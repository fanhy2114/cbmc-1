extern void __VERIFIER_error() __attribute__ ((__noreturn__));
typedef unsigned char __u8;
typedef unsigned short __u16;
typedef int __s32;
typedef unsigned int __u32;
typedef unsigned long long __u64;
typedef signed char s8;
typedef unsigned char u8;
typedef unsigned short u16;
typedef int s32;
typedef unsigned int u32;
typedef long long s64;
typedef unsigned long long u64;
typedef long __kernel_long_t;
typedef unsigned long __kernel_ulong_t;
typedef int __kernel_pid_t;
typedef __kernel_long_t __kernel_suseconds_t;
typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
typedef __kernel_ulong_t __kernel_size_t;
typedef __kernel_long_t __kernel_ssize_t;
typedef long long __kernel_loff_t;
typedef __kernel_long_t __kernel_time_t;
typedef __kernel_long_t __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef __u16 __le16;
struct kernel_symbol {
   unsigned long value ;
   char const *name ;
};
struct module;
typedef __u32 __kernel_dev_t;
typedef __kernel_dev_t dev_t;
typedef unsigned short umode_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_clockid_t clockid_t;
typedef _Bool bool;
typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_loff_t loff_t;
typedef __kernel_size_t size_t;
typedef __kernel_ssize_t ssize_t;
typedef __kernel_time_t time_t;
typedef __s32 int32_t;
typedef __u8 uint8_t;
typedef __u64 uint64_t;
typedef unsigned long sector_t;
typedef unsigned long blkcnt_t;
typedef u64 dma_addr_t;
typedef unsigned int gfp_t;
typedef unsigned int fmode_t;
typedef unsigned int oom_flags_t;
typedef u64 phys_addr_t;
typedef phys_addr_t resource_size_t;
struct __anonstruct_atomic_t_6 {
   int counter ;
};
typedef struct __anonstruct_atomic_t_6 atomic_t;
struct __anonstruct_atomic64_t_7 {
   long counter ;
};
typedef struct __anonstruct_atomic64_t_7 atomic64_t;
struct list_head {
   struct list_head *next ;
   struct list_head *prev ;
};
struct hlist_node;
struct hlist_head {
   struct hlist_node *first ;
};
struct hlist_node {
   struct hlist_node *next ;
   struct hlist_node **pprev ;
};
struct callback_head {
   struct callback_head *next ;
   void (*func)(struct callback_head * ) ;
};
struct pt_regs {
   unsigned long r15 ;
   unsigned long r14 ;
   unsigned long r13 ;
   unsigned long r12 ;
   unsigned long bp ;
   unsigned long bx ;
   unsigned long r11 ;
   unsigned long r10 ;
   unsigned long r9 ;
   unsigned long r8 ;
   unsigned long ax ;
   unsigned long cx ;
   unsigned long dx ;
   unsigned long si ;
   unsigned long di ;
   unsigned long orig_ax ;
   unsigned long ip ;
   unsigned long cs ;
   unsigned long flags ;
   unsigned long sp ;
   unsigned long ss ;
};
struct __anonstruct____missing_field_name_9 {
   unsigned int a ;
   unsigned int b ;
};
struct __anonstruct____missing_field_name_10 {
   u16 limit0 ;
   u16 base0 ;
   unsigned char base1 ;
   unsigned char type : 4 ;
   unsigned char s : 1 ;
   unsigned char dpl : 2 ;
   unsigned char p : 1 ;
   unsigned char limit : 4 ;
   unsigned char avl : 1 ;
   unsigned char l : 1 ;
   unsigned char d : 1 ;
   unsigned char g : 1 ;
   unsigned char base2 ;
};
union __anonunion____missing_field_name_8 {
   struct __anonstruct____missing_field_name_9 __annonCompField4 ;
   struct __anonstruct____missing_field_name_10 __annonCompField5 ;
};
struct desc_struct {
   union __anonunion____missing_field_name_8 __annonCompField6 ;
};
typedef unsigned long pteval_t;
typedef unsigned long pgdval_t;
typedef unsigned long pgprotval_t;
struct __anonstruct_pte_t_11 {
   pteval_t pte ;
};
typedef struct __anonstruct_pte_t_11 pte_t;
struct pgprot {
   pgprotval_t pgprot ;
};
typedef struct pgprot pgprot_t;
struct __anonstruct_pgd_t_12 {
   pgdval_t pgd ;
};
typedef struct __anonstruct_pgd_t_12 pgd_t;
struct page;
typedef struct page *pgtable_t;
struct file;
struct seq_file;
struct thread_struct;
struct mm_struct;
struct task_struct;
struct cpumask;
struct qspinlock {
   atomic_t val ;
};
typedef struct qspinlock arch_spinlock_t;
struct qrwlock {
   atomic_t cnts ;
   arch_spinlock_t lock ;
};
typedef struct qrwlock arch_rwlock_t;
typedef void (*ctor_fn_t)(void);
struct device;
struct file_operations;
struct completion;
struct bug_entry {
   int bug_addr_disp ;
   int file_disp ;
   unsigned short line ;
   unsigned short flags ;
};
struct timespec;
struct compat_timespec;
struct __anonstruct_futex_16 {
   u32 *uaddr ;
   u32 val ;
   u32 flags ;
   u32 bitset ;
   u64 time ;
   u32 *uaddr2 ;
};
struct __anonstruct_nanosleep_17 {
   clockid_t clockid ;
   struct timespec *rmtp ;
   struct compat_timespec *compat_rmtp ;
   u64 expires ;
};
struct pollfd;
struct __anonstruct_poll_18 {
   struct pollfd *ufds ;
   int nfds ;
   int has_timeout ;
   unsigned long tv_sec ;
   unsigned long tv_nsec ;
};
union __anonunion____missing_field_name_15 {
   struct __anonstruct_futex_16 futex ;
   struct __anonstruct_nanosleep_17 nanosleep ;
   struct __anonstruct_poll_18 poll ;
};
struct restart_block {
   long (*fn)(struct restart_block * ) ;
   union __anonunion____missing_field_name_15 __annonCompField7 ;
};
struct kernel_vm86_regs {
   struct pt_regs pt ;
   unsigned short es ;
   unsigned short __esh ;
   unsigned short ds ;
   unsigned short __dsh ;
   unsigned short fs ;
   unsigned short __fsh ;
   unsigned short gs ;
   unsigned short __gsh ;
};
union __anonunion____missing_field_name_19 {
   struct pt_regs *regs ;
   struct kernel_vm86_regs *vm86 ;
};
struct math_emu_info {
   long ___orig_eip ;
   union __anonunion____missing_field_name_19 __annonCompField8 ;
};
struct cpumask {
   unsigned long bits[128U] ;
};
typedef struct cpumask cpumask_t;
typedef struct cpumask *cpumask_var_t;
struct fregs_state {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20U] ;
   u32 status ;
};
struct __anonstruct____missing_field_name_29 {
   u64 rip ;
   u64 rdp ;
};
struct __anonstruct____missing_field_name_30 {
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
};
union __anonunion____missing_field_name_28 {
   struct __anonstruct____missing_field_name_29 __annonCompField12 ;
   struct __anonstruct____missing_field_name_30 __annonCompField13 ;
};
union __anonunion____missing_field_name_31 {
   u32 padding1[12U] ;
   u32 sw_reserved[12U] ;
};
struct fxregs_state {
   u16 cwd ;
   u16 swd ;
   u16 twd ;
   u16 fop ;
   union __anonunion____missing_field_name_28 __annonCompField14 ;
   u32 mxcsr ;
   u32 mxcsr_mask ;
   u32 st_space[32U] ;
   u32 xmm_space[64U] ;
   u32 padding[12U] ;
   union __anonunion____missing_field_name_31 __annonCompField15 ;
};
struct swregs_state {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20U] ;
   u8 ftop ;
   u8 changed ;
   u8 lookahead ;
   u8 no_update ;
   u8 rm ;
   u8 alimit ;
   struct math_emu_info *info ;
   u32 entry_eip ;
};
struct xstate_header {
   u64 xfeatures ;
   u64 xcomp_bv ;
   u64 reserved[6U] ;
};
struct xregs_state {
   struct fxregs_state i387 ;
   struct xstate_header header ;
   u8 __reserved[464U] ;
};
union fpregs_state {
   struct fregs_state fsave ;
   struct fxregs_state fxsave ;
   struct swregs_state soft ;
   struct xregs_state xsave ;
};
struct fpu {
   union fpregs_state state ;
   unsigned int last_cpu ;
   unsigned char fpstate_active ;
   unsigned char fpregs_active ;
   unsigned char counter ;
};
struct seq_operations;
struct perf_event;
struct thread_struct {
   struct desc_struct tls_array[3U] ;
   unsigned long sp0 ;
   unsigned long sp ;
   unsigned short es ;
   unsigned short ds ;
   unsigned short fsindex ;
   unsigned short gsindex ;
   unsigned long fs ;
   unsigned long gs ;
   struct fpu fpu ;
   struct perf_event *ptrace_bps[4U] ;
   unsigned long debugreg6 ;
   unsigned long ptrace_dr7 ;
   unsigned long cr2 ;
   unsigned long trap_nr ;
   unsigned long error_code ;
   unsigned long *io_bitmap_ptr ;
   unsigned long iopl ;
   unsigned int io_bitmap_max ;
};
typedef atomic64_t atomic_long_t;
struct lockdep_map;
struct stack_trace {
   unsigned int nr_entries ;
   unsigned int max_entries ;
   unsigned long *entries ;
   int skip ;
};
struct lockdep_subclass_key {
   char __one_byte ;
};
struct lock_class_key {
   struct lockdep_subclass_key subkeys[8U] ;
};
struct lock_class {
   struct list_head hash_entry ;
   struct list_head lock_entry ;
   struct lockdep_subclass_key *key ;
   unsigned int subclass ;
   unsigned int dep_gen_id ;
   unsigned long usage_mask ;
   struct stack_trace usage_traces[13U] ;
   struct list_head locks_after ;
   struct list_head locks_before ;
   unsigned int version ;
   unsigned long ops ;
   char const *name ;
   int name_version ;
   unsigned long contention_point[4U] ;
   unsigned long contending_point[4U] ;
};
struct lockdep_map {
   struct lock_class_key *key ;
   struct lock_class *class_cache[2U] ;
   char const *name ;
   int cpu ;
   unsigned long ip ;
};
struct held_lock {
   u64 prev_chain_key ;
   unsigned long acquire_ip ;
   struct lockdep_map *instance ;
   struct lockdep_map *nest_lock ;
   u64 waittime_stamp ;
   u64 holdtime_stamp ;
   unsigned short class_idx : 13 ;
   unsigned char irq_context : 2 ;
   unsigned char trylock : 1 ;
   unsigned char read : 2 ;
   unsigned char check : 1 ;
   unsigned char hardirqs_off : 1 ;
   unsigned short references : 12 ;
   unsigned int pin_count ;
};
struct raw_spinlock {
   arch_spinlock_t raw_lock ;
   unsigned int magic ;
   unsigned int owner_cpu ;
   void *owner ;
   struct lockdep_map dep_map ;
};
typedef struct raw_spinlock raw_spinlock_t;
struct __anonstruct____missing_field_name_35 {
   u8 __padding[24U] ;
   struct lockdep_map dep_map ;
};
union __anonunion____missing_field_name_34 {
   struct raw_spinlock rlock ;
   struct __anonstruct____missing_field_name_35 __annonCompField17 ;
};
struct spinlock {
   union __anonunion____missing_field_name_34 __annonCompField18 ;
};
typedef struct spinlock spinlock_t;
struct __anonstruct_rwlock_t_36 {
   arch_rwlock_t raw_lock ;
   unsigned int magic ;
   unsigned int owner_cpu ;
   void *owner ;
   struct lockdep_map dep_map ;
};
typedef struct __anonstruct_rwlock_t_36 rwlock_t;
struct seqcount {
   unsigned int sequence ;
   struct lockdep_map dep_map ;
};
typedef struct seqcount seqcount_t;
struct __anonstruct_seqlock_t_45 {
   struct seqcount seqcount ;
   spinlock_t lock ;
};
typedef struct __anonstruct_seqlock_t_45 seqlock_t;
struct timespec {
   __kernel_time_t tv_sec ;
   long tv_nsec ;
};
struct timeval {
   __kernel_time_t tv_sec ;
   __kernel_suseconds_t tv_usec ;
};
struct user_namespace;
struct __anonstruct_kuid_t_46 {
   uid_t val ;
};
typedef struct __anonstruct_kuid_t_46 kuid_t;
struct __anonstruct_kgid_t_47 {
   gid_t val ;
};
typedef struct __anonstruct_kgid_t_47 kgid_t;
struct kstat {
   u64 ino ;
   dev_t dev ;
   umode_t mode ;
   unsigned int nlink ;
   kuid_t uid ;
   kgid_t gid ;
   dev_t rdev ;
   loff_t size ;
   struct timespec atime ;
   struct timespec mtime ;
   struct timespec ctime ;
   unsigned long blksize ;
   unsigned long long blocks ;
};
struct vm_area_struct;
struct __wait_queue;
typedef struct __wait_queue wait_queue_t;
struct __wait_queue {
   unsigned int flags ;
   void *private ;
   int (*func)(wait_queue_t * , unsigned int , int , void * ) ;
   struct list_head task_list ;
};
struct __wait_queue_head {
   spinlock_t lock ;
   struct list_head task_list ;
};
typedef struct __wait_queue_head wait_queue_head_t;
struct __anonstruct_nodemask_t_48 {
   unsigned long bits[16U] ;
};
typedef struct __anonstruct_nodemask_t_48 nodemask_t;
struct optimistic_spin_queue {
   atomic_t tail ;
};
struct mutex {
   atomic_t count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
   struct task_struct *owner ;
   void *magic ;
   struct lockdep_map dep_map ;
};
struct mutex_waiter {
   struct list_head list ;
   struct task_struct *task ;
   void *magic ;
};
struct rw_semaphore;
struct rw_semaphore {
   long count ;
   struct list_head wait_list ;
   raw_spinlock_t wait_lock ;
   struct optimistic_spin_queue osq ;
   struct task_struct *owner ;
   struct lockdep_map dep_map ;
};
struct completion {
   unsigned int done ;
   wait_queue_head_t wait ;
};
union ktime {
   s64 tv64 ;
};
typedef union ktime ktime_t;
struct notifier_block;
struct timer_list {
   struct hlist_node entry ;
   unsigned long expires ;
   void (*function)(unsigned long ) ;
   unsigned long data ;
   u32 flags ;
   int slack ;
   int start_pid ;
   void *start_site ;
   char start_comm[16U] ;
   struct lockdep_map lockdep_map ;
};
struct hrtimer;
enum hrtimer_restart;
struct rb_node {
   unsigned long __rb_parent_color ;
   struct rb_node *rb_right ;
   struct rb_node *rb_left ;
};
struct rb_root {
   struct rb_node *rb_node ;
};
struct nsproxy;
struct workqueue_struct;
struct work_struct;
struct work_struct {
   atomic_long_t data ;
   struct list_head entry ;
   void (*func)(struct work_struct * ) ;
   struct lockdep_map lockdep_map ;
};
struct delayed_work {
   struct work_struct work ;
   struct timer_list timer ;
   struct workqueue_struct *wq ;
   int cpu ;
};
struct notifier_block {
   int (*notifier_call)(struct notifier_block * , unsigned long , void * ) ;
   struct notifier_block *next ;
   int priority ;
};
struct resource {
   resource_size_t start ;
   resource_size_t end ;
   char const *name ;
   unsigned long flags ;
   struct resource *parent ;
   struct resource *sibling ;
   struct resource *child ;
};
struct pci_dev;
struct pm_message {
   int event ;
};
typedef struct pm_message pm_message_t;
struct dev_pm_ops {
   int (*prepare)(struct device * ) ;
   void (*complete)(struct device * ) ;
   int (*suspend)(struct device * ) ;
   int (*resume)(struct device * ) ;
   int (*freeze)(struct device * ) ;
   int (*thaw)(struct device * ) ;
   int (*poweroff)(struct device * ) ;
   int (*restore)(struct device * ) ;
   int (*suspend_late)(struct device * ) ;
   int (*resume_early)(struct device * ) ;
   int (*freeze_late)(struct device * ) ;
   int (*thaw_early)(struct device * ) ;
   int (*poweroff_late)(struct device * ) ;
   int (*restore_early)(struct device * ) ;
   int (*suspend_noirq)(struct device * ) ;
   int (*resume_noirq)(struct device * ) ;
   int (*freeze_noirq)(struct device * ) ;
   int (*thaw_noirq)(struct device * ) ;
   int (*poweroff_noirq)(struct device * ) ;
   int (*restore_noirq)(struct device * ) ;
   int (*runtime_suspend)(struct device * ) ;
   int (*runtime_resume)(struct device * ) ;
   int (*runtime_idle)(struct device * ) ;
};
enum rpm_status {
    RPM_ACTIVE = 0,
    RPM_RESUMING = 1,
    RPM_SUSPENDED = 2,
    RPM_SUSPENDING = 3
} ;
enum rpm_request {
    RPM_REQ_NONE = 0,
    RPM_REQ_IDLE = 1,
    RPM_REQ_SUSPEND = 2,
    RPM_REQ_AUTOSUSPEND = 3,
    RPM_REQ_RESUME = 4
} ;
struct wakeup_source;
struct wake_irq;
struct pm_subsys_data {
   spinlock_t lock ;
   unsigned int refcount ;
   struct list_head clock_list ;
};
struct dev_pm_qos;
struct dev_pm_info {
   pm_message_t power_state ;
   unsigned char can_wakeup : 1 ;
   unsigned char async_suspend : 1 ;
   bool is_prepared ;
   bool is_suspended ;
   bool is_noirq_suspended ;
   bool is_late_suspended ;
   bool ignore_children ;
   bool early_init ;
   bool direct_complete ;
   spinlock_t lock ;
   struct list_head entry ;
   struct completion completion ;
   struct wakeup_source *wakeup ;
   bool wakeup_path ;
   bool syscore ;
   struct timer_list suspend_timer ;
   unsigned long timer_expires ;
   struct work_struct work ;
   wait_queue_head_t wait_queue ;
   struct wake_irq *wakeirq ;
   atomic_t usage_count ;
   atomic_t child_count ;
   unsigned char disable_depth : 3 ;
   unsigned char idle_notification : 1 ;
   unsigned char request_pending : 1 ;
   unsigned char deferred_resume : 1 ;
   unsigned char run_wake : 1 ;
   unsigned char runtime_auto : 1 ;
   unsigned char no_callbacks : 1 ;
   unsigned char irq_safe : 1 ;
   unsigned char use_autosuspend : 1 ;
   unsigned char timer_autosuspends : 1 ;
   unsigned char memalloc_noio : 1 ;
   enum rpm_request request ;
   enum rpm_status runtime_status ;
   int runtime_error ;
   int autosuspend_delay ;
   unsigned long last_busy ;
   unsigned long active_jiffies ;
   unsigned long suspended_jiffies ;
   unsigned long accounting_timestamp ;
   struct pm_subsys_data *subsys_data ;
   void (*set_latency_tolerance)(struct device * , s32 ) ;
   struct dev_pm_qos *qos ;
};
struct dev_pm_domain {
   struct dev_pm_ops ops ;
   void (*detach)(struct device * , bool ) ;
   int (*activate)(struct device * ) ;
   void (*sync)(struct device * ) ;
   void (*dismiss)(struct device * ) ;
};
struct pci_bus;
struct __anonstruct_mm_context_t_115 {
   void *ldt ;
   int size ;
   unsigned short ia32_compat ;
   struct mutex lock ;
   void *vdso ;
   atomic_t perf_rdpmc_allowed ;
};
typedef struct __anonstruct_mm_context_t_115 mm_context_t;
struct llist_node;
struct llist_node {
   struct llist_node *next ;
};
struct cred;
struct inode;
struct arch_uprobe_task {
   unsigned long saved_scratch_register ;
   unsigned int saved_trap_nr ;
   unsigned int saved_tf ;
};
enum uprobe_task_state {
    UTASK_RUNNING = 0,
    UTASK_SSTEP = 1,
    UTASK_SSTEP_ACK = 2,
    UTASK_SSTEP_TRAPPED = 3
} ;
struct __anonstruct____missing_field_name_148 {
   struct arch_uprobe_task autask ;
   unsigned long vaddr ;
};
struct __anonstruct____missing_field_name_149 {
   struct callback_head dup_xol_work ;
   unsigned long dup_xol_addr ;
};
union __anonunion____missing_field_name_147 {
   struct __anonstruct____missing_field_name_148 __annonCompField33 ;
   struct __anonstruct____missing_field_name_149 __annonCompField34 ;
};
struct uprobe;
struct return_instance;
struct uprobe_task {
   enum uprobe_task_state state ;
   union __anonunion____missing_field_name_147 __annonCompField35 ;
   struct uprobe *active_uprobe ;
   unsigned long xol_vaddr ;
   struct return_instance *return_instances ;
   unsigned int depth ;
};
struct xol_area;
struct uprobes_state {
   struct xol_area *xol_area ;
};
struct address_space;
struct mem_cgroup;
typedef void compound_page_dtor(struct page * );
union __anonunion____missing_field_name_150 {
   struct address_space *mapping ;
   void *s_mem ;
};
union __anonunion____missing_field_name_152 {
   unsigned long index ;
   void *freelist ;
   bool pfmemalloc ;
};
struct __anonstruct____missing_field_name_156 {
   unsigned short inuse ;
   unsigned short objects : 15 ;
   unsigned char frozen : 1 ;
};
union __anonunion____missing_field_name_155 {
   atomic_t _mapcount ;
   struct __anonstruct____missing_field_name_156 __annonCompField38 ;
   int units ;
};
struct __anonstruct____missing_field_name_154 {
   union __anonunion____missing_field_name_155 __annonCompField39 ;
   atomic_t _count ;
};
union __anonunion____missing_field_name_153 {
   unsigned long counters ;
   struct __anonstruct____missing_field_name_154 __annonCompField40 ;
   unsigned int active ;
};
struct __anonstruct____missing_field_name_151 {
   union __anonunion____missing_field_name_152 __annonCompField37 ;
   union __anonunion____missing_field_name_153 __annonCompField41 ;
};
struct __anonstruct____missing_field_name_158 {
   struct page *next ;
   int pages ;
   int pobjects ;
};
struct slab;
struct __anonstruct____missing_field_name_159 {
   compound_page_dtor *compound_dtor ;
   unsigned long compound_order ;
};
union __anonunion____missing_field_name_157 {
   struct list_head lru ;
   struct __anonstruct____missing_field_name_158 __annonCompField43 ;
   struct slab *slab_page ;
   struct callback_head callback_head ;
   struct __anonstruct____missing_field_name_159 __annonCompField44 ;
   pgtable_t pmd_huge_pte ;
};
struct kmem_cache;
union __anonunion____missing_field_name_160 {
   unsigned long private ;
   spinlock_t *ptl ;
   struct kmem_cache *slab_cache ;
   struct page *first_page ;
};
struct page {
   unsigned long flags ;
   union __anonunion____missing_field_name_150 __annonCompField36 ;
   struct __anonstruct____missing_field_name_151 __annonCompField42 ;
   union __anonunion____missing_field_name_157 __annonCompField45 ;
   union __anonunion____missing_field_name_160 __annonCompField46 ;
   struct mem_cgroup *mem_cgroup ;
};
struct page_frag {
   struct page *page ;
   __u32 offset ;
   __u32 size ;
};
struct __anonstruct_shared_161 {
   struct rb_node rb ;
   unsigned long rb_subtree_last ;
};
struct anon_vma;
struct vm_operations_struct;
struct mempolicy;
struct vm_area_struct {
   unsigned long vm_start ;
   unsigned long vm_end ;
   struct vm_area_struct *vm_next ;
   struct vm_area_struct *vm_prev ;
   struct rb_node vm_rb ;
   unsigned long rb_subtree_gap ;
   struct mm_struct *vm_mm ;
   pgprot_t vm_page_prot ;
   unsigned long vm_flags ;
   struct __anonstruct_shared_161 shared ;
   struct list_head anon_vma_chain ;
   struct anon_vma *anon_vma ;
   struct vm_operations_struct const *vm_ops ;
   unsigned long vm_pgoff ;
   struct file *vm_file ;
   void *vm_private_data ;
   struct mempolicy *vm_policy ;
};
struct core_thread {
   struct task_struct *task ;
   struct core_thread *next ;
};
struct core_state {
   atomic_t nr_threads ;
   struct core_thread dumper ;
   struct completion startup ;
};
struct task_rss_stat {
   int events ;
   int count[3U] ;
};
struct mm_rss_stat {
   atomic_long_t count[3U] ;
};
struct kioctx_table;
struct linux_binfmt;
struct mmu_notifier_mm;
struct mm_struct {
   struct vm_area_struct *mmap ;
   struct rb_root mm_rb ;
   u32 vmacache_seqnum ;
   unsigned long (*get_unmapped_area)(struct file * , unsigned long , unsigned long ,
                                      unsigned long , unsigned long ) ;
   unsigned long mmap_base ;
   unsigned long mmap_legacy_base ;
   unsigned long task_size ;
   unsigned long highest_vm_end ;
   pgd_t *pgd ;
   atomic_t mm_users ;
   atomic_t mm_count ;
   atomic_long_t nr_ptes ;
   atomic_long_t nr_pmds ;
   int map_count ;
   spinlock_t page_table_lock ;
   struct rw_semaphore mmap_sem ;
   struct list_head mmlist ;
   unsigned long hiwater_rss ;
   unsigned long hiwater_vm ;
   unsigned long total_vm ;
   unsigned long locked_vm ;
   unsigned long pinned_vm ;
   unsigned long shared_vm ;
   unsigned long exec_vm ;
   unsigned long stack_vm ;
   unsigned long def_flags ;
   unsigned long start_code ;
   unsigned long end_code ;
   unsigned long start_data ;
   unsigned long end_data ;
   unsigned long start_brk ;
   unsigned long brk ;
   unsigned long start_stack ;
   unsigned long arg_start ;
   unsigned long arg_end ;
   unsigned long env_start ;
   unsigned long env_end ;
   unsigned long saved_auxv[46U] ;
   struct mm_rss_stat rss_stat ;
   struct linux_binfmt *binfmt ;
   cpumask_var_t cpu_vm_mask_var ;
   mm_context_t context ;
   unsigned long flags ;
   struct core_state *core_state ;
   spinlock_t ioctx_lock ;
   struct kioctx_table *ioctx_table ;
   struct task_struct *owner ;
   struct file *exe_file ;
   struct mmu_notifier_mm *mmu_notifier_mm ;
   struct cpumask cpumask_allocation ;
   unsigned long numa_next_scan ;
   unsigned long numa_scan_offset ;
   int numa_scan_seq ;
   bool tlb_flush_pending ;
   struct uprobes_state uprobes_state ;
   void *bd_addr ;
};
typedef __u64 Elf64_Addr;
typedef __u16 Elf64_Half;
typedef __u32 Elf64_Word;
typedef __u64 Elf64_Xword;
struct elf64_sym {
   Elf64_Word st_name ;
   unsigned char st_info ;
   unsigned char st_other ;
   Elf64_Half st_shndx ;
   Elf64_Addr st_value ;
   Elf64_Xword st_size ;
};
typedef struct elf64_sym Elf64_Sym;
union __anonunion____missing_field_name_166 {
   unsigned long bitmap[4U] ;
   struct callback_head callback_head ;
};
struct idr_layer {
   int prefix ;
   int layer ;
   struct idr_layer *ary[256U] ;
   int count ;
   union __anonunion____missing_field_name_166 __annonCompField47 ;
};
struct idr {
   struct idr_layer *hint ;
   struct idr_layer *top ;
   int layers ;
   int cur ;
   spinlock_t lock ;
   int id_free_cnt ;
   struct idr_layer *id_free ;
};
struct ida_bitmap {
   long nr_busy ;
   unsigned long bitmap[15U] ;
};
struct ida {
   struct idr idr ;
   struct ida_bitmap *free_bitmap ;
};
struct dentry;
struct iattr;
struct super_block;
struct file_system_type;
struct kernfs_open_node;
struct kernfs_iattrs;
struct kernfs_root;
struct kernfs_elem_dir {
   unsigned long subdirs ;
   struct rb_root children ;
   struct kernfs_root *root ;
};
struct kernfs_node;
struct kernfs_elem_symlink {
   struct kernfs_node *target_kn ;
};
struct kernfs_ops;
struct kernfs_elem_attr {
   struct kernfs_ops const *ops ;
   struct kernfs_open_node *open ;
   loff_t size ;
   struct kernfs_node *notify_next ;
};
union __anonunion____missing_field_name_171 {
   struct kernfs_elem_dir dir ;
   struct kernfs_elem_symlink symlink ;
   struct kernfs_elem_attr attr ;
};
struct kernfs_node {
   atomic_t count ;
   atomic_t active ;
   struct lockdep_map dep_map ;
   struct kernfs_node *parent ;
   char const *name ;
   struct rb_node rb ;
   void const *ns ;
   unsigned int hash ;
   union __anonunion____missing_field_name_171 __annonCompField48 ;
   void *priv ;
   unsigned short flags ;
   umode_t mode ;
   unsigned int ino ;
   struct kernfs_iattrs *iattr ;
};
struct kernfs_syscall_ops {
   int (*remount_fs)(struct kernfs_root * , int * , char * ) ;
   int (*show_options)(struct seq_file * , struct kernfs_root * ) ;
   int (*mkdir)(struct kernfs_node * , char const * , umode_t ) ;
   int (*rmdir)(struct kernfs_node * ) ;
   int (*rename)(struct kernfs_node * , struct kernfs_node * , char const * ) ;
};
struct kernfs_root {
   struct kernfs_node *kn ;
   unsigned int flags ;
   struct ida ino_ida ;
   struct kernfs_syscall_ops *syscall_ops ;
   struct list_head supers ;
   wait_queue_head_t deactivate_waitq ;
};
struct kernfs_open_file {
   struct kernfs_node *kn ;
   struct file *file ;
   void *priv ;
   struct mutex mutex ;
   int event ;
   struct list_head list ;
   char *prealloc_buf ;
   size_t atomic_write_len ;
   bool mmapped ;
   struct vm_operations_struct const *vm_ops ;
};
struct kernfs_ops {
   int (*seq_show)(struct seq_file * , void * ) ;
   void *(*seq_start)(struct seq_file * , loff_t * ) ;
   void *(*seq_next)(struct seq_file * , void * , loff_t * ) ;
   void (*seq_stop)(struct seq_file * , void * ) ;
   ssize_t (*read)(struct kernfs_open_file * , char * , size_t , loff_t ) ;
   size_t atomic_write_len ;
   bool prealloc ;
   ssize_t (*write)(struct kernfs_open_file * , char * , size_t , loff_t ) ;
   int (*mmap)(struct kernfs_open_file * , struct vm_area_struct * ) ;
   struct lock_class_key lockdep_key ;
};
struct sock;
struct kobject;
enum kobj_ns_type {
    KOBJ_NS_TYPE_NONE = 0,
    KOBJ_NS_TYPE_NET = 1,
    KOBJ_NS_TYPES = 2
} ;
struct kobj_ns_type_operations {
   enum kobj_ns_type type ;
   bool (*current_may_mount)(void) ;
   void *(*grab_current_ns)(void) ;
   void const *(*netlink_ns)(struct sock * ) ;
   void const *(*initial_ns)(void) ;
   void (*drop_ns)(void * ) ;
};
struct bin_attribute;
struct attribute {
   char const *name ;
   umode_t mode ;
   bool ignore_lockdep ;
   struct lock_class_key *key ;
   struct lock_class_key skey ;
};
struct attribute_group {
   char const *name ;
   umode_t (*is_visible)(struct kobject * , struct attribute * , int ) ;
   struct attribute **attrs ;
   struct bin_attribute **bin_attrs ;
};
struct bin_attribute {
   struct attribute attr ;
   size_t size ;
   void *private ;
   ssize_t (*read)(struct file * , struct kobject * , struct bin_attribute * , char * ,
                   loff_t , size_t ) ;
   ssize_t (*write)(struct file * , struct kobject * , struct bin_attribute * , char * ,
                    loff_t , size_t ) ;
   int (*mmap)(struct file * , struct kobject * , struct bin_attribute * , struct vm_area_struct * ) ;
};
struct sysfs_ops {
   ssize_t (*show)(struct kobject * , struct attribute * , char * ) ;
   ssize_t (*store)(struct kobject * , struct attribute * , char const * , size_t ) ;
};
struct kref {
   atomic_t refcount ;
};
struct kset;
struct kobj_type;
struct kobject {
   char const *name ;
   struct list_head entry ;
   struct kobject *parent ;
   struct kset *kset ;
   struct kobj_type *ktype ;
   struct kernfs_node *sd ;
   struct kref kref ;
   struct delayed_work release ;
   unsigned char state_initialized : 1 ;
   unsigned char state_in_sysfs : 1 ;
   unsigned char state_add_uevent_sent : 1 ;
   unsigned char state_remove_uevent_sent : 1 ;
   unsigned char uevent_suppress : 1 ;
};
struct kobj_type {
   void (*release)(struct kobject * ) ;
   struct sysfs_ops const *sysfs_ops ;
   struct attribute **default_attrs ;
   struct kobj_ns_type_operations const *(*child_ns_type)(struct kobject * ) ;
   void const *(*namespace)(struct kobject * ) ;
};
struct kobj_uevent_env {
   char *argv[3U] ;
   char *envp[32U] ;
   int envp_idx ;
   char buf[2048U] ;
   int buflen ;
};
struct kset_uevent_ops {
   int (* const filter)(struct kset * , struct kobject * ) ;
   char const *(* const name)(struct kset * , struct kobject * ) ;
   int (* const uevent)(struct kset * , struct kobject * , struct kobj_uevent_env * ) ;
};
struct kset {
   struct list_head list ;
   spinlock_t list_lock ;
   struct kobject kobj ;
   struct kset_uevent_ops const *uevent_ops ;
};
struct kernel_param;
struct kernel_param_ops {
   unsigned int flags ;
   int (*set)(char const * , struct kernel_param const * ) ;
   int (*get)(char * , struct kernel_param const * ) ;
   void (*free)(void * ) ;
};
struct kparam_string;
struct kparam_array;
union __anonunion____missing_field_name_172 {
   void *arg ;
   struct kparam_string const *str ;
   struct kparam_array const *arr ;
};
struct kernel_param {
   char const *name ;
   struct module *mod ;
   struct kernel_param_ops const *ops ;
   u16 const perm ;
   s8 level ;
   u8 flags ;
   union __anonunion____missing_field_name_172 __annonCompField49 ;
};
struct kparam_string {
   unsigned int maxlen ;
   char *string ;
};
struct kparam_array {
   unsigned int max ;
   unsigned int elemsize ;
   unsigned int *num ;
   struct kernel_param_ops const *ops ;
   void *elem ;
};
struct latch_tree_node {
   struct rb_node node[2U] ;
};
struct mod_arch_specific {
};
struct module_param_attrs;
struct module_kobject {
   struct kobject kobj ;
   struct module *mod ;
   struct kobject *drivers_dir ;
   struct module_param_attrs *mp ;
   struct completion *kobj_completion ;
};
struct module_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct module_attribute * , struct module_kobject * , char * ) ;
   ssize_t (*store)(struct module_attribute * , struct module_kobject * , char const * ,
                    size_t ) ;
   void (*setup)(struct module * , char const * ) ;
   int (*test)(struct module * ) ;
   void (*free)(struct module * ) ;
};
struct exception_table_entry;
enum module_state {
    MODULE_STATE_LIVE = 0,
    MODULE_STATE_COMING = 1,
    MODULE_STATE_GOING = 2,
    MODULE_STATE_UNFORMED = 3
} ;
struct mod_tree_node {
   struct module *mod ;
   struct latch_tree_node node ;
};
struct module_sect_attrs;
struct module_notes_attrs;
struct tracepoint;
struct trace_event_call;
struct trace_enum_map;
struct module {
   enum module_state state ;
   struct list_head list ;
   char name[56U] ;
   struct module_kobject mkobj ;
   struct module_attribute *modinfo_attrs ;
   char const *version ;
   char const *srcversion ;
   struct kobject *holders_dir ;
   struct kernel_symbol const *syms ;
   unsigned long const *crcs ;
   unsigned int num_syms ;
   struct mutex param_lock ;
   struct kernel_param *kp ;
   unsigned int num_kp ;
   unsigned int num_gpl_syms ;
   struct kernel_symbol const *gpl_syms ;
   unsigned long const *gpl_crcs ;
   struct kernel_symbol const *unused_syms ;
   unsigned long const *unused_crcs ;
   unsigned int num_unused_syms ;
   unsigned int num_unused_gpl_syms ;
   struct kernel_symbol const *unused_gpl_syms ;
   unsigned long const *unused_gpl_crcs ;
   bool sig_ok ;
   bool async_probe_requested ;
   struct kernel_symbol const *gpl_future_syms ;
   unsigned long const *gpl_future_crcs ;
   unsigned int num_gpl_future_syms ;
   unsigned int num_exentries ;
   struct exception_table_entry *extable ;
   int (*init)(void) ;
   void *module_init ;
   void *module_core ;
   unsigned int init_size ;
   unsigned int core_size ;
   unsigned int init_text_size ;
   unsigned int core_text_size ;
   struct mod_tree_node mtn_core ;
   struct mod_tree_node mtn_init ;
   unsigned int init_ro_size ;
   unsigned int core_ro_size ;
   struct mod_arch_specific arch ;
   unsigned int taints ;
   unsigned int num_bugs ;
   struct list_head bug_list ;
   struct bug_entry *bug_table ;
   Elf64_Sym *symtab ;
   Elf64_Sym *core_symtab ;
   unsigned int num_symtab ;
   unsigned int core_num_syms ;
   char *strtab ;
   char *core_strtab ;
   struct module_sect_attrs *sect_attrs ;
   struct module_notes_attrs *notes_attrs ;
   char *args ;
   void *percpu ;
   unsigned int percpu_size ;
   unsigned int num_tracepoints ;
   struct tracepoint * const *tracepoints_ptrs ;
   unsigned int num_trace_bprintk_fmt ;
   char const **trace_bprintk_fmt_start ;
   struct trace_event_call **trace_events ;
   unsigned int num_trace_events ;
   struct trace_enum_map **trace_enums ;
   unsigned int num_trace_enums ;
   unsigned int num_ftrace_callsites ;
   unsigned long *ftrace_callsites ;
   bool klp_alive ;
   struct list_head source_list ;
   struct list_head target_list ;
   void (*exit)(void) ;
   atomic_t refcnt ;
   ctor_fn_t (**ctors)(void) ;
   unsigned int num_ctors ;
};
struct kernel_cap_struct {
   __u32 cap[2U] ;
};
typedef struct kernel_cap_struct kernel_cap_t;
struct plist_node {
   int prio ;
   struct list_head prio_list ;
   struct list_head node_list ;
};
typedef unsigned long cputime_t;
struct sem_undo_list;
struct sysv_sem {
   struct sem_undo_list *undo_list ;
};
struct user_struct;
struct sysv_shm {
   struct list_head shm_clist ;
};
struct __anonstruct_sigset_t_180 {
   unsigned long sig[1U] ;
};
typedef struct __anonstruct_sigset_t_180 sigset_t;
struct siginfo;
typedef void __signalfn_t(int );
typedef __signalfn_t *__sighandler_t;
typedef void __restorefn_t(void);
typedef __restorefn_t *__sigrestore_t;
union sigval {
   int sival_int ;
   void *sival_ptr ;
};
typedef union sigval sigval_t;
struct __anonstruct__kill_182 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
};
struct __anonstruct__timer_183 {
   __kernel_timer_t _tid ;
   int _overrun ;
   char _pad[0U] ;
   sigval_t _sigval ;
   int _sys_private ;
};
struct __anonstruct__rt_184 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
   sigval_t _sigval ;
};
struct __anonstruct__sigchld_185 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
   int _status ;
   __kernel_clock_t _utime ;
   __kernel_clock_t _stime ;
};
struct __anonstruct__addr_bnd_187 {
   void *_lower ;
   void *_upper ;
};
struct __anonstruct__sigfault_186 {
   void *_addr ;
   short _addr_lsb ;
   struct __anonstruct__addr_bnd_187 _addr_bnd ;
};
struct __anonstruct__sigpoll_188 {
   long _band ;
   int _fd ;
};
struct __anonstruct__sigsys_189 {
   void *_call_addr ;
   int _syscall ;
   unsigned int _arch ;
};
union __anonunion__sifields_181 {
   int _pad[28U] ;
   struct __anonstruct__kill_182 _kill ;
   struct __anonstruct__timer_183 _timer ;
   struct __anonstruct__rt_184 _rt ;
   struct __anonstruct__sigchld_185 _sigchld ;
   struct __anonstruct__sigfault_186 _sigfault ;
   struct __anonstruct__sigpoll_188 _sigpoll ;
   struct __anonstruct__sigsys_189 _sigsys ;
};
struct siginfo {
   int si_signo ;
   int si_errno ;
   int si_code ;
   union __anonunion__sifields_181 _sifields ;
};
typedef struct siginfo siginfo_t;
struct sigpending {
   struct list_head list ;
   sigset_t signal ;
};
struct sigaction {
   __sighandler_t sa_handler ;
   unsigned long sa_flags ;
   __sigrestore_t sa_restorer ;
   sigset_t sa_mask ;
};
struct k_sigaction {
   struct sigaction sa ;
};
enum pid_type {
    PIDTYPE_PID = 0,
    PIDTYPE_PGID = 1,
    PIDTYPE_SID = 2,
    PIDTYPE_MAX = 3
} ;
struct pid_namespace;
struct upid {
   int nr ;
   struct pid_namespace *ns ;
   struct hlist_node pid_chain ;
};
struct pid {
   atomic_t count ;
   unsigned int level ;
   struct hlist_head tasks[3U] ;
   struct callback_head rcu ;
   struct upid numbers[1U] ;
};
struct pid_link {
   struct hlist_node node ;
   struct pid *pid ;
};
struct percpu_counter {
   raw_spinlock_t lock ;
   s64 count ;
   struct list_head list ;
   s32 *counters ;
};
struct seccomp_filter;
struct seccomp {
   int mode ;
   struct seccomp_filter *filter ;
};
struct rt_mutex {
   raw_spinlock_t wait_lock ;
   struct rb_root waiters ;
   struct rb_node *waiters_leftmost ;
   struct task_struct *owner ;
   int save_state ;
   char const *name ;
   char const *file ;
   int line ;
   void *magic ;
};
struct rt_mutex_waiter;
struct rlimit {
   __kernel_ulong_t rlim_cur ;
   __kernel_ulong_t rlim_max ;
};
struct timerqueue_node {
   struct rb_node node ;
   ktime_t expires ;
};
struct timerqueue_head {
   struct rb_root head ;
   struct timerqueue_node *next ;
};
struct hrtimer_clock_base;
struct hrtimer_cpu_base;
enum hrtimer_restart {
    HRTIMER_NORESTART = 0,
    HRTIMER_RESTART = 1
} ;
struct hrtimer {
   struct timerqueue_node node ;
   ktime_t _softexpires ;
   enum hrtimer_restart (*function)(struct hrtimer * ) ;
   struct hrtimer_clock_base *base ;
   unsigned long state ;
   int start_pid ;
   void *start_site ;
   char start_comm[16U] ;
};
struct hrtimer_clock_base {
   struct hrtimer_cpu_base *cpu_base ;
   int index ;
   clockid_t clockid ;
   struct timerqueue_head active ;
   ktime_t (*get_time)(void) ;
   ktime_t offset ;
};
struct hrtimer_cpu_base {
   raw_spinlock_t lock ;
   seqcount_t seq ;
   struct hrtimer *running ;
   unsigned int cpu ;
   unsigned int active_bases ;
   unsigned int clock_was_set_seq ;
   bool migration_enabled ;
   bool nohz_active ;
   unsigned char in_hrtirq : 1 ;
   unsigned char hres_active : 1 ;
   unsigned char hang_detected : 1 ;
   ktime_t expires_next ;
   struct hrtimer *next_timer ;
   unsigned int nr_events ;
   unsigned int nr_retries ;
   unsigned int nr_hangs ;
   unsigned int max_hang_time ;
   struct hrtimer_clock_base clock_base[4U] ;
};
struct task_io_accounting {
   u64 rchar ;
   u64 wchar ;
   u64 syscr ;
   u64 syscw ;
   u64 read_bytes ;
   u64 write_bytes ;
   u64 cancelled_write_bytes ;
};
struct latency_record {
   unsigned long backtrace[12U] ;
   unsigned int count ;
   unsigned long time ;
   unsigned long max ;
};
struct assoc_array_ptr;
struct assoc_array {
   struct assoc_array_ptr *root ;
   unsigned long nr_leaves_on_tree ;
};
typedef int32_t key_serial_t;
typedef u32 key_perm_t;
struct key;
struct signal_struct;
struct key_type;
struct keyring_index_key {
   struct key_type *type ;
   char const *description ;
   size_t desc_len ;
};
union __anonunion____missing_field_name_196 {
   struct list_head graveyard_link ;
   struct rb_node serial_node ;
};
struct key_user;
union __anonunion____missing_field_name_197 {
   time_t expiry ;
   time_t revoked_at ;
};
struct __anonstruct____missing_field_name_199 {
   struct key_type *type ;
   char *description ;
};
union __anonunion____missing_field_name_198 {
   struct keyring_index_key index_key ;
   struct __anonstruct____missing_field_name_199 __annonCompField52 ;
};
union __anonunion_type_data_200 {
   struct list_head link ;
   unsigned long x[2U] ;
   void *p[2U] ;
   int reject_error ;
};
union __anonunion_payload_202 {
   unsigned long value ;
   void *rcudata ;
   void *data ;
   void *data2[2U] ;
};
union __anonunion____missing_field_name_201 {
   union __anonunion_payload_202 payload ;
   struct assoc_array keys ;
};
struct key {
   atomic_t usage ;
   key_serial_t serial ;
   union __anonunion____missing_field_name_196 __annonCompField50 ;
   struct rw_semaphore sem ;
   struct key_user *user ;
   void *security ;
   union __anonunion____missing_field_name_197 __annonCompField51 ;
   time_t last_used_at ;
   kuid_t uid ;
   kgid_t gid ;
   key_perm_t perm ;
   unsigned short quotalen ;
   unsigned short datalen ;
   unsigned long flags ;
   union __anonunion____missing_field_name_198 __annonCompField53 ;
   union __anonunion_type_data_200 type_data ;
   union __anonunion____missing_field_name_201 __annonCompField54 ;
};
struct audit_context;
struct group_info {
   atomic_t usage ;
   int ngroups ;
   int nblocks ;
   kgid_t small_block[32U] ;
   kgid_t *blocks[0U] ;
};
struct cred {
   atomic_t usage ;
   atomic_t subscribers ;
   void *put_addr ;
   unsigned int magic ;
   kuid_t uid ;
   kgid_t gid ;
   kuid_t suid ;
   kgid_t sgid ;
   kuid_t euid ;
   kgid_t egid ;
   kuid_t fsuid ;
   kgid_t fsgid ;
   unsigned int securebits ;
   kernel_cap_t cap_inheritable ;
   kernel_cap_t cap_permitted ;
   kernel_cap_t cap_effective ;
   kernel_cap_t cap_bset ;
   unsigned char jit_keyring ;
   struct key *session_keyring ;
   struct key *process_keyring ;
   struct key *thread_keyring ;
   struct key *request_key_auth ;
   void *security ;
   struct user_struct *user ;
   struct user_namespace *user_ns ;
   struct group_info *group_info ;
   struct callback_head rcu ;
};
struct percpu_ref;
typedef void percpu_ref_func_t(struct percpu_ref * );
struct percpu_ref {
   atomic_long_t count ;
   unsigned long percpu_count_ptr ;
   percpu_ref_func_t *release ;
   percpu_ref_func_t *confirm_switch ;
   bool force_atomic ;
   struct callback_head rcu ;
};
struct cgroup;
struct cgroup_root;
struct cgroup_subsys;
struct cgroup_taskset;
struct cgroup_subsys_state {
   struct cgroup *cgroup ;
   struct cgroup_subsys *ss ;
   struct percpu_ref refcnt ;
   struct cgroup_subsys_state *parent ;
   struct list_head sibling ;
   struct list_head children ;
   int id ;
   unsigned int flags ;
   u64 serial_nr ;
   struct callback_head callback_head ;
   struct work_struct destroy_work ;
};
struct css_set {
   atomic_t refcount ;
   struct hlist_node hlist ;
   struct list_head tasks ;
   struct list_head mg_tasks ;
   struct list_head cgrp_links ;
   struct cgroup *dfl_cgrp ;
   struct cgroup_subsys_state *subsys[12U] ;
   struct list_head mg_preload_node ;
   struct list_head mg_node ;
   struct cgroup *mg_src_cgrp ;
   struct css_set *mg_dst_cset ;
   struct list_head e_cset_node[12U] ;
   struct callback_head callback_head ;
};
struct cgroup {
   struct cgroup_subsys_state self ;
   unsigned long flags ;
   int id ;
   int populated_cnt ;
   struct kernfs_node *kn ;
   struct kernfs_node *procs_kn ;
   struct kernfs_node *populated_kn ;
   unsigned int subtree_control ;
   unsigned int child_subsys_mask ;
   struct cgroup_subsys_state *subsys[12U] ;
   struct cgroup_root *root ;
   struct list_head cset_links ;
   struct list_head e_csets[12U] ;
   struct list_head pidlists ;
   struct mutex pidlist_mutex ;
   wait_queue_head_t offline_waitq ;
   struct work_struct release_agent_work ;
};
struct cgroup_root {
   struct kernfs_root *kf_root ;
   unsigned int subsys_mask ;
   int hierarchy_id ;
   struct cgroup cgrp ;
   atomic_t nr_cgrps ;
   struct list_head root_list ;
   unsigned int flags ;
   struct idr cgroup_idr ;
   char release_agent_path[4096U] ;
   char name[64U] ;
};
struct cftype {
   char name[64U] ;
   int private ;
   umode_t mode ;
   size_t max_write_len ;
   unsigned int flags ;
   struct cgroup_subsys *ss ;
   struct list_head node ;
   struct kernfs_ops *kf_ops ;
   u64 (*read_u64)(struct cgroup_subsys_state * , struct cftype * ) ;
   s64 (*read_s64)(struct cgroup_subsys_state * , struct cftype * ) ;
   int (*seq_show)(struct seq_file * , void * ) ;
   void *(*seq_start)(struct seq_file * , loff_t * ) ;
   void *(*seq_next)(struct seq_file * , void * , loff_t * ) ;
   void (*seq_stop)(struct seq_file * , void * ) ;
   int (*write_u64)(struct cgroup_subsys_state * , struct cftype * , u64 ) ;
   int (*write_s64)(struct cgroup_subsys_state * , struct cftype * , s64 ) ;
   ssize_t (*write)(struct kernfs_open_file * , char * , size_t , loff_t ) ;
   struct lock_class_key lockdep_key ;
};
struct cgroup_subsys {
   struct cgroup_subsys_state *(*css_alloc)(struct cgroup_subsys_state * ) ;
   int (*css_online)(struct cgroup_subsys_state * ) ;
   void (*css_offline)(struct cgroup_subsys_state * ) ;
   void (*css_released)(struct cgroup_subsys_state * ) ;
   void (*css_free)(struct cgroup_subsys_state * ) ;
   void (*css_reset)(struct cgroup_subsys_state * ) ;
   void (*css_e_css_changed)(struct cgroup_subsys_state * ) ;
   int (*can_attach)(struct cgroup_subsys_state * , struct cgroup_taskset * ) ;
   void (*cancel_attach)(struct cgroup_subsys_state * , struct cgroup_taskset * ) ;
   void (*attach)(struct cgroup_subsys_state * , struct cgroup_taskset * ) ;
   void (*fork)(struct task_struct * ) ;
   void (*exit)(struct cgroup_subsys_state * , struct cgroup_subsys_state * , struct task_struct * ) ;
   void (*bind)(struct cgroup_subsys_state * ) ;
   int disabled ;
   int early_init ;
   bool broken_hierarchy ;
   bool warned_broken_hierarchy ;
   int id ;
   char const *name ;
   struct cgroup_root *root ;
   struct idr css_idr ;
   struct list_head cfts ;
   struct cftype *dfl_cftypes ;
   struct cftype *legacy_cftypes ;
   unsigned int depends_on ;
};
struct futex_pi_state;
struct robust_list_head;
struct bio_list;
struct fs_struct;
struct perf_event_context;
struct blk_plug;
struct nameidata;
struct cfs_rq;
struct task_group;
struct sighand_struct {
   atomic_t count ;
   struct k_sigaction action[64U] ;
   spinlock_t siglock ;
   wait_queue_head_t signalfd_wqh ;
};
struct pacct_struct {
   int ac_flag ;
   long ac_exitcode ;
   unsigned long ac_mem ;
   cputime_t ac_utime ;
   cputime_t ac_stime ;
   unsigned long ac_minflt ;
   unsigned long ac_majflt ;
};
struct cpu_itimer {
   cputime_t expires ;
   cputime_t incr ;
   u32 error ;
   u32 incr_error ;
};
struct cputime {
   cputime_t utime ;
   cputime_t stime ;
};
struct task_cputime {
   cputime_t utime ;
   cputime_t stime ;
   unsigned long long sum_exec_runtime ;
};
struct task_cputime_atomic {
   atomic64_t utime ;
   atomic64_t stime ;
   atomic64_t sum_exec_runtime ;
};
struct thread_group_cputimer {
   struct task_cputime_atomic cputime_atomic ;
   int running ;
};
struct autogroup;
struct tty_struct;
struct taskstats;
struct tty_audit_buf;
struct signal_struct {
   atomic_t sigcnt ;
   atomic_t live ;
   int nr_threads ;
   struct list_head thread_head ;
   wait_queue_head_t wait_chldexit ;
   struct task_struct *curr_target ;
   struct sigpending shared_pending ;
   int group_exit_code ;
   int notify_count ;
   struct task_struct *group_exit_task ;
   int group_stop_count ;
   unsigned int flags ;
   unsigned char is_child_subreaper : 1 ;
   unsigned char has_child_subreaper : 1 ;
   int posix_timer_id ;
   struct list_head posix_timers ;
   struct hrtimer real_timer ;
   struct pid *leader_pid ;
   ktime_t it_real_incr ;
   struct cpu_itimer it[2U] ;
   struct thread_group_cputimer cputimer ;
   struct task_cputime cputime_expires ;
   struct list_head cpu_timers[3U] ;
   struct pid *tty_old_pgrp ;
   int leader ;
   struct tty_struct *tty ;
   struct autogroup *autogroup ;
   seqlock_t stats_lock ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t cutime ;
   cputime_t cstime ;
   cputime_t gtime ;
   cputime_t cgtime ;
   struct cputime prev_cputime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   unsigned long cnvcsw ;
   unsigned long cnivcsw ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   unsigned long cmin_flt ;
   unsigned long cmaj_flt ;
   unsigned long inblock ;
   unsigned long oublock ;
   unsigned long cinblock ;
   unsigned long coublock ;
   unsigned long maxrss ;
   unsigned long cmaxrss ;
   struct task_io_accounting ioac ;
   unsigned long long sum_sched_runtime ;
   struct rlimit rlim[16U] ;
   struct pacct_struct pacct ;
   struct taskstats *stats ;
   unsigned int audit_tty ;
   unsigned int audit_tty_log_passwd ;
   struct tty_audit_buf *tty_audit_buf ;
   oom_flags_t oom_flags ;
   short oom_score_adj ;
   short oom_score_adj_min ;
   struct mutex cred_guard_mutex ;
};
struct user_struct {
   atomic_t __count ;
   atomic_t processes ;
   atomic_t sigpending ;
   atomic_t inotify_watches ;
   atomic_t inotify_devs ;
   atomic_t fanotify_listeners ;
   atomic_long_t epoll_watches ;
   unsigned long mq_bytes ;
   unsigned long locked_shm ;
   struct key *uid_keyring ;
   struct key *session_keyring ;
   struct hlist_node uidhash_node ;
   kuid_t uid ;
   atomic_long_t locked_vm ;
};
struct backing_dev_info;
struct reclaim_state;
struct sched_info {
   unsigned long pcount ;
   unsigned long long run_delay ;
   unsigned long long last_arrival ;
   unsigned long long last_queued ;
};
struct task_delay_info {
   spinlock_t lock ;
   unsigned int flags ;
   u64 blkio_start ;
   u64 blkio_delay ;
   u64 swapin_delay ;
   u32 blkio_count ;
   u32 swapin_count ;
   u64 freepages_start ;
   u64 freepages_delay ;
   u32 freepages_count ;
};
struct wake_q_node {
   struct wake_q_node *next ;
};
struct io_context;
struct pipe_inode_info;
struct load_weight {
   unsigned long weight ;
   u32 inv_weight ;
};
struct sched_avg {
   u64 last_runnable_update ;
   s64 decay_count ;
   unsigned long load_avg_contrib ;
   unsigned long utilization_avg_contrib ;
   u32 runnable_avg_sum ;
   u32 avg_period ;
   u32 running_avg_sum ;
};
struct sched_statistics {
   u64 wait_start ;
   u64 wait_max ;
   u64 wait_count ;
   u64 wait_sum ;
   u64 iowait_count ;
   u64 iowait_sum ;
   u64 sleep_start ;
   u64 sleep_max ;
   s64 sum_sleep_runtime ;
   u64 block_start ;
   u64 block_max ;
   u64 exec_max ;
   u64 slice_max ;
   u64 nr_migrations_cold ;
   u64 nr_failed_migrations_affine ;
   u64 nr_failed_migrations_running ;
   u64 nr_failed_migrations_hot ;
   u64 nr_forced_migrations ;
   u64 nr_wakeups ;
   u64 nr_wakeups_sync ;
   u64 nr_wakeups_migrate ;
   u64 nr_wakeups_local ;
   u64 nr_wakeups_remote ;
   u64 nr_wakeups_affine ;
   u64 nr_wakeups_affine_attempts ;
   u64 nr_wakeups_passive ;
   u64 nr_wakeups_idle ;
};
struct sched_entity {
   struct load_weight load ;
   struct rb_node run_node ;
   struct list_head group_node ;
   unsigned int on_rq ;
   u64 exec_start ;
   u64 sum_exec_runtime ;
   u64 vruntime ;
   u64 prev_sum_exec_runtime ;
   u64 nr_migrations ;
   struct sched_statistics statistics ;
   int depth ;
   struct sched_entity *parent ;
   struct cfs_rq *cfs_rq ;
   struct cfs_rq *my_q ;
   struct sched_avg avg ;
};
struct rt_rq;
struct sched_rt_entity {
   struct list_head run_list ;
   unsigned long timeout ;
   unsigned long watchdog_stamp ;
   unsigned int time_slice ;
   struct sched_rt_entity *back ;
   struct sched_rt_entity *parent ;
   struct rt_rq *rt_rq ;
   struct rt_rq *my_q ;
};
struct sched_dl_entity {
   struct rb_node rb_node ;
   u64 dl_runtime ;
   u64 dl_deadline ;
   u64 dl_period ;
   u64 dl_bw ;
   s64 runtime ;
   u64 deadline ;
   unsigned int flags ;
   int dl_throttled ;
   int dl_new ;
   int dl_boosted ;
   int dl_yielded ;
   struct hrtimer dl_timer ;
};
struct memcg_oom_info {
   struct mem_cgroup *memcg ;
   gfp_t gfp_mask ;
   int order ;
   unsigned char may_oom : 1 ;
};
struct sched_class;
struct files_struct;
struct compat_robust_list_head;
struct numa_group;
struct ftrace_ret_stack;
struct task_struct {
   long volatile state ;
   void *stack ;
   atomic_t usage ;
   unsigned int flags ;
   unsigned int ptrace ;
   struct llist_node wake_entry ;
   int on_cpu ;
   struct task_struct *last_wakee ;
   unsigned long wakee_flips ;
   unsigned long wakee_flip_decay_ts ;
   int wake_cpu ;
   int on_rq ;
   int prio ;
   int static_prio ;
   int normal_prio ;
   unsigned int rt_priority ;
   struct sched_class const *sched_class ;
   struct sched_entity se ;
   struct sched_rt_entity rt ;
   struct task_group *sched_task_group ;
   struct sched_dl_entity dl ;
   struct hlist_head preempt_notifiers ;
   unsigned int btrace_seq ;
   unsigned int policy ;
   int nr_cpus_allowed ;
   cpumask_t cpus_allowed ;
   unsigned long rcu_tasks_nvcsw ;
   bool rcu_tasks_holdout ;
   struct list_head rcu_tasks_holdout_list ;
   int rcu_tasks_idle_cpu ;
   struct sched_info sched_info ;
   struct list_head tasks ;
   struct plist_node pushable_tasks ;
   struct rb_node pushable_dl_tasks ;
   struct mm_struct *mm ;
   struct mm_struct *active_mm ;
   u32 vmacache_seqnum ;
   struct vm_area_struct *vmacache[4U] ;
   struct task_rss_stat rss_stat ;
   int exit_state ;
   int exit_code ;
   int exit_signal ;
   int pdeath_signal ;
   unsigned long jobctl ;
   unsigned int personality ;
   unsigned char in_execve : 1 ;
   unsigned char in_iowait : 1 ;
   unsigned char sched_reset_on_fork : 1 ;
   unsigned char sched_contributes_to_load : 1 ;
   unsigned char sched_migrated : 1 ;
   unsigned char memcg_kmem_skip_account : 1 ;
   unsigned char brk_randomized : 1 ;
   unsigned long atomic_flags ;
   struct restart_block restart_block ;
   pid_t pid ;
   pid_t tgid ;
   struct task_struct *real_parent ;
   struct task_struct *parent ;
   struct list_head children ;
   struct list_head sibling ;
   struct task_struct *group_leader ;
   struct list_head ptraced ;
   struct list_head ptrace_entry ;
   struct pid_link pids[3U] ;
   struct list_head thread_group ;
   struct list_head thread_node ;
   struct completion *vfork_done ;
   int *set_child_tid ;
   int *clear_child_tid ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t utimescaled ;
   cputime_t stimescaled ;
   cputime_t gtime ;
   struct cputime prev_cputime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   u64 start_time ;
   u64 real_start_time ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   struct task_cputime cputime_expires ;
   struct list_head cpu_timers[3U] ;
   struct cred const *real_cred ;
   struct cred const *cred ;
   char comm[16U] ;
   struct nameidata *nameidata ;
   struct sysv_sem sysvsem ;
   struct sysv_shm sysvshm ;
   unsigned long last_switch_count ;
   struct thread_struct thread ;
   struct fs_struct *fs ;
   struct files_struct *files ;
   struct nsproxy *nsproxy ;
   struct signal_struct *signal ;
   struct sighand_struct *sighand ;
   sigset_t blocked ;
   sigset_t real_blocked ;
   sigset_t saved_sigmask ;
   struct sigpending pending ;
   unsigned long sas_ss_sp ;
   size_t sas_ss_size ;
   int (*notifier)(void * ) ;
   void *notifier_data ;
   sigset_t *notifier_mask ;
   struct callback_head *task_works ;
   struct audit_context *audit_context ;
   kuid_t loginuid ;
   unsigned int sessionid ;
   struct seccomp seccomp ;
   u32 parent_exec_id ;
   u32 self_exec_id ;
   spinlock_t alloc_lock ;
   raw_spinlock_t pi_lock ;
   struct wake_q_node wake_q ;
   struct rb_root pi_waiters ;
   struct rb_node *pi_waiters_leftmost ;
   struct rt_mutex_waiter *pi_blocked_on ;
   struct mutex_waiter *blocked_on ;
   unsigned int irq_events ;
   unsigned long hardirq_enable_ip ;
   unsigned long hardirq_disable_ip ;
   unsigned int hardirq_enable_event ;
   unsigned int hardirq_disable_event ;
   int hardirqs_enabled ;
   int hardirq_context ;
   unsigned long softirq_disable_ip ;
   unsigned long softirq_enable_ip ;
   unsigned int softirq_disable_event ;
   unsigned int softirq_enable_event ;
   int softirqs_enabled ;
   int softirq_context ;
   u64 curr_chain_key ;
   int lockdep_depth ;
   unsigned int lockdep_recursion ;
   struct held_lock held_locks[48U] ;
   gfp_t lockdep_reclaim_gfp ;
   void *journal_info ;
   struct bio_list *bio_list ;
   struct blk_plug *plug ;
   struct reclaim_state *reclaim_state ;
   struct backing_dev_info *backing_dev_info ;
   struct io_context *io_context ;
   unsigned long ptrace_message ;
   siginfo_t *last_siginfo ;
   struct task_io_accounting ioac ;
   u64 acct_rss_mem1 ;
   u64 acct_vm_mem1 ;
   cputime_t acct_timexpd ;
   nodemask_t mems_allowed ;
   seqcount_t mems_allowed_seq ;
   int cpuset_mem_spread_rotor ;
   int cpuset_slab_spread_rotor ;
   struct css_set *cgroups ;
   struct list_head cg_list ;
   struct robust_list_head *robust_list ;
   struct compat_robust_list_head *compat_robust_list ;
   struct list_head pi_state_list ;
   struct futex_pi_state *pi_state_cache ;
   struct perf_event_context *perf_event_ctxp[2U] ;
   struct mutex perf_event_mutex ;
   struct list_head perf_event_list ;
   struct mempolicy *mempolicy ;
   short il_next ;
   short pref_node_fork ;
   int numa_scan_seq ;
   unsigned int numa_scan_period ;
   unsigned int numa_scan_period_max ;
   int numa_preferred_nid ;
   unsigned long numa_migrate_retry ;
   u64 node_stamp ;
   u64 last_task_numa_placement ;
   u64 last_sum_exec_runtime ;
   struct callback_head numa_work ;
   struct list_head numa_entry ;
   struct numa_group *numa_group ;
   unsigned long *numa_faults ;
   unsigned long total_numa_faults ;
   unsigned long numa_faults_locality[3U] ;
   unsigned long numa_pages_migrated ;
   struct callback_head rcu ;
   struct pipe_inode_info *splice_pipe ;
   struct page_frag task_frag ;
   struct task_delay_info *delays ;
   int make_it_fail ;
   int nr_dirtied ;
   int nr_dirtied_pause ;
   unsigned long dirty_paused_when ;
   int latency_record_count ;
   struct latency_record latency_record[32U] ;
   unsigned long timer_slack_ns ;
   unsigned long default_timer_slack_ns ;
   unsigned int kasan_depth ;
   int curr_ret_stack ;
   struct ftrace_ret_stack *ret_stack ;
   unsigned long long ftrace_timestamp ;
   atomic_t trace_overrun ;
   atomic_t tracing_graph_pause ;
   unsigned long trace ;
   unsigned long trace_recursion ;
   struct memcg_oom_info memcg_oom ;
   struct uprobe_task *utask ;
   unsigned int sequential_io ;
   unsigned int sequential_io_avg ;
   unsigned long task_state_change ;
   int pagefault_disabled ;
};
struct amdgpu_irq_src;
struct drm_encoder;
struct drm_gem_object;
struct drm_connector;
struct amdgpu_device;
struct fence;
struct amdgpu_mode_mc_save;
struct backlight_device;
struct drm_display_mode;
struct drm_crtc;
struct amdgpu_ring;
struct ttm_bo_device;
struct amdgpu_encoder;
struct drm_framebuffer;
struct ttm_buffer_object;
struct drm_minor;
struct device_attribute;
struct fb_var_screeninfo;
struct ttm_tt;
struct drm_device;
struct amdgpu_ib;
struct fb_info;
struct i2c_adapter;
struct mmu_notifier;
struct ttm_mem_reg;
struct drm_file;
enum chipset_type {
    NOT_SUPPORTED = 0,
    SUPPORTED = 1
} ;
struct agp_version {
   u16 major ;
   u16 minor ;
};
struct agp_kern_info {
   struct agp_version version ;
   struct pci_dev *device ;
   enum chipset_type chipset ;
   unsigned long mode ;
   unsigned long aper_base ;
   size_t aper_size ;
   int max_memory ;
   int current_memory ;
   bool cant_use_aperture ;
   unsigned long page_mask ;
   struct vm_operations_struct const *vm_ops ;
};
struct agp_bridge_data;
struct scatterlist;
struct cdev {
   struct kobject kobj ;
   struct module *owner ;
   struct file_operations const *ops ;
   struct list_head list ;
   dev_t dev ;
   unsigned int count ;
};
struct klist_node;
struct klist_node {
   void *n_klist ;
   struct list_head n_node ;
   struct kref n_ref ;
};
struct path;
struct seq_file {
   char *buf ;
   size_t size ;
   size_t from ;
   size_t count ;
   size_t pad_until ;
   loff_t index ;
   loff_t read_pos ;
   u64 version ;
   struct mutex lock ;
   struct seq_operations const *op ;
   int poll_event ;
   struct user_namespace *user_ns ;
   void *private ;
};
struct seq_operations {
   void *(*start)(struct seq_file * , loff_t * ) ;
   void (*stop)(struct seq_file * , void * ) ;
   void *(*next)(struct seq_file * , void * , loff_t * ) ;
   int (*show)(struct seq_file * , void * ) ;
};
struct pinctrl;
struct pinctrl_state;
struct dev_pin_info {
   struct pinctrl *p ;
   struct pinctrl_state *default_state ;
   struct pinctrl_state *sleep_state ;
   struct pinctrl_state *idle_state ;
};
struct dma_map_ops;
struct dev_archdata {
   struct dma_map_ops *dma_ops ;
   void *iommu ;
};
struct pdev_archdata {
};
struct device_private;
struct device_driver;
struct driver_private;
struct class;
struct subsys_private;
struct bus_type;
struct device_node;
struct fwnode_handle;
struct iommu_ops;
struct iommu_group;
struct bus_type {
   char const *name ;
   char const *dev_name ;
   struct device *dev_root ;
   struct device_attribute *dev_attrs ;
   struct attribute_group const **bus_groups ;
   struct attribute_group const **dev_groups ;
   struct attribute_group const **drv_groups ;
   int (*match)(struct device * , struct device_driver * ) ;
   int (*uevent)(struct device * , struct kobj_uevent_env * ) ;
   int (*probe)(struct device * ) ;
   int (*remove)(struct device * ) ;
   void (*shutdown)(struct device * ) ;
   int (*online)(struct device * ) ;
   int (*offline)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct dev_pm_ops const *pm ;
   struct iommu_ops const *iommu_ops ;
   struct subsys_private *p ;
   struct lock_class_key lock_key ;
};
struct device_type;
enum probe_type {
    PROBE_DEFAULT_STRATEGY = 0,
    PROBE_PREFER_ASYNCHRONOUS = 1,
    PROBE_FORCE_SYNCHRONOUS = 2
} ;
struct of_device_id;
struct acpi_device_id;
struct device_driver {
   char const *name ;
   struct bus_type *bus ;
   struct module *owner ;
   char const *mod_name ;
   bool suppress_bind_attrs ;
   enum probe_type probe_type ;
   struct of_device_id const *of_match_table ;
   struct acpi_device_id const *acpi_match_table ;
   int (*probe)(struct device * ) ;
   int (*remove)(struct device * ) ;
   void (*shutdown)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct attribute_group const **groups ;
   struct dev_pm_ops const *pm ;
   struct driver_private *p ;
};
struct class_attribute;
struct class {
   char const *name ;
   struct module *owner ;
   struct class_attribute *class_attrs ;
   struct attribute_group const **dev_groups ;
   struct kobject *dev_kobj ;
   int (*dev_uevent)(struct device * , struct kobj_uevent_env * ) ;
   char *(*devnode)(struct device * , umode_t * ) ;
   void (*class_release)(struct class * ) ;
   void (*dev_release)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct kobj_ns_type_operations const *ns_type ;
   void const *(*namespace)(struct device * ) ;
   struct dev_pm_ops const *pm ;
   struct subsys_private *p ;
};
struct class_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct class * , struct class_attribute * , char * ) ;
   ssize_t (*store)(struct class * , struct class_attribute * , char const * , size_t ) ;
};
struct device_type {
   char const *name ;
   struct attribute_group const **groups ;
   int (*uevent)(struct device * , struct kobj_uevent_env * ) ;
   char *(*devnode)(struct device * , umode_t * , kuid_t * , kgid_t * ) ;
   void (*release)(struct device * ) ;
   struct dev_pm_ops const *pm ;
};
struct device_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct device * , struct device_attribute * , char * ) ;
   ssize_t (*store)(struct device * , struct device_attribute * , char const * ,
                    size_t ) ;
};
struct device_dma_parameters {
   unsigned int max_segment_size ;
   unsigned long segment_boundary_mask ;
};
struct dma_coherent_mem;
struct cma;
struct device {
   struct device *parent ;
   struct device_private *p ;
   struct kobject kobj ;
   char const *init_name ;
   struct device_type const *type ;
   struct mutex mutex ;
   struct bus_type *bus ;
   struct device_driver *driver ;
   void *platform_data ;
   void *driver_data ;
   struct dev_pm_info power ;
   struct dev_pm_domain *pm_domain ;
   struct dev_pin_info *pins ;
   int numa_node ;
   u64 *dma_mask ;
   u64 coherent_dma_mask ;
   unsigned long dma_pfn_offset ;
   struct device_dma_parameters *dma_parms ;
   struct list_head dma_pools ;
   struct dma_coherent_mem *dma_mem ;
   struct cma *cma_area ;
   struct dev_archdata archdata ;
   struct device_node *of_node ;
   struct fwnode_handle *fwnode ;
   dev_t devt ;
   u32 id ;
   spinlock_t devres_lock ;
   struct list_head devres_head ;
   struct klist_node knode_class ;
   struct class *class ;
   struct attribute_group const **groups ;
   void (*release)(struct device * ) ;
   struct iommu_group *iommu_group ;
   bool offline_disabled ;
   bool offline ;
};
struct wakeup_source {
   char const *name ;
   struct list_head entry ;
   spinlock_t lock ;
   struct wake_irq *wakeirq ;
   struct timer_list timer ;
   unsigned long timer_expires ;
   ktime_t total_time ;
   ktime_t max_time ;
   ktime_t last_time ;
   ktime_t start_prevent_time ;
   ktime_t prevent_sleep_time ;
   unsigned long event_count ;
   unsigned long active_count ;
   unsigned long relax_count ;
   unsigned long expire_count ;
   unsigned long wakeup_count ;
   bool active ;
   bool autosleep_enabled ;
};
struct dma_attrs {
   unsigned long flags[1U] ;
};
enum dma_data_direction {
    DMA_BIDIRECTIONAL = 0,
    DMA_TO_DEVICE = 1,
    DMA_FROM_DEVICE = 2,
    DMA_NONE = 3
} ;
struct shrink_control {
   gfp_t gfp_mask ;
   unsigned long nr_to_scan ;
   int nid ;
   struct mem_cgroup *memcg ;
};
struct shrinker {
   unsigned long (*count_objects)(struct shrinker * , struct shrink_control * ) ;
   unsigned long (*scan_objects)(struct shrinker * , struct shrink_control * ) ;
   int seeks ;
   long batch ;
   unsigned long flags ;
   struct list_head list ;
   atomic_long_t *nr_deferred ;
};
struct file_ra_state;
struct writeback_control;
struct bdi_writeback;
struct vm_fault {
   unsigned int flags ;
   unsigned long pgoff ;
   void *virtual_address ;
   struct page *cow_page ;
   struct page *page ;
   unsigned long max_pgoff ;
   pte_t *pte ;
};
struct vm_operations_struct {
   void (*open)(struct vm_area_struct * ) ;
   void (*close)(struct vm_area_struct * ) ;
   int (*fault)(struct vm_area_struct * , struct vm_fault * ) ;
   void (*map_pages)(struct vm_area_struct * , struct vm_fault * ) ;
   int (*page_mkwrite)(struct vm_area_struct * , struct vm_fault * ) ;
   int (*pfn_mkwrite)(struct vm_area_struct * , struct vm_fault * ) ;
   int (*access)(struct vm_area_struct * , unsigned long , void * , int , int ) ;
   char const *(*name)(struct vm_area_struct * ) ;
   int (*set_policy)(struct vm_area_struct * , struct mempolicy * ) ;
   struct mempolicy *(*get_policy)(struct vm_area_struct * , unsigned long ) ;
   struct page *(*find_special_page)(struct vm_area_struct * , unsigned long ) ;
};
struct scatterlist {
   unsigned long sg_magic ;
   unsigned long page_link ;
   unsigned int offset ;
   unsigned int length ;
   dma_addr_t dma_address ;
   unsigned int dma_length ;
};
struct sg_table {
   struct scatterlist *sgl ;
   unsigned int nents ;
   unsigned int orig_nents ;
};
struct dma_map_ops {
   void *(*alloc)(struct device * , size_t , dma_addr_t * , gfp_t , struct dma_attrs * ) ;
   void (*free)(struct device * , size_t , void * , dma_addr_t , struct dma_attrs * ) ;
   int (*mmap)(struct device * , struct vm_area_struct * , void * , dma_addr_t ,
               size_t , struct dma_attrs * ) ;
   int (*get_sgtable)(struct device * , struct sg_table * , void * , dma_addr_t ,
                      size_t , struct dma_attrs * ) ;
   dma_addr_t (*map_page)(struct device * , struct page * , unsigned long , size_t ,
                          enum dma_data_direction , struct dma_attrs * ) ;
   void (*unmap_page)(struct device * , dma_addr_t , size_t , enum dma_data_direction ,
                      struct dma_attrs * ) ;
   int (*map_sg)(struct device * , struct scatterlist * , int , enum dma_data_direction ,
                 struct dma_attrs * ) ;
   void (*unmap_sg)(struct device * , struct scatterlist * , int , enum dma_data_direction ,
                    struct dma_attrs * ) ;
   void (*sync_single_for_cpu)(struct device * , dma_addr_t , size_t , enum dma_data_direction ) ;
   void (*sync_single_for_device)(struct device * , dma_addr_t , size_t , enum dma_data_direction ) ;
   void (*sync_sg_for_cpu)(struct device * , struct scatterlist * , int , enum dma_data_direction ) ;
   void (*sync_sg_for_device)(struct device * , struct scatterlist * , int , enum dma_data_direction ) ;
   int (*mapping_error)(struct device * , dma_addr_t ) ;
   int (*dma_supported)(struct device * , u64 ) ;
   int (*set_dma_mask)(struct device * , u64 ) ;
   int is_phys ;
};
struct vfsmount;
struct hlist_bl_node;
struct hlist_bl_head {
   struct hlist_bl_node *first ;
};
struct hlist_bl_node {
   struct hlist_bl_node *next ;
   struct hlist_bl_node **pprev ;
};
struct __anonstruct____missing_field_name_220 {
   spinlock_t lock ;
   int count ;
};
union __anonunion____missing_field_name_219 {
   struct __anonstruct____missing_field_name_220 __annonCompField58 ;
};
struct lockref {
   union __anonunion____missing_field_name_219 __annonCompField59 ;
};
struct __anonstruct____missing_field_name_222 {
   u32 hash ;
   u32 len ;
};
union __anonunion____missing_field_name_221 {
   struct __anonstruct____missing_field_name_222 __annonCompField60 ;
   u64 hash_len ;
};
struct qstr {
   union __anonunion____missing_field_name_221 __annonCompField61 ;
   unsigned char const *name ;
};
struct dentry_operations;
union __anonunion_d_u_223 {
   struct hlist_node d_alias ;
   struct callback_head d_rcu ;
};
struct dentry {
   unsigned int d_flags ;
   seqcount_t d_seq ;
   struct hlist_bl_node d_hash ;
   struct dentry *d_parent ;
   struct qstr d_name ;
   struct inode *d_inode ;
   unsigned char d_iname[32U] ;
   struct lockref d_lockref ;
   struct dentry_operations const *d_op ;
   struct super_block *d_sb ;
   unsigned long d_time ;
   void *d_fsdata ;
   struct list_head d_lru ;
   struct list_head d_child ;
   struct list_head d_subdirs ;
   union __anonunion_d_u_223 d_u ;
};
struct dentry_operations {
   int (*d_revalidate)(struct dentry * , unsigned int ) ;
   int (*d_weak_revalidate)(struct dentry * , unsigned int ) ;
   int (*d_hash)(struct dentry const * , struct qstr * ) ;
   int (*d_compare)(struct dentry const * , struct dentry const * , unsigned int ,
                    char const * , struct qstr const * ) ;
   int (*d_delete)(struct dentry const * ) ;
   void (*d_release)(struct dentry * ) ;
   void (*d_prune)(struct dentry * ) ;
   void (*d_iput)(struct dentry * , struct inode * ) ;
   char *(*d_dname)(struct dentry * , char * , int ) ;
   struct vfsmount *(*d_automount)(struct path * ) ;
   int (*d_manage)(struct dentry * , bool ) ;
   struct inode *(*d_select_inode)(struct dentry * , unsigned int ) ;
};
struct path {
   struct vfsmount *mnt ;
   struct dentry *dentry ;
};
struct list_lru_one {
   struct list_head list ;
   long nr_items ;
};
struct list_lru_memcg {
   struct list_lru_one *lru[0U] ;
};
struct list_lru_node {
   spinlock_t lock ;
   struct list_lru_one lru ;
   struct list_lru_memcg *memcg_lrus ;
};
struct list_lru {
   struct list_lru_node *node ;
   struct list_head list ;
};
struct __anonstruct____missing_field_name_227 {
   struct radix_tree_node *parent ;
   void *private_data ;
};
union __anonunion____missing_field_name_226 {
   struct __anonstruct____missing_field_name_227 __annonCompField62 ;
   struct callback_head callback_head ;
};
struct radix_tree_node {
   unsigned int path ;
   unsigned int count ;
   union __anonunion____missing_field_name_226 __annonCompField63 ;
   struct list_head private_list ;
   void *slots[64U] ;
   unsigned long tags[3U][1U] ;
};
struct radix_tree_root {
   unsigned int height ;
   gfp_t gfp_mask ;
   struct radix_tree_node *rnode ;
};
struct fiemap_extent {
   __u64 fe_logical ;
   __u64 fe_physical ;
   __u64 fe_length ;
   __u64 fe_reserved64[2U] ;
   __u32 fe_flags ;
   __u32 fe_reserved[3U] ;
};
enum migrate_mode {
    MIGRATE_ASYNC = 0,
    MIGRATE_SYNC_LIGHT = 1,
    MIGRATE_SYNC = 2
} ;
struct block_device;
struct export_operations;
struct kiocb;
struct poll_table_struct;
struct kstatfs;
struct swap_info_struct;
struct iov_iter;
struct iattr {
   unsigned int ia_valid ;
   umode_t ia_mode ;
   kuid_t ia_uid ;
   kgid_t ia_gid ;
   loff_t ia_size ;
   struct timespec ia_atime ;
   struct timespec ia_mtime ;
   struct timespec ia_ctime ;
   struct file *ia_file ;
};
struct dquot;
typedef __kernel_uid32_t projid_t;
struct __anonstruct_kprojid_t_231 {
   projid_t val ;
};
typedef struct __anonstruct_kprojid_t_231 kprojid_t;
enum quota_type {
    USRQUOTA = 0,
    GRPQUOTA = 1,
    PRJQUOTA = 2
} ;
typedef long long qsize_t;
union __anonunion____missing_field_name_232 {
   kuid_t uid ;
   kgid_t gid ;
   kprojid_t projid ;
};
struct kqid {
   union __anonunion____missing_field_name_232 __annonCompField65 ;
   enum quota_type type ;
};
struct mem_dqblk {
   qsize_t dqb_bhardlimit ;
   qsize_t dqb_bsoftlimit ;
   qsize_t dqb_curspace ;
   qsize_t dqb_rsvspace ;
   qsize_t dqb_ihardlimit ;
   qsize_t dqb_isoftlimit ;
   qsize_t dqb_curinodes ;
   time_t dqb_btime ;
   time_t dqb_itime ;
};
struct quota_format_type;
struct mem_dqinfo {
   struct quota_format_type *dqi_format ;
   int dqi_fmt_id ;
   struct list_head dqi_dirty_list ;
   unsigned long dqi_flags ;
   unsigned int dqi_bgrace ;
   unsigned int dqi_igrace ;
   qsize_t dqi_max_spc_limit ;
   qsize_t dqi_max_ino_limit ;
   void *dqi_priv ;
};
struct dquot {
   struct hlist_node dq_hash ;
   struct list_head dq_inuse ;
   struct list_head dq_free ;
   struct list_head dq_dirty ;
   struct mutex dq_lock ;
   atomic_t dq_count ;
   wait_queue_head_t dq_wait_unused ;
   struct super_block *dq_sb ;
   struct kqid dq_id ;
   loff_t dq_off ;
   unsigned long dq_flags ;
   struct mem_dqblk dq_dqb ;
};
struct quota_format_ops {
   int (*check_quota_file)(struct super_block * , int ) ;
   int (*read_file_info)(struct super_block * , int ) ;
   int (*write_file_info)(struct super_block * , int ) ;
   int (*free_file_info)(struct super_block * , int ) ;
   int (*read_dqblk)(struct dquot * ) ;
   int (*commit_dqblk)(struct dquot * ) ;
   int (*release_dqblk)(struct dquot * ) ;
};
struct dquot_operations {
   int (*write_dquot)(struct dquot * ) ;
   struct dquot *(*alloc_dquot)(struct super_block * , int ) ;
   void (*destroy_dquot)(struct dquot * ) ;
   int (*acquire_dquot)(struct dquot * ) ;
   int (*release_dquot)(struct dquot * ) ;
   int (*mark_dirty)(struct dquot * ) ;
   int (*write_info)(struct super_block * , int ) ;
   qsize_t *(*get_reserved_space)(struct inode * ) ;
   int (*get_projid)(struct inode * , kprojid_t * ) ;
};
struct qc_dqblk {
   int d_fieldmask ;
   u64 d_spc_hardlimit ;
   u64 d_spc_softlimit ;
   u64 d_ino_hardlimit ;
   u64 d_ino_softlimit ;
   u64 d_space ;
   u64 d_ino_count ;
   s64 d_ino_timer ;
   s64 d_spc_timer ;
   int d_ino_warns ;
   int d_spc_warns ;
   u64 d_rt_spc_hardlimit ;
   u64 d_rt_spc_softlimit ;
   u64 d_rt_space ;
   s64 d_rt_spc_timer ;
   int d_rt_spc_warns ;
};
struct qc_type_state {
   unsigned int flags ;
   unsigned int spc_timelimit ;
   unsigned int ino_timelimit ;
   unsigned int rt_spc_timelimit ;
   unsigned int spc_warnlimit ;
   unsigned int ino_warnlimit ;
   unsigned int rt_spc_warnlimit ;
   unsigned long long ino ;
   blkcnt_t blocks ;
   blkcnt_t nextents ;
};
struct qc_state {
   unsigned int s_incoredqs ;
   struct qc_type_state s_state[3U] ;
};
struct qc_info {
   int i_fieldmask ;
   unsigned int i_flags ;
   unsigned int i_spc_timelimit ;
   unsigned int i_ino_timelimit ;
   unsigned int i_rt_spc_timelimit ;
   unsigned int i_spc_warnlimit ;
   unsigned int i_ino_warnlimit ;
   unsigned int i_rt_spc_warnlimit ;
};
struct quotactl_ops {
   int (*quota_on)(struct super_block * , int , int , struct path * ) ;
   int (*quota_off)(struct super_block * , int ) ;
   int (*quota_enable)(struct super_block * , unsigned int ) ;
   int (*quota_disable)(struct super_block * , unsigned int ) ;
   int (*quota_sync)(struct super_block * , int ) ;
   int (*set_info)(struct super_block * , int , struct qc_info * ) ;
   int (*get_dqblk)(struct super_block * , struct kqid , struct qc_dqblk * ) ;
   int (*set_dqblk)(struct super_block * , struct kqid , struct qc_dqblk * ) ;
   int (*get_state)(struct super_block * , struct qc_state * ) ;
   int (*rm_xquota)(struct super_block * , unsigned int ) ;
};
struct quota_format_type {
   int qf_fmt_id ;
   struct quota_format_ops const *qf_ops ;
   struct module *qf_owner ;
   struct quota_format_type *qf_next ;
};
struct quota_info {
   unsigned int flags ;
   struct mutex dqio_mutex ;
   struct mutex dqonoff_mutex ;
   struct inode *files[3U] ;
   struct mem_dqinfo info[3U] ;
   struct quota_format_ops const *ops[3U] ;
};
struct kiocb {
   struct file *ki_filp ;
   loff_t ki_pos ;
   void (*ki_complete)(struct kiocb * , long , long ) ;
   void *private ;
   int ki_flags ;
};
struct address_space_operations {
   int (*writepage)(struct page * , struct writeback_control * ) ;
   int (*readpage)(struct file * , struct page * ) ;
   int (*writepages)(struct address_space * , struct writeback_control * ) ;
   int (*set_page_dirty)(struct page * ) ;
   int (*readpages)(struct file * , struct address_space * , struct list_head * ,
                    unsigned int ) ;
   int (*write_begin)(struct file * , struct address_space * , loff_t , unsigned int ,
                      unsigned int , struct page ** , void ** ) ;
   int (*write_end)(struct file * , struct address_space * , loff_t , unsigned int ,
                    unsigned int , struct page * , void * ) ;
   sector_t (*bmap)(struct address_space * , sector_t ) ;
   void (*invalidatepage)(struct page * , unsigned int , unsigned int ) ;
   int (*releasepage)(struct page * , gfp_t ) ;
   void (*freepage)(struct page * ) ;
   ssize_t (*direct_IO)(struct kiocb * , struct iov_iter * , loff_t ) ;
   int (*migratepage)(struct address_space * , struct page * , struct page * , enum migrate_mode ) ;
   int (*launder_page)(struct page * ) ;
   int (*is_partially_uptodate)(struct page * , unsigned long , unsigned long ) ;
   void (*is_dirty_writeback)(struct page * , bool * , bool * ) ;
   int (*error_remove_page)(struct address_space * , struct page * ) ;
   int (*swap_activate)(struct swap_info_struct * , struct file * , sector_t * ) ;
   void (*swap_deactivate)(struct file * ) ;
};
struct address_space {
   struct inode *host ;
   struct radix_tree_root page_tree ;
   spinlock_t tree_lock ;
   atomic_t i_mmap_writable ;
   struct rb_root i_mmap ;
   struct rw_semaphore i_mmap_rwsem ;
   unsigned long nrpages ;
   unsigned long nrshadows ;
   unsigned long writeback_index ;
   struct address_space_operations const *a_ops ;
   unsigned long flags ;
   spinlock_t private_lock ;
   struct list_head private_list ;
   void *private_data ;
};
struct request_queue;
struct hd_struct;
struct gendisk;
struct block_device {
   dev_t bd_dev ;
   int bd_openers ;
   struct inode *bd_inode ;
   struct super_block *bd_super ;
   struct mutex bd_mutex ;
   struct list_head bd_inodes ;
   void *bd_claiming ;
   void *bd_holder ;
   int bd_holders ;
   bool bd_write_holder ;
   struct list_head bd_holder_disks ;
   struct block_device *bd_contains ;
   unsigned int bd_block_size ;
   struct hd_struct *bd_part ;
   unsigned int bd_part_count ;
   int bd_invalidated ;
   struct gendisk *bd_disk ;
   struct request_queue *bd_queue ;
   struct list_head bd_list ;
   unsigned long bd_private ;
   int bd_fsfreeze_count ;
   struct mutex bd_fsfreeze_mutex ;
};
struct posix_acl;
struct inode_operations;
union __anonunion____missing_field_name_235 {
   unsigned int const i_nlink ;
   unsigned int __i_nlink ;
};
union __anonunion____missing_field_name_236 {
   struct hlist_head i_dentry ;
   struct callback_head i_rcu ;
};
struct file_lock_context;
union __anonunion____missing_field_name_237 {
   struct pipe_inode_info *i_pipe ;
   struct block_device *i_bdev ;
   struct cdev *i_cdev ;
   char *i_link ;
};
struct inode {
   umode_t i_mode ;
   unsigned short i_opflags ;
   kuid_t i_uid ;
   kgid_t i_gid ;
   unsigned int i_flags ;
   struct posix_acl *i_acl ;
   struct posix_acl *i_default_acl ;
   struct inode_operations const *i_op ;
   struct super_block *i_sb ;
   struct address_space *i_mapping ;
   void *i_security ;
   unsigned long i_ino ;
   union __anonunion____missing_field_name_235 __annonCompField66 ;
   dev_t i_rdev ;
   loff_t i_size ;
   struct timespec i_atime ;
   struct timespec i_mtime ;
   struct timespec i_ctime ;
   spinlock_t i_lock ;
   unsigned short i_bytes ;
   unsigned int i_blkbits ;
   blkcnt_t i_blocks ;
   unsigned long i_state ;
   struct mutex i_mutex ;
   unsigned long dirtied_when ;
   unsigned long dirtied_time_when ;
   struct hlist_node i_hash ;
   struct list_head i_wb_list ;
   struct bdi_writeback *i_wb ;
   int i_wb_frn_winner ;
   u16 i_wb_frn_avg_time ;
   u16 i_wb_frn_history ;
   struct list_head i_lru ;
   struct list_head i_sb_list ;
   union __anonunion____missing_field_name_236 __annonCompField67 ;
   u64 i_version ;
   atomic_t i_count ;
   atomic_t i_dio_count ;
   atomic_t i_writecount ;
   atomic_t i_readcount ;
   struct file_operations const *i_fop ;
   struct file_lock_context *i_flctx ;
   struct address_space i_data ;
   struct list_head i_devices ;
   union __anonunion____missing_field_name_237 __annonCompField68 ;
   __u32 i_generation ;
   __u32 i_fsnotify_mask ;
   struct hlist_head i_fsnotify_marks ;
   void *i_private ;
};
struct fown_struct {
   rwlock_t lock ;
   struct pid *pid ;
   enum pid_type pid_type ;
   kuid_t uid ;
   kuid_t euid ;
   int signum ;
};
struct file_ra_state {
   unsigned long start ;
   unsigned int size ;
   unsigned int async_size ;
   unsigned int ra_pages ;
   unsigned int mmap_miss ;
   loff_t prev_pos ;
};
union __anonunion_f_u_238 {
   struct llist_node fu_llist ;
   struct callback_head fu_rcuhead ;
};
struct file {
   union __anonunion_f_u_238 f_u ;
   struct path f_path ;
   struct inode *f_inode ;
   struct file_operations const *f_op ;
   spinlock_t f_lock ;
   atomic_long_t f_count ;
   unsigned int f_flags ;
   fmode_t f_mode ;
   struct mutex f_pos_lock ;
   loff_t f_pos ;
   struct fown_struct f_owner ;
   struct cred const *f_cred ;
   struct file_ra_state f_ra ;
   u64 f_version ;
   void *f_security ;
   void *private_data ;
   struct list_head f_ep_links ;
   struct list_head f_tfile_llink ;
   struct address_space *f_mapping ;
};
typedef void *fl_owner_t;
struct file_lock;
struct file_lock_operations {
   void (*fl_copy_lock)(struct file_lock * , struct file_lock * ) ;
   void (*fl_release_private)(struct file_lock * ) ;
};
struct lock_manager_operations {
   int (*lm_compare_owner)(struct file_lock * , struct file_lock * ) ;
   unsigned long (*lm_owner_key)(struct file_lock * ) ;
   fl_owner_t (*lm_get_owner)(fl_owner_t ) ;
   void (*lm_put_owner)(fl_owner_t ) ;
   void (*lm_notify)(struct file_lock * ) ;
   int (*lm_grant)(struct file_lock * , int ) ;
   bool (*lm_break)(struct file_lock * ) ;
   int (*lm_change)(struct file_lock * , int , struct list_head * ) ;
   void (*lm_setup)(struct file_lock * , void ** ) ;
};
struct nlm_lockowner;
struct nfs_lock_info {
   u32 state ;
   struct nlm_lockowner *owner ;
   struct list_head list ;
};
struct nfs4_lock_state;
struct nfs4_lock_info {
   struct nfs4_lock_state *owner ;
};
struct fasync_struct;
struct __anonstruct_afs_240 {
   struct list_head link ;
   int state ;
};
union __anonunion_fl_u_239 {
   struct nfs_lock_info nfs_fl ;
   struct nfs4_lock_info nfs4_fl ;
   struct __anonstruct_afs_240 afs ;
};
struct file_lock {
   struct file_lock *fl_next ;
   struct list_head fl_list ;
   struct hlist_node fl_link ;
   struct list_head fl_block ;
   fl_owner_t fl_owner ;
   unsigned int fl_flags ;
   unsigned char fl_type ;
   unsigned int fl_pid ;
   int fl_link_cpu ;
   struct pid *fl_nspid ;
   wait_queue_head_t fl_wait ;
   struct file *fl_file ;
   loff_t fl_start ;
   loff_t fl_end ;
   struct fasync_struct *fl_fasync ;
   unsigned long fl_break_time ;
   unsigned long fl_downgrade_time ;
   struct file_lock_operations const *fl_ops ;
   struct lock_manager_operations const *fl_lmops ;
   union __anonunion_fl_u_239 fl_u ;
};
struct file_lock_context {
   spinlock_t flc_lock ;
   struct list_head flc_flock ;
   struct list_head flc_posix ;
   struct list_head flc_lease ;
};
struct fasync_struct {
   spinlock_t fa_lock ;
   int magic ;
   int fa_fd ;
   struct fasync_struct *fa_next ;
   struct file *fa_file ;
   struct callback_head fa_rcu ;
};
struct sb_writers {
   struct percpu_counter counter[3U] ;
   wait_queue_head_t wait ;
   int frozen ;
   wait_queue_head_t wait_unfrozen ;
   struct lockdep_map lock_map[3U] ;
};
struct super_operations;
struct xattr_handler;
struct mtd_info;
struct super_block {
   struct list_head s_list ;
   dev_t s_dev ;
   unsigned char s_blocksize_bits ;
   unsigned long s_blocksize ;
   loff_t s_maxbytes ;
   struct file_system_type *s_type ;
   struct super_operations const *s_op ;
   struct dquot_operations const *dq_op ;
   struct quotactl_ops const *s_qcop ;
   struct export_operations const *s_export_op ;
   unsigned long s_flags ;
   unsigned long s_iflags ;
   unsigned long s_magic ;
   struct dentry *s_root ;
   struct rw_semaphore s_umount ;
   int s_count ;
   atomic_t s_active ;
   void *s_security ;
   struct xattr_handler const **s_xattr ;
   struct list_head s_inodes ;
   struct hlist_bl_head s_anon ;
   struct list_head s_mounts ;
   struct block_device *s_bdev ;
   struct backing_dev_info *s_bdi ;
   struct mtd_info *s_mtd ;
   struct hlist_node s_instances ;
   unsigned int s_quota_types ;
   struct quota_info s_dquot ;
   struct sb_writers s_writers ;
   char s_id[32U] ;
   u8 s_uuid[16U] ;
   void *s_fs_info ;
   unsigned int s_max_links ;
   fmode_t s_mode ;
   u32 s_time_gran ;
   struct mutex s_vfs_rename_mutex ;
   char *s_subtype ;
   char *s_options ;
   struct dentry_operations const *s_d_op ;
   int cleancache_poolid ;
   struct shrinker s_shrink ;
   atomic_long_t s_remove_count ;
   int s_readonly_remount ;
   struct workqueue_struct *s_dio_done_wq ;
   struct hlist_head s_pins ;
   struct list_lru s_dentry_lru ;
   struct list_lru s_inode_lru ;
   struct callback_head rcu ;
   int s_stack_depth ;
};
struct fiemap_extent_info {
   unsigned int fi_flags ;
   unsigned int fi_extents_mapped ;
   unsigned int fi_extents_max ;
   struct fiemap_extent *fi_extents_start ;
};
struct dir_context;
struct dir_context {
   int (*actor)(struct dir_context * , char const * , int , loff_t , u64 , unsigned int ) ;
   loff_t pos ;
};
struct file_operations {
   struct module *owner ;
   loff_t (*llseek)(struct file * , loff_t , int ) ;
   ssize_t (*read)(struct file * , char * , size_t , loff_t * ) ;
   ssize_t (*write)(struct file * , char const * , size_t , loff_t * ) ;
   ssize_t (*read_iter)(struct kiocb * , struct iov_iter * ) ;
   ssize_t (*write_iter)(struct kiocb * , struct iov_iter * ) ;
   int (*iterate)(struct file * , struct dir_context * ) ;
   unsigned int (*poll)(struct file * , struct poll_table_struct * ) ;
   long (*unlocked_ioctl)(struct file * , unsigned int , unsigned long ) ;
   long (*compat_ioctl)(struct file * , unsigned int , unsigned long ) ;
   int (*mmap)(struct file * , struct vm_area_struct * ) ;
   int (*mremap)(struct file * , struct vm_area_struct * ) ;
   int (*open)(struct inode * , struct file * ) ;
   int (*flush)(struct file * , fl_owner_t ) ;
   int (*release)(struct inode * , struct file * ) ;
   int (*fsync)(struct file * , loff_t , loff_t , int ) ;
   int (*aio_fsync)(struct kiocb * , int ) ;
   int (*fasync)(int , struct file * , int ) ;
   int (*lock)(struct file * , int , struct file_lock * ) ;
   ssize_t (*sendpage)(struct file * , struct page * , int , size_t , loff_t * ,
                       int ) ;
   unsigned long (*get_unmapped_area)(struct file * , unsigned long , unsigned long ,
                                      unsigned long , unsigned long ) ;
   int (*check_flags)(int ) ;
   int (*flock)(struct file * , int , struct file_lock * ) ;
   ssize_t (*splice_write)(struct pipe_inode_info * , struct file * , loff_t * , size_t ,
                           unsigned int ) ;
   ssize_t (*splice_read)(struct file * , loff_t * , struct pipe_inode_info * , size_t ,
                          unsigned int ) ;
   int (*setlease)(struct file * , long , struct file_lock ** , void ** ) ;
   long (*fallocate)(struct file * , int , loff_t , loff_t ) ;
   void (*show_fdinfo)(struct seq_file * , struct file * ) ;
};
struct inode_operations {
   struct dentry *(*lookup)(struct inode * , struct dentry * , unsigned int ) ;
   char const *(*follow_link)(struct dentry * , void ** ) ;
   int (*permission)(struct inode * , int ) ;
   struct posix_acl *(*get_acl)(struct inode * , int ) ;
   int (*readlink)(struct dentry * , char * , int ) ;
   void (*put_link)(struct inode * , void * ) ;
   int (*create)(struct inode * , struct dentry * , umode_t , bool ) ;
   int (*link)(struct dentry * , struct inode * , struct dentry * ) ;
   int (*unlink)(struct inode * , struct dentry * ) ;
   int (*symlink)(struct inode * , struct dentry * , char const * ) ;
   int (*mkdir)(struct inode * , struct dentry * , umode_t ) ;
   int (*rmdir)(struct inode * , struct dentry * ) ;
   int (*mknod)(struct inode * , struct dentry * , umode_t , dev_t ) ;
   int (*rename)(struct inode * , struct dentry * , struct inode * , struct dentry * ) ;
   int (*rename2)(struct inode * , struct dentry * , struct inode * , struct dentry * ,
                  unsigned int ) ;
   int (*setattr)(struct dentry * , struct iattr * ) ;
   int (*getattr)(struct vfsmount * , struct dentry * , struct kstat * ) ;
   int (*setxattr)(struct dentry * , char const * , void const * , size_t , int ) ;
   ssize_t (*getxattr)(struct dentry * , char const * , void * , size_t ) ;
   ssize_t (*listxattr)(struct dentry * , char * , size_t ) ;
   int (*removexattr)(struct dentry * , char const * ) ;
   int (*fiemap)(struct inode * , struct fiemap_extent_info * , u64 , u64 ) ;
   int (*update_time)(struct inode * , struct timespec * , int ) ;
   int (*atomic_open)(struct inode * , struct dentry * , struct file * , unsigned int ,
                      umode_t , int * ) ;
   int (*tmpfile)(struct inode * , struct dentry * , umode_t ) ;
   int (*set_acl)(struct inode * , struct posix_acl * , int ) ;
};
struct super_operations {
   struct inode *(*alloc_inode)(struct super_block * ) ;
   void (*destroy_inode)(struct inode * ) ;
   void (*dirty_inode)(struct inode * , int ) ;
   int (*write_inode)(struct inode * , struct writeback_control * ) ;
   int (*drop_inode)(struct inode * ) ;
   void (*evict_inode)(struct inode * ) ;
   void (*put_super)(struct super_block * ) ;
   int (*sync_fs)(struct super_block * , int ) ;
   int (*freeze_super)(struct super_block * ) ;
   int (*freeze_fs)(struct super_block * ) ;
   int (*thaw_super)(struct super_block * ) ;
   int (*unfreeze_fs)(struct super_block * ) ;
   int (*statfs)(struct dentry * , struct kstatfs * ) ;
   int (*remount_fs)(struct super_block * , int * , char * ) ;
   void (*umount_begin)(struct super_block * ) ;
   int (*show_options)(struct seq_file * , struct dentry * ) ;
   int (*show_devname)(struct seq_file * , struct dentry * ) ;
   int (*show_path)(struct seq_file * , struct dentry * ) ;
   int (*show_stats)(struct seq_file * , struct dentry * ) ;
   ssize_t (*quota_read)(struct super_block * , int , char * , size_t , loff_t ) ;
   ssize_t (*quota_write)(struct super_block * , int , char const * , size_t ,
                          loff_t ) ;
   struct dquot **(*get_dquots)(struct inode * ) ;
   int (*bdev_try_to_free_page)(struct super_block * , struct page * , gfp_t ) ;
   long (*nr_cached_objects)(struct super_block * , struct shrink_control * ) ;
   long (*free_cached_objects)(struct super_block * , struct shrink_control * ) ;
};
struct file_system_type {
   char const *name ;
   int fs_flags ;
   struct dentry *(*mount)(struct file_system_type * , int , char const * , void * ) ;
   void (*kill_sb)(struct super_block * ) ;
   struct module *owner ;
   struct file_system_type *next ;
   struct hlist_head fs_supers ;
   struct lock_class_key s_lock_key ;
   struct lock_class_key s_umount_key ;
   struct lock_class_key s_vfs_rename_key ;
   struct lock_class_key s_writers_key[3U] ;
   struct lock_class_key i_lock_key ;
   struct lock_class_key i_mutex_key ;
   struct lock_class_key i_mutex_dir_key ;
};
struct exception_table_entry {
   int insn ;
   int fixup ;
};
enum irqreturn {
    IRQ_NONE = 0,
    IRQ_HANDLED = 1,
    IRQ_WAKE_THREAD = 2
} ;
typedef enum irqreturn irqreturn_t;
struct proc_dir_entry;
typedef unsigned long kernel_ulong_t;
struct pci_device_id {
   __u32 vendor ;
   __u32 device ;
   __u32 subvendor ;
   __u32 subdevice ;
   __u32 class ;
   __u32 class_mask ;
   kernel_ulong_t driver_data ;
};
struct acpi_device_id {
   __u8 id[9U] ;
   kernel_ulong_t driver_data ;
};
struct of_device_id {
   char name[32U] ;
   char type[32U] ;
   char compatible[128U] ;
   void const *data ;
};
struct platform_device_id {
   char name[20U] ;
   kernel_ulong_t driver_data ;
};
struct hotplug_slot;
struct pci_slot {
   struct pci_bus *bus ;
   struct list_head list ;
   struct hotplug_slot *hotplug ;
   unsigned char number ;
   struct kobject kobj ;
};
typedef int pci_power_t;
typedef unsigned int pci_channel_state_t;
enum pci_channel_state {
    pci_channel_io_normal = 1,
    pci_channel_io_frozen = 2,
    pci_channel_io_perm_failure = 3
} ;
typedef unsigned short pci_dev_flags_t;
typedef unsigned short pci_bus_flags_t;
struct pcie_link_state;
struct pci_vpd;
struct pci_sriov;
struct pci_ats;
struct pci_driver;
union __anonunion____missing_field_name_252 {
   struct pci_sriov *sriov ;
   struct pci_dev *physfn ;
};
struct pci_dev {
   struct list_head bus_list ;
   struct pci_bus *bus ;
   struct pci_bus *subordinate ;
   void *sysdata ;
   struct proc_dir_entry *procent ;
   struct pci_slot *slot ;
   unsigned int devfn ;
   unsigned short vendor ;
   unsigned short device ;
   unsigned short subsystem_vendor ;
   unsigned short subsystem_device ;
   unsigned int class ;
   u8 revision ;
   u8 hdr_type ;
   u8 pcie_cap ;
   u8 msi_cap ;
   u8 msix_cap ;
   unsigned char pcie_mpss : 3 ;
   u8 rom_base_reg ;
   u8 pin ;
   u16 pcie_flags_reg ;
   u8 dma_alias_devfn ;
   struct pci_driver *driver ;
   u64 dma_mask ;
   struct device_dma_parameters dma_parms ;
   pci_power_t current_state ;
   u8 pm_cap ;
   unsigned char pme_support : 5 ;
   unsigned char pme_interrupt : 1 ;
   unsigned char pme_poll : 1 ;
   unsigned char d1_support : 1 ;
   unsigned char d2_support : 1 ;
   unsigned char no_d1d2 : 1 ;
   unsigned char no_d3cold : 1 ;
   unsigned char d3cold_allowed : 1 ;
   unsigned char mmio_always_on : 1 ;
   unsigned char wakeup_prepared : 1 ;
   unsigned char runtime_d3cold : 1 ;
   unsigned char ignore_hotplug : 1 ;
   unsigned int d3_delay ;
   unsigned int d3cold_delay ;
   struct pcie_link_state *link_state ;
   pci_channel_state_t error_state ;
   struct device dev ;
   int cfg_size ;
   unsigned int irq ;
   struct resource resource[17U] ;
   bool match_driver ;
   unsigned char transparent : 1 ;
   unsigned char multifunction : 1 ;
   unsigned char is_added : 1 ;
   unsigned char is_busmaster : 1 ;
   unsigned char no_msi : 1 ;
   unsigned char no_64bit_msi : 1 ;
   unsigned char block_cfg_access : 1 ;
   unsigned char broken_parity_status : 1 ;
   unsigned char irq_reroute_variant : 2 ;
   unsigned char msi_enabled : 1 ;
   unsigned char msix_enabled : 1 ;
   unsigned char ari_enabled : 1 ;
   unsigned char is_managed : 1 ;
   unsigned char needs_freset : 1 ;
   unsigned char state_saved : 1 ;
   unsigned char is_physfn : 1 ;
   unsigned char is_virtfn : 1 ;
   unsigned char reset_fn : 1 ;
   unsigned char is_hotplug_bridge : 1 ;
   unsigned char __aer_firmware_first_valid : 1 ;
   unsigned char __aer_firmware_first : 1 ;
   unsigned char broken_intx_masking : 1 ;
   unsigned char io_window_1k : 1 ;
   unsigned char irq_managed : 1 ;
   unsigned char has_secondary_link : 1 ;
   pci_dev_flags_t dev_flags ;
   atomic_t enable_cnt ;
   u32 saved_config_space[16U] ;
   struct hlist_head saved_cap_space ;
   struct bin_attribute *rom_attr ;
   int rom_attr_enabled ;
   struct bin_attribute *res_attr[17U] ;
   struct bin_attribute *res_attr_wc[17U] ;
   struct list_head msi_list ;
   struct attribute_group const **msi_irq_groups ;
   struct pci_vpd *vpd ;
   union __anonunion____missing_field_name_252 __annonCompField76 ;
   struct pci_ats *ats ;
   phys_addr_t rom ;
   size_t romlen ;
   char *driver_override ;
};
struct pci_ops;
struct msi_controller;
struct pci_bus {
   struct list_head node ;
   struct pci_bus *parent ;
   struct list_head children ;
   struct list_head devices ;
   struct pci_dev *self ;
   struct list_head slots ;
   struct resource *resource[4U] ;
   struct list_head resources ;
   struct resource busn_res ;
   struct pci_ops *ops ;
   struct msi_controller *msi ;
   void *sysdata ;
   struct proc_dir_entry *procdir ;
   unsigned char number ;
   unsigned char primary ;
   unsigned char max_bus_speed ;
   unsigned char cur_bus_speed ;
   char name[48U] ;
   unsigned short bridge_ctl ;
   pci_bus_flags_t bus_flags ;
   struct device *bridge ;
   struct device dev ;
   struct bin_attribute *legacy_io ;
   struct bin_attribute *legacy_mem ;
   unsigned char is_added : 1 ;
};
struct pci_ops {
   void *(*map_bus)(struct pci_bus * , unsigned int , int ) ;
   int (*read)(struct pci_bus * , unsigned int , int , int , u32 * ) ;
   int (*write)(struct pci_bus * , unsigned int , int , int , u32 ) ;
};
struct pci_dynids {
   spinlock_t lock ;
   struct list_head list ;
};
typedef unsigned int pci_ers_result_t;
struct pci_error_handlers {
   pci_ers_result_t (*error_detected)(struct pci_dev * , enum pci_channel_state ) ;
   pci_ers_result_t (*mmio_enabled)(struct pci_dev * ) ;
   pci_ers_result_t (*link_reset)(struct pci_dev * ) ;
   pci_ers_result_t (*slot_reset)(struct pci_dev * ) ;
   void (*reset_notify)(struct pci_dev * , bool ) ;
   void (*resume)(struct pci_dev * ) ;
};
struct pci_driver {
   struct list_head node ;
   char const *name ;
   struct pci_device_id const *id_table ;
   int (*probe)(struct pci_dev * , struct pci_device_id const * ) ;
   void (*remove)(struct pci_dev * ) ;
   int (*suspend)(struct pci_dev * , pm_message_t ) ;
   int (*suspend_late)(struct pci_dev * , pm_message_t ) ;
   int (*resume_early)(struct pci_dev * ) ;
   int (*resume)(struct pci_dev * ) ;
   void (*shutdown)(struct pci_dev * ) ;
   int (*sriov_configure)(struct pci_dev * , int ) ;
   struct pci_error_handlers const *err_handler ;
   struct device_driver driver ;
   struct pci_dynids dynids ;
};
struct mfd_cell;
struct platform_device {
   char const *name ;
   int id ;
   bool id_auto ;
   struct device dev ;
   u32 num_resources ;
   struct resource *resource ;
   struct platform_device_id const *id_entry ;
   char *driver_override ;
   struct mfd_cell *mfd_cell ;
   struct pdev_archdata archdata ;
};
struct pollfd {
   int fd ;
   short events ;
   short revents ;
};
struct poll_table_struct {
   void (*_qproc)(struct file * , wait_queue_head_t * , struct poll_table_struct * ) ;
   unsigned long _key ;
};
typedef unsigned int drm_magic_t;
struct drm_clip_rect {
   unsigned short x1 ;
   unsigned short y1 ;
   unsigned short x2 ;
   unsigned short y2 ;
};
struct drm_hw_lock {
   unsigned int volatile lock ;
   char padding[60U] ;
};
struct drm_mode_fb_cmd2 {
   __u32 fb_id ;
   __u32 width ;
   __u32 height ;
   __u32 pixel_format ;
   __u32 flags ;
   __u32 handles[4U] ;
   __u32 pitches[4U] ;
   __u32 offsets[4U] ;
   __u64 modifier[4U] ;
};
struct drm_mode_create_dumb {
   u32 height ;
   u32 width ;
   u32 bpp ;
   u32 flags ;
   u32 handle ;
   u32 pitch ;
   uint64_t size ;
};
struct drm_event {
   __u32 type ;
   __u32 length ;
};
struct drm_event_vblank {
   struct drm_event base ;
   __u64 user_data ;
   __u32 tv_sec ;
   __u32 tv_usec ;
   __u32 sequence ;
   __u32 reserved ;
};
struct drm_agp_head {
   struct agp_kern_info agp_info ;
   struct list_head memory ;
   unsigned long mode ;
   struct agp_bridge_data *bridge ;
   int enabled ;
   int acquired ;
   unsigned long base ;
   int agp_mtrr ;
   int cant_use_aperture ;
   unsigned long page_mask ;
};
enum fwnode_type {
    FWNODE_INVALID = 0,
    FWNODE_OF = 1,
    FWNODE_ACPI = 2,
    FWNODE_PDATA = 3
} ;
struct fwnode_handle {
   enum fwnode_type type ;
   struct fwnode_handle *secondary ;
};
typedef u32 phandle;
struct property {
   char *name ;
   int length ;
   void *value ;
   struct property *next ;
   unsigned long _flags ;
   unsigned int unique_id ;
   struct bin_attribute attr ;
};
struct device_node {
   char const *name ;
   char const *type ;
   phandle phandle ;
   char const *full_name ;
   struct fwnode_handle fwnode ;
   struct property *properties ;
   struct property *deadprops ;
   struct device_node *parent ;
   struct device_node *child ;
   struct device_node *sibling ;
   struct kobject kobj ;
   unsigned long _flags ;
   void *data ;
};
struct i2c_msg {
   __u16 addr ;
   __u16 flags ;
   __u16 len ;
   __u8 *buf ;
};
union i2c_smbus_data {
   __u8 byte ;
   __u16 word ;
   __u8 block[34U] ;
};
struct i2c_algorithm;
struct i2c_client;
enum i2c_slave_event;
enum i2c_slave_event;
struct i2c_client {
   unsigned short flags ;
   unsigned short addr ;
   char name[20U] ;
   struct i2c_adapter *adapter ;
   struct device dev ;
   int irq ;
   struct list_head detected ;
   int (*slave_cb)(struct i2c_client * , enum i2c_slave_event , u8 * ) ;
};
enum i2c_slave_event {
    I2C_SLAVE_READ_REQUESTED = 0,
    I2C_SLAVE_WRITE_REQUESTED = 1,
    I2C_SLAVE_READ_PROCESSED = 2,
    I2C_SLAVE_WRITE_RECEIVED = 3,
    I2C_SLAVE_STOP = 4
} ;
struct i2c_algorithm {
   int (*master_xfer)(struct i2c_adapter * , struct i2c_msg * , int ) ;
   int (*smbus_xfer)(struct i2c_adapter * , u16 , unsigned short , char , u8 ,
                     int , union i2c_smbus_data * ) ;
   u32 (*functionality)(struct i2c_adapter * ) ;
   int (*reg_slave)(struct i2c_client * ) ;
   int (*unreg_slave)(struct i2c_client * ) ;
};
struct i2c_bus_recovery_info {
   int (*recover_bus)(struct i2c_adapter * ) ;
   int (*get_scl)(struct i2c_adapter * ) ;
   void (*set_scl)(struct i2c_adapter * , int ) ;
   int (*get_sda)(struct i2c_adapter * ) ;
   void (*prepare_recovery)(struct i2c_adapter * ) ;
   void (*unprepare_recovery)(struct i2c_adapter * ) ;
   int scl_gpio ;
   int sda_gpio ;
};
struct i2c_adapter_quirks {
   u64 flags ;
   int max_num_msgs ;
   u16 max_write_len ;
   u16 max_read_len ;
   u16 max_comb_1st_msg_len ;
   u16 max_comb_2nd_msg_len ;
};
struct i2c_adapter {
   struct module *owner ;
   unsigned int class ;
   struct i2c_algorithm const *algo ;
   void *algo_data ;
   struct rt_mutex bus_lock ;
   int timeout ;
   int retries ;
   struct device dev ;
   int nr ;
   char name[48U] ;
   struct completion dev_released ;
   struct mutex userspace_clients_lock ;
   struct list_head userspace_clients ;
   struct i2c_bus_recovery_info *bus_recovery_info ;
   struct i2c_adapter_quirks const *quirks ;
};
struct fb_fix_screeninfo {
   char id[16U] ;
   unsigned long smem_start ;
   __u32 smem_len ;
   __u32 type ;
   __u32 type_aux ;
   __u32 visual ;
   __u16 xpanstep ;
   __u16 ypanstep ;
   __u16 ywrapstep ;
   __u32 line_length ;
   unsigned long mmio_start ;
   __u32 mmio_len ;
   __u32 accel ;
   __u16 capabilities ;
   __u16 reserved[2U] ;
};
struct fb_bitfield {
   __u32 offset ;
   __u32 length ;
   __u32 msb_right ;
};
struct fb_var_screeninfo {
   __u32 xres ;
   __u32 yres ;
   __u32 xres_virtual ;
   __u32 yres_virtual ;
   __u32 xoffset ;
   __u32 yoffset ;
   __u32 bits_per_pixel ;
   __u32 grayscale ;
   struct fb_bitfield red ;
   struct fb_bitfield green ;
   struct fb_bitfield blue ;
   struct fb_bitfield transp ;
   __u32 nonstd ;
   __u32 activate ;
   __u32 height ;
   __u32 width ;
   __u32 accel_flags ;
   __u32 pixclock ;
   __u32 left_margin ;
   __u32 right_margin ;
   __u32 upper_margin ;
   __u32 lower_margin ;
   __u32 hsync_len ;
   __u32 vsync_len ;
   __u32 sync ;
   __u32 vmode ;
   __u32 rotate ;
   __u32 colorspace ;
   __u32 reserved[4U] ;
};
struct fb_cmap {
   __u32 start ;
   __u32 len ;
   __u16 *red ;
   __u16 *green ;
   __u16 *blue ;
   __u16 *transp ;
};
struct fb_copyarea {
   __u32 dx ;
   __u32 dy ;
   __u32 width ;
   __u32 height ;
   __u32 sx ;
   __u32 sy ;
};
struct fb_fillrect {
   __u32 dx ;
   __u32 dy ;
   __u32 width ;
   __u32 height ;
   __u32 color ;
   __u32 rop ;
};
struct fb_image {
   __u32 dx ;
   __u32 dy ;
   __u32 width ;
   __u32 height ;
   __u32 fg_color ;
   __u32 bg_color ;
   __u8 depth ;
   char const *data ;
   struct fb_cmap cmap ;
};
struct fbcurpos {
   __u16 x ;
   __u16 y ;
};
struct fb_cursor {
   __u16 set ;
   __u16 enable ;
   __u16 rop ;
   char const *mask ;
   struct fbcurpos hot ;
   struct fb_image image ;
};
enum backlight_type {
    BACKLIGHT_RAW = 1,
    BACKLIGHT_PLATFORM = 2,
    BACKLIGHT_FIRMWARE = 3,
    BACKLIGHT_TYPE_MAX = 4
} ;
struct backlight_ops {
   unsigned int options ;
   int (*update_status)(struct backlight_device * ) ;
   int (*get_brightness)(struct backlight_device * ) ;
   int (*check_fb)(struct backlight_device * , struct fb_info * ) ;
};
struct backlight_properties {
   int brightness ;
   int max_brightness ;
   int power ;
   int fb_blank ;
   enum backlight_type type ;
   unsigned int state ;
};
struct backlight_device {
   struct backlight_properties props ;
   struct mutex update_lock ;
   struct mutex ops_lock ;
   struct backlight_ops const *ops ;
   struct notifier_block fb_notif ;
   struct list_head entry ;
   struct device dev ;
   bool fb_bl_on[32U] ;
   int use_count ;
};
struct fb_chroma {
   __u32 redx ;
   __u32 greenx ;
   __u32 bluex ;
   __u32 whitex ;
   __u32 redy ;
   __u32 greeny ;
   __u32 bluey ;
   __u32 whitey ;
};
struct fb_videomode;
struct fb_monspecs {
   struct fb_chroma chroma ;
   struct fb_videomode *modedb ;
   __u8 manufacturer[4U] ;
   __u8 monitor[14U] ;
   __u8 serial_no[14U] ;
   __u8 ascii[14U] ;
   __u32 modedb_len ;
   __u32 model ;
   __u32 serial ;
   __u32 year ;
   __u32 week ;
   __u32 hfmin ;
   __u32 hfmax ;
   __u32 dclkmin ;
   __u32 dclkmax ;
   __u16 input ;
   __u16 dpms ;
   __u16 signal ;
   __u16 vfmin ;
   __u16 vfmax ;
   __u16 gamma ;
   unsigned char gtf : 1 ;
   __u16 misc ;
   __u8 version ;
   __u8 revision ;
   __u8 max_x ;
   __u8 max_y ;
};
struct fb_blit_caps {
   u32 x ;
   u32 y ;
   u32 len ;
   u32 flags ;
};
struct fb_pixmap {
   u8 *addr ;
   u32 size ;
   u32 offset ;
   u32 buf_align ;
   u32 scan_align ;
   u32 access_align ;
   u32 flags ;
   u32 blit_x ;
   u32 blit_y ;
   void (*writeio)(struct fb_info * , void * , void * , unsigned int ) ;
   void (*readio)(struct fb_info * , void * , void * , unsigned int ) ;
};
struct fb_deferred_io {
   unsigned long delay ;
   struct mutex lock ;
   struct list_head pagelist ;
   void (*first_io)(struct fb_info * ) ;
   void (*deferred_io)(struct fb_info * , struct list_head * ) ;
};
struct fb_ops {
   struct module *owner ;
   int (*fb_open)(struct fb_info * , int ) ;
   int (*fb_release)(struct fb_info * , int ) ;
   ssize_t (*fb_read)(struct fb_info * , char * , size_t , loff_t * ) ;
   ssize_t (*fb_write)(struct fb_info * , char const * , size_t , loff_t * ) ;
   int (*fb_check_var)(struct fb_var_screeninfo * , struct fb_info * ) ;
   int (*fb_set_par)(struct fb_info * ) ;
   int (*fb_setcolreg)(unsigned int , unsigned int , unsigned int , unsigned int ,
                       unsigned int , struct fb_info * ) ;
   int (*fb_setcmap)(struct fb_cmap * , struct fb_info * ) ;
   int (*fb_blank)(int , struct fb_info * ) ;
   int (*fb_pan_display)(struct fb_var_screeninfo * , struct fb_info * ) ;
   void (*fb_fillrect)(struct fb_info * , struct fb_fillrect const * ) ;
   void (*fb_copyarea)(struct fb_info * , struct fb_copyarea const * ) ;
   void (*fb_imageblit)(struct fb_info * , struct fb_image const * ) ;
   int (*fb_cursor)(struct fb_info * , struct fb_cursor * ) ;
   void (*fb_rotate)(struct fb_info * , int ) ;
   int (*fb_sync)(struct fb_info * ) ;
   int (*fb_ioctl)(struct fb_info * , unsigned int , unsigned long ) ;
   int (*fb_compat_ioctl)(struct fb_info * , unsigned int , unsigned long ) ;
   int (*fb_mmap)(struct fb_info * , struct vm_area_struct * ) ;
   void (*fb_get_caps)(struct fb_info * , struct fb_blit_caps * , struct fb_var_screeninfo * ) ;
   void (*fb_destroy)(struct fb_info * ) ;
   int (*fb_debug_enter)(struct fb_info * ) ;
   int (*fb_debug_leave)(struct fb_info * ) ;
};
struct fb_tilemap {
   __u32 width ;
   __u32 height ;
   __u32 depth ;
   __u32 length ;
   __u8 const *data ;
};
struct fb_tilerect {
   __u32 sx ;
   __u32 sy ;
   __u32 width ;
   __u32 height ;
   __u32 index ;
   __u32 fg ;
   __u32 bg ;
   __u32 rop ;
};
struct fb_tilearea {
   __u32 sx ;
   __u32 sy ;
   __u32 dx ;
   __u32 dy ;
   __u32 width ;
   __u32 height ;
};
struct fb_tileblit {
   __u32 sx ;
   __u32 sy ;
   __u32 width ;
   __u32 height ;
   __u32 fg ;
   __u32 bg ;
   __u32 length ;
   __u32 *indices ;
};
struct fb_tilecursor {
   __u32 sx ;
   __u32 sy ;
   __u32 mode ;
   __u32 shape ;
   __u32 fg ;
   __u32 bg ;
};
struct fb_tile_ops {
   void (*fb_settile)(struct fb_info * , struct fb_tilemap * ) ;
   void (*fb_tilecopy)(struct fb_info * , struct fb_tilearea * ) ;
   void (*fb_tilefill)(struct fb_info * , struct fb_tilerect * ) ;
   void (*fb_tileblit)(struct fb_info * , struct fb_tileblit * ) ;
   void (*fb_tilecursor)(struct fb_info * , struct fb_tilecursor * ) ;
   int (*fb_get_tilemax)(struct fb_info * ) ;
};
struct aperture {
   resource_size_t base ;
   resource_size_t size ;
};
struct apertures_struct {
   unsigned int count ;
   struct aperture ranges[0U] ;
};
struct fb_info {
   atomic_t count ;
   int node ;
   int flags ;
   struct mutex lock ;
   struct mutex mm_lock ;
   struct fb_var_screeninfo var ;
   struct fb_fix_screeninfo fix ;
   struct fb_monspecs monspecs ;
   struct work_struct queue ;
   struct fb_pixmap pixmap ;
   struct fb_pixmap sprite ;
   struct fb_cmap cmap ;
   struct list_head modelist ;
   struct fb_videomode *mode ;
   struct backlight_device *bl_dev ;
   struct mutex bl_curve_mutex ;
   u8 bl_curve[128U] ;
   struct delayed_work deferred_work ;
   struct fb_deferred_io *fbdefio ;
   struct fb_ops *fbops ;
   struct device *device ;
   struct device *dev ;
   int class_flag ;
   struct fb_tile_ops *tileops ;
   char *screen_base ;
   unsigned long screen_size ;
   void *pseudo_palette ;
   u32 state ;
   void *fbcon_par ;
   void *par ;
   struct apertures_struct *apertures ;
   bool skip_vt_switch ;
};
struct fb_videomode {
   char const *name ;
   u32 refresh ;
   u32 xres ;
   u32 yres ;
   u32 pixclock ;
   u32 left_margin ;
   u32 right_margin ;
   u32 upper_margin ;
   u32 lower_margin ;
   u32 hsync_len ;
   u32 vsync_len ;
   u32 sync ;
   u32 vmode ;
   u32 flag ;
};
enum hdmi_picture_aspect {
    HDMI_PICTURE_ASPECT_NONE = 0,
    HDMI_PICTURE_ASPECT_4_3 = 1,
    HDMI_PICTURE_ASPECT_16_9 = 2,
    HDMI_PICTURE_ASPECT_RESERVED = 3
} ;
struct ww_class {
   atomic_long_t stamp ;
   struct lock_class_key acquire_key ;
   struct lock_class_key mutex_key ;
   char const *acquire_name ;
   char const *mutex_name ;
};
struct ww_mutex;
struct ww_acquire_ctx {
   struct task_struct *task ;
   unsigned long stamp ;
   unsigned int acquired ;
   unsigned int done_acquire ;
   struct ww_class *ww_class ;
   struct ww_mutex *contending_lock ;
   struct lockdep_map dep_map ;
   unsigned int deadlock_inject_interval ;
   unsigned int deadlock_inject_countdown ;
};
struct ww_mutex {
   struct mutex base ;
   struct ww_acquire_ctx *ctx ;
   struct ww_class *ww_class ;
};
struct drm_modeset_lock;
struct drm_modeset_acquire_ctx {
   struct ww_acquire_ctx ww_ctx ;
   struct drm_modeset_lock *contended ;
   struct list_head locked ;
   bool trylock_only ;
};
struct drm_modeset_lock {
   struct ww_mutex mutex ;
   struct list_head head ;
};
struct drm_plane;
struct drm_mode_set;
struct drm_object_properties;
struct drm_mode_object {
   u32 id ;
   u32 type ;
   struct drm_object_properties *properties ;
};
struct drm_property;
struct drm_object_properties {
   int count ;
   int atomic_count ;
   struct drm_property *properties[24U] ;
   uint64_t values[24U] ;
};
enum drm_connector_force {
    DRM_FORCE_UNSPECIFIED = 0,
    DRM_FORCE_OFF = 1,
    DRM_FORCE_ON = 2,
    DRM_FORCE_ON_DIGITAL = 3
} ;
enum drm_mode_status {
    MODE_OK = 0,
    MODE_HSYNC = 1,
    MODE_VSYNC = 2,
    MODE_H_ILLEGAL = 3,
    MODE_V_ILLEGAL = 4,
    MODE_BAD_WIDTH = 5,
    MODE_NOMODE = 6,
    MODE_NO_INTERLACE = 7,
    MODE_NO_DBLESCAN = 8,
    MODE_NO_VSCAN = 9,
    MODE_MEM = 10,
    MODE_VIRTUAL_X = 11,
    MODE_VIRTUAL_Y = 12,
    MODE_MEM_VIRT = 13,
    MODE_NOCLOCK = 14,
    MODE_CLOCK_HIGH = 15,
    MODE_CLOCK_LOW = 16,
    MODE_CLOCK_RANGE = 17,
    MODE_BAD_HVALUE = 18,
    MODE_BAD_VVALUE = 19,
    MODE_BAD_VSCAN = 20,
    MODE_HSYNC_NARROW = 21,
    MODE_HSYNC_WIDE = 22,
    MODE_HBLANK_NARROW = 23,
    MODE_HBLANK_WIDE = 24,
    MODE_VSYNC_NARROW = 25,
    MODE_VSYNC_WIDE = 26,
    MODE_VBLANK_NARROW = 27,
    MODE_VBLANK_WIDE = 28,
    MODE_PANEL = 29,
    MODE_INTERLACE_WIDTH = 30,
    MODE_ONE_WIDTH = 31,
    MODE_ONE_HEIGHT = 32,
    MODE_ONE_SIZE = 33,
    MODE_NO_REDUCED = 34,
    MODE_NO_STEREO = 35,
    MODE_UNVERIFIED = -3,
    MODE_BAD = -2,
    MODE_ERROR = -1
} ;
struct drm_display_mode {
   struct list_head head ;
   struct drm_mode_object base ;
   char name[32U] ;
   enum drm_mode_status status ;
   unsigned int type ;
   int clock ;
   int hdisplay ;
   int hsync_start ;
   int hsync_end ;
   int htotal ;
   int hskew ;
   int vdisplay ;
   int vsync_start ;
   int vsync_end ;
   int vtotal ;
   int vscan ;
   unsigned int flags ;
   int width_mm ;
   int height_mm ;
   int crtc_clock ;
   int crtc_hdisplay ;
   int crtc_hblank_start ;
   int crtc_hblank_end ;
   int crtc_hsync_start ;
   int crtc_hsync_end ;
   int crtc_htotal ;
   int crtc_hskew ;
   int crtc_vdisplay ;
   int crtc_vblank_start ;
   int crtc_vblank_end ;
   int crtc_vsync_start ;
   int crtc_vsync_end ;
   int crtc_vtotal ;
   int *private ;
   int private_flags ;
   int vrefresh ;
   int hsync ;
   enum hdmi_picture_aspect picture_aspect_ratio ;
};
struct drm_cmdline_mode {
   bool specified ;
   bool refresh_specified ;
   bool bpp_specified ;
   int xres ;
   int yres ;
   int bpp ;
   int refresh ;
   bool rb ;
   bool interlace ;
   bool cvt ;
   bool margins ;
   enum drm_connector_force force ;
};
enum drm_connector_status {
    connector_status_connected = 1,
    connector_status_disconnected = 2,
    connector_status_unknown = 3
} ;
enum subpixel_order {
    SubPixelUnknown = 0,
    SubPixelHorizontalRGB = 1,
    SubPixelHorizontalBGR = 2,
    SubPixelVerticalRGB = 3,
    SubPixelVerticalBGR = 4,
    SubPixelNone = 5
} ;
struct drm_display_info {
   char name[32U] ;
   unsigned int width_mm ;
   unsigned int height_mm ;
   unsigned int min_vfreq ;
   unsigned int max_vfreq ;
   unsigned int min_hfreq ;
   unsigned int max_hfreq ;
   unsigned int pixel_clock ;
   unsigned int bpc ;
   enum subpixel_order subpixel_order ;
   u32 color_formats ;
   u32 const *bus_formats ;
   unsigned int num_bus_formats ;
   u8 edid_hdmi_dc_modes ;
   u8 cea_rev ;
};
struct drm_tile_group {
   struct kref refcount ;
   struct drm_device *dev ;
   int id ;
   u8 group_data[8U] ;
};
struct drm_framebuffer_funcs {
   void (*destroy)(struct drm_framebuffer * ) ;
   int (*create_handle)(struct drm_framebuffer * , struct drm_file * , unsigned int * ) ;
   int (*dirty)(struct drm_framebuffer * , struct drm_file * , unsigned int , unsigned int ,
                struct drm_clip_rect * , unsigned int ) ;
};
struct drm_framebuffer {
   struct drm_device *dev ;
   struct kref refcount ;
   struct list_head head ;
   struct drm_mode_object base ;
   struct drm_framebuffer_funcs const *funcs ;
   unsigned int pitches[4U] ;
   unsigned int offsets[4U] ;
   uint64_t modifier[4U] ;
   unsigned int width ;
   unsigned int height ;
   unsigned int depth ;
   int bits_per_pixel ;
   int flags ;
   u32 pixel_format ;
   struct list_head filp_head ;
   void *helper_private ;
};
struct drm_property_blob {
   struct drm_mode_object base ;
   struct drm_device *dev ;
   struct kref refcount ;
   struct list_head head_global ;
   struct list_head head_file ;
   size_t length ;
   unsigned char data[] ;
};
struct drm_property {
   struct list_head head ;
   struct drm_mode_object base ;
   u32 flags ;
   char name[32U] ;
   u32 num_values ;
   uint64_t *values ;
   struct drm_device *dev ;
   struct list_head enum_list ;
};
struct drm_pending_vblank_event;
struct drm_bridge;
struct drm_atomic_state;
struct drm_crtc_state {
   struct drm_crtc *crtc ;
   bool enable ;
   bool active ;
   bool planes_changed ;
   bool mode_changed ;
   bool active_changed ;
   u32 plane_mask ;
   u32 last_vblank_count ;
   struct drm_display_mode adjusted_mode ;
   struct drm_display_mode mode ;
   struct drm_property_blob *mode_blob ;
   struct drm_pending_vblank_event *event ;
   struct drm_atomic_state *state ;
};
struct drm_crtc_funcs {
   void (*save)(struct drm_crtc * ) ;
   void (*restore)(struct drm_crtc * ) ;
   void (*reset)(struct drm_crtc * ) ;
   int (*cursor_set)(struct drm_crtc * , struct drm_file * , u32 , u32 , u32 ) ;
   int (*cursor_set2)(struct drm_crtc * , struct drm_file * , u32 , u32 , u32 ,
                      int32_t , int32_t ) ;
   int (*cursor_move)(struct drm_crtc * , int , int ) ;
   void (*gamma_set)(struct drm_crtc * , u16 * , u16 * , u16 * , u32 , u32 ) ;
   void (*destroy)(struct drm_crtc * ) ;
   int (*set_config)(struct drm_mode_set * ) ;
   int (*page_flip)(struct drm_crtc * , struct drm_framebuffer * , struct drm_pending_vblank_event * ,
                    u32 ) ;
   int (*set_property)(struct drm_crtc * , struct drm_property * , uint64_t ) ;
   struct drm_crtc_state *(*atomic_duplicate_state)(struct drm_crtc * ) ;
   void (*atomic_destroy_state)(struct drm_crtc * , struct drm_crtc_state * ) ;
   int (*atomic_set_property)(struct drm_crtc * , struct drm_crtc_state * , struct drm_property * ,
                              uint64_t ) ;
   int (*atomic_get_property)(struct drm_crtc * , struct drm_crtc_state const * ,
                              struct drm_property * , uint64_t * ) ;
};
struct drm_crtc {
   struct drm_device *dev ;
   struct device_node *port ;
   struct list_head head ;
   struct drm_modeset_lock mutex ;
   struct drm_mode_object base ;
   struct drm_plane *primary ;
   struct drm_plane *cursor ;
   int cursor_x ;
   int cursor_y ;
   bool enabled ;
   struct drm_display_mode mode ;
   struct drm_display_mode hwmode ;
   bool invert_dimensions ;
   int x ;
   int y ;
   struct drm_crtc_funcs const *funcs ;
   u32 gamma_size ;
   u16 *gamma_store ;
   int framedur_ns ;
   int linedur_ns ;
   int pixeldur_ns ;
   void const *helper_private ;
   struct drm_object_properties properties ;
   struct drm_crtc_state *state ;
   struct drm_modeset_acquire_ctx *acquire_ctx ;
};
struct drm_connector_state {
   struct drm_connector *connector ;
   struct drm_crtc *crtc ;
   struct drm_encoder *best_encoder ;
   struct drm_atomic_state *state ;
};
struct drm_connector_funcs {
   void (*dpms)(struct drm_connector * , int ) ;
   void (*save)(struct drm_connector * ) ;
   void (*restore)(struct drm_connector * ) ;
   void (*reset)(struct drm_connector * ) ;
   enum drm_connector_status (*detect)(struct drm_connector * , bool ) ;
   int (*fill_modes)(struct drm_connector * , u32 , u32 ) ;
   int (*set_property)(struct drm_connector * , struct drm_property * , uint64_t ) ;
   void (*destroy)(struct drm_connector * ) ;
   void (*force)(struct drm_connector * ) ;
   struct drm_connector_state *(*atomic_duplicate_state)(struct drm_connector * ) ;
   void (*atomic_destroy_state)(struct drm_connector * , struct drm_connector_state * ) ;
   int (*atomic_set_property)(struct drm_connector * , struct drm_connector_state * ,
                              struct drm_property * , uint64_t ) ;
   int (*atomic_get_property)(struct drm_connector * , struct drm_connector_state const * ,
                              struct drm_property * , uint64_t * ) ;
};
struct drm_encoder_funcs {
   void (*reset)(struct drm_encoder * ) ;
   void (*destroy)(struct drm_encoder * ) ;
};
struct drm_encoder {
   struct drm_device *dev ;
   struct list_head head ;
   struct drm_mode_object base ;
   char *name ;
   int encoder_type ;
   u32 possible_crtcs ;
   u32 possible_clones ;
   struct drm_crtc *crtc ;
   struct drm_bridge *bridge ;
   struct drm_encoder_funcs const *funcs ;
   void const *helper_private ;
};
struct drm_connector {
   struct drm_device *dev ;
   struct device *kdev ;
   struct device_attribute *attr ;
   struct list_head head ;
   struct drm_mode_object base ;
   char *name ;
   int connector_type ;
   int connector_type_id ;
   bool interlace_allowed ;
   bool doublescan_allowed ;
   bool stereo_allowed ;
   struct list_head modes ;
   enum drm_connector_status status ;
   struct list_head probed_modes ;
   struct drm_display_info display_info ;
   struct drm_connector_funcs const *funcs ;
   struct drm_property_blob *edid_blob_ptr ;
   struct drm_object_properties properties ;
   struct drm_property_blob *path_blob_ptr ;
   struct drm_property_blob *tile_blob_ptr ;
   uint8_t polled ;
   int dpms ;
   void const *helper_private ;
   struct drm_cmdline_mode cmdline_mode ;
   enum drm_connector_force force ;
   bool override_edid ;
   u32 encoder_ids[3U] ;
   struct drm_encoder *encoder ;
   uint8_t eld[128U] ;
   bool dvi_dual ;
   int max_tmds_clock ;
   bool latency_present[2U] ;
   int video_latency[2U] ;
   int audio_latency[2U] ;
   int null_edid_counter ;
   unsigned int bad_edid_counter ;
   bool edid_corrupt ;
   struct dentry *debugfs_entry ;
   struct drm_connector_state *state ;
   bool has_tile ;
   struct drm_tile_group *tile_group ;
   bool tile_is_single_monitor ;
   uint8_t num_h_tile ;
   uint8_t num_v_tile ;
   uint8_t tile_h_loc ;
   uint8_t tile_v_loc ;
   u16 tile_h_size ;
   u16 tile_v_size ;
   struct list_head destroy_list ;
};
struct drm_plane_state {
   struct drm_plane *plane ;
   struct drm_crtc *crtc ;
   struct drm_framebuffer *fb ;
   struct fence *fence ;
   int32_t crtc_x ;
   int32_t crtc_y ;
   u32 crtc_w ;
   u32 crtc_h ;
   u32 src_x ;
   u32 src_y ;
   u32 src_h ;
   u32 src_w ;
   unsigned int rotation ;
   struct drm_atomic_state *state ;
};
struct drm_plane_funcs {
   int (*update_plane)(struct drm_plane * , struct drm_crtc * , struct drm_framebuffer * ,
                       int , int , unsigned int , unsigned int , u32 , u32 ,
                       u32 , u32 ) ;
   int (*disable_plane)(struct drm_plane * ) ;
   void (*destroy)(struct drm_plane * ) ;
   void (*reset)(struct drm_plane * ) ;
   int (*set_property)(struct drm_plane * , struct drm_property * , uint64_t ) ;
   struct drm_plane_state *(*atomic_duplicate_state)(struct drm_plane * ) ;
   void (*atomic_destroy_state)(struct drm_plane * , struct drm_plane_state * ) ;
   int (*atomic_set_property)(struct drm_plane * , struct drm_plane_state * , struct drm_property * ,
                              uint64_t ) ;
   int (*atomic_get_property)(struct drm_plane * , struct drm_plane_state const * ,
                              struct drm_property * , uint64_t * ) ;
};
enum drm_plane_type {
    DRM_PLANE_TYPE_OVERLAY = 0,
    DRM_PLANE_TYPE_PRIMARY = 1,
    DRM_PLANE_TYPE_CURSOR = 2
} ;
struct drm_plane {
   struct drm_device *dev ;
   struct list_head head ;
   struct drm_modeset_lock mutex ;
   struct drm_mode_object base ;
   u32 possible_crtcs ;
   u32 *format_types ;
   u32 format_count ;
   bool format_default ;
   struct drm_crtc *crtc ;
   struct drm_framebuffer *fb ;
   struct drm_framebuffer *old_fb ;
   struct drm_plane_funcs const *funcs ;
   struct drm_object_properties properties ;
   enum drm_plane_type type ;
   void const *helper_private ;
   struct drm_plane_state *state ;
};
struct drm_bridge_funcs {
   int (*attach)(struct drm_bridge * ) ;
   bool (*mode_fixup)(struct drm_bridge * , struct drm_display_mode const * , struct drm_display_mode * ) ;
   void (*disable)(struct drm_bridge * ) ;
   void (*post_disable)(struct drm_bridge * ) ;
   void (*mode_set)(struct drm_bridge * , struct drm_display_mode * , struct drm_display_mode * ) ;
   void (*pre_enable)(struct drm_bridge * ) ;
   void (*enable)(struct drm_bridge * ) ;
};
struct drm_bridge {
   struct drm_device *dev ;
   struct drm_encoder *encoder ;
   struct drm_bridge *next ;
   struct device_node *of_node ;
   struct list_head list ;
   struct drm_bridge_funcs const *funcs ;
   void *driver_private ;
};
struct drm_atomic_state {
   struct drm_device *dev ;
   bool allow_modeset ;
   bool legacy_cursor_update ;
   struct drm_plane **planes ;
   struct drm_plane_state **plane_states ;
   struct drm_crtc **crtcs ;
   struct drm_crtc_state **crtc_states ;
   int num_connector ;
   struct drm_connector **connectors ;
   struct drm_connector_state **connector_states ;
   struct drm_modeset_acquire_ctx *acquire_ctx ;
};
struct drm_mode_set {
   struct drm_framebuffer *fb ;
   struct drm_crtc *crtc ;
   struct drm_display_mode *mode ;
   u32 x ;
   u32 y ;
   struct drm_connector **connectors ;
   size_t num_connectors ;
};
struct drm_mode_config_funcs {
   struct drm_framebuffer *(*fb_create)(struct drm_device * , struct drm_file * ,
                                        struct drm_mode_fb_cmd2 * ) ;
   void (*output_poll_changed)(struct drm_device * ) ;
   int (*atomic_check)(struct drm_device * , struct drm_atomic_state * ) ;
   int (*atomic_commit)(struct drm_device * , struct drm_atomic_state * , bool ) ;
   struct drm_atomic_state *(*atomic_state_alloc)(struct drm_device * ) ;
   void (*atomic_state_clear)(struct drm_atomic_state * ) ;
   void (*atomic_state_free)(struct drm_atomic_state * ) ;
};
struct drm_mode_group {
   u32 num_crtcs ;
   u32 num_encoders ;
   u32 num_connectors ;
   u32 *id_list ;
};
struct drm_mode_config {
   struct mutex mutex ;
   struct drm_modeset_lock connection_mutex ;
   struct drm_modeset_acquire_ctx *acquire_ctx ;
   struct mutex idr_mutex ;
   struct idr crtc_idr ;
   struct idr tile_idr ;
   struct mutex fb_lock ;
   int num_fb ;
   struct list_head fb_list ;
   int num_connector ;
   struct list_head connector_list ;
   int num_encoder ;
   struct list_head encoder_list ;
   int num_overlay_plane ;
   int num_total_plane ;
   struct list_head plane_list ;
   int num_crtc ;
   struct list_head crtc_list ;
   struct list_head property_list ;
   int min_width ;
   int min_height ;
   int max_width ;
   int max_height ;
   struct drm_mode_config_funcs const *funcs ;
   resource_size_t fb_base ;
   bool poll_enabled ;
   bool poll_running ;
   bool delayed_event ;
   struct delayed_work output_poll_work ;
   struct mutex blob_lock ;
   struct list_head property_blob_list ;
   struct drm_property *edid_property ;
   struct drm_property *dpms_property ;
   struct drm_property *path_property ;
   struct drm_property *tile_property ;
   struct drm_property *plane_type_property ;
   struct drm_property *rotation_property ;
   struct drm_property *prop_src_x ;
   struct drm_property *prop_src_y ;
   struct drm_property *prop_src_w ;
   struct drm_property *prop_src_h ;
   struct drm_property *prop_crtc_x ;
   struct drm_property *prop_crtc_y ;
   struct drm_property *prop_crtc_w ;
   struct drm_property *prop_crtc_h ;
   struct drm_property *prop_fb_id ;
   struct drm_property *prop_crtc_id ;
   struct drm_property *prop_active ;
   struct drm_property *prop_mode_id ;
   struct drm_property *dvi_i_subconnector_property ;
   struct drm_property *dvi_i_select_subconnector_property ;
   struct drm_property *tv_subconnector_property ;
   struct drm_property *tv_select_subconnector_property ;
   struct drm_property *tv_mode_property ;
   struct drm_property *tv_left_margin_property ;
   struct drm_property *tv_right_margin_property ;
   struct drm_property *tv_top_margin_property ;
   struct drm_property *tv_bottom_margin_property ;
   struct drm_property *tv_brightness_property ;
   struct drm_property *tv_contrast_property ;
   struct drm_property *tv_flicker_reduction_property ;
   struct drm_property *tv_overscan_property ;
   struct drm_property *tv_saturation_property ;
   struct drm_property *tv_hue_property ;
   struct drm_property *scaling_mode_property ;
   struct drm_property *aspect_ratio_property ;
   struct drm_property *dirty_info_property ;
   struct drm_property *suggested_x_property ;
   struct drm_property *suggested_y_property ;
   u32 preferred_depth ;
   u32 prefer_shadow ;
   bool async_page_flip ;
   bool allow_fb_modifiers ;
   u32 cursor_width ;
   u32 cursor_height ;
};
struct edid;
enum drm_global_types {
    DRM_GLOBAL_TTM_MEM = 0,
    DRM_GLOBAL_TTM_BO = 1,
    DRM_GLOBAL_TTM_OBJECT = 2,
    DRM_GLOBAL_NUM = 3
} ;
struct drm_global_reference {
   enum drm_global_types global_type ;
   size_t size ;
   void *object ;
   int (*init)(struct drm_global_reference * ) ;
   void (*release)(struct drm_global_reference * ) ;
};
struct drm_open_hash {
   struct hlist_head *table ;
   u8 order ;
};
struct drm_mm;
struct drm_mm_node {
   struct list_head node_list ;
   struct list_head hole_stack ;
   unsigned char hole_follows : 1 ;
   unsigned char scanned_block : 1 ;
   unsigned char scanned_prev_free : 1 ;
   unsigned char scanned_next_free : 1 ;
   unsigned char scanned_preceeds_hole : 1 ;
   unsigned char allocated : 1 ;
   unsigned long color ;
   u64 start ;
   u64 size ;
   struct drm_mm *mm ;
};
struct drm_mm {
   struct list_head hole_stack ;
   struct drm_mm_node head_node ;
   unsigned char scan_check_range : 1 ;
   unsigned int scan_alignment ;
   unsigned long scan_color ;
   u64 scan_size ;
   u64 scan_hit_start ;
   u64 scan_hit_end ;
   unsigned int scanned_blocks ;
   u64 scan_start ;
   u64 scan_end ;
   struct drm_mm_node *prev_scanned_node ;
   void (*color_adjust)(struct drm_mm_node * , unsigned long , u64 * , u64 * ) ;
};
struct drm_vma_offset_node {
   rwlock_t vm_lock ;
   struct drm_mm_node vm_node ;
   struct rb_node vm_rb ;
   struct rb_root vm_files ;
};
struct drm_vma_offset_manager {
   rwlock_t vm_lock ;
   struct rb_root vm_addr_space_rb ;
   struct drm_mm vm_addr_space_mm ;
};
struct drm_local_map;
struct drm_device_dma;
struct reservation_object;
struct dma_buf_attachment;
typedef int drm_ioctl_t(struct drm_device * , void * , struct drm_file * );
struct drm_ioctl_desc {
   unsigned int cmd ;
   int flags ;
   drm_ioctl_t *func ;
   char const *name ;
};
struct drm_pending_event {
   struct drm_event *event ;
   struct list_head link ;
   struct drm_file *file_priv ;
   pid_t pid ;
   void (*destroy)(struct drm_pending_event * ) ;
};
struct drm_prime_file_private {
   struct list_head head ;
   struct mutex lock ;
};
struct drm_master;
struct drm_file {
   unsigned char authenticated : 1 ;
   unsigned char is_master : 1 ;
   unsigned char stereo_allowed : 1 ;
   unsigned char universal_planes : 1 ;
   unsigned char atomic : 1 ;
   struct pid *pid ;
   kuid_t uid ;
   drm_magic_t magic ;
   struct list_head lhead ;
   struct drm_minor *minor ;
   unsigned long lock_count ;
   struct idr object_idr ;
   spinlock_t table_lock ;
   struct file *filp ;
   void *driver_priv ;
   struct drm_master *master ;
   struct list_head fbs ;
   struct mutex fbs_lock ;
   struct list_head blobs ;
   wait_queue_head_t event_wait ;
   struct list_head event_list ;
   int event_space ;
   struct drm_prime_file_private prime ;
};
struct drm_lock_data {
   struct drm_hw_lock *hw_lock ;
   struct drm_file *file_priv ;
   wait_queue_head_t lock_queue ;
   unsigned long lock_time ;
   spinlock_t spinlock ;
   u32 kernel_waiters ;
   u32 user_waiters ;
   int idle_has_lock ;
};
struct drm_master {
   struct kref refcount ;
   struct drm_minor *minor ;
   char *unique ;
   int unique_len ;
   struct idr magic_map ;
   struct drm_lock_data lock ;
   void *driver_priv ;
};
struct dma_buf;
struct drm_driver {
   int (*load)(struct drm_device * , unsigned long ) ;
   int (*firstopen)(struct drm_device * ) ;
   int (*open)(struct drm_device * , struct drm_file * ) ;
   void (*preclose)(struct drm_device * , struct drm_file * ) ;
   void (*postclose)(struct drm_device * , struct drm_file * ) ;
   void (*lastclose)(struct drm_device * ) ;
   int (*unload)(struct drm_device * ) ;
   int (*suspend)(struct drm_device * , pm_message_t ) ;
   int (*resume)(struct drm_device * ) ;
   int (*dma_ioctl)(struct drm_device * , void * , struct drm_file * ) ;
   int (*dma_quiescent)(struct drm_device * ) ;
   int (*context_dtor)(struct drm_device * , int ) ;
   int (*set_busid)(struct drm_device * , struct drm_master * ) ;
   u32 (*get_vblank_counter)(struct drm_device * , int ) ;
   int (*enable_vblank)(struct drm_device * , int ) ;
   void (*disable_vblank)(struct drm_device * , int ) ;
   int (*device_is_agp)(struct drm_device * ) ;
   int (*get_scanout_position)(struct drm_device * , int , unsigned int , int * ,
                               int * , ktime_t * , ktime_t * ) ;
   int (*get_vblank_timestamp)(struct drm_device * , int , int * , struct timeval * ,
                               unsigned int ) ;
   irqreturn_t (*irq_handler)(int , void * ) ;
   void (*irq_preinstall)(struct drm_device * ) ;
   int (*irq_postinstall)(struct drm_device * ) ;
   void (*irq_uninstall)(struct drm_device * ) ;
   int (*master_create)(struct drm_device * , struct drm_master * ) ;
   void (*master_destroy)(struct drm_device * , struct drm_master * ) ;
   int (*master_set)(struct drm_device * , struct drm_file * , bool ) ;
   void (*master_drop)(struct drm_device * , struct drm_file * , bool ) ;
   int (*debugfs_init)(struct drm_minor * ) ;
   void (*debugfs_cleanup)(struct drm_minor * ) ;
   void (*gem_free_object)(struct drm_gem_object * ) ;
   int (*gem_open_object)(struct drm_gem_object * , struct drm_file * ) ;
   void (*gem_close_object)(struct drm_gem_object * , struct drm_file * ) ;
   int (*prime_handle_to_fd)(struct drm_device * , struct drm_file * , u32 , u32 ,
                             int * ) ;
   int (*prime_fd_to_handle)(struct drm_device * , struct drm_file * , int , u32 * ) ;
   struct dma_buf *(*gem_prime_export)(struct drm_device * , struct drm_gem_object * ,
                                       int ) ;
   struct drm_gem_object *(*gem_prime_import)(struct drm_device * , struct dma_buf * ) ;
   int (*gem_prime_pin)(struct drm_gem_object * ) ;
   void (*gem_prime_unpin)(struct drm_gem_object * ) ;
   struct reservation_object *(*gem_prime_res_obj)(struct drm_gem_object * ) ;
   struct sg_table *(*gem_prime_get_sg_table)(struct drm_gem_object * ) ;
   struct drm_gem_object *(*gem_prime_import_sg_table)(struct drm_device * , struct dma_buf_attachment * ,
                                                       struct sg_table * ) ;
   void *(*gem_prime_vmap)(struct drm_gem_object * ) ;
   void (*gem_prime_vunmap)(struct drm_gem_object * , void * ) ;
   int (*gem_prime_mmap)(struct drm_gem_object * , struct vm_area_struct * ) ;
   void (*vgaarb_irq)(struct drm_device * , bool ) ;
   int (*dumb_create)(struct drm_file * , struct drm_device * , struct drm_mode_create_dumb * ) ;
   int (*dumb_map_offset)(struct drm_file * , struct drm_device * , u32 , uint64_t * ) ;
   int (*dumb_destroy)(struct drm_file * , struct drm_device * , u32 ) ;
   struct vm_operations_struct const *gem_vm_ops ;
   int major ;
   int minor ;
   int patchlevel ;
   char *name ;
   char *desc ;
   char *date ;
   u32 driver_features ;
   int dev_priv_size ;
   struct drm_ioctl_desc const *ioctls ;
   int num_ioctls ;
   struct file_operations const *fops ;
   struct list_head legacy_dev_list ;
};
struct drm_info_list {
   char const *name ;
   int (*show)(struct seq_file * , void * ) ;
   u32 driver_features ;
   void *data ;
};
struct drm_minor {
   int index ;
   int type ;
   struct device *kdev ;
   struct drm_device *dev ;
   struct dentry *debugfs_root ;
   struct list_head debugfs_list ;
   struct mutex debugfs_lock ;
   struct drm_master *master ;
   struct drm_mode_group mode_group ;
};
struct drm_pending_vblank_event {
   struct drm_pending_event base ;
   int pipe ;
   struct drm_event_vblank event ;
};
struct drm_vblank_crtc {
   struct drm_device *dev ;
   wait_queue_head_t queue ;
   struct timer_list disable_timer ;
   unsigned long count ;
   struct timeval time[2U] ;
   atomic_t refcount ;
   u32 last ;
   u32 last_wait ;
   unsigned int inmodeset ;
   int crtc ;
   bool enabled ;
};
struct virtio_device;
struct drm_sg_mem;
struct __anonstruct_sigdata_257 {
   int context ;
   struct drm_hw_lock *lock ;
};
struct drm_device {
   struct list_head legacy_dev_list ;
   int if_version ;
   struct kref ref ;
   struct device *dev ;
   struct drm_driver *driver ;
   void *dev_private ;
   struct drm_minor *control ;
   struct drm_minor *primary ;
   struct drm_minor *render ;
   atomic_t unplugged ;
   struct inode *anon_inode ;
   char *unique ;
   struct mutex struct_mutex ;
   struct mutex master_mutex ;
   int open_count ;
   spinlock_t buf_lock ;
   int buf_use ;
   atomic_t buf_alloc ;
   struct list_head filelist ;
   struct list_head maplist ;
   struct drm_open_hash map_hash ;
   struct list_head ctxlist ;
   struct mutex ctxlist_mutex ;
   struct idr ctx_idr ;
   struct list_head vmalist ;
   struct drm_device_dma *dma ;
   long volatile context_flag ;
   int last_context ;
   bool irq_enabled ;
   int irq ;
   bool vblank_disable_allowed ;
   bool vblank_disable_immediate ;
   struct drm_vblank_crtc *vblank ;
   spinlock_t vblank_time_lock ;
   spinlock_t vbl_lock ;
   u32 max_vblank_count ;
   struct list_head vblank_event_list ;
   spinlock_t event_lock ;
   struct drm_agp_head *agp ;
   struct pci_dev *pdev ;
   struct platform_device *platformdev ;
   struct virtio_device *virtdev ;
   struct drm_sg_mem *sg ;
   unsigned int num_crtcs ;
   sigset_t sigmask ;
   struct __anonstruct_sigdata_257 sigdata ;
   struct drm_local_map *agp_buffer_map ;
   unsigned int agp_buffer_token ;
   struct drm_mode_config mode_config ;
   struct mutex object_name_lock ;
   struct idr object_name_idr ;
   struct drm_vma_offset_manager *vma_offset_manager ;
   int switch_power_state ;
};
struct drm_gem_object {
   struct kref refcount ;
   unsigned int handle_count ;
   struct drm_device *dev ;
   struct file *filp ;
   struct drm_vma_offset_node vma_node ;
   size_t size ;
   int name ;
   u32 read_domains ;
   u32 write_domain ;
   u32 pending_read_domains ;
   u32 pending_write_domain ;
   struct dma_buf *dma_buf ;
   struct dma_buf_attachment *import_attach ;
};
struct firmware {
   size_t size ;
   u8 const *data ;
   struct page **pages ;
   void *priv ;
};
enum amdgpu_asic_type {
    CHIP_BONAIRE = 0,
    CHIP_KAVERI = 1,
    CHIP_KABINI = 2,
    CHIP_HAWAII = 3,
    CHIP_MULLINS = 4,
    CHIP_TOPAZ = 5,
    CHIP_TONGA = 6,
    CHIP_CARRIZO = 7,
    CHIP_LAST = 8
} ;
enum vga_switcheroo_state {
    VGA_SWITCHEROO_OFF = 0,
    VGA_SWITCHEROO_ON = 1,
    VGA_SWITCHEROO_INIT = 2,
    VGA_SWITCHEROO_NOT_FOUND = 3
} ;
struct fence_ops;
struct fence {
   struct kref refcount ;
   struct fence_ops const *ops ;
   struct callback_head rcu ;
   struct list_head cb_list ;
   spinlock_t *lock ;
   unsigned int context ;
   unsigned int seqno ;
   unsigned long flags ;
   ktime_t timestamp ;
   int status ;
};
struct fence_ops {
   char const *(*get_driver_name)(struct fence * ) ;
   char const *(*get_timeline_name)(struct fence * ) ;
   bool (*enable_signaling)(struct fence * ) ;
   bool (*signaled)(struct fence * ) ;
   long (*wait)(struct fence * , bool , long ) ;
   void (*release)(struct fence * ) ;
   int (*fill_driver_data)(struct fence * , void * , int ) ;
   void (*fence_value_str)(struct fence * , char * , int ) ;
   void (*timeline_value_str)(struct fence * , char * , int ) ;
};
struct reservation_object_list {
   struct callback_head rcu ;
   u32 shared_count ;
   u32 shared_max ;
   struct fence *shared[] ;
};
struct reservation_object {
   struct ww_mutex lock ;
   seqcount_t seq ;
   struct fence *fence_excl ;
   struct reservation_object_list *fence ;
   struct reservation_object_list *staged ;
};
struct ttm_place {
   unsigned int fpfn ;
   unsigned int lpfn ;
   u32 flags ;
};
struct ttm_placement {
   unsigned int num_placement ;
   struct ttm_place const *placement ;
   unsigned int num_busy_placement ;
   struct ttm_place const *busy_placement ;
};
struct ttm_bus_placement {
   void *addr ;
   unsigned long base ;
   unsigned long size ;
   unsigned long offset ;
   bool is_iomem ;
   bool io_reserved_vm ;
   uint64_t io_reserved_count ;
};
struct ttm_mem_reg {
   void *mm_node ;
   unsigned long start ;
   unsigned long size ;
   unsigned long num_pages ;
   u32 page_alignment ;
   u32 mem_type ;
   u32 placement ;
   struct ttm_bus_placement bus ;
};
enum ttm_bo_type {
    ttm_bo_type_device = 0,
    ttm_bo_type_kernel = 1,
    ttm_bo_type_sg = 2
} ;
struct ttm_bo_global;
struct ttm_buffer_object {
   struct ttm_bo_global *glob ;
   struct ttm_bo_device *bdev ;
   enum ttm_bo_type type ;
   void (*destroy)(struct ttm_buffer_object * ) ;
   unsigned long num_pages ;
   size_t acc_size ;
   struct kref kref ;
   struct kref list_kref ;
   struct ttm_mem_reg mem ;
   struct file *persistent_swap_storage ;
   struct ttm_tt *ttm ;
   bool evicted ;
   atomic_t cpu_writers ;
   struct list_head lru ;
   struct list_head ddestroy ;
   struct list_head swap ;
   struct list_head io_reserve_lru ;
   unsigned long priv_flags ;
   struct drm_vma_offset_node vma_node ;
   uint64_t offset ;
   u32 cur_placement ;
   struct sg_table *sg ;
   struct reservation_object *resv ;
   struct reservation_object ttm_resv ;
   struct mutex wu_mutex ;
};
enum ldv_29096 {
    ttm_bo_map_iomap = 129,
    ttm_bo_map_vmap = 2,
    ttm_bo_map_kmap = 3,
    ttm_bo_map_premapped = 132
} ;
struct ttm_bo_kmap_obj {
   void *virtual ;
   struct page *page ;
   enum ldv_29096 bo_kmap_type ;
   struct ttm_buffer_object *bo ;
};
struct ttm_mem_shrink {
   int (*do_shrink)(struct ttm_mem_shrink * ) ;
};
struct ttm_mem_zone;
struct ttm_mem_global {
   struct kobject kobj ;
   struct ttm_mem_shrink *shrink ;
   struct workqueue_struct *swap_queue ;
   struct work_struct work ;
   spinlock_t lock ;
   struct ttm_mem_zone *zones[2U] ;
   unsigned int num_zones ;
   struct ttm_mem_zone *zone_kernel ;
   struct ttm_mem_zone *zone_dma32 ;
};
struct ttm_backend_func {
   int (*bind)(struct ttm_tt * , struct ttm_mem_reg * ) ;
   int (*unbind)(struct ttm_tt * ) ;
   void (*destroy)(struct ttm_tt * ) ;
};
enum ttm_caching_state {
    tt_uncached = 0,
    tt_wc = 1,
    tt_cached = 2
} ;
enum ldv_29183 {
    tt_bound = 0,
    tt_unbound = 1,
    tt_unpopulated = 2
} ;
struct ttm_tt {
   struct ttm_bo_device *bdev ;
   struct ttm_backend_func *func ;
   struct page *dummy_read_page ;
   struct page **pages ;
   u32 page_flags ;
   unsigned long num_pages ;
   struct sg_table *sg ;
   struct ttm_bo_global *glob ;
   struct file *swap_storage ;
   enum ttm_caching_state caching_state ;
   enum ldv_29183 state ;
};
struct ttm_mem_type_manager;
struct ttm_mem_type_manager_func {
   int (*init)(struct ttm_mem_type_manager * , unsigned long ) ;
   int (*takedown)(struct ttm_mem_type_manager * ) ;
   int (*get_node)(struct ttm_mem_type_manager * , struct ttm_buffer_object * , struct ttm_place const * ,
                   struct ttm_mem_reg * ) ;
   void (*put_node)(struct ttm_mem_type_manager * , struct ttm_mem_reg * ) ;
   void (*debug)(struct ttm_mem_type_manager * , char const * ) ;
};
struct ttm_mem_type_manager {
   struct ttm_bo_device *bdev ;
   bool has_type ;
   bool use_type ;
   u32 flags ;
   uint64_t gpu_offset ;
   uint64_t size ;
   u32 available_caching ;
   u32 default_caching ;
   struct ttm_mem_type_manager_func const *func ;
   void *priv ;
   struct mutex io_reserve_mutex ;
   bool use_io_reserve_lru ;
   bool io_reserve_fastpath ;
   struct list_head io_reserve_lru ;
   struct list_head lru ;
};
struct ttm_bo_driver {
   struct ttm_tt *(*ttm_tt_create)(struct ttm_bo_device * , unsigned long , u32 ,
                                   struct page * ) ;
   int (*ttm_tt_populate)(struct ttm_tt * ) ;
   void (*ttm_tt_unpopulate)(struct ttm_tt * ) ;
   int (*invalidate_caches)(struct ttm_bo_device * , u32 ) ;
   int (*init_mem_type)(struct ttm_bo_device * , u32 , struct ttm_mem_type_manager * ) ;
   void (*evict_flags)(struct ttm_buffer_object * , struct ttm_placement * ) ;
   int (*move)(struct ttm_buffer_object * , bool , bool , bool , struct ttm_mem_reg * ) ;
   int (*verify_access)(struct ttm_buffer_object * , struct file * ) ;
   void (*move_notify)(struct ttm_buffer_object * , struct ttm_mem_reg * ) ;
   int (*fault_reserve_notify)(struct ttm_buffer_object * ) ;
   void (*swap_notify)(struct ttm_buffer_object * ) ;
   int (*io_mem_reserve)(struct ttm_bo_device * , struct ttm_mem_reg * ) ;
   void (*io_mem_free)(struct ttm_bo_device * , struct ttm_mem_reg * ) ;
};
struct ttm_bo_global_ref {
   struct drm_global_reference ref ;
   struct ttm_mem_global *mem_glob ;
};
struct ttm_bo_global {
   struct kobject kobj ;
   struct ttm_mem_global *mem_glob ;
   struct page *dummy_read_page ;
   struct ttm_mem_shrink shrink ;
   struct mutex device_list_mutex ;
   spinlock_t lru_lock ;
   struct list_head device_list ;
   struct list_head swap_lru ;
   atomic_t bo_count ;
};
struct ttm_bo_device {
   struct list_head device_list ;
   struct ttm_bo_global *glob ;
   struct ttm_bo_driver *driver ;
   struct ttm_mem_type_manager man[8U] ;
   struct drm_vma_offset_manager vma_manager ;
   struct list_head ddestroy ;
   u32 val_seq ;
   struct address_space *dev_mapping ;
   struct delayed_work wq ;
   bool need_dma32 ;
};
struct ttm_validate_buffer {
   struct list_head head ;
   struct ttm_buffer_object *bo ;
   bool shared ;
};
enum amd_ip_block_type {
    AMD_IP_BLOCK_TYPE_COMMON = 0,
    AMD_IP_BLOCK_TYPE_GMC = 1,
    AMD_IP_BLOCK_TYPE_IH = 2,
    AMD_IP_BLOCK_TYPE_SMC = 3,
    AMD_IP_BLOCK_TYPE_DCE = 4,
    AMD_IP_BLOCK_TYPE_GFX = 5,
    AMD_IP_BLOCK_TYPE_SDMA = 6,
    AMD_IP_BLOCK_TYPE_UVD = 7,
    AMD_IP_BLOCK_TYPE_VCE = 8
} ;
enum amd_clockgating_state {
    AMD_CG_STATE_GATE = 0,
    AMD_CG_STATE_UNGATE = 1
} ;
enum amd_powergating_state {
    AMD_PG_STATE_GATE = 0,
    AMD_PG_STATE_UNGATE = 1
} ;
struct amd_ip_funcs {
   int (*early_init)(void * ) ;
   int (*late_init)(void * ) ;
   int (*sw_init)(void * ) ;
   int (*sw_fini)(void * ) ;
   int (*hw_init)(void * ) ;
   int (*hw_fini)(void * ) ;
   int (*suspend)(void * ) ;
   int (*resume)(void * ) ;
   bool (*is_idle)(void * ) ;
   int (*wait_for_idle)(void * ) ;
   int (*soft_reset)(void * ) ;
   void (*print_status)(void * ) ;
   int (*set_clockgating_state)(void * , enum amd_clockgating_state ) ;
   int (*set_powergating_state)(void * , enum amd_powergating_state ) ;
};
struct est_timings {
   u8 t1 ;
   u8 t2 ;
   u8 mfg_rsvd ;
};
struct std_timing {
   u8 hsize ;
   u8 vfreq_aspect ;
};
struct detailed_pixel_timing {
   u8 hactive_lo ;
   u8 hblank_lo ;
   u8 hactive_hblank_hi ;
   u8 vactive_lo ;
   u8 vblank_lo ;
   u8 vactive_vblank_hi ;
   u8 hsync_offset_lo ;
   u8 hsync_pulse_width_lo ;
   u8 vsync_offset_pulse_width_lo ;
   u8 hsync_vsync_offset_pulse_width_hi ;
   u8 width_mm_lo ;
   u8 height_mm_lo ;
   u8 width_height_mm_hi ;
   u8 hborder ;
   u8 vborder ;
   u8 misc ;
};
struct detailed_data_string {
   u8 str[13U] ;
};
struct __anonstruct_gtf2_268 {
   u8 reserved ;
   u8 hfreq_start_khz ;
   u8 c ;
   __le16 m ;
   u8 k ;
   u8 j ;
};
struct __anonstruct_cvt_269 {
   u8 version ;
   u8 data1 ;
   u8 data2 ;
   u8 supported_aspects ;
   u8 flags ;
   u8 supported_scalings ;
   u8 preferred_refresh ;
};
union __anonunion_formula_267 {
   struct __anonstruct_gtf2_268 gtf2 ;
   struct __anonstruct_cvt_269 cvt ;
};
struct detailed_data_monitor_range {
   u8 min_vfreq ;
   u8 max_vfreq ;
   u8 min_hfreq_khz ;
   u8 max_hfreq_khz ;
   u8 pixel_clock_mhz ;
   u8 flags ;
   union __anonunion_formula_267 formula ;
};
struct detailed_data_wpindex {
   u8 white_yx_lo ;
   u8 white_x_hi ;
   u8 white_y_hi ;
   u8 gamma ;
};
struct cvt_timing {
   u8 code[3U] ;
};
union __anonunion_data_270 {
   struct detailed_data_string str ;
   struct detailed_data_monitor_range range ;
   struct detailed_data_wpindex color ;
   struct std_timing timings[6U] ;
   struct cvt_timing cvt[4U] ;
};
struct detailed_non_pixel {
   u8 pad1 ;
   u8 type ;
   u8 pad2 ;
   union __anonunion_data_270 data ;
};
union __anonunion_data_271 {
   struct detailed_pixel_timing pixel_data ;
   struct detailed_non_pixel other_data ;
};
struct detailed_timing {
   __le16 pixel_clock ;
   union __anonunion_data_271 data ;
};
struct edid {
   u8 header[8U] ;
   u8 mfg_id[2U] ;
   u8 prod_code[2U] ;
   u32 serial ;
   u8 mfg_week ;
   u8 mfg_year ;
   u8 version ;
   u8 revision ;
   u8 input ;
   u8 width_cm ;
   u8 height_cm ;
   u8 gamma ;
   u8 features ;
   u8 red_green_lo ;
   u8 black_white_lo ;
   u8 red_x ;
   u8 red_y ;
   u8 green_x ;
   u8 green_y ;
   u8 blue_x ;
   u8 blue_y ;
   u8 white_x ;
   u8 white_y ;
   struct est_timings established_timings ;
   struct std_timing standard_timings[8U] ;
   struct detailed_timing detailed_timings[4U] ;
   u8 extensions ;
   u8 checksum ;
};
struct drm_dp_aux_msg {
   unsigned int address ;
   u8 request ;
   u8 reply ;
   void *buffer ;
   size_t size ;
};
struct drm_dp_aux {
   char const *name ;
   struct i2c_adapter ddc ;
   struct device *dev ;
   struct mutex hw_mutex ;
   ssize_t (*transfer)(struct drm_dp_aux * , struct drm_dp_aux_msg * ) ;
   unsigned int i2c_nack_count ;
   unsigned int i2c_defer_count ;
};
union dfixed {
   u32 full ;
};
typedef union dfixed fixed20_12;
struct i2c_algo_bit_data {
   void *data ;
   void (*setsda)(void * , int ) ;
   void (*setscl)(void * , int ) ;
   int (*getsda)(void * ) ;
   int (*getscl)(void * ) ;
   int (*pre_xfer)(struct i2c_adapter * ) ;
   void (*post_xfer)(struct i2c_adapter * ) ;
   int udelay ;
   int timeout ;
};
struct amdgpu_bo;
struct amdgpu_router;
struct amdgpu_hpd;
enum amdgpu_rmx_type {
    RMX_OFF = 0,
    RMX_FULL = 1,
    RMX_CENTER = 2,
    RMX_ASPECT = 3
} ;
enum amdgpu_underscan_type {
    UNDERSCAN_OFF = 0,
    UNDERSCAN_ON = 1,
    UNDERSCAN_AUTO = 2
} ;
enum amdgpu_hpd_id {
    AMDGPU_HPD_1 = 0,
    AMDGPU_HPD_2 = 1,
    AMDGPU_HPD_3 = 2,
    AMDGPU_HPD_4 = 3,
    AMDGPU_HPD_5 = 4,
    AMDGPU_HPD_6 = 5,
    AMDGPU_HPD_LAST = 6,
    AMDGPU_HPD_NONE = 255
} ;
enum amdgpu_flip_status {
    AMDGPU_FLIP_NONE = 0,
    AMDGPU_FLIP_PENDING = 1,
    AMDGPU_FLIP_SUBMITTED = 2
} ;
struct amdgpu_i2c_bus_rec {
   bool valid ;
   uint8_t i2c_id ;
   enum amdgpu_hpd_id hpd ;
   bool hw_capable ;
   bool mm_i2c ;
   u32 mask_clk_reg ;
   u32 mask_data_reg ;
   u32 a_clk_reg ;
   u32 a_data_reg ;
   u32 en_clk_reg ;
   u32 en_data_reg ;
   u32 y_clk_reg ;
   u32 y_data_reg ;
   u32 mask_clk_mask ;
   u32 mask_data_mask ;
   u32 a_clk_mask ;
   u32 a_data_mask ;
   u32 en_clk_mask ;
   u32 en_data_mask ;
   u32 y_clk_mask ;
   u32 y_data_mask ;
};
struct amdgpu_pll {
   u32 reference_freq ;
   u32 reference_div ;
   u32 post_div ;
   u32 pll_in_min ;
   u32 pll_in_max ;
   u32 pll_out_min ;
   u32 pll_out_max ;
   u32 lcd_pll_out_min ;
   u32 lcd_pll_out_max ;
   u32 best_vco ;
   u32 min_ref_div ;
   u32 max_ref_div ;
   u32 min_post_div ;
   u32 max_post_div ;
   u32 min_feedback_div ;
   u32 max_feedback_div ;
   u32 min_frac_feedback_div ;
   u32 max_frac_feedback_div ;
   u32 flags ;
   u32 id ;
};
struct amdgpu_i2c_chan {
   struct i2c_adapter adapter ;
   struct drm_device *dev ;
   struct i2c_algo_bit_data bit ;
   struct amdgpu_i2c_bus_rec rec ;
   struct drm_dp_aux aux ;
   bool has_aux ;
   struct mutex mutex ;
};
struct amdgpu_fbdev;
struct amdgpu_audio_pin;
struct amdgpu_afmt {
   bool enabled ;
   int offset ;
   bool last_buffer_filled_status ;
   int id ;
   struct amdgpu_audio_pin *pin ;
};
struct amdgpu_audio_pin {
   int channels ;
   int rate ;
   int bits_per_sample ;
   u8 status_bits ;
   u8 category_code ;
   u32 offset ;
   bool connected ;
   u32 id ;
};
struct amdgpu_audio {
   bool enabled ;
   struct amdgpu_audio_pin pin[7U] ;
   int num_pins ;
};
struct amdgpu_mode_mc_save {
   u32 vga_render_control ;
   u32 vga_hdp_control ;
   bool crtc_enabled[6U] ;
};
struct amdgpu_display_funcs {
   void (*set_vga_render_state)(struct amdgpu_device * , bool ) ;
   void (*bandwidth_update)(struct amdgpu_device * ) ;
   u32 (*vblank_get_counter)(struct amdgpu_device * , int ) ;
   void (*vblank_wait)(struct amdgpu_device * , int ) ;
   bool (*is_display_hung)(struct amdgpu_device * ) ;
   void (*backlight_set_level)(struct amdgpu_encoder * , u8 ) ;
   u8 (*backlight_get_level)(struct amdgpu_encoder * ) ;
   bool (*hpd_sense)(struct amdgpu_device * , enum amdgpu_hpd_id ) ;
   void (*hpd_set_polarity)(struct amdgpu_device * , enum amdgpu_hpd_id ) ;
   u32 (*hpd_get_gpio_reg)(struct amdgpu_device * ) ;
   void (*page_flip)(struct amdgpu_device * , int , u64 ) ;
   int (*page_flip_get_scanoutpos)(struct amdgpu_device * , int , u32 * , u32 * ) ;
   void (*add_encoder)(struct amdgpu_device * , u32 , u32 , u16 ) ;
   void (*add_connector)(struct amdgpu_device * , u32 , u32 , int , struct amdgpu_i2c_bus_rec * ,
                         u16 , struct amdgpu_hpd * , struct amdgpu_router * ) ;
   void (*stop_mc_access)(struct amdgpu_device * , struct amdgpu_mode_mc_save * ) ;
   void (*resume_mc_access)(struct amdgpu_device * , struct amdgpu_mode_mc_save * ) ;
};
struct atom_context;
struct card_info;
struct amdgpu_crtc;
struct amdgpu_mode_info {
   struct atom_context *atom_context ;
   struct card_info *atom_card_info ;
   bool mode_config_initialized ;
   struct amdgpu_crtc *crtcs[6U] ;
   struct amdgpu_afmt *afmt[7U] ;
   struct drm_property *coherent_mode_property ;
   struct drm_property *load_detect_property ;
   struct drm_property *underscan_property ;
   struct drm_property *underscan_hborder_property ;
   struct drm_property *underscan_vborder_property ;
   struct drm_property *audio_property ;
   struct drm_property *dither_property ;
   struct edid *bios_hardcoded_edid ;
   int bios_hardcoded_edid_size ;
   struct amdgpu_fbdev *rfbdev ;
   u16 firmware_flags ;
   struct amdgpu_encoder *bl_encoder ;
   struct amdgpu_audio audio ;
   int num_crtc ;
   int num_hpd ;
   int num_dig ;
   int disp_priority ;
   struct amdgpu_display_funcs const *funcs ;
};
struct amdgpu_atom_ss {
   u16 percentage ;
   u16 percentage_divider ;
   uint8_t type ;
   u16 step ;
   uint8_t delay ;
   uint8_t range ;
   uint8_t refdiv ;
   u16 rate ;
   u16 amount ;
};
struct amdgpu_flip_work;
struct amdgpu_crtc {
   struct drm_crtc base ;
   int crtc_id ;
   u16 lut_r[256U] ;
   u16 lut_g[256U] ;
   u16 lut_b[256U] ;
   bool enabled ;
   bool can_tile ;
   u32 crtc_offset ;
   struct drm_gem_object *cursor_bo ;
   uint64_t cursor_addr ;
   int cursor_width ;
   int cursor_height ;
   int max_cursor_width ;
   int max_cursor_height ;
   enum amdgpu_rmx_type rmx_type ;
   u8 h_border ;
   u8 v_border ;
   fixed20_12 vsc ;
   fixed20_12 hsc ;
   struct drm_display_mode native_mode ;
   u32 pll_id ;
   struct workqueue_struct *pflip_queue ;
   struct amdgpu_flip_work *pflip_works ;
   enum amdgpu_flip_status pflip_status ;
   int deferred_flip_completion ;
   struct amdgpu_atom_ss ss ;
   bool ss_enabled ;
   u32 adjusted_clock ;
   int bpc ;
   u32 pll_reference_div ;
   u32 pll_post_div ;
   u32 pll_flags ;
   struct drm_encoder *encoder ;
   struct drm_connector *connector ;
   u32 line_time ;
   u32 wm_low ;
   u32 wm_high ;
   struct drm_display_mode hw_mode ;
};
struct amdgpu_encoder {
   struct drm_encoder base ;
   u32 encoder_enum ;
   u32 encoder_id ;
   u32 devices ;
   u32 active_device ;
   u32 flags ;
   u32 pixel_clock ;
   enum amdgpu_rmx_type rmx_type ;
   enum amdgpu_underscan_type underscan_type ;
   u32 underscan_hborder ;
   u32 underscan_vborder ;
   struct drm_display_mode native_mode ;
   void *enc_priv ;
   int audio_polling_active ;
   bool is_ext_encoder ;
   u16 caps ;
};
struct amdgpu_gpio_rec {
   bool valid ;
   u8 id ;
   u32 reg ;
   u32 mask ;
   u32 shift ;
};
struct amdgpu_hpd {
   enum amdgpu_hpd_id hpd ;
   u8 plugged_state ;
   struct amdgpu_gpio_rec gpio ;
};
struct amdgpu_router {
   u32 router_id ;
   struct amdgpu_i2c_bus_rec i2c_info ;
   u8 i2c_addr ;
   bool ddc_valid ;
   u8 ddc_mux_type ;
   u8 ddc_mux_control_pin ;
   u8 ddc_mux_state ;
   bool cd_valid ;
   u8 cd_mux_type ;
   u8 cd_mux_control_pin ;
   u8 cd_mux_state ;
};
struct amdgpu_ih_ring {
   struct amdgpu_bo *ring_obj ;
   u32 volatile *ring ;
   unsigned int rptr ;
   unsigned int ring_size ;
   uint64_t gpu_addr ;
   u32 ptr_mask ;
   atomic_t lock ;
   bool enabled ;
   unsigned int wptr_offs ;
   unsigned int rptr_offs ;
   u32 doorbell_index ;
   bool use_doorbell ;
   bool use_bus_addr ;
   dma_addr_t rb_dma_addr ;
};
struct amdgpu_iv_entry {
   unsigned int src_id ;
   unsigned int src_data ;
   unsigned int ring_id ;
   unsigned int vm_id ;
   unsigned int pas_id ;
};
enum amdgpu_interrupt_state {
    AMDGPU_IRQ_STATE_DISABLE = 0,
    AMDGPU_IRQ_STATE_ENABLE = 1
} ;
struct amdgpu_irq_src_funcs;
struct amdgpu_irq_src {
   unsigned int num_types ;
   atomic_t *enabled_types ;
   struct amdgpu_irq_src_funcs const *funcs ;
};
struct amdgpu_irq_src_funcs {
   int (*set)(struct amdgpu_device * , struct amdgpu_irq_src * , unsigned int , enum amdgpu_interrupt_state ) ;
   int (*process)(struct amdgpu_device * , struct amdgpu_irq_src * , struct amdgpu_iv_entry * ) ;
};
struct amdgpu_ih_funcs;
struct amdgpu_irq {
   bool installed ;
   spinlock_t lock ;
   struct amdgpu_irq_src *sources[256U] ;
   bool msi_enabled ;
   struct amdgpu_ih_ring ih ;
   struct amdgpu_ih_funcs const *ih_funcs ;
};
enum AMDGPU_UCODE_ID {
    AMDGPU_UCODE_ID_SDMA0 = 0,
    AMDGPU_UCODE_ID_SDMA1 = 1,
    AMDGPU_UCODE_ID_CP_CE = 2,
    AMDGPU_UCODE_ID_CP_PFP = 3,
    AMDGPU_UCODE_ID_CP_ME = 4,
    AMDGPU_UCODE_ID_CP_MEC1 = 5,
    AMDGPU_UCODE_ID_CP_MEC2 = 6,
    AMDGPU_UCODE_ID_RLC_G = 7,
    AMDGPU_UCODE_ID_MAXIMUM = 8
} ;
struct amdgpu_firmware_info {
   enum AMDGPU_UCODE_ID ucode_id ;
   struct firmware const *fw ;
   uint64_t mc_addr ;
   void *kaddr ;
};
struct amdgpu_gds_asic_info {
   u32 total_size ;
   u32 gfx_partition_size ;
   u32 cs_partition_size ;
};
struct amdgpu_gds {
   struct amdgpu_gds_asic_info mem ;
   struct amdgpu_gds_asic_info gws ;
   struct amdgpu_gds_asic_info oa ;
   struct amdgpu_bo *gds_gfx_bo ;
   struct amdgpu_bo *gws_gfx_bo ;
   struct amdgpu_bo *oa_gfx_bo ;
};
struct amdgpu_fence;
struct amdgpu_vm;
struct amdgpu_semaphore;
struct amdgpu_cs_parser;
struct amdgpu_ip_block_version {
   enum amd_ip_block_type type ;
   u32 major ;
   u32 minor ;
   u32 rev ;
   struct amd_ip_funcs const *funcs ;
};
struct amdgpu_buffer_funcs {
   u32 copy_max_bytes ;
   unsigned int copy_num_dw ;
   void (*emit_copy_buffer)(struct amdgpu_ring * , uint64_t , uint64_t , u32 ) ;
   u32 fill_max_bytes ;
   unsigned int fill_num_dw ;
   void (*emit_fill_buffer)(struct amdgpu_ring * , u32 , uint64_t , u32 ) ;
};
struct amdgpu_vm_pte_funcs {
   void (*copy_pte)(struct amdgpu_ib * , uint64_t , uint64_t , unsigned int ) ;
   void (*write_pte)(struct amdgpu_ib * , uint64_t , uint64_t , unsigned int ,
                     u32 , u32 ) ;
   void (*set_pte_pde)(struct amdgpu_ib * , uint64_t , uint64_t , unsigned int ,
                       u32 , u32 ) ;
   void (*pad_ib)(struct amdgpu_ib * ) ;
};
struct amdgpu_gart_funcs {
   void (*flush_gpu_tlb)(struct amdgpu_device * , u32 ) ;
   int (*set_pte_pde)(struct amdgpu_device * , void * , u32 , uint64_t , u32 ) ;
};
struct amdgpu_ih_funcs {
   u32 (*get_wptr)(struct amdgpu_device * ) ;
   void (*decode_iv)(struct amdgpu_device * , struct amdgpu_iv_entry * ) ;
   void (*set_rptr)(struct amdgpu_device * ) ;
};
struct amdgpu_ring_funcs {
   u32 (*get_rptr)(struct amdgpu_ring * ) ;
   u32 (*get_wptr)(struct amdgpu_ring * ) ;
   void (*set_wptr)(struct amdgpu_ring * ) ;
   int (*parse_cs)(struct amdgpu_cs_parser * , u32 ) ;
   void (*emit_ib)(struct amdgpu_ring * , struct amdgpu_ib * ) ;
   void (*emit_fence)(struct amdgpu_ring * , uint64_t , uint64_t , unsigned int ) ;
   bool (*emit_semaphore)(struct amdgpu_ring * , struct amdgpu_semaphore * , bool ) ;
   void (*emit_vm_flush)(struct amdgpu_ring * , unsigned int , uint64_t ) ;
   void (*emit_hdp_flush)(struct amdgpu_ring * ) ;
   void (*emit_gds_switch)(struct amdgpu_ring * , u32 , u32 , u32 , u32 , u32 ,
                           u32 , u32 ) ;
   int (*test_ring)(struct amdgpu_ring * ) ;
   int (*test_ib)(struct amdgpu_ring * ) ;
   bool (*is_lockup)(struct amdgpu_ring * ) ;
};
struct amdgpu_dummy_page {
   struct page *page ;
   dma_addr_t addr ;
};
struct amdgpu_clock {
   struct amdgpu_pll ppll[3U] ;
   struct amdgpu_pll spll ;
   struct amdgpu_pll mpll ;
   u32 default_mclk ;
   u32 default_sclk ;
   u32 default_dispclk ;
   u32 current_dispclk ;
   u32 dp_extclk ;
   u32 max_pixel_clock ;
};
struct amdgpu_fence_driver {
   struct amdgpu_ring *ring ;
   uint64_t gpu_addr ;
   u32 volatile *cpu_addr ;
   uint64_t sync_seq[16U] ;
   atomic64_t last_seq ;
   bool initialized ;
   bool delayed_irq ;
   struct amdgpu_irq_src *irq_src ;
   unsigned int irq_type ;
   struct delayed_work lockup_work ;
};
struct amdgpu_fence {
   struct fence base ;
   struct amdgpu_ring *ring ;
   uint64_t seq ;
   void *owner ;
   wait_queue_t fence_wake ;
};
struct amdgpu_user_fence {
   struct amdgpu_bo *bo ;
   u32 offset ;
};
struct amdgpu_mman {
   struct ttm_bo_global_ref bo_global_ref ;
   struct drm_global_reference mem_global_ref ;
   struct ttm_bo_device bdev ;
   bool mem_global_referenced ;
   bool initialized ;
   struct dentry *vram ;
   struct dentry *gtt ;
   struct amdgpu_buffer_funcs const *buffer_funcs ;
   struct amdgpu_ring *buffer_funcs_ring ;
};
struct amdgpu_bo_va;
struct amdgpu_bo_list_entry {
   struct amdgpu_bo *robj ;
   struct ttm_validate_buffer tv ;
   struct amdgpu_bo_va *bo_va ;
   unsigned int prefered_domains ;
   unsigned int allowed_domains ;
   u32 priority ;
};
struct amdgpu_bo_va {
   struct list_head bo_list ;
   uint64_t addr ;
   struct amdgpu_fence *last_pt_update ;
   unsigned int ref_count ;
   struct list_head mappings ;
   struct list_head vm_status ;
   struct amdgpu_vm *vm ;
   struct amdgpu_bo *bo ;
};
struct amdgpu_mn;
struct amdgpu_bo {
   struct list_head list ;
   u32 initial_domain ;
   struct ttm_place placements[4U] ;
   struct ttm_placement placement ;
   struct ttm_buffer_object tbo ;
   struct ttm_bo_kmap_obj kmap ;
   u64 flags ;
   unsigned int pin_count ;
   void *kptr ;
   u64 tiling_flags ;
   u64 metadata_flags ;
   void *metadata ;
   u32 metadata_size ;
   struct list_head va ;
   struct amdgpu_device *adev ;
   struct drm_gem_object gem_base ;
   struct ttm_bo_kmap_obj dma_buf_vmap ;
   pid_t pid ;
   struct amdgpu_mn *mn ;
   struct list_head mn_list ;
};
struct amdgpu_sa_manager {
   wait_queue_head_t wq ;
   struct amdgpu_bo *bo ;
   struct list_head *hole ;
   struct list_head flist[16U] ;
   struct list_head olist ;
   unsigned int size ;
   uint64_t gpu_addr ;
   void *cpu_ptr ;
   u32 domain ;
   u32 align ;
};
struct amdgpu_sa_bo;
struct amdgpu_sa_bo {
   struct list_head olist ;
   struct list_head flist ;
   struct amdgpu_sa_manager *manager ;
   unsigned int soffset ;
   unsigned int eoffset ;
   struct amdgpu_fence *fence ;
};
struct amdgpu_gem {
   struct mutex mutex ;
   struct list_head objects ;
};
struct amdgpu_semaphore {
   struct amdgpu_sa_bo *sa_bo ;
   int waiters ;
   uint64_t gpu_addr ;
};
struct amdgpu_sync {
   struct amdgpu_semaphore *semaphores[4U] ;
   struct amdgpu_fence *sync_to[16U] ;
   struct amdgpu_fence *last_vm_update ;
};
struct amdgpu_mc;
struct amdgpu_gart {
   dma_addr_t table_addr ;
   struct amdgpu_bo *robj ;
   void *ptr ;
   unsigned int num_gpu_pages ;
   unsigned int num_cpu_pages ;
   unsigned int table_size ;
   struct page **pages ;
   dma_addr_t *pages_addr ;
   bool ready ;
   struct amdgpu_gart_funcs const *gart_funcs ;
};
struct amdgpu_mc {
   resource_size_t aper_size ;
   resource_size_t aper_base ;
   resource_size_t agp_base ;
   u64 mc_vram_size ;
   u64 visible_vram_size ;
   u64 gtt_size ;
   u64 gtt_start ;
   u64 gtt_end ;
   u64 vram_start ;
   u64 vram_end ;
   unsigned int vram_width ;
   u64 real_vram_size ;
   int vram_mtrr ;
   u64 gtt_base_align ;
   u64 mc_mask ;
   struct firmware const *fw ;
   u32 fw_version ;
   struct amdgpu_irq_src vm_fault ;
   u32 vram_type ;
};
struct amdgpu_doorbell {
   resource_size_t base ;
   resource_size_t size ;
   u32 *ptr ;
   u32 num_doorbells ;
};
struct amdgpu_flip_work {
   struct work_struct flip_work ;
   struct work_struct unpin_work ;
   struct amdgpu_device *adev ;
   int crtc_id ;
   uint64_t base ;
   struct drm_pending_vblank_event *event ;
   struct amdgpu_bo *old_rbo ;
   struct fence *fence ;
};
struct amdgpu_ctx;
struct amdgpu_ib {
   struct amdgpu_sa_bo *sa_bo ;
   u32 length_dw ;
   uint64_t gpu_addr ;
   u32 *ptr ;
   struct amdgpu_ring *ring ;
   struct amdgpu_fence *fence ;
   struct amdgpu_user_fence *user ;
   struct amdgpu_vm *vm ;
   struct amdgpu_ctx *ctx ;
   struct amdgpu_sync sync ;
   u32 gds_base ;
   u32 gds_size ;
   u32 gws_base ;
   u32 gws_size ;
   u32 oa_base ;
   u32 oa_size ;
   u32 flags ;
};
enum amdgpu_ring_type {
    AMDGPU_RING_TYPE_GFX = 0,
    AMDGPU_RING_TYPE_COMPUTE = 1,
    AMDGPU_RING_TYPE_SDMA = 2,
    AMDGPU_RING_TYPE_UVD = 3,
    AMDGPU_RING_TYPE_VCE = 4
} ;
struct amdgpu_ring {
   struct amdgpu_device *adev ;
   struct amdgpu_ring_funcs const *funcs ;
   struct amdgpu_fence_driver fence_drv ;
   struct mutex *ring_lock ;
   struct amdgpu_bo *ring_obj ;
   u32 volatile *ring ;
   unsigned int rptr_offs ;
   u64 next_rptr_gpu_addr ;
   u32 volatile *next_rptr_cpu_addr ;
   unsigned int wptr ;
   unsigned int wptr_old ;
   unsigned int ring_size ;
   unsigned int ring_free_dw ;
   int count_dw ;
   atomic_t last_rptr ;
   atomic64_t last_activity ;
   uint64_t gpu_addr ;
   u32 align_mask ;
   u32 ptr_mask ;
   bool ready ;
   u32 nop ;
   u32 idx ;
   u64 last_semaphore_signal_addr ;
   u64 last_semaphore_wait_addr ;
   u32 me ;
   u32 pipe ;
   u32 queue ;
   struct amdgpu_bo *mqd_obj ;
   u32 doorbell_index ;
   bool use_doorbell ;
   unsigned int wptr_offs ;
   unsigned int next_rptr_offs ;
   unsigned int fence_offs ;
   struct amdgpu_ctx *current_ctx ;
   enum amdgpu_ring_type type ;
   char name[16U] ;
};
struct amdgpu_vm_pt {
   struct amdgpu_bo *bo ;
   uint64_t addr ;
};
struct amdgpu_vm_id {
   unsigned int id ;
   uint64_t pd_gpu_addr ;
   struct amdgpu_fence *flushed_updates ;
   struct amdgpu_fence *last_id_use ;
};
struct amdgpu_vm {
   struct mutex mutex ;
   struct rb_root va ;
   spinlock_t status_lock ;
   struct list_head invalidated ;
   struct list_head freed ;
   struct amdgpu_bo *page_directory ;
   unsigned int max_pde_used ;
   struct amdgpu_vm_pt *page_tables ;
   struct amdgpu_vm_id ids[16U] ;
};
struct amdgpu_vm_manager {
   struct amdgpu_fence *active[16U] ;
   u32 max_pfn ;
   unsigned int nvm ;
   u64 vram_base_offset ;
   bool enabled ;
   u32 saved_table_addr[16U] ;
   struct amdgpu_vm_pte_funcs const *vm_pte_funcs ;
   struct amdgpu_ring *vm_pte_funcs_ring ;
};
struct amdgpu_ctx_state {
   uint64_t flags ;
   u32 hangs ;
};
struct amdgpu_fpriv;
struct amdgpu_ctx {
   struct kref refcount ;
   struct amdgpu_fpriv *fpriv ;
   struct amdgpu_ctx_state state ;
   u32 id ;
   unsigned int reset_counter ;
};
struct amdgpu_ctx_mgr {
   struct amdgpu_device *adev ;
   struct idr ctx_handles ;
   struct mutex lock ;
};
struct amdgpu_fpriv {
   struct amdgpu_vm vm ;
   struct mutex bo_list_lock ;
   struct idr bo_list_handles ;
   struct amdgpu_ctx_mgr ctx_mgr ;
};
struct amdgpu_bo_list {
   struct mutex lock ;
   struct amdgpu_bo *gds_obj ;
   struct amdgpu_bo *gws_obj ;
   struct amdgpu_bo *oa_obj ;
   bool has_userptr ;
   unsigned int num_entries ;
   struct amdgpu_bo_list_entry *array ;
};
enum section_id {
    SECT_NONE = 0,
    SECT_CONTEXT = 1,
    SECT_CLEAR = 2,
    SECT_CTRLCONST = 3
} ;
struct cs_extent_def {
   unsigned int const *extent ;
   unsigned int const reg_index ;
   unsigned int const reg_count ;
};
struct cs_section_def {
   struct cs_extent_def const *section ;
   enum section_id const id ;
};
struct amdgpu_rlc {
   struct amdgpu_bo *save_restore_obj ;
   uint64_t save_restore_gpu_addr ;
   u32 volatile *sr_ptr ;
   u32 const *reg_list ;
   u32 reg_list_size ;
   struct amdgpu_bo *clear_state_obj ;
   uint64_t clear_state_gpu_addr ;
   u32 volatile *cs_ptr ;
   struct cs_section_def const *cs_data ;
   u32 clear_state_size ;
   struct amdgpu_bo *cp_table_obj ;
   uint64_t cp_table_gpu_addr ;
   u32 volatile *cp_table_ptr ;
   u32 cp_table_size ;
};
struct amdgpu_mec {
   struct amdgpu_bo *hpd_eop_obj ;
   u64 hpd_eop_gpu_addr ;
   u32 num_pipe ;
   u32 num_mec ;
   u32 num_queue ;
};
struct amdgpu_scratch {
   unsigned int num_reg ;
   u32 reg_base ;
   bool free[32U] ;
   u32 reg[32U] ;
};
struct amdgpu_gca_config {
   unsigned int max_shader_engines ;
   unsigned int max_tile_pipes ;
   unsigned int max_cu_per_sh ;
   unsigned int max_sh_per_se ;
   unsigned int max_backends_per_se ;
   unsigned int max_texture_channel_caches ;
   unsigned int max_gprs ;
   unsigned int max_gs_threads ;
   unsigned int max_hw_contexts ;
   unsigned int sc_prim_fifo_size_frontend ;
   unsigned int sc_prim_fifo_size_backend ;
   unsigned int sc_hiz_tile_fifo_size ;
   unsigned int sc_earlyz_tile_fifo_size ;
   unsigned int num_tile_pipes ;
   unsigned int backend_enable_mask ;
   unsigned int mem_max_burst_length_bytes ;
   unsigned int mem_row_size_in_kb ;
   unsigned int shader_engine_tile_size ;
   unsigned int num_gpus ;
   unsigned int multi_gpu_tile_size ;
   unsigned int mc_arb_ramcfg ;
   unsigned int gb_addr_config ;
   u32 tile_mode_array[32U] ;
   u32 macrotile_mode_array[16U] ;
};
struct amdgpu_gfx {
   struct mutex gpu_clock_mutex ;
   struct amdgpu_gca_config config ;
   struct amdgpu_rlc rlc ;
   struct amdgpu_mec mec ;
   struct amdgpu_scratch scratch ;
   struct firmware const *me_fw ;
   u32 me_fw_version ;
   struct firmware const *pfp_fw ;
   u32 pfp_fw_version ;
   struct firmware const *ce_fw ;
   u32 ce_fw_version ;
   struct firmware const *rlc_fw ;
   u32 rlc_fw_version ;
   struct firmware const *mec_fw ;
   u32 mec_fw_version ;
   struct firmware const *mec2_fw ;
   u32 mec2_fw_version ;
   u32 me_feature_version ;
   u32 ce_feature_version ;
   u32 pfp_feature_version ;
   struct amdgpu_ring gfx_ring[1U] ;
   unsigned int num_gfx_rings ;
   struct amdgpu_ring compute_ring[8U] ;
   unsigned int num_compute_rings ;
   struct amdgpu_irq_src eop_irq ;
   struct amdgpu_irq_src priv_reg_irq ;
   struct amdgpu_irq_src priv_inst_irq ;
   u32 gfx_current_status ;
   unsigned int ce_sync_offs ;
   unsigned int ce_ram_size ;
};
struct amdgpu_cs_chunk {
   u32 chunk_id ;
   u32 length_dw ;
   u32 *kdata ;
   void *user_ptr ;
};
struct amdgpu_cs_parser {
   struct amdgpu_device *adev ;
   struct drm_file *filp ;
   struct amdgpu_ctx *ctx ;
   struct amdgpu_bo_list *bo_list ;
   unsigned int nchunks ;
   struct amdgpu_cs_chunk *chunks ;
   struct amdgpu_bo_list_entry *vm_bos ;
   struct list_head validated ;
   struct amdgpu_ib *ibs ;
   u32 num_ibs ;
   struct ww_acquire_ctx ticket ;
   struct amdgpu_user_fence uf ;
};
struct amdgpu_wb {
   struct amdgpu_bo *wb_obj ;
   u32 volatile *wb ;
   uint64_t gpu_addr ;
   u32 num_wb ;
   unsigned long used[16U] ;
};
enum amdgpu_pm_state_type {
    POWER_STATE_TYPE_DEFAULT = 0,
    POWER_STATE_TYPE_POWERSAVE = 1,
    POWER_STATE_TYPE_BATTERY = 2,
    POWER_STATE_TYPE_BALANCED = 3,
    POWER_STATE_TYPE_PERFORMANCE = 4,
    POWER_STATE_TYPE_INTERNAL_UVD = 5,
    POWER_STATE_TYPE_INTERNAL_UVD_SD = 6,
    POWER_STATE_TYPE_INTERNAL_UVD_HD = 7,
    POWER_STATE_TYPE_INTERNAL_UVD_HD2 = 8,
    POWER_STATE_TYPE_INTERNAL_UVD_MVC = 9,
    POWER_STATE_TYPE_INTERNAL_BOOT = 10,
    POWER_STATE_TYPE_INTERNAL_THERMAL = 11,
    POWER_STATE_TYPE_INTERNAL_ACPI = 12,
    POWER_STATE_TYPE_INTERNAL_ULV = 13,
    POWER_STATE_TYPE_INTERNAL_3DPERF = 14
} ;
enum amdgpu_int_thermal_type {
    THERMAL_TYPE_NONE = 0,
    THERMAL_TYPE_EXTERNAL = 1,
    THERMAL_TYPE_EXTERNAL_GPIO = 2,
    THERMAL_TYPE_RV6XX = 3,
    THERMAL_TYPE_RV770 = 4,
    THERMAL_TYPE_ADT7473_WITH_INTERNAL = 5,
    THERMAL_TYPE_EVERGREEN = 6,
    THERMAL_TYPE_SUMO = 7,
    THERMAL_TYPE_NI = 8,
    THERMAL_TYPE_SI = 9,
    THERMAL_TYPE_EMC2103_WITH_INTERNAL = 10,
    THERMAL_TYPE_CI = 11,
    THERMAL_TYPE_KV = 12
} ;
enum amdgpu_vce_level {
    AMDGPU_VCE_LEVEL_AC_ALL = 0,
    AMDGPU_VCE_LEVEL_DC_EE = 1,
    AMDGPU_VCE_LEVEL_DC_LL_LOW = 2,
    AMDGPU_VCE_LEVEL_DC_LL_HIGH = 3,
    AMDGPU_VCE_LEVEL_DC_GP_LOW = 4,
    AMDGPU_VCE_LEVEL_DC_GP_HIGH = 5
} ;
struct amdgpu_ps {
   u32 caps ;
   u32 class ;
   u32 class2 ;
   u32 vclk ;
   u32 dclk ;
   u32 evclk ;
   u32 ecclk ;
   bool vce_active ;
   enum amdgpu_vce_level vce_level ;
   void *ps_priv ;
};
struct amdgpu_dpm_thermal {
   struct work_struct work ;
   int min_temp ;
   int max_temp ;
   bool high_to_low ;
   struct amdgpu_irq_src irq ;
};
struct amdgpu_clock_and_voltage_limits {
   u32 sclk ;
   u32 mclk ;
   u16 vddc ;
   u16 vddci ;
};
struct amdgpu_clock_array {
   u32 count ;
   u32 *values ;
};
struct amdgpu_clock_voltage_dependency_entry {
   u32 clk ;
   u16 v ;
};
struct amdgpu_clock_voltage_dependency_table {
   u32 count ;
   struct amdgpu_clock_voltage_dependency_entry *entries ;
};
struct __anonstruct____missing_field_name_272 {
   u16 vddc ;
   u32 leakage ;
};
struct __anonstruct____missing_field_name_273 {
   u16 vddc1 ;
   u16 vddc2 ;
   u16 vddc3 ;
};
union amdgpu_cac_leakage_entry {
   struct __anonstruct____missing_field_name_272 __annonCompField79 ;
   struct __anonstruct____missing_field_name_273 __annonCompField80 ;
};
struct amdgpu_cac_leakage_table {
   u32 count ;
   union amdgpu_cac_leakage_entry *entries ;
};
struct amdgpu_phase_shedding_limits_entry {
   u16 voltage ;
   u32 sclk ;
   u32 mclk ;
};
struct amdgpu_phase_shedding_limits_table {
   u32 count ;
   struct amdgpu_phase_shedding_limits_entry *entries ;
};
struct amdgpu_uvd_clock_voltage_dependency_entry {
   u32 vclk ;
   u32 dclk ;
   u16 v ;
};
struct amdgpu_uvd_clock_voltage_dependency_table {
   u8 count ;
   struct amdgpu_uvd_clock_voltage_dependency_entry *entries ;
};
struct amdgpu_vce_clock_voltage_dependency_entry {
   u32 ecclk ;
   u32 evclk ;
   u16 v ;
};
struct amdgpu_vce_clock_voltage_dependency_table {
   u8 count ;
   struct amdgpu_vce_clock_voltage_dependency_entry *entries ;
};
struct amdgpu_ppm_table {
   u8 ppm_design ;
   u16 cpu_core_number ;
   u32 platform_tdp ;
   u32 small_ac_platform_tdp ;
   u32 platform_tdc ;
   u32 small_ac_platform_tdc ;
   u32 apu_tdp ;
   u32 dgpu_tdp ;
   u32 dgpu_ulv_power ;
   u32 tj_max ;
};
struct amdgpu_cac_tdp_table {
   u16 tdp ;
   u16 configurable_tdp ;
   u16 tdc ;
   u16 battery_power_limit ;
   u16 small_power_limit ;
   u16 low_cac_leakage ;
   u16 high_cac_leakage ;
   u16 maximum_power_delivery_limit ;
};
struct amdgpu_dpm_dynamic_state {
   struct amdgpu_clock_voltage_dependency_table vddc_dependency_on_sclk ;
   struct amdgpu_clock_voltage_dependency_table vddci_dependency_on_mclk ;
   struct amdgpu_clock_voltage_dependency_table vddc_dependency_on_mclk ;
   struct amdgpu_clock_voltage_dependency_table mvdd_dependency_on_mclk ;
   struct amdgpu_clock_voltage_dependency_table vddc_dependency_on_dispclk ;
   struct amdgpu_uvd_clock_voltage_dependency_table uvd_clock_voltage_dependency_table ;
   struct amdgpu_vce_clock_voltage_dependency_table vce_clock_voltage_dependency_table ;
   struct amdgpu_clock_voltage_dependency_table samu_clock_voltage_dependency_table ;
   struct amdgpu_clock_voltage_dependency_table acp_clock_voltage_dependency_table ;
   struct amdgpu_clock_voltage_dependency_table vddgfx_dependency_on_sclk ;
   struct amdgpu_clock_array valid_sclk_values ;
   struct amdgpu_clock_array valid_mclk_values ;
   struct amdgpu_clock_and_voltage_limits max_clock_voltage_on_dc ;
   struct amdgpu_clock_and_voltage_limits max_clock_voltage_on_ac ;
   u32 mclk_sclk_ratio ;
   u32 sclk_mclk_delta ;
   u16 vddc_vddci_delta ;
   u16 min_vddc_for_pcie_gen2 ;
   struct amdgpu_cac_leakage_table cac_leakage_table ;
   struct amdgpu_phase_shedding_limits_table phase_shedding_limits_table ;
   struct amdgpu_ppm_table *ppm_table ;
   struct amdgpu_cac_tdp_table *cac_tdp_table ;
};
struct amdgpu_dpm_fan {
   u16 t_min ;
   u16 t_med ;
   u16 t_high ;
   u16 pwm_min ;
   u16 pwm_med ;
   u16 pwm_high ;
   u8 t_hyst ;
   u32 cycle_delay ;
   u16 t_max ;
   u8 control_mode ;
   u16 default_max_fan_pwm ;
   u16 default_fan_output_sensitivity ;
   u16 fan_output_sensitivity ;
   bool ucode_fan_control ;
};
enum amdgpu_dpm_forced_level {
    AMDGPU_DPM_FORCED_LEVEL_AUTO = 0,
    AMDGPU_DPM_FORCED_LEVEL_LOW = 1,
    AMDGPU_DPM_FORCED_LEVEL_HIGH = 2
} ;
struct amdgpu_vce_state {
   u32 evclk ;
   u32 ecclk ;
   u32 sclk ;
   u32 mclk ;
   u8 clk_idx ;
   u8 pstate ;
};
struct amdgpu_dpm_funcs {
   int (*get_temperature)(struct amdgpu_device * ) ;
   int (*pre_set_power_state)(struct amdgpu_device * ) ;
   int (*set_power_state)(struct amdgpu_device * ) ;
   void (*post_set_power_state)(struct amdgpu_device * ) ;
   void (*display_configuration_changed)(struct amdgpu_device * ) ;
   u32 (*get_sclk)(struct amdgpu_device * , bool ) ;
   u32 (*get_mclk)(struct amdgpu_device * , bool ) ;
   void (*print_power_state)(struct amdgpu_device * , struct amdgpu_ps * ) ;
   void (*debugfs_print_current_performance_level)(struct amdgpu_device * , struct seq_file * ) ;
   int (*force_performance_level)(struct amdgpu_device * , enum amdgpu_dpm_forced_level ) ;
   bool (*vblank_too_short)(struct amdgpu_device * ) ;
   void (*powergate_uvd)(struct amdgpu_device * , bool ) ;
   void (*powergate_vce)(struct amdgpu_device * , bool ) ;
   void (*enable_bapm)(struct amdgpu_device * , bool ) ;
   void (*set_fan_control_mode)(struct amdgpu_device * , u32 ) ;
   u32 (*get_fan_control_mode)(struct amdgpu_device * ) ;
   int (*set_fan_speed_percent)(struct amdgpu_device * , u32 ) ;
   int (*get_fan_speed_percent)(struct amdgpu_device * , u32 * ) ;
};
struct amdgpu_dpm {
   struct amdgpu_ps *ps ;
   int num_ps ;
   struct amdgpu_ps *current_ps ;
   struct amdgpu_ps *requested_ps ;
   struct amdgpu_ps *boot_ps ;
   struct amdgpu_ps *uvd_ps ;
   struct amdgpu_vce_state vce_states[6U] ;
   enum amdgpu_vce_level vce_level ;
   enum amdgpu_pm_state_type state ;
   enum amdgpu_pm_state_type user_state ;
   u32 platform_caps ;
   u32 voltage_response_time ;
   u32 backbias_response_time ;
   void *priv ;
   u32 new_active_crtcs ;
   int new_active_crtc_count ;
   u32 current_active_crtcs ;
   int current_active_crtc_count ;
   struct amdgpu_dpm_dynamic_state dyn_state ;
   struct amdgpu_dpm_fan fan ;
   u32 tdp_limit ;
   u32 near_tdp_limit ;
   u32 near_tdp_limit_adjusted ;
   u32 sq_ramping_threshold ;
   u32 cac_leakage ;
   u16 tdp_od_limit ;
   u32 tdp_adjustment ;
   u16 load_line_slope ;
   bool power_control ;
   bool ac_power ;
   bool thermal_active ;
   bool uvd_active ;
   bool vce_active ;
   struct amdgpu_dpm_thermal thermal ;
   enum amdgpu_dpm_forced_level forced_level ;
};
struct amdgpu_pm {
   struct mutex mutex ;
   u32 current_sclk ;
   u32 current_mclk ;
   u32 default_sclk ;
   u32 default_mclk ;
   struct amdgpu_i2c_chan *i2c_bus ;
   enum amdgpu_int_thermal_type int_thermal_type ;
   struct device *int_hwmon_dev ;
   bool no_fan ;
   u8 fan_pulses_per_revolution ;
   u8 fan_min_rpm ;
   u8 fan_max_rpm ;
   bool dpm_enabled ;
   struct amdgpu_dpm dpm ;
   struct firmware const *fw ;
   u32 fw_version ;
   struct amdgpu_dpm_funcs const *funcs ;
};
struct amdgpu_uvd {
   struct amdgpu_bo *vcpu_bo ;
   void *cpu_addr ;
   uint64_t gpu_addr ;
   void *saved_bo ;
   atomic_t handles[10U] ;
   struct drm_file *filp[10U] ;
   struct delayed_work idle_work ;
   struct firmware const *fw ;
   struct amdgpu_ring ring ;
   struct amdgpu_irq_src irq ;
   bool address_64_bit ;
};
struct amdgpu_vce {
   struct amdgpu_bo *vcpu_bo ;
   uint64_t gpu_addr ;
   unsigned int fw_version ;
   unsigned int fb_version ;
   atomic_t handles[16U] ;
   struct drm_file *filp[16U] ;
   u32 img_size[16U] ;
   struct delayed_work idle_work ;
   struct firmware const *fw ;
   struct amdgpu_ring ring[2U] ;
   struct amdgpu_irq_src irq ;
};
struct amdgpu_sdma {
   struct firmware const *fw ;
   u32 fw_version ;
   struct amdgpu_ring ring ;
};
struct amdgpu_firmware {
   struct amdgpu_firmware_info ucode[8U] ;
   bool smu_load ;
   struct amdgpu_bo *fw_buf ;
   unsigned int fw_size ;
};
struct amdgpu_debugfs {
   struct drm_info_list *files ;
   unsigned int num_files ;
};
struct amdgpu_smumgr_funcs {
   int (*check_fw_load_finish)(struct amdgpu_device * , u32 ) ;
   int (*request_smu_load_fw)(struct amdgpu_device * ) ;
   int (*request_smu_specific_fw)(struct amdgpu_device * , u32 ) ;
};
struct amdgpu_smumgr {
   struct amdgpu_bo *toc_buf ;
   struct amdgpu_bo *smu_buf ;
   void *priv ;
   spinlock_t smu_lock ;
   struct amdgpu_smumgr_funcs const *smumgr_funcs ;
   u32 fw_flags ;
};
struct amdgpu_cu_info {
   u32 number ;
   u32 ao_cu_mask ;
   u32 bitmap[4U][4U] ;
};
struct amdgpu_asic_funcs {
   bool (*read_disabled_bios)(struct amdgpu_device * ) ;
   int (*read_register)(struct amdgpu_device * , u32 , u32 , u32 , u32 * ) ;
   void (*set_vga_state)(struct amdgpu_device * , bool ) ;
   int (*reset)(struct amdgpu_device * ) ;
   int (*wait_for_mc_idle)(struct amdgpu_device * ) ;
   u32 (*get_xclk)(struct amdgpu_device * ) ;
   uint64_t (*get_gpu_clock_counter)(struct amdgpu_device * ) ;
   int (*get_cu_info)(struct amdgpu_device * , struct amdgpu_cu_info * ) ;
   int (*set_uvd_clocks)(struct amdgpu_device * , u32 , u32 ) ;
   int (*set_vce_clocks)(struct amdgpu_device * , u32 , u32 ) ;
};
struct amdgpu_vram_scratch {
   struct amdgpu_bo *robj ;
   u32 volatile *ptr ;
   u64 gpu_addr ;
};
struct amdgpu_atif_notification_cfg {
   bool enabled ;
   int command_code ;
};
struct amdgpu_atif_notifications {
   bool display_switch ;
   bool expansion_mode_change ;
   bool thermal_state ;
   bool forced_power_state ;
   bool system_power_state ;
   bool display_conf_change ;
   bool px_gfx_switch ;
   bool brightness_change ;
   bool dgpu_display_event ;
};
struct amdgpu_atif_functions {
   bool system_params ;
   bool sbios_requests ;
   bool select_active_disp ;
   bool lid_state ;
   bool get_tv_standard ;
   bool set_tv_standard ;
   bool get_panel_expansion_mode ;
   bool set_panel_expansion_mode ;
   bool temperature_change ;
   bool graphics_device_types ;
};
struct amdgpu_atif {
   struct amdgpu_atif_notifications notifications ;
   struct amdgpu_atif_functions functions ;
   struct amdgpu_atif_notification_cfg notification_cfg ;
   struct amdgpu_encoder *encoder_for_bl ;
};
struct amdgpu_atcs_functions {
   bool get_ext_state ;
   bool pcie_perf_req ;
   bool pcie_dev_rdy ;
   bool pcie_bus_width ;
};
struct amdgpu_atcs {
   struct amdgpu_atcs_functions functions ;
};
struct amdgpu_device {
   struct device *dev ;
   struct drm_device *ddev ;
   struct pci_dev *pdev ;
   struct rw_semaphore exclusive_lock ;
   enum amdgpu_asic_type asic_type ;
   u32 family ;
   u32 rev_id ;
   u32 external_rev_id ;
   unsigned long flags ;
   int usec_timeout ;
   struct amdgpu_asic_funcs const *asic_funcs ;
   bool shutdown ;
   bool suspend ;
   bool need_dma32 ;
   bool accel_working ;
   bool needs_reset ;
   struct work_struct reset_work ;
   struct notifier_block acpi_nb ;
   struct amdgpu_i2c_chan *i2c_bus[16U] ;
   struct amdgpu_debugfs debugfs[32U] ;
   unsigned int debugfs_count ;
   struct dentry *debugfs_regs ;
   struct amdgpu_atif atif ;
   struct amdgpu_atcs atcs ;
   struct mutex srbm_mutex ;
   struct mutex grbm_idx_mutex ;
   struct dev_pm_domain vga_pm_domain ;
   bool have_disp_power_ref ;
   uint8_t *bios ;
   bool is_atom_bios ;
   u16 bios_header_start ;
   struct amdgpu_bo *stollen_vga_memory ;
   u32 bios_scratch[8U] ;
   resource_size_t rmmio_base ;
   resource_size_t rmmio_size ;
   void *rmmio ;
   spinlock_t mmio_idx_lock ;
   spinlock_t smc_idx_lock ;
   u32 (*smc_rreg)(struct amdgpu_device * , u32 ) ;
   void (*smc_wreg)(struct amdgpu_device * , u32 , u32 ) ;
   spinlock_t pcie_idx_lock ;
   u32 (*pcie_rreg)(struct amdgpu_device * , u32 ) ;
   void (*pcie_wreg)(struct amdgpu_device * , u32 , u32 ) ;
   spinlock_t uvd_ctx_idx_lock ;
   u32 (*uvd_ctx_rreg)(struct amdgpu_device * , u32 ) ;
   void (*uvd_ctx_wreg)(struct amdgpu_device * , u32 , u32 ) ;
   spinlock_t didt_idx_lock ;
   u32 (*didt_rreg)(struct amdgpu_device * , u32 ) ;
   void (*didt_wreg)(struct amdgpu_device * , u32 , u32 ) ;
   spinlock_t audio_endpt_idx_lock ;
   u32 (*audio_endpt_rreg)(struct amdgpu_device * , u32 , u32 ) ;
   void (*audio_endpt_wreg)(struct amdgpu_device * , u32 , u32 , u32 ) ;
   void *rio_mem ;
   resource_size_t rio_mem_size ;
   struct amdgpu_doorbell doorbell ;
   struct amdgpu_clock clock ;
   struct amdgpu_mc mc ;
   struct amdgpu_gart gart ;
   struct amdgpu_dummy_page dummy_page ;
   struct amdgpu_vm_manager vm_manager ;
   struct amdgpu_mman mman ;
   struct amdgpu_gem gem ;
   struct amdgpu_vram_scratch vram_scratch ;
   struct amdgpu_wb wb ;
   atomic64_t vram_usage ;
   atomic64_t vram_vis_usage ;
   atomic64_t gtt_usage ;
   atomic64_t num_bytes_moved ;
   atomic_t gpu_reset_counter ;
   struct amdgpu_mode_info mode_info ;
   struct work_struct hotplug_work ;
   struct amdgpu_irq_src crtc_irq ;
   struct amdgpu_irq_src pageflip_irq ;
   struct amdgpu_irq_src hpd_irq ;
   wait_queue_head_t fence_queue ;
   unsigned int fence_context ;
   struct mutex ring_lock ;
   unsigned int num_rings ;
   struct amdgpu_ring *rings[16U] ;
   bool ib_pool_ready ;
   struct amdgpu_sa_manager ring_tmp_bo ;
   struct amdgpu_irq irq ;
   struct amdgpu_pm pm ;
   u32 cg_flags ;
   u32 pg_flags ;
   struct amdgpu_smumgr smu ;
   struct amdgpu_gfx gfx ;
   struct amdgpu_sdma sdma[2U] ;
   struct amdgpu_irq_src sdma_trap_irq ;
   struct amdgpu_irq_src sdma_illegal_inst_irq ;
   bool has_uvd ;
   struct amdgpu_uvd uvd ;
   struct amdgpu_vce vce ;
   struct amdgpu_firmware firmware ;
   struct amdgpu_gds gds ;
   struct amdgpu_ip_block_version const *ip_blocks ;
   int num_ip_blocks ;
   bool *ip_block_enabled ;
   struct mutex mn_lock ;
   struct hlist_head mn_hash[128U] ;
   u64 vram_pin_size ;
   u64 gart_pin_size ;
};
typedef bool ldv_func_ret_type;
typedef bool ldv_func_ret_type___0;
typedef bool ldv_func_ret_type___1;
typedef bool ldv_func_ret_type___2;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct vga_switcheroo_client_ops {
   void (*set_gpu_state)(struct pci_dev * , enum vga_switcheroo_state ) ;
   void (*reprobe)(struct pci_dev * ) ;
   bool (*can_switch)(struct pci_dev * ) ;
};
struct amdgpu_framebuffer {
   struct drm_framebuffer base ;
   struct drm_gem_object *obj ;
};
struct card_info {
   struct drm_device *dev ;
   void (*reg_write)(struct card_info * , u32 , u32 ) ;
   u32 (*reg_read)(struct card_info * , u32 ) ;
   void (*ioreg_write)(struct card_info * , u32 , u32 ) ;
   u32 (*ioreg_read)(struct card_info * , u32 ) ;
   void (*mc_write)(struct card_info * , u32 , u32 ) ;
   u32 (*mc_read)(struct card_info * , u32 ) ;
   void (*pll_write)(struct card_info * , u32 , u32 ) ;
   u32 (*pll_read)(struct card_info * , u32 ) ;
};
struct atom_context {
   struct card_info *card ;
   struct mutex mutex ;
   void *bios ;
   u32 cmd_table ;
   u32 data_table ;
   u16 *iio ;
   u16 data_block ;
   u32 fb_base ;
   u32 divmul[2U] ;
   u16 io_attr ;
   u16 reg_block ;
   uint8_t shift ;
   int cs_equal ;
   int cs_above ;
   int io_mode ;
   u32 *scratch ;
   int scratch_size_bytes ;
};
struct _ddebug {
   char const *modname ;
   char const *function ;
   char const *filename ;
   char const *format ;
   unsigned int lineno : 18 ;
   unsigned char flags ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct __anonstruct_mode_crtc_263 {
   u32 id ;
   u32 _pad ;
};
struct __anonstruct_query_hw_ip_264 {
   u32 type ;
   u32 ip_instance ;
};
struct __anonstruct_read_mmr_reg_265 {
   u32 dword_offset ;
   u32 count ;
   u32 instance ;
   u32 flags ;
};
struct __anonstruct_query_fw_266 {
   u32 fw_type ;
   u32 ip_instance ;
   u32 index ;
   u32 _pad ;
};
union __anonunion____missing_field_name_262 {
   struct __anonstruct_mode_crtc_263 mode_crtc ;
   struct __anonstruct_query_hw_ip_264 query_hw_ip ;
   struct __anonstruct_read_mmr_reg_265 read_mmr_reg ;
   struct __anonstruct_query_fw_266 query_fw ;
};
struct drm_amdgpu_info {
   uint64_t return_pointer ;
   u32 return_size ;
   u32 query ;
   union __anonunion____missing_field_name_262 __annonCompField78 ;
};
struct drm_amdgpu_info_gds {
   u32 gds_gfx_partition_size ;
   u32 compute_partition_size ;
   u32 gds_total_size ;
   u32 gws_per_gfx_partition ;
   u32 gws_per_compute_partition ;
   u32 oa_per_gfx_partition ;
   u32 oa_per_compute_partition ;
   u32 _pad ;
};
struct drm_amdgpu_info_vram_gtt {
   uint64_t vram_size ;
   uint64_t vram_cpu_accessible_size ;
   uint64_t gtt_size ;
};
struct drm_amdgpu_info_firmware {
   u32 ver ;
   u32 feature ;
};
struct drm_amdgpu_info_device {
   u32 device_id ;
   u32 chip_rev ;
   u32 external_rev ;
   u32 pci_rev ;
   u32 family ;
   u32 num_shader_engines ;
   u32 num_shader_arrays_per_engine ;
   u32 gpu_counter_freq ;
   uint64_t max_engine_clock ;
   uint64_t max_memory_clock ;
   u32 cu_active_number ;
   u32 cu_ao_mask ;
   u32 cu_bitmap[4U][4U] ;
   u32 enabled_rb_pipes_mask ;
   u32 num_rb_pipes ;
   u32 num_hw_gfx_contexts ;
   u32 _pad ;
   uint64_t ids_flags ;
   uint64_t virtual_address_offset ;
   uint64_t virtual_address_max ;
   u32 virtual_address_alignment ;
   u32 pte_fragment_size ;
   u32 gart_page_size ;
   u32 ce_ram_size ;
   u32 vram_type ;
   u32 vram_bit_width ;
};
struct drm_amdgpu_info_hw_ip {
   u32 hw_ip_version_major ;
   u32 hw_ip_version_minor ;
   uint64_t capabilities_flags ;
   u32 ib_start_alignment ;
   u32 ib_size_alignment ;
   u32 available_rings ;
   u32 _pad ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct __anonstruct____missing_field_name_275 {
   unsigned short frac_fb_div : 14 ;
   unsigned short whole_fb_div : 12 ;
   unsigned char reserved : 6 ;
};
union __anonunion____missing_field_name_274 {
   struct __anonstruct____missing_field_name_275 __annonCompField81 ;
   u32 fb_div ;
};
struct atom_clock_dividers {
   u32 post_div ;
   union __anonunion____missing_field_name_274 __annonCompField82 ;
   u32 ref_div ;
   bool enable_post_div ;
   bool enable_dithen ;
   u32 vco_mode ;
   u32 real_clock ;
   u32 post_divider ;
   u32 flags ;
};
struct __anonstruct____missing_field_name_277 {
   unsigned short clkf : 12 ;
   unsigned short clkfrac : 12 ;
   unsigned char reserved ;
};
union __anonunion____missing_field_name_276 {
   struct __anonstruct____missing_field_name_277 __annonCompField83 ;
   u32 fb_div ;
};
struct atom_mpll_param {
   union __anonunion____missing_field_name_276 __annonCompField84 ;
   u32 post_div ;
   u32 bwcntl ;
   u32 dll_speed ;
   u32 vco_mode ;
   u32 yclk_sel ;
   u32 qdr ;
   u32 half_rate ;
};
struct atom_mc_reg_entry {
   u32 mclk_max ;
   u32 mc_data[32U] ;
};
struct atom_mc_register_address {
   u16 s1 ;
   u8 pre_reg_data ;
};
struct atom_mc_reg_table {
   u8 last ;
   u8 num_entries ;
   struct atom_mc_reg_entry mc_reg_table_entry[20U] ;
   struct atom_mc_register_address mc_reg_address[32U] ;
};
struct atom_voltage_table_entry {
   u16 value ;
   u32 smio_low ;
};
struct atom_voltage_table {
   u32 count ;
   u32 mask_low ;
   u32 phase_delay ;
   struct atom_voltage_table_entry entries[32U] ;
};
typedef u16 USHORT;
typedef u32 ULONG;
typedef uint8_t UCHAR;
struct _ATOM_COMMON_TABLE_HEADER {
   USHORT usStructureSize ;
   UCHAR ucTableFormatRevision ;
   UCHAR ucTableContentRevision ;
};
typedef struct _ATOM_COMMON_TABLE_HEADER ATOM_COMMON_TABLE_HEADER;
struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS {
   ULONG ulClock ;
   UCHAR ucAction ;
   UCHAR ucReserved ;
   UCHAR ucFbDiv ;
   UCHAR ucPostDiv ;
};
typedef struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS;
struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 {
   ULONG ulClock ;
   UCHAR ucAction ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
};
struct _ATOM_COMPUTE_CLOCK_FREQ {
   unsigned int ulClockFreq : 24 ;
   unsigned char ulComputeClockFlag ;
};
typedef struct _ATOM_COMPUTE_CLOCK_FREQ ATOM_COMPUTE_CLOCK_FREQ;
struct _ATOM_S_MPLL_FB_DIVIDER {
   USHORT usFbDivFrac ;
   USHORT usFbDiv ;
};
typedef struct _ATOM_S_MPLL_FB_DIVIDER ATOM_S_MPLL_FB_DIVIDER;
union __anonunion____missing_field_name_278 {
   ATOM_COMPUTE_CLOCK_FREQ ulClock ;
   ATOM_S_MPLL_FB_DIVIDER ulFbDiv ;
};
struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 {
   union __anonunion____missing_field_name_278 __annonCompField85 ;
   UCHAR ucRefDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucCntlFlag ;
   UCHAR ucReserved ;
};
struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 {
   unsigned int ulClock : 24 ;
   unsigned char ucPostDiv ;
};
typedef struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4;
union __anonunion____missing_field_name_279 {
   ATOM_COMPUTE_CLOCK_FREQ ulClock ;
   ATOM_S_MPLL_FB_DIVIDER ulFbDiv ;
};
union __anonunion____missing_field_name_280 {
   UCHAR ucCntlFlag ;
   UCHAR ucInputFlag ;
};
struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 {
   union __anonunion____missing_field_name_279 __annonCompField86 ;
   UCHAR ucRefDiv ;
   UCHAR ucPostDiv ;
   union __anonunion____missing_field_name_280 __annonCompField87 ;
   UCHAR ucReserved ;
};
struct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 {
   ATOM_COMPUTE_CLOCK_FREQ ulClock ;
   ULONG ulReserved[2U] ;
};
struct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 {
   COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 ulClock ;
   ATOM_S_MPLL_FB_DIVIDER ulFbDiv ;
   UCHAR ucPllRefDiv ;
   UCHAR ucPllPostDiv ;
   UCHAR ucPllCntlFlag ;
   UCHAR ucReserved ;
};
union __anonunion____missing_field_name_281 {
   ULONG ulClock ;
   ATOM_S_MPLL_FB_DIVIDER ulFbDiv ;
};
union __anonunion____missing_field_name_282 {
   UCHAR ucInputFlag ;
   UCHAR ucPllCntlFlag ;
};
struct _COMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 {
   union __anonunion____missing_field_name_281 __annonCompField88 ;
   UCHAR ucDllSpeed ;
   UCHAR ucPostDiv ;
   union __anonunion____missing_field_name_282 __annonCompField89 ;
   UCHAR ucBWCntl ;
};
typedef struct _COMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 COMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1;
struct _SET_ENGINE_CLOCK_PS_ALLOCATION {
   ULONG ulTargetEngineClock ;
   COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS sReserved ;
};
typedef struct _SET_ENGINE_CLOCK_PS_ALLOCATION SET_ENGINE_CLOCK_PS_ALLOCATION;
struct _SET_MEMORY_CLOCK_PS_ALLOCATION {
   ULONG ulTargetMemoryClock ;
   COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS sReserved ;
};
typedef struct _SET_MEMORY_CLOCK_PS_ALLOCATION SET_MEMORY_CLOCK_PS_ALLOCATION;
struct _GET_MEMORY_CLOCK_PARAMETERS {
   ULONG ulReturnMemoryClock ;
};
typedef struct _GET_MEMORY_CLOCK_PARAMETERS GET_MEMORY_CLOCK_PARAMETERS;
struct _GET_ENGINE_CLOCK_PARAMETERS {
   ULONG ulReturnEngineClock ;
};
typedef struct _GET_ENGINE_CLOCK_PARAMETERS GET_ENGINE_CLOCK_PARAMETERS;
struct _WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS {
   USHORT usPrescale ;
   USHORT usByteOffset ;
   UCHAR ucData ;
   UCHAR ucStatus ;
   UCHAR ucSlaveAddr ;
   UCHAR ucLineNumber ;
};
typedef struct _WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS;
struct _SET_VOLTAGE_PARAMETERS {
   UCHAR ucVoltageType ;
   UCHAR ucVoltageMode ;
   UCHAR ucVoltageIndex ;
   UCHAR ucReserved ;
};
typedef struct _SET_VOLTAGE_PARAMETERS SET_VOLTAGE_PARAMETERS;
struct _SET_VOLTAGE_PARAMETERS_V2 {
   UCHAR ucVoltageType ;
   UCHAR ucVoltageMode ;
   USHORT usVoltageLevel ;
};
struct _SET_VOLTAGE_PARAMETERS_V1_3 {
   UCHAR ucVoltageType ;
   UCHAR ucVoltageMode ;
   USHORT usVoltageLevel ;
};
struct _SET_VOLTAGE_PS_ALLOCATION {
   SET_VOLTAGE_PARAMETERS sASICSetVoltage ;
   WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS sReserved ;
};
struct _GET_VOLTAGE_INFO_INPUT_PARAMETER_V1_2 {
   UCHAR ucVoltageType ;
   UCHAR ucVoltageMode ;
   USHORT usVoltageLevel ;
   ULONG ulSCLKFreq ;
};
struct _GET_EVV_VOLTAGE_INFO_OUTPUT_PARAMETER_V1_2 {
   USHORT usVoltageLevel ;
   USHORT usVoltageId ;
   USHORT usTDP_Current ;
   USHORT usTDP_Power ;
};
struct _ATOM_FIRMWARE_CAPABILITY {
   unsigned char FirmwarePosted : 1 ;
   unsigned char DualCRTC_Support : 1 ;
   unsigned char ExtendedDesktopSupport : 1 ;
   unsigned char MemoryClockSS_Support : 1 ;
   unsigned char EngineClockSS_Support : 1 ;
   unsigned char GPUControlsBL : 1 ;
   unsigned char WMI_SUPPORT : 1 ;
   unsigned char PPMode_Assigned : 1 ;
   unsigned char HyperMemory_Support : 1 ;
   unsigned char HyperMemory_Size : 4 ;
   unsigned char PostWithoutModeSet : 1 ;
   unsigned char SCL2Redefined : 1 ;
   unsigned char Reserved : 1 ;
};
typedef struct _ATOM_FIRMWARE_CAPABILITY ATOM_FIRMWARE_CAPABILITY;
union _ATOM_FIRMWARE_CAPABILITY_ACCESS {
   ATOM_FIRMWARE_CAPABILITY sbfAccess ;
   USHORT susAccess ;
};
typedef union _ATOM_FIRMWARE_CAPABILITY_ACCESS ATOM_FIRMWARE_CAPABILITY_ACCESS;
struct _ATOM_FIRMWARE_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulDriverTargetEngineClock ;
   ULONG ulDriverTargetMemoryClock ;
   ULONG ulMaxEngineClockPLL_Output ;
   ULONG ulMaxMemoryClockPLL_Output ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulASICMaxEngineClock ;
   ULONG ulASICMaxMemoryClock ;
   UCHAR ucASICMaxTemperature ;
   UCHAR ucPadding[3U] ;
   ULONG aulReservedForBIOS[3U] ;
   USHORT usMinEngineClockPLL_Input ;
   USHORT usMaxEngineClockPLL_Input ;
   USHORT usMinEngineClockPLL_Output ;
   USHORT usMinMemoryClockPLL_Input ;
   USHORT usMaxMemoryClockPLL_Input ;
   USHORT usMinMemoryClockPLL_Output ;
   USHORT usMaxPixelClock ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usMinPixelClockPLL_Output ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usReferenceClock ;
   USHORT usPM_RTS_Location ;
   UCHAR ucPM_RTS_StreamSize ;
   UCHAR ucDesign_ID ;
   UCHAR ucMemoryModule_ID ;
};
typedef struct _ATOM_FIRMWARE_INFO ATOM_FIRMWARE_INFO;
struct _ATOM_FIRMWARE_INFO_V1_2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulDriverTargetEngineClock ;
   ULONG ulDriverTargetMemoryClock ;
   ULONG ulMaxEngineClockPLL_Output ;
   ULONG ulMaxMemoryClockPLL_Output ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulASICMaxEngineClock ;
   ULONG ulASICMaxMemoryClock ;
   UCHAR ucASICMaxTemperature ;
   UCHAR ucMinAllowedBL_Level ;
   UCHAR ucPadding[2U] ;
   ULONG aulReservedForBIOS[2U] ;
   ULONG ulMinPixelClockPLL_Output ;
   USHORT usMinEngineClockPLL_Input ;
   USHORT usMaxEngineClockPLL_Input ;
   USHORT usMinEngineClockPLL_Output ;
   USHORT usMinMemoryClockPLL_Input ;
   USHORT usMaxMemoryClockPLL_Input ;
   USHORT usMinMemoryClockPLL_Output ;
   USHORT usMaxPixelClock ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usMinPixelClockPLL_Output ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usReferenceClock ;
   USHORT usPM_RTS_Location ;
   UCHAR ucPM_RTS_StreamSize ;
   UCHAR ucDesign_ID ;
   UCHAR ucMemoryModule_ID ;
};
typedef struct _ATOM_FIRMWARE_INFO_V1_2 ATOM_FIRMWARE_INFO_V1_2;
struct _ATOM_FIRMWARE_INFO_V1_3 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulDriverTargetEngineClock ;
   ULONG ulDriverTargetMemoryClock ;
   ULONG ulMaxEngineClockPLL_Output ;
   ULONG ulMaxMemoryClockPLL_Output ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulASICMaxEngineClock ;
   ULONG ulASICMaxMemoryClock ;
   UCHAR ucASICMaxTemperature ;
   UCHAR ucMinAllowedBL_Level ;
   UCHAR ucPadding[2U] ;
   ULONG aulReservedForBIOS ;
   ULONG ul3DAccelerationEngineClock ;
   ULONG ulMinPixelClockPLL_Output ;
   USHORT usMinEngineClockPLL_Input ;
   USHORT usMaxEngineClockPLL_Input ;
   USHORT usMinEngineClockPLL_Output ;
   USHORT usMinMemoryClockPLL_Input ;
   USHORT usMaxMemoryClockPLL_Input ;
   USHORT usMinMemoryClockPLL_Output ;
   USHORT usMaxPixelClock ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usMinPixelClockPLL_Output ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usReferenceClock ;
   USHORT usPM_RTS_Location ;
   UCHAR ucPM_RTS_StreamSize ;
   UCHAR ucDesign_ID ;
   UCHAR ucMemoryModule_ID ;
};
typedef struct _ATOM_FIRMWARE_INFO_V1_3 ATOM_FIRMWARE_INFO_V1_3;
struct _ATOM_FIRMWARE_INFO_V1_4 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulDriverTargetEngineClock ;
   ULONG ulDriverTargetMemoryClock ;
   ULONG ulMaxEngineClockPLL_Output ;
   ULONG ulMaxMemoryClockPLL_Output ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulASICMaxEngineClock ;
   ULONG ulASICMaxMemoryClock ;
   UCHAR ucASICMaxTemperature ;
   UCHAR ucMinAllowedBL_Level ;
   USHORT usBootUpVDDCVoltage ;
   USHORT usLcdMinPixelClockPLL_Output ;
   USHORT usLcdMaxPixelClockPLL_Output ;
   ULONG ul3DAccelerationEngineClock ;
   ULONG ulMinPixelClockPLL_Output ;
   USHORT usMinEngineClockPLL_Input ;
   USHORT usMaxEngineClockPLL_Input ;
   USHORT usMinEngineClockPLL_Output ;
   USHORT usMinMemoryClockPLL_Input ;
   USHORT usMaxMemoryClockPLL_Input ;
   USHORT usMinMemoryClockPLL_Output ;
   USHORT usMaxPixelClock ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usMinPixelClockPLL_Output ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usReferenceClock ;
   USHORT usPM_RTS_Location ;
   UCHAR ucPM_RTS_StreamSize ;
   UCHAR ucDesign_ID ;
   UCHAR ucMemoryModule_ID ;
};
typedef struct _ATOM_FIRMWARE_INFO_V1_4 ATOM_FIRMWARE_INFO_V1_4;
struct _ATOM_FIRMWARE_INFO_V2_1 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulReserved1 ;
   ULONG ulReserved2 ;
   ULONG ulMaxEngineClockPLL_Output ;
   ULONG ulMaxMemoryClockPLL_Output ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulBinaryAlteredInfo ;
   ULONG ulDefaultDispEngineClkFreq ;
   UCHAR ucReserved1 ;
   UCHAR ucMinAllowedBL_Level ;
   USHORT usBootUpVDDCVoltage ;
   USHORT usLcdMinPixelClockPLL_Output ;
   USHORT usLcdMaxPixelClockPLL_Output ;
   ULONG ulReserved4 ;
   ULONG ulMinPixelClockPLL_Output ;
   USHORT usMinEngineClockPLL_Input ;
   USHORT usMaxEngineClockPLL_Input ;
   USHORT usMinEngineClockPLL_Output ;
   USHORT usMinMemoryClockPLL_Input ;
   USHORT usMaxMemoryClockPLL_Input ;
   USHORT usMinMemoryClockPLL_Output ;
   USHORT usMaxPixelClock ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usMinPixelClockPLL_Output ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usCoreReferenceClock ;
   USHORT usMemoryReferenceClock ;
   USHORT usUniphyDPModeExtClkFreq ;
   UCHAR ucMemoryModule_ID ;
   UCHAR ucReserved4[3U] ;
};
typedef struct _ATOM_FIRMWARE_INFO_V2_1 ATOM_FIRMWARE_INFO_V2_1;
struct _PRODUCT_BRANDING {
   unsigned char ucEMBEDDED_CAP : 2 ;
   unsigned char ucReserved : 2 ;
   unsigned char ucBRANDING_ID : 4 ;
};
typedef struct _PRODUCT_BRANDING PRODUCT_BRANDING;
struct _ATOM_FIRMWARE_INFO_V2_2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulFirmwareRevision ;
   ULONG ulDefaultEngineClock ;
   ULONG ulDefaultMemoryClock ;
   ULONG ulSPLL_OutputFreq ;
   ULONG ulGPUPLL_OutputFreq ;
   ULONG ulReserved1 ;
   ULONG ulReserved2 ;
   ULONG ulMaxPixelClockPLL_Output ;
   ULONG ulBinaryAlteredInfo ;
   ULONG ulDefaultDispEngineClkFreq ;
   UCHAR ucReserved3 ;
   UCHAR ucMinAllowedBL_Level ;
   USHORT usBootUpVDDCVoltage ;
   USHORT usLcdMinPixelClockPLL_Output ;
   USHORT usLcdMaxPixelClockPLL_Output ;
   ULONG ulReserved4 ;
   ULONG ulMinPixelClockPLL_Output ;
   UCHAR ucRemoteDisplayConfig ;
   UCHAR ucReserved5[3U] ;
   ULONG ulReserved6 ;
   ULONG ulReserved7 ;
   USHORT usReserved11 ;
   USHORT usMinPixelClockPLL_Input ;
   USHORT usMaxPixelClockPLL_Input ;
   USHORT usBootUpVDDCIVoltage ;
   ATOM_FIRMWARE_CAPABILITY_ACCESS usFirmwareCapability ;
   USHORT usCoreReferenceClock ;
   USHORT usMemoryReferenceClock ;
   USHORT usUniphyDPModeExtClkFreq ;
   UCHAR ucMemoryModule_ID ;
   UCHAR ucCoolingSolution_ID ;
   PRODUCT_BRANDING ucProductBranding ;
   UCHAR ucReserved9 ;
   USHORT usBootUpMVDDCVoltage ;
   USHORT usBootUpVDDGFXVoltage ;
   ULONG ulReserved10[3U] ;
};
typedef struct _ATOM_FIRMWARE_INFO_V2_2 ATOM_FIRMWARE_INFO_V2_2;
struct _ATOM_INTEGRATED_SYSTEM_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulBootUpMemoryClock ;
   ULONG ulMaxSystemMemoryClock ;
   ULONG ulMinSystemMemoryClock ;
   UCHAR ucNumberOfCyclesInPeriodHi ;
   UCHAR ucLCDTimingSel ;
   USHORT usReserved1 ;
   USHORT usInterNBVoltageLow ;
   USHORT usInterNBVoltageHigh ;
   ULONG ulReserved[2U] ;
   USHORT usFSBClock ;
   USHORT usCapabilityFlag ;
   USHORT usPCIENBCfgReg7 ;
   USHORT usK8MemoryClock ;
   USHORT usK8SyncStartDelay ;
   USHORT usK8DataReturnTime ;
   UCHAR ucMaxNBVoltage ;
   UCHAR ucMinNBVoltage ;
   UCHAR ucMemoryType ;
   UCHAR ucNumberOfCyclesInPeriod ;
   UCHAR ucStartingPWM_HighTime ;
   UCHAR ucHTLinkWidth ;
   UCHAR ucMaxNBVoltageHigh ;
   UCHAR ucMinNBVoltageHigh ;
};
struct _ATOM_INTEGRATED_SYSTEM_INFO_V2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulReserved1[2U] ;
   ULONG ulBootUpUMAClock ;
   ULONG ulBootUpSidePortClock ;
   ULONG ulMinSidePortClock ;
   ULONG ulReserved2[6U] ;
   ULONG ulSystemConfig ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulOtherDisplayMisc ;
   ULONG ulDDISlot1Config ;
   ULONG ulDDISlot2Config ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   UCHAR ucDockingPinBit ;
   UCHAR ucDockingPinPolarity ;
   ULONG ulDockingPinCFGInfo ;
   ULONG ulCPUCapInfo ;
   USHORT usNumberOfCyclesInPeriod ;
   USHORT usMaxNBVoltage ;
   USHORT usMinNBVoltage ;
   USHORT usBootUpNBVoltage ;
   ULONG ulHTLinkFreq ;
   USHORT usMinHTLinkWidth ;
   USHORT usMaxHTLinkWidth ;
   USHORT usUMASyncStartDelay ;
   USHORT usUMADataReturnTime ;
   USHORT usLinkStatusZeroTime ;
   USHORT usDACEfuse ;
   ULONG ulHighVoltageHTLinkFreq ;
   ULONG ulLowVoltageHTLinkFreq ;
   USHORT usMaxUpStreamHTLinkWidth ;
   USHORT usMaxDownStreamHTLinkWidth ;
   USHORT usMinUpStreamHTLinkWidth ;
   USHORT usMinDownStreamHTLinkWidth ;
   USHORT usFirmwareVersion ;
   USHORT usFullT0Time ;
   ULONG ulReserved3[96U] ;
};
struct _ATOM_I2C_ID_CONFIG {
   unsigned char bfI2C_LineMux : 4 ;
   unsigned char bfHW_EngineID : 3 ;
   unsigned char bfHW_Capable : 1 ;
};
typedef struct _ATOM_I2C_ID_CONFIG ATOM_I2C_ID_CONFIG;
union _ATOM_I2C_ID_CONFIG_ACCESS {
   ATOM_I2C_ID_CONFIG sbfAccess ;
   UCHAR ucAccess ;
};
typedef union _ATOM_I2C_ID_CONFIG_ACCESS ATOM_I2C_ID_CONFIG_ACCESS;
struct _ATOM_GPIO_I2C_ASSIGMENT {
   USHORT usClkMaskRegisterIndex ;
   USHORT usClkEnRegisterIndex ;
   USHORT usClkY_RegisterIndex ;
   USHORT usClkA_RegisterIndex ;
   USHORT usDataMaskRegisterIndex ;
   USHORT usDataEnRegisterIndex ;
   USHORT usDataY_RegisterIndex ;
   USHORT usDataA_RegisterIndex ;
   ATOM_I2C_ID_CONFIG_ACCESS sucI2cId ;
   UCHAR ucClkMaskShift ;
   UCHAR ucClkEnShift ;
   UCHAR ucClkY_Shift ;
   UCHAR ucClkA_Shift ;
   UCHAR ucDataMaskShift ;
   UCHAR ucDataEnShift ;
   UCHAR ucDataY_Shift ;
   UCHAR ucDataA_Shift ;
   UCHAR ucReserved1 ;
   UCHAR ucReserved2 ;
};
typedef struct _ATOM_GPIO_I2C_ASSIGMENT ATOM_GPIO_I2C_ASSIGMENT;
struct _ATOM_GPIO_I2C_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_GPIO_I2C_ASSIGMENT asGPIO_Info[16U] ;
};
struct _ATOM_GPIO_PIN_ASSIGNMENT {
   USHORT usGpioPin_AIndex ;
   UCHAR ucGpioPinBitShift ;
   UCHAR ucGPIO_ID ;
};
typedef struct _ATOM_GPIO_PIN_ASSIGNMENT ATOM_GPIO_PIN_ASSIGNMENT;
struct _ATOM_GPIO_PIN_LUT {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_GPIO_PIN_ASSIGNMENT asGPIO_Pin[1U] ;
};
struct _ATOM_OBJECT_HEADER {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   USHORT usDeviceSupport ;
   USHORT usConnectorObjectTableOffset ;
   USHORT usRouterObjectTableOffset ;
   USHORT usEncoderObjectTableOffset ;
   USHORT usProtectionObjectTableOffset ;
   USHORT usDisplayPathTableOffset ;
};
typedef struct _ATOM_OBJECT_HEADER ATOM_OBJECT_HEADER;
struct _ATOM_DISPLAY_OBJECT_PATH {
   USHORT usDeviceTag ;
   USHORT usSize ;
   USHORT usConnObjectId ;
   USHORT usGPUObjectId ;
   USHORT usGraphicObjIds[1U] ;
};
typedef struct _ATOM_DISPLAY_OBJECT_PATH ATOM_DISPLAY_OBJECT_PATH;
struct _ATOM_DISPLAY_OBJECT_PATH_TABLE {
   UCHAR ucNumOfDispPath ;
   UCHAR ucVersion ;
   UCHAR ucPadding[2U] ;
   ATOM_DISPLAY_OBJECT_PATH asDispPath[1U] ;
};
typedef struct _ATOM_DISPLAY_OBJECT_PATH_TABLE ATOM_DISPLAY_OBJECT_PATH_TABLE;
struct _ATOM_OBJECT {
   USHORT usObjectID ;
   USHORT usSrcDstTableOffset ;
   USHORT usRecordOffset ;
   USHORT usReserved ;
};
typedef struct _ATOM_OBJECT ATOM_OBJECT;
struct _ATOM_OBJECT_TABLE {
   UCHAR ucNumberOfObjects ;
   UCHAR ucPadding[3U] ;
   ATOM_OBJECT asObjects[1U] ;
};
typedef struct _ATOM_OBJECT_TABLE ATOM_OBJECT_TABLE;
struct _ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT {
   UCHAR ucNumberOfSrc ;
   USHORT usSrcObjectID[1U] ;
   UCHAR ucNumberOfDst ;
   USHORT usDstObjectID[1U] ;
};
typedef struct _ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT;
struct _ATOM_DP_CONN_CHANNEL_MAPPING {
   unsigned char ucDP_Lane0_Source : 2 ;
   unsigned char ucDP_Lane1_Source : 2 ;
   unsigned char ucDP_Lane2_Source : 2 ;
   unsigned char ucDP_Lane3_Source : 2 ;
};
typedef struct _ATOM_DP_CONN_CHANNEL_MAPPING ATOM_DP_CONN_CHANNEL_MAPPING;
struct _ATOM_DVI_CONN_CHANNEL_MAPPING {
   unsigned char ucDVI_DATA2_Source : 2 ;
   unsigned char ucDVI_DATA1_Source : 2 ;
   unsigned char ucDVI_DATA0_Source : 2 ;
   unsigned char ucDVI_CLK_Source : 2 ;
};
typedef struct _ATOM_DVI_CONN_CHANNEL_MAPPING ATOM_DVI_CONN_CHANNEL_MAPPING;
union __anonunion____missing_field_name_301 {
   UCHAR ucChannelMapping ;
   ATOM_DP_CONN_CHANNEL_MAPPING asDPMapping ;
   ATOM_DVI_CONN_CHANNEL_MAPPING asDVIMapping ;
};
struct _EXT_DISPLAY_PATH {
   USHORT usDeviceTag ;
   USHORT usDeviceACPIEnum ;
   USHORT usDeviceConnector ;
   UCHAR ucExtAUXDDCLutIndex ;
   UCHAR ucExtHPDPINLutIndex ;
   USHORT usExtEncoderObjId ;
   union __anonunion____missing_field_name_301 __annonCompField108 ;
   UCHAR ucChPNInvert ;
   USHORT usCaps ;
   USHORT usReserved ;
};
typedef struct _EXT_DISPLAY_PATH EXT_DISPLAY_PATH;
struct _ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   UCHAR ucGuid[16U] ;
   EXT_DISPLAY_PATH sPath[7U] ;
   UCHAR ucChecksum ;
   UCHAR uc3DStereoPinId ;
   UCHAR ucRemoteDisplayConfig ;
   UCHAR uceDPToLVDSRxId ;
   UCHAR ucFixDPVoltageSwing ;
   UCHAR Reserved[3U] ;
};
typedef struct _ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO;
struct _ATOM_COMMON_RECORD_HEADER {
   UCHAR ucRecordType ;
   UCHAR ucRecordSize ;
};
typedef struct _ATOM_COMMON_RECORD_HEADER ATOM_COMMON_RECORD_HEADER;
struct _ATOM_I2C_RECORD {
   ATOM_COMMON_RECORD_HEADER sheader ;
   ATOM_I2C_ID_CONFIG sucI2cId ;
   UCHAR ucI2CAddr ;
};
typedef struct _ATOM_I2C_RECORD ATOM_I2C_RECORD;
struct _ATOM_HPD_INT_RECORD {
   ATOM_COMMON_RECORD_HEADER sheader ;
   UCHAR ucHPDIntGPIOID ;
   UCHAR ucPlugged_PinState ;
};
typedef struct _ATOM_HPD_INT_RECORD ATOM_HPD_INT_RECORD;
struct __anonstruct____missing_field_name_303 {
   unsigned char usHBR2Cap : 1 ;
   unsigned char usHBR2En : 1 ;
   unsigned short usReserved : 14 ;
};
union __anonunion____missing_field_name_302 {
   USHORT usEncoderCap ;
   struct __anonstruct____missing_field_name_303 __annonCompField109 ;
};
struct _ATOM_ENCODER_CAP_RECORD {
   ATOM_COMMON_RECORD_HEADER sheader ;
   union __anonunion____missing_field_name_302 __annonCompField110 ;
};
typedef struct _ATOM_ENCODER_CAP_RECORD ATOM_ENCODER_CAP_RECORD;
struct _ATOM_ROUTER_DDC_PATH_SELECT_RECORD {
   ATOM_COMMON_RECORD_HEADER sheader ;
   UCHAR ucMuxType ;
   UCHAR ucMuxControlPin ;
   UCHAR ucMuxState[2U] ;
};
typedef struct _ATOM_ROUTER_DDC_PATH_SELECT_RECORD ATOM_ROUTER_DDC_PATH_SELECT_RECORD;
struct _ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD {
   ATOM_COMMON_RECORD_HEADER sheader ;
   UCHAR ucMuxType ;
   UCHAR ucMuxControlPin ;
   UCHAR ucMuxState[2U] ;
};
typedef struct _ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD;
struct _ATOM_VOLTAGE_FORMULA {
   USHORT usVoltageBaseLevel ;
   USHORT usVoltageStep ;
   UCHAR ucNumOfVoltageEntries ;
   UCHAR ucFlag ;
   UCHAR ucBaseVID ;
   UCHAR ucReserved ;
   UCHAR ucVIDAdjustEntries[32U] ;
};
typedef struct _ATOM_VOLTAGE_FORMULA ATOM_VOLTAGE_FORMULA;
struct _VOLTAGE_LUT_ENTRY {
   USHORT usVoltageCode ;
   USHORT usVoltageValue ;
};
typedef struct _VOLTAGE_LUT_ENTRY VOLTAGE_LUT_ENTRY;
struct _ATOM_VOLTAGE_FORMULA_V2 {
   UCHAR ucNumOfVoltageEntries ;
   UCHAR ucReserved[3U] ;
   VOLTAGE_LUT_ENTRY asVIDAdjustEntries[32U] ;
};
typedef struct _ATOM_VOLTAGE_FORMULA_V2 ATOM_VOLTAGE_FORMULA_V2;
struct _ATOM_VOLTAGE_CONTROL {
   UCHAR ucVoltageControlId ;
   UCHAR ucVoltageControlI2cLine ;
   UCHAR ucVoltageControlAddress ;
   UCHAR ucVoltageControlOffset ;
   USHORT usGpioPin_AIndex ;
   UCHAR ucGpioPinBitShift[9U] ;
   UCHAR ucReserved ;
};
typedef struct _ATOM_VOLTAGE_CONTROL ATOM_VOLTAGE_CONTROL;
struct _ATOM_VOLTAGE_OBJECT {
   UCHAR ucVoltageType ;
   UCHAR ucSize ;
   ATOM_VOLTAGE_CONTROL asControl ;
   ATOM_VOLTAGE_FORMULA asFormula ;
};
typedef struct _ATOM_VOLTAGE_OBJECT ATOM_VOLTAGE_OBJECT;
struct _ATOM_VOLTAGE_OBJECT_V2 {
   UCHAR ucVoltageType ;
   UCHAR ucSize ;
   ATOM_VOLTAGE_CONTROL asControl ;
   ATOM_VOLTAGE_FORMULA_V2 asFormula ;
};
typedef struct _ATOM_VOLTAGE_OBJECT_V2 ATOM_VOLTAGE_OBJECT_V2;
struct _ATOM_VOLTAGE_OBJECT_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_VOLTAGE_OBJECT asVoltageObj[3U] ;
};
struct _ATOM_VOLTAGE_OBJECT_INFO_V2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_VOLTAGE_OBJECT_V2 asVoltageObj[3U] ;
};
struct _ATOM_VOLTAGE_OBJECT_HEADER_V3 {
   UCHAR ucVoltageType ;
   UCHAR ucVoltageMode ;
   USHORT usSize ;
};
typedef struct _ATOM_VOLTAGE_OBJECT_HEADER_V3 ATOM_VOLTAGE_OBJECT_HEADER_V3;
struct _VOLTAGE_LUT_ENTRY_V2 {
   ULONG ulVoltageId ;
   USHORT usVoltageValue ;
};
typedef struct _VOLTAGE_LUT_ENTRY_V2 VOLTAGE_LUT_ENTRY_V2;
struct _LEAKAGE_VOLTAGE_LUT_ENTRY_V2 {
   USHORT usVoltageLevel ;
   USHORT usVoltageId ;
   USHORT usLeakageId ;
};
typedef struct _LEAKAGE_VOLTAGE_LUT_ENTRY_V2 LEAKAGE_VOLTAGE_LUT_ENTRY_V2;
struct _ATOM_I2C_VOLTAGE_OBJECT_V3 {
   ATOM_VOLTAGE_OBJECT_HEADER_V3 sHeader ;
   UCHAR ucVoltageRegulatorId ;
   UCHAR ucVoltageControlI2cLine ;
   UCHAR ucVoltageControlAddress ;
   UCHAR ucVoltageControlOffset ;
   UCHAR ucVoltageControlFlag ;
   UCHAR ulReserved[3U] ;
   VOLTAGE_LUT_ENTRY asVolI2cLut[1U] ;
};
typedef struct _ATOM_I2C_VOLTAGE_OBJECT_V3 ATOM_I2C_VOLTAGE_OBJECT_V3;
struct _ATOM_GPIO_VOLTAGE_OBJECT_V3 {
   ATOM_VOLTAGE_OBJECT_HEADER_V3 sHeader ;
   UCHAR ucVoltageGpioCntlId ;
   UCHAR ucGpioEntryNum ;
   UCHAR ucPhaseDelay ;
   UCHAR ucReserved ;
   ULONG ulGpioMaskVal ;
   VOLTAGE_LUT_ENTRY_V2 asVolGpioLut[1U] ;
};
typedef struct _ATOM_GPIO_VOLTAGE_OBJECT_V3 ATOM_GPIO_VOLTAGE_OBJECT_V3;
struct _ATOM_LEAKAGE_VOLTAGE_OBJECT_V3 {
   ATOM_VOLTAGE_OBJECT_HEADER_V3 sHeader ;
   UCHAR ucLeakageCntlId ;
   UCHAR ucLeakageEntryNum ;
   UCHAR ucReserved[2U] ;
   ULONG ulMaxVoltageLevel ;
   LEAKAGE_VOLTAGE_LUT_ENTRY_V2 asLeakageIdLut[1U] ;
};
typedef struct _ATOM_LEAKAGE_VOLTAGE_OBJECT_V3 ATOM_LEAKAGE_VOLTAGE_OBJECT_V3;
struct _ATOM_SVID2_VOLTAGE_OBJECT_V3 {
   ATOM_VOLTAGE_OBJECT_HEADER_V3 sHeader ;
   USHORT usLoadLine_PSI ;
   UCHAR ucSVDGpioId ;
   UCHAR ucSVCGpioId ;
   ULONG ulReserved ;
};
typedef struct _ATOM_SVID2_VOLTAGE_OBJECT_V3 ATOM_SVID2_VOLTAGE_OBJECT_V3;
union _ATOM_VOLTAGE_OBJECT_V3 {
   ATOM_GPIO_VOLTAGE_OBJECT_V3 asGpioVoltageObj ;
   ATOM_I2C_VOLTAGE_OBJECT_V3 asI2cVoltageObj ;
   ATOM_LEAKAGE_VOLTAGE_OBJECT_V3 asLeakageObj ;
   ATOM_SVID2_VOLTAGE_OBJECT_V3 asSVID2Obj ;
};
typedef union _ATOM_VOLTAGE_OBJECT_V3 ATOM_VOLTAGE_OBJECT_V3;
struct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_VOLTAGE_OBJECT_V3 asVoltageObj[3U] ;
};
typedef struct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 ATOM_VOLTAGE_OBJECT_INFO_V3_1;
struct _ATOM_ASIC_PROFILING_INFO_V2_1 {
   ATOM_COMMON_TABLE_HEADER asHeader ;
   UCHAR ucLeakageBinNum ;
   USHORT usLeakageBinArrayOffset ;
   UCHAR ucElbVDDC_Num ;
   USHORT usElbVDDC_IdArrayOffset ;
   USHORT usElbVDDC_LevelArrayOffset ;
   UCHAR ucElbVDDCI_Num ;
   USHORT usElbVDDCI_IdArrayOffset ;
   USHORT usElbVDDCI_LevelArrayOffset ;
};
typedef struct _ATOM_ASIC_PROFILING_INFO_V2_1 ATOM_ASIC_PROFILING_INFO_V2_1;
struct _ATOM_CLK_VOLT_CAPABILITY {
   ULONG ulVoltageIndex ;
   ULONG ulMaximumSupportedCLK ;
};
typedef struct _ATOM_CLK_VOLT_CAPABILITY ATOM_CLK_VOLT_CAPABILITY;
struct _ATOM_CLK_VOLT_CAPABILITY_V2 {
   USHORT usVoltageLevel ;
   ULONG ulMaximumSupportedCLK ;
};
typedef struct _ATOM_CLK_VOLT_CAPABILITY_V2 ATOM_CLK_VOLT_CAPABILITY_V2;
struct _ATOM_AVAILABLE_SCLK_LIST {
   ULONG ulSupportedSCLK ;
   USHORT usVoltageIndex ;
   USHORT usVoltageID ;
};
typedef struct _ATOM_AVAILABLE_SCLK_LIST ATOM_AVAILABLE_SCLK_LIST;
struct _ATOM_INTEGRATED_SYSTEM_INFO_V6 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulDentistVCOFreq ;
   ULONG ulBootUpUMAClock ;
   ATOM_CLK_VOLT_CAPABILITY sDISPCLK_Voltage[4U] ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulOtherDisplayMisc ;
   ULONG ulGPUCapInfo ;
   ULONG ulSB_MMIO_Base_Addr ;
   USHORT usRequestedPWMFreqInHz ;
   UCHAR ucHtcTmpLmt ;
   UCHAR ucHtcHystLmt ;
   ULONG ulMinEngineClock ;
   ULONG ulSystemConfig ;
   ULONG ulCPUCapInfo ;
   USHORT usNBP0Voltage ;
   USHORT usNBP1Voltage ;
   USHORT usBootUpNBVoltage ;
   USHORT usExtDispConnInfoOffset ;
   USHORT usPanelRefreshRateRange ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   ULONG ulCSR_M3_ARB_CNTL_DEFAULT[10U] ;
   ULONG ulCSR_M3_ARB_CNTL_UVD[10U] ;
   ULONG ulCSR_M3_ARB_CNTL_FS3D[10U] ;
   ATOM_AVAILABLE_SCLK_LIST sAvail_SCLK[5U] ;
   ULONG ulGMCRestoreResetTime ;
   ULONG ulMinimumNClk ;
   ULONG ulIdleNClk ;
   ULONG ulDDR_DLL_PowerUpTime ;
   ULONG ulDDR_PLL_PowerUpTime ;
   USHORT usPCIEClkSSPercentage ;
   USHORT usPCIEClkSSType ;
   USHORT usLvdsSSPercentage ;
   USHORT usLvdsSSpreadRateIn10Hz ;
   USHORT usHDMISSPercentage ;
   USHORT usHDMISSpreadRateIn10Hz ;
   USHORT usDVISSPercentage ;
   USHORT usDVISSpreadRateIn10Hz ;
   ULONG SclkDpmBoostMargin ;
   ULONG SclkDpmThrottleMargin ;
   USHORT SclkDpmTdpLimitPG ;
   USHORT SclkDpmTdpLimitBoost ;
   ULONG ulBoostEngineCLock ;
   UCHAR ulBoostVid_2bit ;
   UCHAR EnableBoost ;
   USHORT GnbTdpLimit ;
   USHORT usMaxLVDSPclkFreqInSingleLink ;
   UCHAR ucLvdsMisc ;
   UCHAR ucLVDSReserved ;
   ULONG ulReserved3[15U] ;
   ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO sExtDispConnInfo ;
};
struct _ATOM_TDP_CONFIG_BITS {
   unsigned char uCTDP_Enable : 2 ;
   unsigned short uCTDP_Value : 14 ;
   unsigned short uTDP_Value : 14 ;
   unsigned char uReserved : 2 ;
};
typedef struct _ATOM_TDP_CONFIG_BITS ATOM_TDP_CONFIG_BITS;
union _ATOM_TDP_CONFIG {
   ATOM_TDP_CONFIG_BITS TDP_config ;
   ULONG TDP_config_all ;
};
typedef union _ATOM_TDP_CONFIG ATOM_TDP_CONFIG;
struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulDentistVCOFreq ;
   ULONG ulBootUpUMAClock ;
   ATOM_CLK_VOLT_CAPABILITY sDISPCLK_Voltage[4U] ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulOtherDisplayMisc ;
   ULONG ulGPUCapInfo ;
   ULONG ulSB_MMIO_Base_Addr ;
   USHORT usRequestedPWMFreqInHz ;
   UCHAR ucHtcTmpLmt ;
   UCHAR ucHtcHystLmt ;
   ULONG ulMinEngineClock ;
   ULONG ulSystemConfig ;
   ULONG ulCPUCapInfo ;
   USHORT usNBP0Voltage ;
   USHORT usNBP1Voltage ;
   USHORT usBootUpNBVoltage ;
   USHORT usExtDispConnInfoOffset ;
   USHORT usPanelRefreshRateRange ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   UCHAR strVBIOSMsg[40U] ;
   ATOM_TDP_CONFIG asTdpConfig ;
   ULONG ulReserved[19U] ;
   ATOM_AVAILABLE_SCLK_LIST sAvail_SCLK[5U] ;
   ULONG ulGMCRestoreResetTime ;
   ULONG ulMinimumNClk ;
   ULONG ulIdleNClk ;
   ULONG ulDDR_DLL_PowerUpTime ;
   ULONG ulDDR_PLL_PowerUpTime ;
   USHORT usPCIEClkSSPercentage ;
   USHORT usPCIEClkSSType ;
   USHORT usLvdsSSPercentage ;
   USHORT usLvdsSSpreadRateIn10Hz ;
   USHORT usHDMISSPercentage ;
   USHORT usHDMISSpreadRateIn10Hz ;
   USHORT usDVISSPercentage ;
   USHORT usDVISSpreadRateIn10Hz ;
   ULONG SclkDpmBoostMargin ;
   ULONG SclkDpmThrottleMargin ;
   USHORT SclkDpmTdpLimitPG ;
   USHORT SclkDpmTdpLimitBoost ;
   ULONG ulBoostEngineCLock ;
   UCHAR ulBoostVid_2bit ;
   UCHAR EnableBoost ;
   USHORT GnbTdpLimit ;
   USHORT usMaxLVDSPclkFreqInSingleLink ;
   UCHAR ucLvdsMisc ;
   UCHAR ucTravisLVDSVolAdjust ;
   UCHAR ucLVDSPwrOnSeqDIGONtoDE_in4Ms ;
   UCHAR ucLVDSPwrOnSeqDEtoVARY_BL_in4Ms ;
   UCHAR ucLVDSPwrOffSeqVARY_BLtoDE_in4Ms ;
   UCHAR ucLVDSPwrOffSeqDEtoDIGON_in4Ms ;
   UCHAR ucLVDSOffToOnDelay_in4Ms ;
   UCHAR ucLVDSPwrOnSeqVARY_BLtoBLON_in4Ms ;
   UCHAR ucLVDSPwrOffSeqBLONtoVARY_BL_in4Ms ;
   UCHAR ucMinAllowedBL_Level ;
   ULONG ulLCDBitDepthControlVal ;
   ULONG ulNbpStateMemclkFreq[4U] ;
   USHORT usNBP2Voltage ;
   USHORT usNBP3Voltage ;
   ULONG ulNbpStateNClkFreq[4U] ;
   UCHAR ucNBDPMEnable ;
   UCHAR ucReserved[3U] ;
   UCHAR ucDPMState0VclkFid ;
   UCHAR ucDPMState0DclkFid ;
   UCHAR ucDPMState1VclkFid ;
   UCHAR ucDPMState1DclkFid ;
   UCHAR ucDPMState2VclkFid ;
   UCHAR ucDPMState2DclkFid ;
   UCHAR ucDPMState3VclkFid ;
   UCHAR ucDPMState3DclkFid ;
   ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO sExtDispConnInfo ;
};
struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulDentistVCOFreq ;
   ULONG ulBootUpUMAClock ;
   ATOM_CLK_VOLT_CAPABILITY sDISPCLK_Voltage[4U] ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulVBIOSMisc ;
   ULONG ulGPUCapInfo ;
   ULONG ulDISP_CLK2Freq ;
   USHORT usRequestedPWMFreqInHz ;
   UCHAR ucHtcTmpLmt ;
   UCHAR ucHtcHystLmt ;
   ULONG ulReserved2 ;
   ULONG ulSystemConfig ;
   ULONG ulCPUCapInfo ;
   ULONG ulReserved3 ;
   USHORT usGPUReservedSysMemSize ;
   USHORT usExtDispConnInfoOffset ;
   USHORT usPanelRefreshRateRange ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   UCHAR strVBIOSMsg[40U] ;
   ATOM_TDP_CONFIG asTdpConfig ;
   ULONG ulReserved[19U] ;
   ATOM_AVAILABLE_SCLK_LIST sAvail_SCLK[5U] ;
   ULONG ulGMCRestoreResetTime ;
   ULONG ulReserved4 ;
   ULONG ulIdleNClk ;
   ULONG ulDDR_DLL_PowerUpTime ;
   ULONG ulDDR_PLL_PowerUpTime ;
   USHORT usPCIEClkSSPercentage ;
   USHORT usPCIEClkSSType ;
   USHORT usLvdsSSPercentage ;
   USHORT usLvdsSSpreadRateIn10Hz ;
   USHORT usHDMISSPercentage ;
   USHORT usHDMISSpreadRateIn10Hz ;
   USHORT usDVISSPercentage ;
   USHORT usDVISSpreadRateIn10Hz ;
   ULONG ulGPUReservedSysMemBaseAddrLo ;
   ULONG ulGPUReservedSysMemBaseAddrHi ;
   ATOM_CLK_VOLT_CAPABILITY s5thDISPCLK_Voltage ;
   ULONG ulReserved5 ;
   USHORT usMaxLVDSPclkFreqInSingleLink ;
   UCHAR ucLvdsMisc ;
   UCHAR ucTravisLVDSVolAdjust ;
   UCHAR ucLVDSPwrOnSeqDIGONtoDE_in4Ms ;
   UCHAR ucLVDSPwrOnSeqDEtoVARY_BL_in4Ms ;
   UCHAR ucLVDSPwrOffSeqVARY_BLtoDE_in4Ms ;
   UCHAR ucLVDSPwrOffSeqDEtoDIGON_in4Ms ;
   UCHAR ucLVDSOffToOnDelay_in4Ms ;
   UCHAR ucLVDSPwrOnSeqVARY_BLtoBLON_in4Ms ;
   UCHAR ucLVDSPwrOffSeqBLONtoVARY_BL_in4Ms ;
   UCHAR ucMinAllowedBL_Level ;
   ULONG ulLCDBitDepthControlVal ;
   ULONG ulNbpStateMemclkFreq[4U] ;
   ULONG ulPSPVersion ;
   ULONG ulNbpStateNClkFreq[4U] ;
   USHORT usNBPStateVoltage[4U] ;
   USHORT usBootUpNBVoltage ;
   USHORT usReserved2 ;
   ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO sExtDispConnInfo ;
};
struct _ATOM_I2C_REG_INFO {
   UCHAR ucI2cRegIndex ;
   UCHAR ucI2cRegVal ;
};
typedef struct _ATOM_I2C_REG_INFO ATOM_I2C_REG_INFO;
struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulDentistVCOFreq ;
   ULONG ulBootUpUMAClock ;
   ATOM_CLK_VOLT_CAPABILITY sDISPCLK_Voltage[4U] ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulVBIOSMisc ;
   ULONG ulGPUCapInfo ;
   ULONG ulDISP_CLK2Freq ;
   USHORT usRequestedPWMFreqInHz ;
   UCHAR ucHtcTmpLmt ;
   UCHAR ucHtcHystLmt ;
   ULONG ulReserved2 ;
   ULONG ulSystemConfig ;
   ULONG ulCPUCapInfo ;
   ULONG ulReserved3 ;
   USHORT usGPUReservedSysMemSize ;
   USHORT usExtDispConnInfoOffset ;
   USHORT usPanelRefreshRateRange ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   UCHAR strVBIOSMsg[40U] ;
   ATOM_TDP_CONFIG asTdpConfig ;
   UCHAR ucExtHDMIReDrvSlvAddr ;
   UCHAR ucExtHDMIReDrvRegNum ;
   ATOM_I2C_REG_INFO asExtHDMIRegSetting[9U] ;
   ULONG ulReserved[2U] ;
   ATOM_CLK_VOLT_CAPABILITY_V2 sDispClkVoltageMapping[8U] ;
   ATOM_AVAILABLE_SCLK_LIST sAvail_SCLK[5U] ;
   ULONG ulGMCRestoreResetTime ;
   ULONG ulReserved4 ;
   ULONG ulIdleNClk ;
   ULONG ulDDR_DLL_PowerUpTime ;
   ULONG ulDDR_PLL_PowerUpTime ;
   USHORT usPCIEClkSSPercentage ;
   USHORT usPCIEClkSSType ;
   USHORT usLvdsSSPercentage ;
   USHORT usLvdsSSpreadRateIn10Hz ;
   USHORT usHDMISSPercentage ;
   USHORT usHDMISSpreadRateIn10Hz ;
   USHORT usDVISSPercentage ;
   USHORT usDVISSpreadRateIn10Hz ;
   ULONG ulGPUReservedSysMemBaseAddrLo ;
   ULONG ulGPUReservedSysMemBaseAddrHi ;
   ULONG ulReserved5[3U] ;
   USHORT usMaxLVDSPclkFreqInSingleLink ;
   UCHAR ucLvdsMisc ;
   UCHAR ucTravisLVDSVolAdjust ;
   UCHAR ucLVDSPwrOnSeqDIGONtoDE_in4Ms ;
   UCHAR ucLVDSPwrOnSeqDEtoVARY_BL_in4Ms ;
   UCHAR ucLVDSPwrOffSeqVARY_BLtoDE_in4Ms ;
   UCHAR ucLVDSPwrOffSeqDEtoDIGON_in4Ms ;
   UCHAR ucLVDSOffToOnDelay_in4Ms ;
   UCHAR ucLVDSPwrOnSeqVARY_BLtoBLON_in4Ms ;
   UCHAR ucLVDSPwrOffSeqBLONtoVARY_BL_in4Ms ;
   UCHAR ucMinAllowedBL_Level ;
   ULONG ulLCDBitDepthControlVal ;
   ULONG ulNbpStateMemclkFreq[4U] ;
   ULONG ulPSPVersion ;
   ULONG ulNbpStateNClkFreq[4U] ;
   USHORT usNBPStateVoltage[4U] ;
   USHORT usBootUpNBVoltage ;
   UCHAR ucEDPv1_4VSMode ;
   UCHAR ucReserved2 ;
   ATOM_EXTERNAL_DISPLAY_CONNECTION_INFO sExtDispConnInfo ;
};
struct _ATOM_ASIC_SS_ASSIGNMENT {
   ULONG ulTargetClockRange ;
   USHORT usSpreadSpectrumPercentage ;
   USHORT usSpreadRateInKhz ;
   UCHAR ucClockIndication ;
   UCHAR ucSpreadSpectrumMode ;
   UCHAR ucReserved[2U] ;
};
typedef struct _ATOM_ASIC_SS_ASSIGNMENT ATOM_ASIC_SS_ASSIGNMENT;
struct _ATOM_ASIC_SS_ASSIGNMENT_V2 {
   ULONG ulTargetClockRange ;
   USHORT usSpreadSpectrumPercentage ;
   USHORT usSpreadRateIn10Hz ;
   UCHAR ucClockIndication ;
   UCHAR ucSpreadSpectrumMode ;
   UCHAR ucReserved[2U] ;
};
typedef struct _ATOM_ASIC_SS_ASSIGNMENT_V2 ATOM_ASIC_SS_ASSIGNMENT_V2;
struct _ATOM_ASIC_INTERNAL_SS_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_ASIC_SS_ASSIGNMENT asSpreadSpectrum[4U] ;
};
struct _ATOM_ASIC_INTERNAL_SS_INFO_V2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_ASIC_SS_ASSIGNMENT_V2 asSpreadSpectrum[1U] ;
};
struct _ATOM_ASIC_SS_ASSIGNMENT_V3 {
   ULONG ulTargetClockRange ;
   USHORT usSpreadSpectrumPercentage ;
   USHORT usSpreadRateIn10Hz ;
   UCHAR ucClockIndication ;
   UCHAR ucSpreadSpectrumMode ;
   UCHAR ucReserved[2U] ;
};
typedef struct _ATOM_ASIC_SS_ASSIGNMENT_V3 ATOM_ASIC_SS_ASSIGNMENT_V3;
struct _ATOM_ASIC_INTERNAL_SS_INFO_V3 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_ASIC_SS_ASSIGNMENT_V3 asSpreadSpectrum[1U] ;
};
struct _ATOM_MEMORY_SETTING_ID_CONFIG {
   unsigned int ulMemClockRange : 24 ;
   unsigned char ucMemBlkId ;
};
typedef struct _ATOM_MEMORY_SETTING_ID_CONFIG ATOM_MEMORY_SETTING_ID_CONFIG;
union _ATOM_MEMORY_SETTING_ID_CONFIG_ACCESS {
   ATOM_MEMORY_SETTING_ID_CONFIG slAccess ;
   ULONG ulAccess ;
};
typedef union _ATOM_MEMORY_SETTING_ID_CONFIG_ACCESS ATOM_MEMORY_SETTING_ID_CONFIG_ACCESS;
struct _ATOM_MEMORY_SETTING_DATA_BLOCK {
   ATOM_MEMORY_SETTING_ID_CONFIG_ACCESS ulMemoryID ;
   ULONG aulMemData[1U] ;
};
typedef struct _ATOM_MEMORY_SETTING_DATA_BLOCK ATOM_MEMORY_SETTING_DATA_BLOCK;
struct _ATOM_INIT_REG_INDEX_FORMAT {
   USHORT usRegIndex ;
   UCHAR ucPreRegDataLength ;
};
typedef struct _ATOM_INIT_REG_INDEX_FORMAT ATOM_INIT_REG_INDEX_FORMAT;
struct _ATOM_INIT_REG_BLOCK {
   USHORT usRegIndexTblSize ;
   USHORT usRegDataBlkSize ;
   ATOM_INIT_REG_INDEX_FORMAT asRegIndexBuf[1U] ;
   ATOM_MEMORY_SETTING_DATA_BLOCK asRegDataBuf[1U] ;
};
typedef struct _ATOM_INIT_REG_BLOCK ATOM_INIT_REG_BLOCK;
union __anonunion____missing_field_name_305 {
   USHORT usMRS ;
   USHORT usDDR3_MR0 ;
};
union __anonunion____missing_field_name_306 {
   USHORT usEMRS ;
   USHORT usDDR3_MR1 ;
};
struct __anonstruct____missing_field_name_308 {
   UCHAR ucflag ;
   UCHAR ucReserved ;
};
union __anonunion____missing_field_name_307 {
   struct __anonstruct____missing_field_name_308 __annonCompField114 ;
   USHORT usDDR3_MR2 ;
};
struct _ATOM_MEMORY_TIMING_FORMAT {
   ULONG ulClkRange ;
   union __anonunion____missing_field_name_305 __annonCompField112 ;
   union __anonunion____missing_field_name_306 __annonCompField113 ;
   UCHAR ucCL ;
   UCHAR ucWL ;
   UCHAR uctRAS ;
   UCHAR uctRC ;
   UCHAR uctRFC ;
   UCHAR uctRCDR ;
   UCHAR uctRCDW ;
   UCHAR uctRP ;
   UCHAR uctRRD ;
   UCHAR uctWR ;
   UCHAR uctWTR ;
   UCHAR uctPDIX ;
   UCHAR uctFAW ;
   UCHAR uctAOND ;
   union __anonunion____missing_field_name_307 __annonCompField115 ;
};
typedef struct _ATOM_MEMORY_TIMING_FORMAT ATOM_MEMORY_TIMING_FORMAT;
union __anonunion____missing_field_name_309 {
   USHORT usEMRS2Value ;
   USHORT usDDR3_Reserved ;
};
union __anonunion____missing_field_name_310 {
   USHORT usEMRS3Value ;
   USHORT usDDR3_MR3 ;
};
struct _ATOM_MEMORY_FORMAT {
   ULONG ulDllDisClock ;
   union __anonunion____missing_field_name_309 __annonCompField116 ;
   union __anonunion____missing_field_name_310 __annonCompField117 ;
   UCHAR ucMemoryType ;
   UCHAR ucMemoryVenderID ;
   UCHAR ucRow ;
   UCHAR ucColumn ;
   UCHAR ucBank ;
   UCHAR ucRank ;
   UCHAR ucBurstSize ;
   UCHAR ucDllDisBit ;
   UCHAR ucRefreshRateFactor ;
   UCHAR ucDensity ;
   UCHAR ucPreamble ;
   UCHAR ucMemAttrib ;
   ATOM_MEMORY_TIMING_FORMAT asMemTiming[5U] ;
};
typedef struct _ATOM_MEMORY_FORMAT ATOM_MEMORY_FORMAT;
struct _ATOM_VRAM_MODULE_V3 {
   ULONG ulChannelMapCfg ;
   USHORT usSize ;
   USHORT usDefaultMVDDQ ;
   USHORT usDefaultMVDDC ;
   UCHAR ucExtMemoryID ;
   UCHAR ucChannelNum ;
   UCHAR ucChannelSize ;
   UCHAR ucVREFI ;
   UCHAR ucNPL_RT ;
   UCHAR ucFlag ;
   ATOM_MEMORY_FORMAT asMemory ;
};
typedef struct _ATOM_VRAM_MODULE_V3 ATOM_VRAM_MODULE_V3;
union __anonunion____missing_field_name_311 {
   USHORT usEMRS2Value ;
   USHORT usDDR3_Reserved ;
};
union __anonunion____missing_field_name_312 {
   USHORT usEMRS3Value ;
   USHORT usDDR3_MR3 ;
};
struct _ATOM_VRAM_MODULE_V4 {
   ULONG ulChannelMapCfg ;
   USHORT usModuleSize ;
   USHORT usPrivateReserved ;
   USHORT usReserved ;
   UCHAR ucExtMemoryID ;
   UCHAR ucMemoryType ;
   UCHAR ucChannelNum ;
   UCHAR ucChannelWidth ;
   UCHAR ucDensity ;
   UCHAR ucFlag ;
   UCHAR ucMisc ;
   UCHAR ucVREFI ;
   UCHAR ucNPL_RT ;
   UCHAR ucPreamble ;
   UCHAR ucMemorySize ;
   UCHAR ucReserved[3U] ;
   union __anonunion____missing_field_name_311 __annonCompField118 ;
   union __anonunion____missing_field_name_312 __annonCompField119 ;
   UCHAR ucMemoryVenderID ;
   UCHAR ucRefreshRateFactor ;
   UCHAR ucReserved2[2U] ;
   ATOM_MEMORY_TIMING_FORMAT asMemTiming[5U] ;
};
typedef struct _ATOM_VRAM_MODULE_V4 ATOM_VRAM_MODULE_V4;
struct _ATOM_VRAM_MODULE_V7 {
   ULONG ulChannelMapCfg ;
   USHORT usModuleSize ;
   USHORT usPrivateReserved ;
   USHORT usEnableChannels ;
   UCHAR ucExtMemoryID ;
   UCHAR ucMemoryType ;
   UCHAR ucChannelNum ;
   UCHAR ucChannelWidth ;
   UCHAR ucDensity ;
   UCHAR ucReserve ;
   UCHAR ucMisc ;
   UCHAR ucVREFI ;
   UCHAR ucNPL_RT ;
   UCHAR ucPreamble ;
   UCHAR ucMemorySize ;
   USHORT usSEQSettingOffset ;
   UCHAR ucReserved ;
   USHORT usEMRS2Value ;
   USHORT usEMRS3Value ;
   UCHAR ucMemoryVenderID ;
   UCHAR ucRefreshRateFactor ;
   UCHAR ucFIFODepth ;
   UCHAR ucCDR_Bandwidth ;
   char strMemPNString[20U] ;
};
typedef struct _ATOM_VRAM_MODULE_V7 ATOM_VRAM_MODULE_V7;
struct _ATOM_VRAM_INFO_V3 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   USHORT usMemAdjustTblOffset ;
   USHORT usMemClkPatchTblOffset ;
   USHORT usRerseved ;
   UCHAR aVID_PinsShift[9U] ;
   UCHAR ucNumOfVRAMModule ;
   ATOM_VRAM_MODULE_V3 aVramInfo[16U] ;
   ATOM_INIT_REG_BLOCK asMemPatch ;
};
struct _ATOM_VRAM_INFO_V4 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   USHORT usMemAdjustTblOffset ;
   USHORT usMemClkPatchTblOffset ;
   USHORT usRerseved ;
   UCHAR ucMemDQ7_0ByteRemap ;
   ULONG ulMemDQ7_0BitRemap ;
   UCHAR ucReservde[4U] ;
   UCHAR ucNumOfVRAMModule ;
   ATOM_VRAM_MODULE_V4 aVramInfo[16U] ;
   ATOM_INIT_REG_BLOCK asMemPatch ;
};
struct _ATOM_VRAM_INFO_HEADER_V2_1 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   USHORT usMemAdjustTblOffset ;
   USHORT usMemClkPatchTblOffset ;
   USHORT usPerBytePresetOffset ;
   USHORT usReserved[3U] ;
   UCHAR ucNumOfVRAMModule ;
   UCHAR ucMemoryClkPatchTblVer ;
   UCHAR ucVramModuleVer ;
   UCHAR ucReserved ;
   ATOM_VRAM_MODULE_V7 aVramInfo[16U] ;
};
union firmware_info {
   ATOM_FIRMWARE_INFO info ;
   ATOM_FIRMWARE_INFO_V1_2 info_12 ;
   ATOM_FIRMWARE_INFO_V1_3 info_13 ;
   ATOM_FIRMWARE_INFO_V1_4 info_14 ;
   ATOM_FIRMWARE_INFO_V2_1 info_21 ;
   ATOM_FIRMWARE_INFO_V2_2 info_22 ;
};
union igp_info {
   struct _ATOM_INTEGRATED_SYSTEM_INFO info ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9 ;
};
union asic_ss_info {
   struct _ATOM_ASIC_INTERNAL_SS_INFO info ;
   struct _ATOM_ASIC_INTERNAL_SS_INFO_V2 info_2 ;
   struct _ATOM_ASIC_INTERNAL_SS_INFO_V3 info_3 ;
};
union asic_ss_assignment {
   struct _ATOM_ASIC_SS_ASSIGNMENT v1 ;
   struct _ATOM_ASIC_SS_ASSIGNMENT_V2 v2 ;
   struct _ATOM_ASIC_SS_ASSIGNMENT_V3 v3 ;
};
union get_clock_dividers {
   struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS v1 ;
   struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V2 v2 ;
   struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V3 v3 ;
   struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V4 v4 ;
   struct _COMPUTE_MEMORY_ENGINE_PLL_PARAMETERS_V5 v5 ;
   struct _COMPUTE_GPU_CLOCK_INPUT_PARAMETERS_V1_6 v6_in ;
   struct _COMPUTE_GPU_CLOCK_OUTPUT_PARAMETERS_V1_6 v6_out ;
};
union set_voltage {
   struct _SET_VOLTAGE_PS_ALLOCATION alloc ;
   struct _SET_VOLTAGE_PARAMETERS v1 ;
   struct _SET_VOLTAGE_PARAMETERS_V2 v2 ;
   struct _SET_VOLTAGE_PARAMETERS_V1_3 v3 ;
};
union get_voltage_info {
   struct _GET_VOLTAGE_INFO_INPUT_PARAMETER_V1_2 in ;
   struct _GET_EVV_VOLTAGE_INFO_OUTPUT_PARAMETER_V1_2 evv_out ;
};
union voltage_object_info {
   struct _ATOM_VOLTAGE_OBJECT_INFO v1 ;
   struct _ATOM_VOLTAGE_OBJECT_INFO_V2 v2 ;
   struct _ATOM_VOLTAGE_OBJECT_INFO_V3_1 v3 ;
};
union voltage_object {
   struct _ATOM_VOLTAGE_OBJECT v1 ;
   struct _ATOM_VOLTAGE_OBJECT_V2 v2 ;
   union _ATOM_VOLTAGE_OBJECT_V3 v3 ;
};
union vram_info {
   struct _ATOM_VRAM_INFO_V3 v1_3 ;
   struct _ATOM_VRAM_INFO_V4 v1_4 ;
   struct _ATOM_VRAM_INFO_HEADER_V2_1 v2_1 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_encoder_atom_dig {
   bool linkb ;
   bool coherent_mode ;
   int dig_encoder ;
   u32 lcd_misc ;
   u16 panel_pwr_delay ;
   u32 lcd_ss_id ;
   struct drm_display_mode native_mode ;
   struct backlight_device *bl_dev ;
   int dpms_mode ;
   uint8_t backlight_level ;
   int panel_mode ;
   struct amdgpu_afmt *afmt ;
};
struct amdgpu_connector_atom_dig {
   u8 dpcd[15U] ;
   u8 dp_sink_type ;
   int dp_clock ;
   int dp_lane_count ;
   bool edp_on ;
};
enum amdgpu_connector_audio {
    AMDGPU_AUDIO_DISABLE = 0,
    AMDGPU_AUDIO_ENABLE = 1,
    AMDGPU_AUDIO_AUTO = 2
} ;
enum amdgpu_connector_dither {
    AMDGPU_FMT_DITHER_DISABLE = 0,
    AMDGPU_FMT_DITHER_ENABLE = 1
} ;
struct amdgpu_connector {
   struct drm_connector base ;
   u32 connector_id ;
   u32 devices ;
   struct amdgpu_i2c_chan *ddc_bus ;
   bool shared_ddc ;
   bool use_digital ;
   struct edid *edid ;
   void *con_priv ;
   bool dac_load_detect ;
   bool detected_by_load ;
   u16 connector_object_id ;
   struct amdgpu_hpd hpd ;
   struct amdgpu_router router ;
   struct amdgpu_i2c_chan *router_bus ;
   enum amdgpu_connector_audio audio ;
   enum amdgpu_connector_dither dither ;
   unsigned int pixelclock_for_modeset ;
};
struct _ENABLE_DISP_POWER_GATING_PARAMETERS_V2_1 {
   UCHAR ucDispPipeId ;
   UCHAR ucEnable ;
   UCHAR ucPadding[2U] ;
};
typedef struct _ENABLE_DISP_POWER_GATING_PARAMETERS_V2_1 ENABLE_DISP_POWER_GATING_PARAMETERS_V2_1;
struct _BLANK_CRTC_PARAMETERS {
   UCHAR ucCRTC ;
   UCHAR ucBlanking ;
   USHORT usBlackColorRCr ;
   USHORT usBlackColorGY ;
   USHORT usBlackColorBCb ;
};
typedef struct _BLANK_CRTC_PARAMETERS BLANK_CRTC_PARAMETERS;
struct _ENABLE_CRTC_PARAMETERS {
   UCHAR ucCRTC ;
   UCHAR ucEnable ;
   UCHAR ucPadding[2U] ;
};
typedef struct _ENABLE_CRTC_PARAMETERS ENABLE_CRTC_PARAMETERS;
struct _SET_CRTC_OVERSCAN_PARAMETERS {
   USHORT usOverscanRight ;
   USHORT usOverscanLeft ;
   USHORT usOverscanBottom ;
   USHORT usOverscanTop ;
   UCHAR ucCRTC ;
   UCHAR ucPadding[3U] ;
};
typedef struct _SET_CRTC_OVERSCAN_PARAMETERS SET_CRTC_OVERSCAN_PARAMETERS;
struct _PIXEL_CLOCK_PARAMETERS {
   USHORT usPixelClock ;
   USHORT usRefDiv ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucFracFbDiv ;
   UCHAR ucPpll ;
   UCHAR ucRefDivSrc ;
   UCHAR ucCRTC ;
   UCHAR ucPadding ;
};
typedef struct _PIXEL_CLOCK_PARAMETERS PIXEL_CLOCK_PARAMETERS;
struct _PIXEL_CLOCK_PARAMETERS_V2 {
   USHORT usPixelClock ;
   USHORT usRefDiv ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucFracFbDiv ;
   UCHAR ucPpll ;
   UCHAR ucRefDivSrc ;
   UCHAR ucCRTC ;
   UCHAR ucMiscInfo ;
};
typedef struct _PIXEL_CLOCK_PARAMETERS_V2 PIXEL_CLOCK_PARAMETERS_V2;
union __anonunion____missing_field_name_291 {
   UCHAR ucEncoderMode ;
   UCHAR ucDVOConfig ;
};
struct _PIXEL_CLOCK_PARAMETERS_V3 {
   USHORT usPixelClock ;
   USHORT usRefDiv ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucFracFbDiv ;
   UCHAR ucPpll ;
   UCHAR ucTransmitterId ;
   union __anonunion____missing_field_name_291 __annonCompField98 ;
   UCHAR ucMiscInfo ;
};
typedef struct _PIXEL_CLOCK_PARAMETERS_V3 PIXEL_CLOCK_PARAMETERS_V3;
union __anonunion____missing_field_name_292 {
   UCHAR ucReserved ;
   UCHAR ucFracFbDiv ;
};
struct _PIXEL_CLOCK_PARAMETERS_V5 {
   UCHAR ucCRTC ;
   union __anonunion____missing_field_name_292 __annonCompField99 ;
   USHORT usPixelClock ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucRefDiv ;
   UCHAR ucPpll ;
   UCHAR ucTransmitterID ;
   UCHAR ucEncoderMode ;
   UCHAR ucMiscInfo ;
   ULONG ulFbDivDecFrac ;
};
typedef struct _PIXEL_CLOCK_PARAMETERS_V5 PIXEL_CLOCK_PARAMETERS_V5;
struct _CRTC_PIXEL_CLOCK_FREQ {
   unsigned int ulPixelClock : 24 ;
   unsigned char ucCRTC ;
};
typedef struct _CRTC_PIXEL_CLOCK_FREQ CRTC_PIXEL_CLOCK_FREQ;
union __anonunion____missing_field_name_293 {
   CRTC_PIXEL_CLOCK_FREQ ulCrtcPclkFreq ;
   ULONG ulDispEngClkFreq ;
};
struct _PIXEL_CLOCK_PARAMETERS_V6 {
   union __anonunion____missing_field_name_293 __annonCompField100 ;
   USHORT usFbDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucRefDiv ;
   UCHAR ucPpll ;
   UCHAR ucTransmitterID ;
   UCHAR ucEncoderMode ;
   UCHAR ucMiscInfo ;
   ULONG ulFbDivDecFrac ;
};
typedef struct _PIXEL_CLOCK_PARAMETERS_V6 PIXEL_CLOCK_PARAMETERS_V6;
union __anonunion____missing_field_name_294 {
   UCHAR ucDVOConfig ;
   UCHAR ucConfig ;
};
struct _ADJUST_DISPLAY_PLL_PARAMETERS {
   USHORT usPixelClock ;
   UCHAR ucTransmitterID ;
   UCHAR ucEncodeMode ;
   union __anonunion____missing_field_name_294 __annonCompField101 ;
   UCHAR ucReserved[3U] ;
};
typedef struct _ADJUST_DISPLAY_PLL_PARAMETERS ADJUST_DISPLAY_PLL_PARAMETERS;
struct _ADJUST_DISPLAY_PLL_INPUT_PARAMETERS_V3 {
   USHORT usPixelClock ;
   UCHAR ucTransmitterID ;
   UCHAR ucEncodeMode ;
   UCHAR ucDispPllConfig ;
   UCHAR ucExtTransmitterID ;
   UCHAR ucReserved[2U] ;
};
typedef struct _ADJUST_DISPLAY_PLL_INPUT_PARAMETERS_V3 ADJUST_DISPLAY_PLL_INPUT_PARAMETERS_V3;
struct _ADJUST_DISPLAY_PLL_OUTPUT_PARAMETERS_V3 {
   ULONG ulDispPllFreq ;
   UCHAR ucRefDiv ;
   UCHAR ucPostDiv ;
   UCHAR ucReserved[2U] ;
};
typedef struct _ADJUST_DISPLAY_PLL_OUTPUT_PARAMETERS_V3 ADJUST_DISPLAY_PLL_OUTPUT_PARAMETERS_V3;
union __anonunion____missing_field_name_295 {
   ADJUST_DISPLAY_PLL_INPUT_PARAMETERS_V3 sInput ;
   ADJUST_DISPLAY_PLL_OUTPUT_PARAMETERS_V3 sOutput ;
};
struct _ADJUST_DISPLAY_PLL_PS_ALLOCATION_V3 {
   union __anonunion____missing_field_name_295 __annonCompField102 ;
};
typedef struct _ADJUST_DISPLAY_PLL_PS_ALLOCATION_V3 ADJUST_DISPLAY_PLL_PS_ALLOCATION_V3;
struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL {
   USHORT usSpreadSpectrumPercentage ;
   UCHAR ucSpreadSpectrumType ;
   UCHAR ucSpreadSpectrumStep ;
   UCHAR ucEnable ;
   UCHAR ucSpreadSpectrumDelay ;
   UCHAR ucSpreadSpectrumRange ;
   UCHAR ucPpll ;
};
typedef struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL ENABLE_SPREAD_SPECTRUM_ON_PPLL;
struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL_V2 {
   USHORT usSpreadSpectrumPercentage ;
   UCHAR ucSpreadSpectrumType ;
   UCHAR ucEnable ;
   USHORT usSpreadSpectrumAmount ;
   USHORT usSpreadSpectrumStep ;
};
typedef struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL_V2 ENABLE_SPREAD_SPECTRUM_ON_PPLL_V2;
struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL_V3 {
   USHORT usSpreadSpectrumAmountFrac ;
   UCHAR ucSpreadSpectrumType ;
   UCHAR ucEnable ;
   USHORT usSpreadSpectrumAmount ;
   USHORT usSpreadSpectrumStep ;
};
typedef struct _ENABLE_SPREAD_SPECTRUM_ON_PPLL_V3 ENABLE_SPREAD_SPECTRUM_ON_PPLL_V3;
struct _SET_PIXEL_CLOCK_PS_ALLOCATION {
   PIXEL_CLOCK_PARAMETERS sPCLKInput ;
   ENABLE_SPREAD_SPECTRUM_ON_PPLL sReserved ;
};
typedef struct _SET_PIXEL_CLOCK_PS_ALLOCATION SET_PIXEL_CLOCK_PS_ALLOCATION;
struct _ATOM_MODE_MISC_INFO {
   unsigned char HorizontalCutOff : 1 ;
   unsigned char HSyncPolarity : 1 ;
   unsigned char VSyncPolarity : 1 ;
   unsigned char VerticalCutOff : 1 ;
   unsigned char H_ReplicationBy2 : 1 ;
   unsigned char V_ReplicationBy2 : 1 ;
   unsigned char CompositeSync : 1 ;
   unsigned char Interlace : 1 ;
   unsigned char DoubleClock : 1 ;
   unsigned char RGB888 : 1 ;
   unsigned char Reserved : 6 ;
};
typedef struct _ATOM_MODE_MISC_INFO ATOM_MODE_MISC_INFO;
union _ATOM_MODE_MISC_INFO_ACCESS {
   ATOM_MODE_MISC_INFO sbfAccess ;
   USHORT usAccess ;
};
typedef union _ATOM_MODE_MISC_INFO_ACCESS ATOM_MODE_MISC_INFO_ACCESS;
struct _SET_CRTC_USING_DTD_TIMING_PARAMETERS {
   USHORT usH_Size ;
   USHORT usH_Blanking_Time ;
   USHORT usV_Size ;
   USHORT usV_Blanking_Time ;
   USHORT usH_SyncOffset ;
   USHORT usH_SyncWidth ;
   USHORT usV_SyncOffset ;
   USHORT usV_SyncWidth ;
   ATOM_MODE_MISC_INFO_ACCESS susModeMiscInfo ;
   UCHAR ucH_Border ;
   UCHAR ucV_Border ;
   UCHAR ucCRTC ;
   UCHAR ucPadding[3U] ;
};
typedef struct _SET_CRTC_USING_DTD_TIMING_PARAMETERS SET_CRTC_USING_DTD_TIMING_PARAMETERS;
struct _ENABLE_SCALER_PARAMETERS {
   UCHAR ucScaler ;
   UCHAR ucEnable ;
   UCHAR ucTVStandard ;
   UCHAR ucPadding[1U] ;
};
typedef struct _ENABLE_SCALER_PARAMETERS ENABLE_SCALER_PARAMETERS;
union atom_enable_ss {
   ENABLE_SPREAD_SPECTRUM_ON_PPLL v1 ;
   ENABLE_SPREAD_SPECTRUM_ON_PPLL_V2 v2 ;
   ENABLE_SPREAD_SPECTRUM_ON_PPLL_V3 v3 ;
};
union adjust_pixel_clock {
   ADJUST_DISPLAY_PLL_PARAMETERS v1 ;
   ADJUST_DISPLAY_PLL_PS_ALLOCATION_V3 v3 ;
};
union set_pixel_clock {
   SET_PIXEL_CLOCK_PS_ALLOCATION base ;
   PIXEL_CLOCK_PARAMETERS v1 ;
   PIXEL_CLOCK_PARAMETERS_V2 v2 ;
   PIXEL_CLOCK_PARAMETERS_V3 v3 ;
   PIXEL_CLOCK_PARAMETERS_V5 v5 ;
   PIXEL_CLOCK_PARAMETERS_V6 v6 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_encoder_helper_funcs {
   void (*dpms)(struct drm_encoder * , int ) ;
   void (*save)(struct drm_encoder * ) ;
   void (*restore)(struct drm_encoder * ) ;
   bool (*mode_fixup)(struct drm_encoder * , struct drm_display_mode const * , struct drm_display_mode * ) ;
   void (*prepare)(struct drm_encoder * ) ;
   void (*commit)(struct drm_encoder * ) ;
   void (*mode_set)(struct drm_encoder * , struct drm_display_mode * , struct drm_display_mode * ) ;
   struct drm_crtc *(*get_crtc)(struct drm_encoder * ) ;
   enum drm_connector_status (*detect)(struct drm_encoder * , struct drm_connector * ) ;
   void (*disable)(struct drm_encoder * ) ;
   void (*enable)(struct drm_encoder * ) ;
   int (*atomic_check)(struct drm_encoder * , struct drm_crtc_state * , struct drm_connector_state * ) ;
};
struct drm_connector_helper_funcs {
   int (*get_modes)(struct drm_connector * ) ;
   enum drm_mode_status (*mode_valid)(struct drm_connector * , struct drm_display_mode * ) ;
   struct drm_encoder *(*best_encoder)(struct drm_connector * ) ;
};
struct mode_size {
   int w ;
   int h ;
};
typedef __u32 __le32;
struct paravirt_callee_save {
   void *func ;
};
struct pv_irq_ops {
   struct paravirt_callee_save save_fl ;
   struct paravirt_callee_save restore_fl ;
   struct paravirt_callee_save irq_disable ;
   struct paravirt_callee_save irq_enable ;
   void (*safe_halt)(void) ;
   void (*halt)(void) ;
   void (*adjust_exception_frame)(void) ;
};
enum hrtimer_restart;
struct _ATOM_FIRMWARE_VRAM_RESERVE_INFO {
   ULONG ulStartAddrUsedByFirmware ;
   USHORT usFirmwareUseInKb ;
   USHORT usReserved ;
};
typedef struct _ATOM_FIRMWARE_VRAM_RESERVE_INFO ATOM_FIRMWARE_VRAM_RESERVE_INFO;
struct _ATOM_VRAM_USAGE_BY_FIRMWARE {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_FIRMWARE_VRAM_RESERVE_INFO asFirmwareVramReserveInfo[1U] ;
};
struct __anonstruct_atom_exec_context_320 {
   struct atom_context *ctx ;
   u32 *ps ;
   u32 *ws ;
   int ps_shift ;
   u16 start ;
   unsigned int last_jump ;
   unsigned long last_jump_jiffies ;
   bool abort ;
};
typedef struct __anonstruct_atom_exec_context_320 atom_exec_context;
struct __anonstruct_opcode_table_321 {
   void (*func)(atom_exec_context * , int * , int ) ;
   int arg ;
};
struct static_key;
struct __anonstruct_mm_segment_t_33 {
   unsigned long seg ;
};
typedef struct __anonstruct_mm_segment_t_33 mm_segment_t;
struct thread_info {
   struct task_struct *task ;
   __u32 flags ;
   __u32 status ;
   __u32 cpu ;
   int saved_preempt_count ;
   mm_segment_t addr_limit ;
   void *sysenter_return ;
   unsigned char sig_on_uaccess_error : 1 ;
   unsigned char uaccess_err : 1 ;
};
typedef int pao_T__;
typedef int pao_T_____0;
struct static_key {
   atomic_t enabled ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_info_node {
   struct list_head list ;
   struct drm_minor *minor ;
   struct drm_info_list const *info_ent ;
   struct dentry *dent ;
};
struct fence_cb;
struct fence_cb {
   struct list_head node ;
   void (*func)(struct fence * , struct fence_cb * ) ;
};
struct tracepoint_func {
   void *func ;
   void *data ;
};
struct tracepoint {
   char const *name ;
   struct static_key key ;
   void (*regfunc)(void) ;
   void (*unregfunc)(void) ;
   struct tracepoint_func *funcs ;
};
struct trace_enum_map {
   char const *system ;
   char const *enum_string ;
   unsigned long enum_value ;
};
union __anonunion___u_311 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_313 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_315 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_317 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_319 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_321 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
struct amdgpu_wait_cb {
   struct fence_cb base ;
   struct task_struct *task ;
};
enum hrtimer_restart;
struct ttm_dma_tt {
   struct ttm_tt ttm ;
   void **cpu_address ;
   dma_addr_t *dma_address ;
   struct list_head pages_list ;
};
struct sg_page_iter {
   struct scatterlist *sg ;
   unsigned int sg_pgoffset ;
   unsigned int __nents ;
   int __pg_advance ;
};
enum i2c_slave_event;
enum i2c_slave_event;
struct taskstats {
   __u16 version ;
   __u32 ac_exitcode ;
   __u8 ac_flag ;
   __u8 ac_nice ;
   __u64 cpu_count ;
   __u64 cpu_delay_total ;
   __u64 blkio_count ;
   __u64 blkio_delay_total ;
   __u64 swapin_count ;
   __u64 swapin_delay_total ;
   __u64 cpu_run_real_total ;
   __u64 cpu_run_virtual_total ;
   char ac_comm[32U] ;
   __u8 ac_sched ;
   __u8 ac_pad[3U] ;
   __u32 ac_uid ;
   __u32 ac_gid ;
   __u32 ac_pid ;
   __u32 ac_ppid ;
   __u32 ac_btime ;
   __u64 ac_etime ;
   __u64 ac_utime ;
   __u64 ac_stime ;
   __u64 ac_minflt ;
   __u64 ac_majflt ;
   __u64 coremem ;
   __u64 virtmem ;
   __u64 hiwater_rss ;
   __u64 hiwater_vm ;
   __u64 read_char ;
   __u64 write_char ;
   __u64 read_syscalls ;
   __u64 write_syscalls ;
   __u64 read_bytes ;
   __u64 write_bytes ;
   __u64 cancelled_write_bytes ;
   __u64 nvcsw ;
   __u64 nivcsw ;
   __u64 ac_utimescaled ;
   __u64 ac_stimescaled ;
   __u64 cpu_scaled_run_real_total ;
   __u64 freepages_count ;
   __u64 freepages_delay_total ;
};
struct reclaim_state {
   unsigned long reclaimed_slab ;
};
struct swap_extent {
   struct list_head list ;
   unsigned long start_page ;
   unsigned long nr_pages ;
   sector_t start_block ;
};
struct swap_cluster_info {
   unsigned int data : 24 ;
   unsigned char flags ;
};
struct percpu_cluster {
   struct swap_cluster_info index ;
   unsigned int next ;
};
struct swap_info_struct {
   unsigned long flags ;
   short prio ;
   struct plist_node list ;
   struct plist_node avail_list ;
   signed char type ;
   unsigned int max ;
   unsigned char *swap_map ;
   struct swap_cluster_info *cluster_info ;
   struct swap_cluster_info free_cluster_head ;
   struct swap_cluster_info free_cluster_tail ;
   unsigned int lowest_bit ;
   unsigned int highest_bit ;
   unsigned int pages ;
   unsigned int inuse_pages ;
   unsigned int cluster_next ;
   unsigned int cluster_nr ;
   struct percpu_cluster *percpu_cluster ;
   struct swap_extent *curr_swap_extent ;
   struct swap_extent first_swap_extent ;
   struct block_device *bdev ;
   struct file *swap_file ;
   unsigned int old_block_size ;
   unsigned long *frontswap_map ;
   atomic_t frontswap_pages ;
   spinlock_t lock ;
   struct work_struct discard_work ;
   struct swap_cluster_info discard_cluster_head ;
   struct swap_cluster_info discard_cluster_tail ;
};
struct amdgpu_ttm_tt {
   struct ttm_dma_tt ttm ;
   struct amdgpu_device *adev ;
   u64 offset ;
   uint64_t userptr ;
   struct mm_struct *usermm ;
   u32 userflags ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
union __anonunion___u_275 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_277 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_prop_enum_list {
   int type ;
   char *name ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_fb_helper;
struct drm_fb_offset {
   int x ;
   int y ;
};
struct drm_fb_helper_crtc {
   struct drm_mode_set mode_set ;
   struct drm_display_mode *desired_mode ;
   int x ;
   int y ;
};
struct drm_fb_helper_surface_size {
   u32 fb_width ;
   u32 fb_height ;
   u32 surface_width ;
   u32 surface_height ;
   u32 surface_bpp ;
   u32 surface_depth ;
};
struct drm_fb_helper_funcs {
   void (*gamma_set)(struct drm_crtc * , u16 , u16 , u16 , int ) ;
   void (*gamma_get)(struct drm_crtc * , u16 * , u16 * , u16 * , int ) ;
   int (*fb_probe)(struct drm_fb_helper * , struct drm_fb_helper_surface_size * ) ;
   bool (*initial_config)(struct drm_fb_helper * , struct drm_fb_helper_crtc ** ,
                          struct drm_display_mode ** , struct drm_fb_offset * , bool * ,
                          int , int ) ;
};
struct drm_fb_helper_connector {
   struct drm_connector *connector ;
};
struct drm_fb_helper {
   struct drm_framebuffer *fb ;
   struct drm_device *dev ;
   int crtc_count ;
   struct drm_fb_helper_crtc *crtc_info ;
   int connector_count ;
   int connector_info_alloc_count ;
   struct drm_fb_helper_connector **connector_info ;
   struct drm_fb_helper_funcs const *funcs ;
   struct fb_info *fbdev ;
   u32 pseudo_palette[17U] ;
   struct list_head kernel_fb_list ;
   bool delayed_hotplug ;
};
struct amdgpu_fbdev {
   struct drm_fb_helper helper ;
   struct amdgpu_framebuffer rfb ;
   struct list_head fbdev_list ;
   struct amdgpu_device *adev ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_amdgpu_gem_create_in {
   uint64_t bo_size ;
   uint64_t alignment ;
   uint64_t domains ;
   uint64_t domain_flags ;
};
struct drm_amdgpu_gem_create_out {
   u32 handle ;
   u32 _pad ;
};
union drm_amdgpu_gem_create {
   struct drm_amdgpu_gem_create_in in ;
   struct drm_amdgpu_gem_create_out out ;
};
struct drm_amdgpu_gem_userptr {
   uint64_t addr ;
   uint64_t size ;
   u32 flags ;
   u32 handle ;
};
struct __anonstruct_data_260 {
   uint64_t flags ;
   uint64_t tiling_info ;
   u32 data_size_bytes ;
   u32 data[64U] ;
};
struct drm_amdgpu_gem_metadata {
   u32 handle ;
   u32 op ;
   struct __anonstruct_data_260 data ;
};
struct drm_amdgpu_gem_mmap_in {
   u32 handle ;
   u32 _pad ;
};
struct drm_amdgpu_gem_mmap_out {
   uint64_t addr_ptr ;
};
union drm_amdgpu_gem_mmap {
   struct drm_amdgpu_gem_mmap_in in ;
   struct drm_amdgpu_gem_mmap_out out ;
};
struct drm_amdgpu_gem_wait_idle_in {
   u32 handle ;
   u32 flags ;
   uint64_t timeout ;
};
struct drm_amdgpu_gem_wait_idle_out {
   u32 status ;
   u32 domain ;
};
union drm_amdgpu_gem_wait_idle {
   struct drm_amdgpu_gem_wait_idle_in in ;
   struct drm_amdgpu_gem_wait_idle_out out ;
};
struct drm_amdgpu_gem_op {
   u32 handle ;
   u32 op ;
   uint64_t value ;
};
struct drm_amdgpu_gem_va {
   u32 handle ;
   u32 _pad ;
   u32 operation ;
   u32 flags ;
   uint64_t va_address ;
   uint64_t offset_in_bo ;
   uint64_t map_size ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_amdgpu_wait_cs_in {
   uint64_t handle ;
   uint64_t timeout ;
   u32 ip_type ;
   u32 ip_instance ;
   u32 ring ;
   u32 ctx_id ;
};
struct drm_amdgpu_wait_cs_out {
   uint64_t status ;
};
union drm_amdgpu_wait_cs {
   struct drm_amdgpu_wait_cs_in in ;
   struct drm_amdgpu_wait_cs_out out ;
};
struct drm_amdgpu_cs_chunk {
   u32 chunk_id ;
   u32 length_dw ;
   uint64_t chunk_data ;
};
struct drm_amdgpu_cs_in {
   u32 ctx_id ;
   u32 bo_list_handle ;
   u32 num_chunks ;
   u32 _pad ;
   uint64_t chunks ;
};
struct drm_amdgpu_cs_out {
   uint64_t handle ;
};
union drm_amdgpu_cs {
   struct drm_amdgpu_cs_in in ;
   struct drm_amdgpu_cs_out out ;
};
struct drm_amdgpu_cs_chunk_ib {
   u32 _pad ;
   u32 flags ;
   uint64_t va_start ;
   u32 ib_bytes ;
   u32 ip_type ;
   u32 ip_instance ;
   u32 ring ;
};
struct drm_amdgpu_cs_chunk_dep {
   u32 ip_type ;
   u32 ip_instance ;
   u32 ring ;
   u32 ctx_id ;
   uint64_t handle ;
};
struct drm_amdgpu_cs_chunk_fence {
   u32 handle ;
   u32 offset ;
};
struct interval_tree_node {
   struct rb_node rb ;
   unsigned long start ;
   unsigned long last ;
   unsigned long __subtree_last ;
};
struct amdgpu_bo_va_mapping {
   struct list_head list ;
   struct interval_tree_node it ;
   uint64_t offset ;
   u32 flags ;
};
union __anonunion___u_279 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_281 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
struct amdgpu_cs_buckets {
   struct list_head bucket[33U] ;
};
enum hrtimer_restart;
struct acpi_device;
enum i2c_slave_event;
enum i2c_slave_event;
struct __anonstruct_AMD_ACPI_DESCRIPTION_HEADER_313 {
   ULONG Signature ;
   ULONG TableLength ;
   UCHAR Revision ;
   UCHAR Checksum ;
   UCHAR OemId[6U] ;
   UCHAR OemTableId[8U] ;
   ULONG OemRevision ;
   ULONG CreatorId ;
   ULONG CreatorRevision ;
};
typedef struct __anonstruct_AMD_ACPI_DESCRIPTION_HEADER_313 AMD_ACPI_DESCRIPTION_HEADER;
struct __anonstruct_UEFI_ACPI_VFCT_314 {
   AMD_ACPI_DESCRIPTION_HEADER SHeader ;
   UCHAR TableUUID[16U] ;
   ULONG VBIOSImageOffset ;
   ULONG Lib1ImageOffset ;
   ULONG Reserved[4U] ;
};
typedef struct __anonstruct_UEFI_ACPI_VFCT_314 UEFI_ACPI_VFCT;
struct __anonstruct_VFCT_IMAGE_HEADER_315 {
   ULONG PCIBus ;
   ULONG PCIDevice ;
   ULONG PCIFunction ;
   USHORT VendorID ;
   USHORT DeviceID ;
   USHORT SSVID ;
   USHORT SSID ;
   ULONG Revision ;
   ULONG ImageLength ;
};
typedef struct __anonstruct_VFCT_IMAGE_HEADER_315 VFCT_IMAGE_HEADER;
struct __anonstruct_GOP_VBIOS_CONTENT_316 {
   VFCT_IMAGE_HEADER VbiosHeader ;
   UCHAR VbiosContent[1U] ;
};
typedef struct __anonstruct_GOP_VBIOS_CONTENT_316 GOP_VBIOS_CONTENT;
typedef u64 acpi_size;
typedef u64 acpi_io_address;
typedef u32 acpi_status;
typedef char *acpi_string;
typedef void *acpi_handle;
typedef u32 acpi_object_type;
struct __anonstruct_integer_320 {
   acpi_object_type type ;
   u64 value ;
};
struct __anonstruct_string_321 {
   acpi_object_type type ;
   u32 length ;
   char *pointer ;
};
struct __anonstruct_buffer_322 {
   acpi_object_type type ;
   u32 length ;
   u8 *pointer ;
};
struct __anonstruct_package_323 {
   acpi_object_type type ;
   u32 count ;
   union acpi_object *elements ;
};
struct __anonstruct_reference_324 {
   acpi_object_type type ;
   acpi_object_type actual_type ;
   acpi_handle handle ;
};
struct __anonstruct_processor_325 {
   acpi_object_type type ;
   u32 proc_id ;
   acpi_io_address pblk_address ;
   u32 pblk_length ;
};
struct __anonstruct_power_resource_326 {
   acpi_object_type type ;
   u32 system_level ;
   u32 resource_order ;
};
union acpi_object {
   acpi_object_type type ;
   struct __anonstruct_integer_320 integer ;
   struct __anonstruct_string_321 string ;
   struct __anonstruct_buffer_322 buffer ;
   struct __anonstruct_package_323 package ;
   struct __anonstruct_reference_324 reference ;
   struct __anonstruct_processor_325 processor ;
   struct __anonstruct_power_resource_326 power_resource ;
};
struct acpi_object_list {
   u32 count ;
   union acpi_object *pointer ;
};
struct acpi_buffer {
   acpi_size length ;
   void *pointer ;
};
struct acpi_table_header {
   char signature[4U] ;
   u32 length ;
   u8 revision ;
   u8 checksum ;
   char oem_id[6U] ;
   char oem_table_id[8U] ;
   u32 oem_revision ;
   char asl_compiler_id[4U] ;
   u32 asl_compiler_revision ;
};
struct acpi_driver;
struct acpi_hotplug_profile {
   struct kobject kobj ;
   int (*scan_dependent)(struct acpi_device * ) ;
   void (*notify_online)(struct acpi_device * ) ;
   bool enabled ;
   bool demand_offline ;
};
struct acpi_scan_handler {
   struct acpi_device_id const *ids ;
   struct list_head list_node ;
   bool (*match)(char * , struct acpi_device_id const ** ) ;
   int (*attach)(struct acpi_device * , struct acpi_device_id const * ) ;
   void (*detach)(struct acpi_device * ) ;
   void (*bind)(struct device * ) ;
   void (*unbind)(struct device * ) ;
   struct acpi_hotplug_profile hotplug ;
};
struct acpi_hotplug_context {
   struct acpi_device *self ;
   int (*notify)(struct acpi_device * , u32 ) ;
   void (*uevent)(struct acpi_device * , u32 ) ;
   void (*fixup)(struct acpi_device * ) ;
};
struct acpi_device_ops {
   int (*add)(struct acpi_device * ) ;
   int (*remove)(struct acpi_device * ) ;
   void (*notify)(struct acpi_device * , u32 ) ;
};
struct acpi_driver {
   char name[80U] ;
   char class[80U] ;
   struct acpi_device_id const *ids ;
   unsigned int flags ;
   struct acpi_device_ops ops ;
   struct device_driver drv ;
   struct module *owner ;
};
struct acpi_device_status {
   unsigned char present : 1 ;
   unsigned char enabled : 1 ;
   unsigned char show_in_ui : 1 ;
   unsigned char functional : 1 ;
   unsigned char battery_present : 1 ;
   unsigned int reserved : 27 ;
};
struct acpi_device_flags {
   unsigned char dynamic_status : 1 ;
   unsigned char removable : 1 ;
   unsigned char ejectable : 1 ;
   unsigned char power_manageable : 1 ;
   unsigned char match_driver : 1 ;
   unsigned char initialized : 1 ;
   unsigned char visited : 1 ;
   unsigned char hotplug_notify : 1 ;
   unsigned char is_dock_station : 1 ;
   unsigned char of_compatible_ok : 1 ;
   unsigned char coherent_dma : 1 ;
   unsigned char cca_seen : 1 ;
   unsigned int reserved : 20 ;
};
struct acpi_device_dir {
   struct proc_dir_entry *entry ;
};
typedef char acpi_bus_id[8U];
typedef unsigned long acpi_bus_address;
typedef char acpi_device_name[40U];
typedef char acpi_device_class[20U];
struct acpi_pnp_type {
   unsigned char hardware_id : 1 ;
   unsigned char bus_address : 1 ;
   unsigned char platform_id : 1 ;
   unsigned int reserved : 29 ;
};
struct acpi_device_pnp {
   acpi_bus_id bus_id ;
   struct acpi_pnp_type type ;
   acpi_bus_address bus_address ;
   char *unique_id ;
   struct list_head ids ;
   acpi_device_name device_name ;
   acpi_device_class device_class ;
   union acpi_object *str_obj ;
};
struct acpi_device_power_flags {
   unsigned char explicit_get : 1 ;
   unsigned char power_resources : 1 ;
   unsigned char inrush_current : 1 ;
   unsigned char power_removed : 1 ;
   unsigned char ignore_parent : 1 ;
   unsigned char dsw_present : 1 ;
   unsigned int reserved : 26 ;
};
struct __anonstruct_flags_327 {
   unsigned char valid : 1 ;
   unsigned char explicit_set : 1 ;
   unsigned char reserved : 6 ;
};
struct acpi_device_power_state {
   struct __anonstruct_flags_327 flags ;
   int power ;
   int latency ;
   struct list_head resources ;
};
struct acpi_device_power {
   int state ;
   struct acpi_device_power_flags flags ;
   struct acpi_device_power_state states[5U] ;
};
struct acpi_device_perf_flags {
   u8 reserved ;
};
struct __anonstruct_flags_328 {
   unsigned char valid : 1 ;
   unsigned char reserved : 7 ;
};
struct acpi_device_perf_state {
   struct __anonstruct_flags_328 flags ;
   u8 power ;
   u8 performance ;
   int latency ;
};
struct acpi_device_perf {
   int state ;
   struct acpi_device_perf_flags flags ;
   int state_count ;
   struct acpi_device_perf_state *states ;
};
struct acpi_device_wakeup_flags {
   unsigned char valid : 1 ;
   unsigned char run_wake : 1 ;
   unsigned char notifier_present : 1 ;
   unsigned char enabled : 1 ;
};
struct acpi_device_wakeup_context {
   struct work_struct work ;
   struct device *dev ;
};
struct acpi_device_wakeup {
   acpi_handle gpe_device ;
   u64 gpe_number ;
   u64 sleep_state ;
   struct list_head resources ;
   struct acpi_device_wakeup_flags flags ;
   struct acpi_device_wakeup_context context ;
   struct wakeup_source *ws ;
   int prepare_count ;
};
struct acpi_device_data {
   union acpi_object const *pointer ;
   union acpi_object const *properties ;
   union acpi_object const *of_compatible ;
};
struct acpi_gpio_mapping;
struct acpi_device {
   int device_type ;
   acpi_handle handle ;
   struct fwnode_handle fwnode ;
   struct acpi_device *parent ;
   struct list_head children ;
   struct list_head node ;
   struct list_head wakeup_list ;
   struct list_head del_list ;
   struct acpi_device_status status ;
   struct acpi_device_flags flags ;
   struct acpi_device_pnp pnp ;
   struct acpi_device_power power ;
   struct acpi_device_wakeup wakeup ;
   struct acpi_device_perf performance ;
   struct acpi_device_dir dir ;
   struct acpi_device_data data ;
   struct acpi_scan_handler *handler ;
   struct acpi_hotplug_context *hp ;
   struct acpi_driver *driver ;
   struct acpi_gpio_mapping const *driver_gpios ;
   void *driver_data ;
   struct device dev ;
   unsigned int physical_node_count ;
   unsigned int dep_unmet ;
   struct list_head physical_node_list ;
   struct mutex physical_node_lock ;
   void (*remove)(struct acpi_device * ) ;
};
struct acpi_gpio_params {
   unsigned int crs_entry_index ;
   unsigned int line_index ;
   bool active_low ;
};
struct acpi_gpio_mapping {
   char const *name ;
   struct acpi_gpio_params const *data ;
   unsigned int size ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct sensor_device_attribute {
   struct device_attribute dev_attr ;
   int index ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
union __anonunion____missing_field_name_309___0 {
   UCHAR ucReplyStatus ;
   UCHAR ucDelay ;
};
struct _PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS {
   USHORT lpAuxRequest ;
   USHORT lpDataOut ;
   UCHAR ucChannelID ;
   union __anonunion____missing_field_name_309___0 __annonCompField116 ;
   UCHAR ucDataOutLen ;
   UCHAR ucReserved ;
};
typedef struct _PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS;
union __anonunion____missing_field_name_310___0 {
   UCHAR ucReplyStatus ;
   UCHAR ucDelay ;
};
struct _PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS_V2 {
   USHORT lpAuxRequest ;
   USHORT lpDataOut ;
   UCHAR ucChannelID ;
   union __anonunion____missing_field_name_310___0 __annonCompField117 ;
   UCHAR ucDataOutLen ;
   UCHAR ucHPD_ID ;
};
typedef struct _PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS_V2 PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS_V2;
union __anonunion____missing_field_name_311___0 {
   UCHAR ucConfig ;
   UCHAR ucI2cId ;
};
struct _DP_ENCODER_SERVICE_PARAMETERS {
   USHORT ucLinkClock ;
   union __anonunion____missing_field_name_311___0 __annonCompField118 ;
   UCHAR ucAction ;
   UCHAR ucStatus ;
   UCHAR ucLaneNum ;
   UCHAR ucReserved[2U] ;
};
typedef struct _DP_ENCODER_SERVICE_PARAMETERS DP_ENCODER_SERVICE_PARAMETERS;
union aux_channel_transaction {
   PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS v1 ;
   PROCESS_AUX_CHANNEL_TRANSACTION_PARAMETERS_V2 v2 ;
};
struct amdgpu_atombios_dp_link_train_info {
   struct amdgpu_device *adev ;
   struct drm_encoder *encoder ;
   struct drm_connector *connector ;
   int dp_clock ;
   int dp_lane_count ;
   bool tp3_supported ;
   u8 dpcd[15U] ;
   u8 train_set[4U] ;
   u8 link_status[6U] ;
   u8 tries ;
   struct drm_dp_aux *aux ;
};
struct amdgpu_afmt_acr {
   u32 clock ;
   int n_32khz ;
   int cts_32khz ;
   int n_44_1khz ;
   int cts_44_1khz ;
   int n_48khz ;
   int cts_48khz ;
};
enum hrtimer_restart;
struct uts_namespace;
struct net;
enum i2c_slave_event;
enum i2c_slave_event;
struct ring_buffer;
struct ring_buffer_iter;
struct trace_seq;
struct seq_buf {
   char *buffer ;
   size_t size ;
   size_t len ;
   loff_t readpos ;
};
struct trace_seq {
   unsigned char buffer[4096U] ;
   struct seq_buf seq ;
   int full ;
};
union __anonunion____missing_field_name_330 {
   __u64 sample_period ;
   __u64 sample_freq ;
};
union __anonunion____missing_field_name_331 {
   __u32 wakeup_events ;
   __u32 wakeup_watermark ;
};
union __anonunion____missing_field_name_332 {
   __u64 bp_addr ;
   __u64 config1 ;
};
union __anonunion____missing_field_name_333 {
   __u64 bp_len ;
   __u64 config2 ;
};
struct perf_event_attr {
   __u32 type ;
   __u32 size ;
   __u64 config ;
   union __anonunion____missing_field_name_330 __annonCompField81 ;
   __u64 sample_type ;
   __u64 read_format ;
   unsigned char disabled : 1 ;
   unsigned char inherit : 1 ;
   unsigned char pinned : 1 ;
   unsigned char exclusive : 1 ;
   unsigned char exclude_user : 1 ;
   unsigned char exclude_kernel : 1 ;
   unsigned char exclude_hv : 1 ;
   unsigned char exclude_idle : 1 ;
   unsigned char mmap : 1 ;
   unsigned char comm : 1 ;
   unsigned char freq : 1 ;
   unsigned char inherit_stat : 1 ;
   unsigned char enable_on_exec : 1 ;
   unsigned char task : 1 ;
   unsigned char watermark : 1 ;
   unsigned char precise_ip : 2 ;
   unsigned char mmap_data : 1 ;
   unsigned char sample_id_all : 1 ;
   unsigned char exclude_host : 1 ;
   unsigned char exclude_guest : 1 ;
   unsigned char exclude_callchain_kernel : 1 ;
   unsigned char exclude_callchain_user : 1 ;
   unsigned char mmap2 : 1 ;
   unsigned char comm_exec : 1 ;
   unsigned char use_clockid : 1 ;
   unsigned long __reserved_1 : 38 ;
   union __anonunion____missing_field_name_331 __annonCompField82 ;
   __u32 bp_type ;
   union __anonunion____missing_field_name_332 __annonCompField83 ;
   union __anonunion____missing_field_name_333 __annonCompField84 ;
   __u64 branch_sample_type ;
   __u64 sample_regs_user ;
   __u32 sample_stack_user ;
   __s32 clockid ;
   __u64 sample_regs_intr ;
   __u32 aux_watermark ;
   __u32 __reserved_2 ;
};
struct __anonstruct____missing_field_name_336 {
   unsigned char mem_op : 5 ;
   unsigned short mem_lvl : 14 ;
   unsigned char mem_snoop : 5 ;
   unsigned char mem_lock : 2 ;
   unsigned char mem_dtlb : 7 ;
   unsigned int mem_rsvd : 31 ;
};
union perf_mem_data_src {
   __u64 val ;
   struct __anonstruct____missing_field_name_336 __annonCompField87 ;
};
struct perf_branch_entry {
   __u64 from ;
   __u64 to ;
   unsigned char mispred : 1 ;
   unsigned char predicted : 1 ;
   unsigned char in_tx : 1 ;
   unsigned char abort : 1 ;
   unsigned long reserved : 60 ;
};
struct mnt_namespace;
struct ipc_namespace;
struct nsproxy {
   atomic_t count ;
   struct uts_namespace *uts_ns ;
   struct ipc_namespace *ipc_ns ;
   struct mnt_namespace *mnt_ns ;
   struct pid_namespace *pid_ns_for_children ;
   struct net *net_ns ;
};
struct proc_ns_operations;
struct ns_common {
   atomic_long_t stashed ;
   struct proc_ns_operations const *ops ;
   unsigned int inum ;
};
struct pidmap {
   atomic_t nr_free ;
   void *page ;
};
struct fs_pin;
struct pid_namespace {
   struct kref kref ;
   struct pidmap pidmap[128U] ;
   struct callback_head rcu ;
   int last_pid ;
   unsigned int nr_hashed ;
   struct task_struct *child_reaper ;
   struct kmem_cache *pid_cachep ;
   unsigned int level ;
   struct pid_namespace *parent ;
   struct vfsmount *proc_mnt ;
   struct dentry *proc_self ;
   struct dentry *proc_thread_self ;
   struct fs_pin *bacct ;
   struct user_namespace *user_ns ;
   struct work_struct proc_work ;
   kgid_t pid_gid ;
   int hide_pid ;
   int reboot ;
   struct ns_common ns ;
};
struct __anonstruct_local_t_344 {
   atomic_long_t a ;
};
typedef struct __anonstruct_local_t_344 local_t;
struct __anonstruct_local64_t_345 {
   local_t a ;
};
typedef struct __anonstruct_local64_t_345 local64_t;
struct arch_hw_breakpoint {
   unsigned long address ;
   unsigned long mask ;
   u8 len ;
   u8 type ;
};
struct pmu;
typedef s32 compat_time_t;
struct compat_timespec {
   compat_time_t tv_sec ;
   s32 tv_nsec ;
};
struct ftrace_hash;
struct ftrace_ops;
struct ftrace_ops_hash {
   struct ftrace_hash *notrace_hash ;
   struct ftrace_hash *filter_hash ;
   struct mutex regex_lock ;
};
struct ftrace_ops {
   void (*func)(unsigned long , unsigned long , struct ftrace_ops * , struct pt_regs * ) ;
   struct ftrace_ops *next ;
   unsigned long flags ;
   void *private ;
   int *disabled ;
   int nr_trampolines ;
   struct ftrace_ops_hash local_hash ;
   struct ftrace_ops_hash *func_hash ;
   struct ftrace_ops_hash old_hash ;
   unsigned long trampoline ;
   unsigned long trampoline_size ;
};
struct ftrace_ret_stack {
   unsigned long ret ;
   unsigned long func ;
   unsigned long long calltime ;
   unsigned long long subtime ;
   unsigned long fp ;
};
struct irq_work {
   unsigned long flags ;
   struct llist_node llnode ;
   void (*func)(struct irq_work * ) ;
};
struct perf_regs {
   __u64 abi ;
   struct pt_regs *regs ;
};
struct perf_callchain_entry {
   __u64 nr ;
   __u64 ip[127U] ;
};
struct perf_raw_record {
   u32 size ;
   void *data ;
};
struct perf_branch_stack {
   __u64 nr ;
   struct perf_branch_entry entries[0U] ;
};
struct hw_perf_event_extra {
   u64 config ;
   unsigned int reg ;
   int alloc ;
   int idx ;
};
struct __anonstruct____missing_field_name_362 {
   u64 config ;
   u64 last_tag ;
   unsigned long config_base ;
   unsigned long event_base ;
   int event_base_rdpmc ;
   int idx ;
   int last_cpu ;
   int flags ;
   struct hw_perf_event_extra extra_reg ;
   struct hw_perf_event_extra branch_reg ;
};
struct __anonstruct____missing_field_name_363 {
   struct hrtimer hrtimer ;
};
struct __anonstruct____missing_field_name_364 {
   struct list_head tp_list ;
};
struct __anonstruct____missing_field_name_365 {
   int cqm_state ;
   u32 cqm_rmid ;
   struct list_head cqm_events_entry ;
   struct list_head cqm_groups_entry ;
   struct list_head cqm_group_entry ;
};
struct __anonstruct____missing_field_name_366 {
   int itrace_started ;
};
struct __anonstruct____missing_field_name_367 {
   struct arch_hw_breakpoint info ;
   struct list_head bp_list ;
};
union __anonunion____missing_field_name_361 {
   struct __anonstruct____missing_field_name_362 __annonCompField88 ;
   struct __anonstruct____missing_field_name_363 __annonCompField89 ;
   struct __anonstruct____missing_field_name_364 __annonCompField90 ;
   struct __anonstruct____missing_field_name_365 __annonCompField91 ;
   struct __anonstruct____missing_field_name_366 __annonCompField92 ;
   struct __anonstruct____missing_field_name_367 __annonCompField93 ;
};
struct hw_perf_event {
   union __anonunion____missing_field_name_361 __annonCompField94 ;
   struct task_struct *target ;
   int state ;
   local64_t prev_count ;
   u64 sample_period ;
   u64 last_period ;
   local64_t period_left ;
   u64 interrupts_seq ;
   u64 interrupts ;
   u64 freq_time_stamp ;
   u64 freq_count_stamp ;
};
struct perf_cpu_context;
struct pmu {
   struct list_head entry ;
   struct module *module ;
   struct device *dev ;
   struct attribute_group const **attr_groups ;
   char const *name ;
   int type ;
   int capabilities ;
   int *pmu_disable_count ;
   struct perf_cpu_context *pmu_cpu_context ;
   atomic_t exclusive_cnt ;
   int task_ctx_nr ;
   int hrtimer_interval_ms ;
   void (*pmu_enable)(struct pmu * ) ;
   void (*pmu_disable)(struct pmu * ) ;
   int (*event_init)(struct perf_event * ) ;
   void (*event_mapped)(struct perf_event * ) ;
   void (*event_unmapped)(struct perf_event * ) ;
   int (*add)(struct perf_event * , int ) ;
   void (*del)(struct perf_event * , int ) ;
   void (*start)(struct perf_event * , int ) ;
   void (*stop)(struct perf_event * , int ) ;
   void (*read)(struct perf_event * ) ;
   void (*start_txn)(struct pmu * ) ;
   int (*commit_txn)(struct pmu * ) ;
   void (*cancel_txn)(struct pmu * ) ;
   int (*event_idx)(struct perf_event * ) ;
   void (*sched_task)(struct perf_event_context * , bool ) ;
   size_t task_ctx_size ;
   u64 (*count)(struct perf_event * ) ;
   void *(*setup_aux)(int , void ** , int , bool ) ;
   void (*free_aux)(void * ) ;
   int (*filter_match)(struct perf_event * ) ;
};
enum perf_event_active_state {
    PERF_EVENT_STATE_EXIT = -3,
    PERF_EVENT_STATE_ERROR = -2,
    PERF_EVENT_STATE_OFF = -1,
    PERF_EVENT_STATE_INACTIVE = 0,
    PERF_EVENT_STATE_ACTIVE = 1
} ;
struct perf_sample_data;
struct perf_cgroup;
struct event_filter;
struct perf_event {
   struct list_head event_entry ;
   struct list_head group_entry ;
   struct list_head sibling_list ;
   struct list_head migrate_entry ;
   struct hlist_node hlist_entry ;
   struct list_head active_entry ;
   int nr_siblings ;
   int group_flags ;
   struct perf_event *group_leader ;
   struct pmu *pmu ;
   enum perf_event_active_state state ;
   unsigned int attach_state ;
   local64_t count ;
   atomic64_t child_count ;
   u64 total_time_enabled ;
   u64 total_time_running ;
   u64 tstamp_enabled ;
   u64 tstamp_running ;
   u64 tstamp_stopped ;
   u64 shadow_ctx_time ;
   struct perf_event_attr attr ;
   u16 header_size ;
   u16 id_header_size ;
   u16 read_size ;
   struct hw_perf_event hw ;
   struct perf_event_context *ctx ;
   atomic_long_t refcount ;
   atomic64_t child_total_time_enabled ;
   atomic64_t child_total_time_running ;
   struct mutex child_mutex ;
   struct list_head child_list ;
   struct perf_event *parent ;
   int oncpu ;
   int cpu ;
   struct list_head owner_entry ;
   struct task_struct *owner ;
   struct mutex mmap_mutex ;
   atomic_t mmap_count ;
   struct ring_buffer *rb ;
   struct list_head rb_entry ;
   unsigned long rcu_batches ;
   int rcu_pending ;
   wait_queue_head_t waitq ;
   struct fasync_struct *fasync ;
   int pending_wakeup ;
   int pending_kill ;
   int pending_disable ;
   struct irq_work pending ;
   atomic_t event_limit ;
   void (*destroy)(struct perf_event * ) ;
   struct callback_head callback_head ;
   struct pid_namespace *ns ;
   u64 id ;
   u64 (*clock)(void) ;
   void (*overflow_handler)(struct perf_event * , struct perf_sample_data * , struct pt_regs * ) ;
   void *overflow_handler_context ;
   struct trace_event_call *tp_event ;
   struct event_filter *filter ;
   struct ftrace_ops ftrace_ops ;
   struct perf_cgroup *cgrp ;
   int cgrp_defer_enabled ;
};
struct perf_event_context {
   struct pmu *pmu ;
   raw_spinlock_t lock ;
   struct mutex mutex ;
   struct list_head active_ctx_list ;
   struct list_head pinned_groups ;
   struct list_head flexible_groups ;
   struct list_head event_list ;
   int nr_events ;
   int nr_active ;
   int is_active ;
   int nr_stat ;
   int nr_freq ;
   int rotate_disable ;
   atomic_t refcount ;
   struct task_struct *task ;
   u64 time ;
   u64 timestamp ;
   struct perf_event_context *parent_ctx ;
   u64 parent_gen ;
   u64 generation ;
   int pin_count ;
   int nr_cgroups ;
   void *task_ctx_data ;
   struct callback_head callback_head ;
   struct delayed_work orphans_remove ;
   bool orphans_remove_sched ;
};
struct perf_cpu_context {
   struct perf_event_context ctx ;
   struct perf_event_context *task_ctx ;
   int active_oncpu ;
   int exclusive ;
   raw_spinlock_t hrtimer_lock ;
   struct hrtimer hrtimer ;
   ktime_t hrtimer_interval ;
   unsigned int hrtimer_active ;
   struct pmu *unique_pmu ;
   struct perf_cgroup *cgrp ;
};
struct perf_cgroup_info {
   u64 time ;
   u64 timestamp ;
};
struct perf_cgroup {
   struct cgroup_subsys_state css ;
   struct perf_cgroup_info *info ;
};
struct __anonstruct_tid_entry_369 {
   u32 pid ;
   u32 tid ;
};
struct __anonstruct_cpu_entry_370 {
   u32 cpu ;
   u32 reserved ;
};
struct perf_sample_data {
   u64 addr ;
   struct perf_raw_record *raw ;
   struct perf_branch_stack *br_stack ;
   u64 period ;
   u64 weight ;
   u64 txn ;
   union perf_mem_data_src data_src ;
   u64 type ;
   u64 ip ;
   struct __anonstruct_tid_entry_369 tid_entry ;
   u64 time ;
   u64 id ;
   u64 stream_id ;
   struct __anonstruct_cpu_entry_370 cpu_entry ;
   struct perf_callchain_entry *callchain ;
   struct perf_regs regs_user ;
   struct pt_regs regs_user_copy ;
   struct perf_regs regs_intr ;
   u64 stack_user_size ;
};
struct trace_array;
struct trace_buffer;
struct tracer;
struct bpf_prog;
struct trace_iterator;
struct trace_event;
struct trace_entry {
   unsigned short type ;
   unsigned char flags ;
   unsigned char preempt_count ;
   int pid ;
};
struct trace_iterator {
   struct trace_array *tr ;
   struct tracer *trace ;
   struct trace_buffer *trace_buffer ;
   void *private ;
   int cpu_file ;
   struct mutex mutex ;
   struct ring_buffer_iter **buffer_iter ;
   unsigned long iter_flags ;
   struct trace_seq tmp_seq ;
   cpumask_var_t started ;
   bool snapshot ;
   struct trace_seq seq ;
   struct trace_entry *ent ;
   unsigned long lost_events ;
   int leftover ;
   int ent_size ;
   int cpu ;
   u64 ts ;
   loff_t pos ;
   long idx ;
};
enum print_line_t;
struct trace_event_functions {
   enum print_line_t (*trace)(struct trace_iterator * , int , struct trace_event * ) ;
   enum print_line_t (*raw)(struct trace_iterator * , int , struct trace_event * ) ;
   enum print_line_t (*hex)(struct trace_iterator * , int , struct trace_event * ) ;
   enum print_line_t (*binary)(struct trace_iterator * , int , struct trace_event * ) ;
};
struct trace_event {
   struct hlist_node node ;
   struct list_head list ;
   int type ;
   struct trace_event_functions *funcs ;
};
enum print_line_t {
    TRACE_TYPE_PARTIAL_LINE = 0,
    TRACE_TYPE_HANDLED = 1,
    TRACE_TYPE_UNHANDLED = 2,
    TRACE_TYPE_NO_CONSUME = 3
} ;
enum trace_reg {
    TRACE_REG_REGISTER = 0,
    TRACE_REG_UNREGISTER = 1,
    TRACE_REG_PERF_REGISTER = 2,
    TRACE_REG_PERF_UNREGISTER = 3,
    TRACE_REG_PERF_OPEN = 4,
    TRACE_REG_PERF_CLOSE = 5,
    TRACE_REG_PERF_ADD = 6,
    TRACE_REG_PERF_DEL = 7
} ;
struct trace_event_class {
   char const *system ;
   void *probe ;
   void *perf_probe ;
   int (*reg)(struct trace_event_call * , enum trace_reg , void * ) ;
   int (*define_fields)(struct trace_event_call * ) ;
   struct list_head *(*get_fields)(struct trace_event_call * ) ;
   struct list_head fields ;
   int (*raw_init)(struct trace_event_call * ) ;
};
union __anonunion____missing_field_name_371 {
   char *name ;
   struct tracepoint *tp ;
};
struct trace_event_call {
   struct list_head list ;
   struct trace_event_class *class ;
   union __anonunion____missing_field_name_371 __annonCompField96 ;
   struct trace_event event ;
   char *print_fmt ;
   struct event_filter *filter ;
   void *mod ;
   void *data ;
   int flags ;
   int perf_refcount ;
   struct hlist_head *perf_events ;
   struct bpf_prog *prog ;
   int (*perf_perm)(struct trace_event_call * , struct perf_event * ) ;
};
struct trace_event_raw_amdgpu_bo_create {
   struct trace_entry ent ;
   struct amdgpu_bo *bo ;
   u32 pages ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_cs {
   struct trace_entry ent ;
   struct amdgpu_bo_list *bo_list ;
   u32 ring ;
   u32 dw ;
   u32 fences ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_grab_id {
   struct trace_entry ent ;
   u32 vmid ;
   u32 ring ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_bo_map {
   struct trace_entry ent ;
   struct amdgpu_bo *bo ;
   long start ;
   long last ;
   u64 offset ;
   u32 flags ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_bo_unmap {
   struct trace_entry ent ;
   struct amdgpu_bo *bo ;
   long start ;
   long last ;
   u64 offset ;
   u32 flags ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_bo_update {
   struct trace_entry ent ;
   u64 soffset ;
   u64 eoffset ;
   u32 flags ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_set_page {
   struct trace_entry ent ;
   u64 pe ;
   u64 addr ;
   u32 count ;
   u32 incr ;
   u32 flags ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_vm_flush {
   struct trace_entry ent ;
   u64 pd_addr ;
   u32 ring ;
   u32 id ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_bo_list_set {
   struct trace_entry ent ;
   struct amdgpu_bo_list *list ;
   struct amdgpu_bo *bo ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_fence_request {
   struct trace_entry ent ;
   u32 dev ;
   int ring ;
   u32 seqno ;
   char __data[0U] ;
};
struct trace_event_raw_amdgpu_semaphore_request {
   struct trace_entry ent ;
   int ring ;
   int waiters ;
   uint64_t gpu_addr ;
   char __data[0U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_backlight_privdata {
   struct amdgpu_encoder *encoder ;
   uint8_t negative ;
};
struct _DAC_LOAD_DETECTION_PARAMETERS {
   USHORT usDeviceID ;
   UCHAR ucDacType ;
   UCHAR ucMisc ;
};
typedef struct _DAC_LOAD_DETECTION_PARAMETERS DAC_LOAD_DETECTION_PARAMETERS;
struct _DAC_LOAD_DETECTION_PS_ALLOCATION {
   DAC_LOAD_DETECTION_PARAMETERS sDacload ;
   ULONG Reserved[2U] ;
};
typedef struct _DAC_LOAD_DETECTION_PS_ALLOCATION DAC_LOAD_DETECTION_PS_ALLOCATION;
struct _DAC_ENCODER_CONTROL_PARAMETERS {
   USHORT usPixelClock ;
   UCHAR ucDacStandard ;
   UCHAR ucAction ;
};
typedef struct _DAC_ENCODER_CONTROL_PARAMETERS DAC_ENCODER_CONTROL_PARAMETERS;
struct _DIG_ENCODER_CONTROL_PARAMETERS {
   USHORT usPixelClock ;
   UCHAR ucConfig ;
   UCHAR ucAction ;
   UCHAR ucEncoderMode ;
   UCHAR ucLaneNum ;
   UCHAR ucReserved[2U] ;
};
typedef struct _DIG_ENCODER_CONTROL_PARAMETERS DIG_ENCODER_CONTROL_PARAMETERS;
struct _ATOM_DIG_ENCODER_CONFIG_V2 {
   unsigned char ucDPLinkRate : 1 ;
   unsigned char ucReserved : 1 ;
   unsigned char ucLinkSel : 1 ;
   unsigned char ucTransmitterSel : 2 ;
   unsigned char ucReserved1 : 2 ;
};
typedef struct _ATOM_DIG_ENCODER_CONFIG_V2 ATOM_DIG_ENCODER_CONFIG_V2;
struct _DIG_ENCODER_CONTROL_PARAMETERS_V2 {
   USHORT usPixelClock ;
   ATOM_DIG_ENCODER_CONFIG_V2 acConfig ;
   UCHAR ucAction ;
   UCHAR ucEncoderMode ;
   UCHAR ucLaneNum ;
   UCHAR ucStatus ;
   UCHAR ucReserved ;
};
typedef struct _DIG_ENCODER_CONTROL_PARAMETERS_V2 DIG_ENCODER_CONTROL_PARAMETERS_V2;
struct _ATOM_DIG_ENCODER_CONFIG_V3 {
   unsigned char ucDPLinkRate : 1 ;
   unsigned char ucReserved : 3 ;
   unsigned char ucDigSel : 3 ;
   unsigned char ucReserved1 : 1 ;
};
typedef struct _ATOM_DIG_ENCODER_CONFIG_V3 ATOM_DIG_ENCODER_CONFIG_V3;
union __anonunion____missing_field_name_279___0 {
   UCHAR ucEncoderMode ;
   UCHAR ucPanelMode ;
};
struct _DIG_ENCODER_CONTROL_PARAMETERS_V3 {
   USHORT usPixelClock ;
   ATOM_DIG_ENCODER_CONFIG_V3 acConfig ;
   UCHAR ucAction ;
   union __anonunion____missing_field_name_279___0 __annonCompField86 ;
   UCHAR ucLaneNum ;
   UCHAR ucBitPerColor ;
   UCHAR ucReserved ;
};
typedef struct _DIG_ENCODER_CONTROL_PARAMETERS_V3 DIG_ENCODER_CONTROL_PARAMETERS_V3;
struct _ATOM_DIG_ENCODER_CONFIG_V4 {
   unsigned char ucDPLinkRate : 2 ;
   unsigned char ucReserved : 2 ;
   unsigned char ucDigSel : 3 ;
   unsigned char ucReserved1 : 1 ;
};
typedef struct _ATOM_DIG_ENCODER_CONFIG_V4 ATOM_DIG_ENCODER_CONFIG_V4;
union __anonunion____missing_field_name_280___0 {
   ATOM_DIG_ENCODER_CONFIG_V4 acConfig ;
   UCHAR ucConfig ;
};
union __anonunion____missing_field_name_281___0 {
   UCHAR ucEncoderMode ;
   UCHAR ucPanelMode ;
};
struct _DIG_ENCODER_CONTROL_PARAMETERS_V4 {
   USHORT usPixelClock ;
   union __anonunion____missing_field_name_280___0 __annonCompField87 ;
   UCHAR ucAction ;
   union __anonunion____missing_field_name_281___0 __annonCompField88 ;
   UCHAR ucLaneNum ;
   UCHAR ucBitPerColor ;
   UCHAR ucHPD_ID ;
};
typedef struct _DIG_ENCODER_CONTROL_PARAMETERS_V4 DIG_ENCODER_CONTROL_PARAMETERS_V4;
struct _ATOM_DP_VS_MODE {
   UCHAR ucLaneSel ;
   UCHAR ucLaneSet ;
};
typedef struct _ATOM_DP_VS_MODE ATOM_DP_VS_MODE;
union __anonunion____missing_field_name_282___0 {
   USHORT usPixelClock ;
   USHORT usInitInfo ;
   ATOM_DP_VS_MODE asMode ;
};
struct _DIG_TRANSMITTER_CONTROL_PARAMETERS {
   union __anonunion____missing_field_name_282___0 __annonCompField89 ;
   UCHAR ucConfig ;
   UCHAR ucAction ;
   UCHAR ucReserved[4U] ;
};
typedef struct _DIG_TRANSMITTER_CONTROL_PARAMETERS DIG_TRANSMITTER_CONTROL_PARAMETERS;
struct _ATOM_DIG_TRANSMITTER_CONFIG_V2 {
   unsigned char fDualLinkConnector : 1 ;
   unsigned char fCoherentMode : 1 ;
   unsigned char ucLinkSel : 1 ;
   unsigned char ucEncoderSel : 1 ;
   unsigned char fDPConnector : 1 ;
   unsigned char ucReserved : 1 ;
   unsigned char ucTransmitterSel : 2 ;
};
typedef struct _ATOM_DIG_TRANSMITTER_CONFIG_V2 ATOM_DIG_TRANSMITTER_CONFIG_V2;
union __anonunion____missing_field_name_283 {
   USHORT usPixelClock ;
   USHORT usInitInfo ;
   ATOM_DP_VS_MODE asMode ;
};
struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V2 {
   union __anonunion____missing_field_name_283 __annonCompField90 ;
   ATOM_DIG_TRANSMITTER_CONFIG_V2 acConfig ;
   UCHAR ucAction ;
   UCHAR ucReserved[4U] ;
};
typedef struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V2 DIG_TRANSMITTER_CONTROL_PARAMETERS_V2;
struct _ATOM_DIG_TRANSMITTER_CONFIG_V3 {
   unsigned char fDualLinkConnector : 1 ;
   unsigned char fCoherentMode : 1 ;
   unsigned char ucLinkSel : 1 ;
   unsigned char ucEncoderSel : 1 ;
   unsigned char ucRefClkSource : 2 ;
   unsigned char ucTransmitterSel : 2 ;
};
typedef struct _ATOM_DIG_TRANSMITTER_CONFIG_V3 ATOM_DIG_TRANSMITTER_CONFIG_V3;
union __anonunion____missing_field_name_284 {
   USHORT usPixelClock ;
   USHORT usInitInfo ;
   ATOM_DP_VS_MODE asMode ;
};
struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V3 {
   union __anonunion____missing_field_name_284 __annonCompField91 ;
   ATOM_DIG_TRANSMITTER_CONFIG_V3 acConfig ;
   UCHAR ucAction ;
   UCHAR ucLaneNum ;
   UCHAR ucReserved[3U] ;
};
typedef struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V3 DIG_TRANSMITTER_CONTROL_PARAMETERS_V3;
struct __anonstruct____missing_field_name_286 {
   unsigned char ucVOLTAGE_SWING : 3 ;
   unsigned char ucPRE_EMPHASIS : 3 ;
   unsigned char ucPOST_CURSOR2 : 2 ;
};
union __anonunion____missing_field_name_285 {
   UCHAR ucLaneSet ;
   struct __anonstruct____missing_field_name_286 __annonCompField92 ;
};
struct _ATOM_DP_VS_MODE_V4 {
   UCHAR ucLaneSel ;
   union __anonunion____missing_field_name_285 __annonCompField93 ;
};
typedef struct _ATOM_DP_VS_MODE_V4 ATOM_DP_VS_MODE_V4;
struct _ATOM_DIG_TRANSMITTER_CONFIG_V4 {
   unsigned char fDualLinkConnector : 1 ;
   unsigned char fCoherentMode : 1 ;
   unsigned char ucLinkSel : 1 ;
   unsigned char ucEncoderSel : 1 ;
   unsigned char ucRefClkSource : 2 ;
   unsigned char ucTransmitterSel : 2 ;
};
typedef struct _ATOM_DIG_TRANSMITTER_CONFIG_V4 ATOM_DIG_TRANSMITTER_CONFIG_V4;
union __anonunion____missing_field_name_287 {
   USHORT usPixelClock ;
   USHORT usInitInfo ;
   ATOM_DP_VS_MODE_V4 asMode ;
};
union __anonunion____missing_field_name_288 {
   ATOM_DIG_TRANSMITTER_CONFIG_V4 acConfig ;
   UCHAR ucConfig ;
};
struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V4 {
   union __anonunion____missing_field_name_287 __annonCompField94 ;
   union __anonunion____missing_field_name_288 __annonCompField95 ;
   UCHAR ucAction ;
   UCHAR ucLaneNum ;
   UCHAR ucReserved[3U] ;
};
typedef struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V4 DIG_TRANSMITTER_CONTROL_PARAMETERS_V4;
struct _ATOM_DIG_TRANSMITTER_CONFIG_V5 {
   unsigned char ucReserved : 1 ;
   unsigned char ucCoherentMode : 1 ;
   unsigned char ucPhyClkSrcId : 2 ;
   unsigned char ucHPDSel : 3 ;
   unsigned char ucReservd1 : 1 ;
};
typedef struct _ATOM_DIG_TRANSMITTER_CONFIG_V5 ATOM_DIG_TRANSMITTER_CONFIG_V5;
union __anonunion____missing_field_name_289 {
   ATOM_DIG_TRANSMITTER_CONFIG_V5 asConfig ;
   UCHAR ucConfig ;
};
struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V1_5 {
   USHORT usSymClock ;
   UCHAR ucPhyId ;
   UCHAR ucAction ;
   UCHAR ucLaneNum ;
   UCHAR ucConnObjId ;
   UCHAR ucDigMode ;
   union __anonunion____missing_field_name_289 __annonCompField96 ;
   UCHAR ucDigEncoderSel ;
   UCHAR ucDPLaneSet ;
   UCHAR ucReserved ;
   UCHAR ucReserved1 ;
};
typedef struct _DIG_TRANSMITTER_CONTROL_PARAMETERS_V1_5 DIG_TRANSMITTER_CONTROL_PARAMETERS_V1_5;
union __anonunion____missing_field_name_290 {
   USHORT usPixelClock ;
   USHORT usConnectorId ;
};
struct _EXTERNAL_ENCODER_CONTROL_PARAMETERS_V3 {
   union __anonunion____missing_field_name_290 __annonCompField97 ;
   UCHAR ucConfig ;
   UCHAR ucAction ;
   UCHAR ucEncoderMode ;
   UCHAR ucLaneNum ;
   UCHAR ucBitPerColor ;
   UCHAR ucReserved ;
};
typedef struct _EXTERNAL_ENCODER_CONTROL_PARAMETERS_V3 EXTERNAL_ENCODER_CONTROL_PARAMETERS_V3;
struct _EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION_V3 {
   EXTERNAL_ENCODER_CONTROL_PARAMETERS_V3 sExtEncoder ;
   ULONG ulReserved[2U] ;
};
typedef struct _EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION_V3 EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION_V3;
struct _SELECT_CRTC_SOURCE_PARAMETERS {
   UCHAR ucCRTC ;
   UCHAR ucDevice ;
   UCHAR ucPadding[2U] ;
};
typedef struct _SELECT_CRTC_SOURCE_PARAMETERS SELECT_CRTC_SOURCE_PARAMETERS;
struct _SELECT_CRTC_SOURCE_PARAMETERS_V2 {
   UCHAR ucCRTC ;
   UCHAR ucEncoderID ;
   UCHAR ucEncodeMode ;
   UCHAR ucPadding ;
};
typedef struct _SELECT_CRTC_SOURCE_PARAMETERS_V2 SELECT_CRTC_SOURCE_PARAMETERS_V2;
struct _SELECT_CRTC_SOURCE_PARAMETERS_V3 {
   UCHAR ucCRTC ;
   UCHAR ucEncoderID ;
   UCHAR ucEncodeMode ;
   UCHAR ucDstBpc ;
};
typedef struct _SELECT_CRTC_SOURCE_PARAMETERS_V3 SELECT_CRTC_SOURCE_PARAMETERS_V3;
struct _ENABLE_EXTERNAL_TMDS_ENCODER_PARAMETERS {
   UCHAR ucEnable ;
   UCHAR ucMisc ;
   UCHAR ucPadding[2U] ;
};
typedef struct _ENABLE_EXTERNAL_TMDS_ENCODER_PARAMETERS ENABLE_EXTERNAL_TMDS_ENCODER_PARAMETERS;
struct _ENABLE_EXTERNAL_TMDS_ENCODER_PS_ALLOCATION {
   ENABLE_EXTERNAL_TMDS_ENCODER_PARAMETERS sXTmdsEncoder ;
   WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS sReserved ;
};
typedef struct _ENABLE_EXTERNAL_TMDS_ENCODER_PS_ALLOCATION ENABLE_EXTERNAL_TMDS_ENCODER_PS_ALLOCATION;
struct _EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION {
   DIG_ENCODER_CONTROL_PARAMETERS sDigEncoder ;
   WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS sReserved ;
};
typedef struct _EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION;
struct _DVO_ENCODER_CONTROL_PARAMETERS_V3 {
   USHORT usPixelClock ;
   UCHAR ucDVOConfig ;
   UCHAR ucAction ;
   UCHAR ucReseved[4U] ;
};
typedef struct _DVO_ENCODER_CONTROL_PARAMETERS_V3 DVO_ENCODER_CONTROL_PARAMETERS_V3;
struct _DVO_ENCODER_CONTROL_PARAMETERS_V1_4 {
   USHORT usPixelClock ;
   UCHAR ucDVOConfig ;
   UCHAR ucAction ;
   UCHAR ucBitPerColor ;
   UCHAR ucReseved[3U] ;
};
typedef struct _DVO_ENCODER_CONTROL_PARAMETERS_V1_4 DVO_ENCODER_CONTROL_PARAMETERS_V1_4;
struct _ATOM_DTD_FORMAT {
   USHORT usPixClk ;
   USHORT usHActive ;
   USHORT usHBlanking_Time ;
   USHORT usVActive ;
   USHORT usVBlanking_Time ;
   USHORT usHSyncOffset ;
   USHORT usHSyncWidth ;
   USHORT usVSyncOffset ;
   USHORT usVSyncWidth ;
   USHORT usImageHSize ;
   USHORT usImageVSize ;
   UCHAR ucHBorder ;
   UCHAR ucVBorder ;
   ATOM_MODE_MISC_INFO_ACCESS susModeMiscInfo ;
   UCHAR ucInternalModeNumber ;
   UCHAR ucRefreshRate ;
};
typedef struct _ATOM_DTD_FORMAT ATOM_DTD_FORMAT;
struct _ATOM_LVDS_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_DTD_FORMAT sLCDTiming ;
   USHORT usModePatchTableOffset ;
   USHORT usSupportedRefreshRate ;
   USHORT usOffDelayInMs ;
   UCHAR ucPowerSequenceDigOntoDEin10Ms ;
   UCHAR ucPowerSequenceDEtoBLOnin10Ms ;
   UCHAR ucLVDS_Misc ;
   UCHAR ucPanelDefaultRefreshRate ;
   UCHAR ucPanelIdentification ;
   UCHAR ucSS_Id ;
};
struct _ATOM_LVDS_INFO_V12 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ATOM_DTD_FORMAT sLCDTiming ;
   USHORT usExtInfoTableOffset ;
   USHORT usSupportedRefreshRate ;
   USHORT usOffDelayInMs ;
   UCHAR ucPowerSequenceDigOntoDEin10Ms ;
   UCHAR ucPowerSequenceDEtoBLOnin10Ms ;
   UCHAR ucLVDS_Misc ;
   UCHAR ucPanelDefaultRefreshRate ;
   UCHAR ucPanelIdentification ;
   UCHAR ucSS_Id ;
   USHORT usLCDVenderID ;
   USHORT usLCDProductID ;
   UCHAR ucLCDPanel_SpecialHandlingCap ;
   UCHAR ucPanelInfoSize ;
   UCHAR ucReserved[2U] ;
};
struct _ATOM_FAKE_EDID_PATCH_RECORD {
   UCHAR ucRecordType ;
   UCHAR ucFakeEDIDLength ;
   UCHAR ucFakeEDIDString[1U] ;
};
typedef struct _ATOM_FAKE_EDID_PATCH_RECORD ATOM_FAKE_EDID_PATCH_RECORD;
struct _ATOM_PANEL_RESOLUTION_PATCH_RECORD {
   UCHAR ucRecordType ;
   USHORT usHSize ;
   USHORT usVSize ;
};
typedef struct _ATOM_PANEL_RESOLUTION_PATCH_RECORD ATOM_PANEL_RESOLUTION_PATCH_RECORD;
struct _ATOM_ENCODER_ANALOG_ATTRIBUTE {
   UCHAR ucTVStandard ;
   UCHAR ucPadding[1U] ;
};
typedef struct _ATOM_ENCODER_ANALOG_ATTRIBUTE ATOM_ENCODER_ANALOG_ATTRIBUTE;
struct _ATOM_ENCODER_DIGITAL_ATTRIBUTE {
   UCHAR ucAttribute ;
   UCHAR ucPadding[1U] ;
};
typedef struct _ATOM_ENCODER_DIGITAL_ATTRIBUTE ATOM_ENCODER_DIGITAL_ATTRIBUTE;
union _ATOM_ENCODER_ATTRIBUTE {
   ATOM_ENCODER_ANALOG_ATTRIBUTE sAlgAttrib ;
   ATOM_ENCODER_DIGITAL_ATTRIBUTE sDigAttrib ;
};
typedef union _ATOM_ENCODER_ATTRIBUTE ATOM_ENCODER_ATTRIBUTE;
struct _DVO_ENCODER_CONTROL_PARAMETERS {
   USHORT usPixelClock ;
   USHORT usEncoderID ;
   UCHAR ucDeviceType ;
   UCHAR ucAction ;
   ATOM_ENCODER_ATTRIBUTE usDevAttr ;
};
typedef struct _DVO_ENCODER_CONTROL_PARAMETERS DVO_ENCODER_CONTROL_PARAMETERS;
struct _DVO_ENCODER_CONTROL_PS_ALLOCATION {
   DVO_ENCODER_CONTROL_PARAMETERS sDVOEncoder ;
   WRITE_ONE_BYTE_HW_I2C_DATA_PARAMETERS sReserved ;
};
typedef struct _DVO_ENCODER_CONTROL_PS_ALLOCATION DVO_ENCODER_CONTROL_PS_ALLOCATION;
struct amdgpu_legacy_backlight_privdata;
union dvo_encoder_control {
   ENABLE_EXTERNAL_TMDS_ENCODER_PS_ALLOCATION ext_tmds ;
   DVO_ENCODER_CONTROL_PS_ALLOCATION dvo ;
   DVO_ENCODER_CONTROL_PARAMETERS_V3 dvo_v3 ;
   DVO_ENCODER_CONTROL_PARAMETERS_V1_4 dvo_v4 ;
};
union dig_encoder_control {
   DIG_ENCODER_CONTROL_PARAMETERS v1 ;
   DIG_ENCODER_CONTROL_PARAMETERS_V2 v2 ;
   DIG_ENCODER_CONTROL_PARAMETERS_V3 v3 ;
   DIG_ENCODER_CONTROL_PARAMETERS_V4 v4 ;
};
union dig_transmitter_control {
   DIG_TRANSMITTER_CONTROL_PARAMETERS v1 ;
   DIG_TRANSMITTER_CONTROL_PARAMETERS_V2 v2 ;
   DIG_TRANSMITTER_CONTROL_PARAMETERS_V3 v3 ;
   DIG_TRANSMITTER_CONTROL_PARAMETERS_V4 v4 ;
   DIG_TRANSMITTER_CONTROL_PARAMETERS_V1_5 v5 ;
};
union external_encoder_control {
   EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION v1 ;
   EXTERNAL_ENCODER_CONTROL_PS_ALLOCATION_V3 v3 ;
};
union crtc_source_param {
   SELECT_CRTC_SOURCE_PARAMETERS v1 ;
   SELECT_CRTC_SOURCE_PARAMETERS_V2 v2 ;
   SELECT_CRTC_SOURCE_PARAMETERS_V3 v3 ;
};
union lvds_info {
   struct _ATOM_LVDS_INFO info ;
   struct _ATOM_LVDS_INFO_V12 info_12 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
union __anonunion___u_323 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_325 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_327 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_329 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
union __anonunion____missing_field_name_312___0 {
   UCHAR ucRegIndex ;
   UCHAR ucStatus ;
};
struct _PROCESS_I2C_CHANNEL_TRANSACTION_PARAMETERS {
   UCHAR ucI2CSpeed ;
   union __anonunion____missing_field_name_312___0 __annonCompField119 ;
   USHORT lpI2CDataOut ;
   UCHAR ucFlag ;
   UCHAR ucTransBytes ;
   UCHAR ucSlaveAddr ;
   UCHAR ucLineNumber ;
};
typedef struct _PROCESS_I2C_CHANNEL_TRANSACTION_PARAMETERS PROCESS_I2C_CHANNEL_TRANSACTION_PARAMETERS;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct dma_buf_ops {
   int (*attach)(struct dma_buf * , struct device * , struct dma_buf_attachment * ) ;
   void (*detach)(struct dma_buf * , struct dma_buf_attachment * ) ;
   struct sg_table *(*map_dma_buf)(struct dma_buf_attachment * , enum dma_data_direction ) ;
   void (*unmap_dma_buf)(struct dma_buf_attachment * , struct sg_table * , enum dma_data_direction ) ;
   void (*release)(struct dma_buf * ) ;
   int (*begin_cpu_access)(struct dma_buf * , size_t , size_t , enum dma_data_direction ) ;
   void (*end_cpu_access)(struct dma_buf * , size_t , size_t , enum dma_data_direction ) ;
   void *(*kmap_atomic)(struct dma_buf * , unsigned long ) ;
   void (*kunmap_atomic)(struct dma_buf * , unsigned long , void * ) ;
   void *(*kmap)(struct dma_buf * , unsigned long ) ;
   void (*kunmap)(struct dma_buf * , unsigned long , void * ) ;
   int (*mmap)(struct dma_buf * , struct vm_area_struct * ) ;
   void *(*vmap)(struct dma_buf * ) ;
   void (*vunmap)(struct dma_buf * , void * ) ;
};
struct dma_buf_poll_cb_t {
   struct fence_cb cb ;
   wait_queue_head_t *poll ;
   unsigned long active ;
};
struct dma_buf {
   size_t size ;
   struct file *file ;
   struct list_head attachments ;
   struct dma_buf_ops const *ops ;
   struct mutex lock ;
   unsigned int vmapping_counter ;
   void *vmap_ptr ;
   char const *exp_name ;
   struct module *owner ;
   struct list_head list_node ;
   void *priv ;
   struct reservation_object *resv ;
   wait_queue_head_t poll ;
   struct dma_buf_poll_cb_t cb_excl ;
   struct dma_buf_poll_cb_t cb_shared ;
};
struct dma_buf_attachment {
   struct dma_buf *dmabuf ;
   struct device *dev ;
   struct list_head node ;
   void *priv ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
union __anonunion___u_283 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_285 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_287 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_289 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_291 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_293 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_295 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_297 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_299 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_301 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_303 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_305 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct common_firmware_header {
   u32 size_bytes ;
   u32 header_size_bytes ;
   u16 header_version_major ;
   u16 header_version_minor ;
   u16 ip_version_major ;
   u16 ip_version_minor ;
   u32 ucode_version ;
   u32 ucode_size_bytes ;
   u32 ucode_array_offset_bytes ;
   u32 crc32 ;
};
struct mc_firmware_header_v1_0 {
   struct common_firmware_header header ;
   u32 io_debug_size_bytes ;
   u32 io_debug_array_offset_bytes ;
};
struct smc_firmware_header_v1_0 {
   struct common_firmware_header header ;
   u32 ucode_start_addr ;
};
struct gfx_firmware_header_v1_0 {
   struct common_firmware_header header ;
   u32 ucode_feature_version ;
   u32 jt_offset ;
   u32 jt_size ;
};
struct rlc_firmware_header_v1_0 {
   struct common_firmware_header header ;
   u32 ucode_feature_version ;
   u32 save_and_restore_offset ;
   u32 clear_state_descriptor_offset ;
   u32 avail_scratch_ram_locations ;
   u32 master_pkt_description_offset ;
};
struct rlc_firmware_header_v2_0 {
   struct common_firmware_header header ;
   u32 ucode_feature_version ;
   u32 jt_offset ;
   u32 jt_size ;
   u32 save_and_restore_offset ;
   u32 clear_state_descriptor_offset ;
   u32 avail_scratch_ram_locations ;
   u32 reg_restore_list_size ;
   u32 reg_list_format_start ;
   u32 reg_list_format_separate_start ;
   u32 starting_offsets_start ;
   u32 reg_list_format_size_bytes ;
   u32 reg_list_format_array_offset_bytes ;
   u32 reg_list_size_bytes ;
   u32 reg_list_array_offset_bytes ;
   u32 reg_list_format_separate_size_bytes ;
   u32 reg_list_format_separate_array_offset_bytes ;
   u32 reg_list_separate_size_bytes ;
   u32 reg_list_separate_array_offset_bytes ;
};
struct sdma_firmware_header_v1_0 {
   struct common_firmware_header header ;
   u32 ucode_feature_version ;
   u32 ucode_change_version ;
   u32 jt_offset ;
   u32 jt_size ;
};
struct sdma_firmware_header_v1_1 {
   struct sdma_firmware_header_v1_0 v1_0 ;
   u32 digest_size ;
};
union amdgpu_firmware_header {
   struct common_firmware_header common ;
   struct mc_firmware_header_v1_0 mc ;
   struct smc_firmware_header_v1_0 smc ;
   struct gfx_firmware_header_v1_0 gfx ;
   struct rlc_firmware_header_v1_0 rlc ;
   struct rlc_firmware_header_v2_0 rlc_v2_0 ;
   struct sdma_firmware_header_v1_0 sdma ;
   struct sdma_firmware_header_v1_1 sdma_v1_1 ;
   uint8_t raw[256U] ;
};
enum hrtimer_restart;
union __anonunion___u_168 {
   struct idr_layer *__val ;
   char __c[1U] ;
};
union __anonunion___u_170 {
   struct idr_layer *__val ;
   char __c[1U] ;
};
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_amdgpu_bo_list_in {
   u32 operation ;
   u32 list_handle ;
   u32 bo_number ;
   u32 bo_info_size ;
   uint64_t bo_info_ptr ;
};
struct drm_amdgpu_bo_list_entry {
   u32 bo_handle ;
   u32 bo_priority ;
};
struct drm_amdgpu_bo_list_out {
   u32 list_handle ;
   u32 _pad ;
};
union drm_amdgpu_bo_list {
   struct drm_amdgpu_bo_list_in in ;
   struct drm_amdgpu_bo_list_out out ;
};
union __anonunion___u_307 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
union __anonunion___u_309 {
   struct tracepoint_func *__val ;
   char __c[1U] ;
};
enum hrtimer_restart;
union __anonunion___u_168___0 {
   struct idr_layer *__val ;
   char __c[1U] ;
};
union __anonunion___u_170___0 {
   struct idr_layer *__val ;
   char __c[1U] ;
};
enum i2c_slave_event;
enum i2c_slave_event;
struct drm_amdgpu_ctx_in {
   u32 op ;
   u32 flags ;
   u32 ctx_id ;
   u32 _pad ;
};
struct __anonstruct_alloc_258 {
   u32 ctx_id ;
   u32 _pad ;
};
struct __anonstruct_state_259 {
   uint64_t flags ;
   u32 hangs ;
   u32 reset_status ;
};
union drm_amdgpu_ctx_out {
   struct __anonstruct_alloc_258 alloc ;
   struct __anonstruct_state_259 state ;
};
union drm_amdgpu_ctx {
   struct drm_amdgpu_ctx_in in ;
   union drm_amdgpu_ctx_out out ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_allowed_register_entry {
   u32 reg_offset ;
   bool untouched ;
   bool grbm_indexed ;
};
struct kv_reset_save_regs {
   u32 gmcon_reng_execute ;
   u32 gmcon_misc ;
   u32 gmcon_misc3 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
typedef u16 PPSMC_Msg;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct _ATOM_INTEGRATED_SYSTEM_INFO_V5 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   ULONG ulBootUpEngineClock ;
   ULONG ulDentistVCOFreq ;
   ULONG ulLClockFreq ;
   ULONG ulBootUpUMAClock ;
   ULONG ulReserved1[8U] ;
   ULONG ulBootUpReqDisplayVector ;
   ULONG ulOtherDisplayMisc ;
   ULONG ulReserved2[4U] ;
   ULONG ulSystemConfig ;
   ULONG ulCPUCapInfo ;
   USHORT usMaxNBVoltage ;
   USHORT usMinNBVoltage ;
   USHORT usBootUpNBVoltage ;
   UCHAR ucHtcTmpLmt ;
   UCHAR ucTjOffset ;
   ULONG ulReserved3[4U] ;
   ULONG ulDDISlot1Config ;
   ULONG ulDDISlot2Config ;
   ULONG ulDDISlot3Config ;
   ULONG ulDDISlot4Config ;
   ULONG ulReserved4[4U] ;
   UCHAR ucMemoryType ;
   UCHAR ucUMAChannelNumber ;
   USHORT usReserved ;
   ULONG ulReserved5[4U] ;
   ULONG ulCSR_M3_ARB_CNTL_DEFAULT[10U] ;
   ULONG ulCSR_M3_ARB_CNTL_UVD[10U] ;
   ULONG ulCSR_M3_ARB_CNTL_FS3D[10U] ;
   ULONG ulReserved6[61U] ;
};
struct _ATOM_POWERMODE_INFO {
   ULONG ulMiscInfo ;
   ULONG ulReserved1 ;
   ULONG ulReserved2 ;
   USHORT usEngineClock ;
   USHORT usMemoryClock ;
   UCHAR ucVoltageDropIndex ;
   UCHAR ucSelectedPanel_RefreshRate ;
   UCHAR ucMinTemperature ;
   UCHAR ucMaxTemperature ;
   UCHAR ucNumPciELanes ;
};
typedef struct _ATOM_POWERMODE_INFO ATOM_POWERMODE_INFO;
struct _ATOM_POWERMODE_INFO_V2 {
   ULONG ulMiscInfo ;
   ULONG ulMiscInfo2 ;
   ULONG ulEngineClock ;
   ULONG ulMemoryClock ;
   UCHAR ucVoltageDropIndex ;
   UCHAR ucSelectedPanel_RefreshRate ;
   UCHAR ucMinTemperature ;
   UCHAR ucMaxTemperature ;
   UCHAR ucNumPciELanes ;
};
typedef struct _ATOM_POWERMODE_INFO_V2 ATOM_POWERMODE_INFO_V2;
struct _ATOM_POWERMODE_INFO_V3 {
   ULONG ulMiscInfo ;
   ULONG ulMiscInfo2 ;
   ULONG ulEngineClock ;
   ULONG ulMemoryClock ;
   UCHAR ucVoltageDropIndex ;
   UCHAR ucSelectedPanel_RefreshRate ;
   UCHAR ucMinTemperature ;
   UCHAR ucMaxTemperature ;
   UCHAR ucNumPciELanes ;
   UCHAR ucVDDCI_VoltageDropIndex ;
};
typedef struct _ATOM_POWERMODE_INFO_V3 ATOM_POWERMODE_INFO_V3;
struct _ATOM_POWERPLAY_INFO {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   UCHAR ucOverdriveThermalController ;
   UCHAR ucOverdriveI2cLine ;
   UCHAR ucOverdriveIntBitmap ;
   UCHAR ucOverdriveControllerAddress ;
   UCHAR ucSizeOfPowerModeEntry ;
   UCHAR ucNumOfPowerModeEntries ;
   ATOM_POWERMODE_INFO asPowerPlayInfo[8U] ;
};
struct _ATOM_POWERPLAY_INFO_V2 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   UCHAR ucOverdriveThermalController ;
   UCHAR ucOverdriveI2cLine ;
   UCHAR ucOverdriveIntBitmap ;
   UCHAR ucOverdriveControllerAddress ;
   UCHAR ucSizeOfPowerModeEntry ;
   UCHAR ucNumOfPowerModeEntries ;
   ATOM_POWERMODE_INFO_V2 asPowerPlayInfo[8U] ;
};
struct _ATOM_POWERPLAY_INFO_V3 {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   UCHAR ucOverdriveThermalController ;
   UCHAR ucOverdriveI2cLine ;
   UCHAR ucOverdriveIntBitmap ;
   UCHAR ucOverdriveControllerAddress ;
   UCHAR ucSizeOfPowerModeEntry ;
   UCHAR ucNumOfPowerModeEntries ;
   ATOM_POWERMODE_INFO_V3 asPowerPlayInfo[8U] ;
};
struct _ATOM_PPLIB_THERMALCONTROLLER {
   UCHAR ucType ;
   UCHAR ucI2cLine ;
   UCHAR ucI2cAddress ;
   UCHAR ucFanParameters ;
   UCHAR ucFanMinRPM ;
   UCHAR ucFanMaxRPM ;
   UCHAR ucReserved ;
   UCHAR ucFlags ;
};
typedef struct _ATOM_PPLIB_THERMALCONTROLLER ATOM_PPLIB_THERMALCONTROLLER;
struct _ATOM_PPLIB_STATE {
   UCHAR ucNonClockStateIndex ;
   UCHAR ucClockStateIndices[1U] ;
};
struct _ATOM_PPLIB_POWERPLAYTABLE {
   ATOM_COMMON_TABLE_HEADER sHeader ;
   UCHAR ucDataRevision ;
   UCHAR ucNumStates ;
   UCHAR ucStateEntrySize ;
   UCHAR ucClockInfoSize ;
   UCHAR ucNonClockSize ;
   USHORT usStateArrayOffset ;
   USHORT usClockInfoArrayOffset ;
   USHORT usNonClockInfoArrayOffset ;
   USHORT usBackbiasTime ;
   USHORT usVoltageTime ;
   USHORT usTableSize ;
   ULONG ulPlatformCaps ;
   ATOM_PPLIB_THERMALCONTROLLER sThermalController ;
   USHORT usBootClockInfoOffset ;
   USHORT usBootNonClockInfoOffset ;
};
typedef struct _ATOM_PPLIB_POWERPLAYTABLE ATOM_PPLIB_POWERPLAYTABLE;
struct _ATOM_PPLIB_POWERPLAYTABLE2 {
   ATOM_PPLIB_POWERPLAYTABLE basicTable ;
   UCHAR ucNumCustomThermalPolicy ;
   USHORT usCustomThermalPolicyArrayOffset ;
};
typedef struct _ATOM_PPLIB_POWERPLAYTABLE2 ATOM_PPLIB_POWERPLAYTABLE2;
struct _ATOM_PPLIB_POWERPLAYTABLE3 {
   ATOM_PPLIB_POWERPLAYTABLE2 basicTable2 ;
   USHORT usFormatID ;
   USHORT usFanTableOffset ;
   USHORT usExtendendedHeaderOffset ;
};
struct _ATOM_PPLIB_NONCLOCK_INFO {
   USHORT usClassification ;
   UCHAR ucMinTemperature ;
   UCHAR ucMaxTemperature ;
   ULONG ulCapsAndSettings ;
   UCHAR ucRequiredPower ;
   USHORT usClassification2 ;
   ULONG ulVCLK ;
   ULONG ulDCLK ;
   UCHAR ucUnused[5U] ;
};
typedef struct _ATOM_PPLIB_NONCLOCK_INFO ATOM_PPLIB_NONCLOCK_INFO;
struct _ATOM_PPLIB_R600_CLOCK_INFO {
   USHORT usEngineClockLow ;
   UCHAR ucEngineClockHigh ;
   USHORT usMemoryClockLow ;
   UCHAR ucMemoryClockHigh ;
   USHORT usVDDC ;
   USHORT usUnused1 ;
   USHORT usUnused2 ;
   ULONG ulFlags ;
};
struct _ATOM_PPLIB_RS780_CLOCK_INFO {
   USHORT usLowEngineClockLow ;
   UCHAR ucLowEngineClockHigh ;
   USHORT usHighEngineClockLow ;
   UCHAR ucHighEngineClockHigh ;
   USHORT usMemoryClockLow ;
   UCHAR ucMemoryClockHigh ;
   UCHAR ucPadding ;
   USHORT usVDDC ;
   UCHAR ucMaxHTLinkWidth ;
   UCHAR ucMinHTLinkWidth ;
   USHORT usHTLinkFreq ;
   ULONG ulFlags ;
};
struct _ATOM_PPLIB_EVERGREEN_CLOCK_INFO {
   USHORT usEngineClockLow ;
   UCHAR ucEngineClockHigh ;
   USHORT usMemoryClockLow ;
   UCHAR ucMemoryClockHigh ;
   USHORT usVDDC ;
   USHORT usVDDCI ;
   USHORT usUnused ;
   ULONG ulFlags ;
};
struct _ATOM_PPLIB_SUMO_CLOCK_INFO {
   USHORT usEngineClockLow ;
   UCHAR ucEngineClockHigh ;
   UCHAR vddcIndex ;
   USHORT tdpLimit ;
   USHORT rsv1 ;
   ULONG rsv2[2U] ;
};
struct _ATOM_PPLIB_STATE_V2 {
   UCHAR ucNumDPMLevels ;
   UCHAR nonClockInfoIndex ;
   UCHAR clockInfoIndex[1U] ;
};
typedef struct _ATOM_PPLIB_STATE_V2 ATOM_PPLIB_STATE_V2;
struct _StateArray {
   UCHAR ucNumEntries ;
   ATOM_PPLIB_STATE_V2 states[1U] ;
};
struct _ClockInfoArray {
   UCHAR ucNumEntries ;
   UCHAR ucEntrySize ;
   UCHAR clockInfo[1U] ;
};
struct _NonClockInfoArray {
   UCHAR ucNumEntries ;
   UCHAR ucEntrySize ;
   ATOM_PPLIB_NONCLOCK_INFO nonClockInfo[1U] ;
};
struct SMU7_Fusion_GraphicsLevel {
   u32 MinVddNb ;
   u32 SclkFrequency ;
   uint8_t Vid ;
   uint8_t VidOffset ;
   u16 AT ;
   uint8_t PowerThrottle ;
   uint8_t GnbSlow ;
   uint8_t ForceNbPs1 ;
   uint8_t SclkDid ;
   uint8_t DisplayWatermark ;
   uint8_t EnabledForActivity ;
   uint8_t EnabledForThrottle ;
   uint8_t UpH ;
   uint8_t DownH ;
   uint8_t VoltageDownH ;
   uint8_t DeepSleepDivId ;
   uint8_t ClkBypassCntl ;
   u32 reserved ;
};
typedef struct SMU7_Fusion_GraphicsLevel SMU7_Fusion_GraphicsLevel;
struct SMU7_Fusion_UvdLevel {
   u32 VclkFrequency ;
   u32 DclkFrequency ;
   u16 MinVddNb ;
   uint8_t VclkDivider ;
   uint8_t DclkDivider ;
   uint8_t VClkBypassCntl ;
   uint8_t DClkBypassCntl ;
   uint8_t padding[2U] ;
};
typedef struct SMU7_Fusion_UvdLevel SMU7_Fusion_UvdLevel;
struct SMU7_Fusion_ExtClkLevel {
   u32 Frequency ;
   u16 MinVoltage ;
   uint8_t Divider ;
   uint8_t ClkBypassCntl ;
   u32 Reserved ;
};
typedef struct SMU7_Fusion_ExtClkLevel SMU7_Fusion_ExtClkLevel;
struct SMU7_Fusion_ACPILevel {
   u32 Flags ;
   u32 MinVddNb ;
   u32 SclkFrequency ;
   uint8_t SclkDid ;
   uint8_t GnbSlow ;
   uint8_t ForceNbPs1 ;
   uint8_t DisplayWatermark ;
   uint8_t DeepSleepDivId ;
   uint8_t padding[3U] ;
};
typedef struct SMU7_Fusion_ACPILevel SMU7_Fusion_ACPILevel;
struct sumo_vid_mapping_entry {
   u16 vid_2bit ;
   u16 vid_7bit ;
};
struct sumo_vid_mapping_table {
   u32 num_entries ;
   struct sumo_vid_mapping_entry entries[4U] ;
};
struct sumo_sclk_voltage_mapping_entry {
   u32 sclk_frequency ;
   u16 vid_2bit ;
   u16 rsv ;
};
struct sumo_sclk_voltage_mapping_table {
   u32 num_max_dpm_entries ;
   struct sumo_sclk_voltage_mapping_entry entries[5U] ;
};
enum kv_pt_config_reg_type {
    KV_CONFIGREG_MMR = 0,
    KV_CONFIGREG_SMC_IND = 1,
    KV_CONFIGREG_DIDT_IND = 2,
    KV_CONFIGREG_CACHE = 3,
    KV_CONFIGREG_MAX = 4
} ;
struct kv_pt_config_reg {
   u32 offset ;
   u32 mask ;
   u32 shift ;
   u32 value ;
   enum kv_pt_config_reg_type type ;
};
struct kv_pl {
   u32 sclk ;
   u8 vddc_index ;
   u8 ds_divider_index ;
   u8 ss_divider_index ;
   u8 allow_gnb_slow ;
   u8 force_nbp_state ;
   u8 display_wm ;
   u8 vce_wm ;
};
struct kv_ps {
   struct kv_pl levels[5U] ;
   u32 num_levels ;
   bool need_dfs_bypass ;
   u8 dpm0_pg_nb_ps_lo ;
   u8 dpm0_pg_nb_ps_hi ;
   u8 dpmx_nb_ps_lo ;
   u8 dpmx_nb_ps_hi ;
};
struct kv_sys_info {
   u32 bootup_uma_clk ;
   u32 bootup_sclk ;
   u32 dentist_vco_freq ;
   u32 nb_dpm_enable ;
   u32 nbp_memory_clock[4U] ;
   u32 nbp_n_clock[4U] ;
   u16 bootup_nb_voltage_index ;
   u8 htc_tmp_lmt ;
   u8 htc_hyst_lmt ;
   struct sumo_sclk_voltage_mapping_table sclk_voltage_mapping_table ;
   struct sumo_vid_mapping_table vid_mapping_table ;
   u32 uma_channel_number ;
};
struct kv_power_info {
   u32 at[5U] ;
   u32 voltage_drop_t ;
   struct kv_sys_info sys_info ;
   struct kv_pl boot_pl ;
   bool enable_nb_ps_policy ;
   bool disable_nb_ps3_in_battery ;
   bool video_start ;
   bool battery_state ;
   u32 lowest_valid ;
   u32 highest_valid ;
   u16 high_voltage_t ;
   bool cac_enabled ;
   bool bapm_enable ;
   u32 sram_end ;
   u32 dpm_table_start ;
   u32 soft_regs_start ;
   u8 graphics_dpm_level_count ;
   u8 uvd_level_count ;
   u8 vce_level_count ;
   u8 acp_level_count ;
   u8 samu_level_count ;
   u16 fps_high_t ;
   SMU7_Fusion_GraphicsLevel graphics_level[8U] ;
   SMU7_Fusion_ACPILevel acpi_level ;
   SMU7_Fusion_UvdLevel uvd_level[8U] ;
   SMU7_Fusion_ExtClkLevel vce_level[8U] ;
   SMU7_Fusion_ExtClkLevel acp_level[8U] ;
   SMU7_Fusion_ExtClkLevel samu_level[8U] ;
   u8 uvd_boot_level ;
   u8 vce_boot_level ;
   u8 acp_boot_level ;
   u8 samu_boot_level ;
   u8 uvd_interval ;
   u8 vce_interval ;
   u8 acp_interval ;
   u8 samu_interval ;
   u8 graphics_boot_level ;
   u8 graphics_interval ;
   u8 graphics_therm_throttle_enable ;
   u8 graphics_voltage_change_enable ;
   u8 graphics_clk_slow_enable ;
   u8 graphics_clk_slow_divider ;
   u8 fps_low_t ;
   u32 low_sclk_interrupt_t ;
   bool uvd_power_gated ;
   bool vce_power_gated ;
   bool acp_power_gated ;
   bool samu_power_gated ;
   bool nb_dpm_enabled ;
   bool enable_didt ;
   bool enable_dpm ;
   bool enable_auto_thermal_throttling ;
   bool enable_nb_dpm ;
   bool caps_cac ;
   bool caps_power_containment ;
   bool caps_sq_ramping ;
   bool caps_db_ramping ;
   bool caps_td_ramping ;
   bool caps_tcp_ramping ;
   bool caps_sclk_throttle_low_notification ;
   bool caps_fps ;
   bool caps_uvd_dpm ;
   bool caps_uvd_pg ;
   bool caps_vce_pg ;
   bool caps_samu_pg ;
   bool caps_acp_pg ;
   bool caps_stable_p_state ;
   bool caps_enable_dfs_bypass ;
   bool caps_sclk_ds ;
   struct amdgpu_ps current_rps ;
   struct kv_ps current_ps ;
   struct amdgpu_ps requested_rps ;
   struct kv_ps requested_ps ;
};
union igp_info___0 {
   struct _ATOM_INTEGRATED_SYSTEM_INFO info ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V2 info_2 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V5 info_5 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V6 info_6 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8 ;
};
union power_info {
   struct _ATOM_POWERPLAY_INFO info ;
   struct _ATOM_POWERPLAY_INFO_V2 info_2 ;
   struct _ATOM_POWERPLAY_INFO_V3 info_3 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE pplib ;
   struct _ATOM_PPLIB_POWERPLAYTABLE2 pplib2 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE3 pplib3 ;
};
union pplib_clock_info {
   struct _ATOM_PPLIB_R600_CLOCK_INFO r600 ;
   struct _ATOM_PPLIB_RS780_CLOCK_INFO rs780 ;
   struct _ATOM_PPLIB_EVERGREEN_CLOCK_INFO evergreen ;
   struct _ATOM_PPLIB_SUMO_CLOCK_INFO sumo ;
};
union pplib_power_state {
   struct _ATOM_PPLIB_STATE v1 ;
   struct _ATOM_PPLIB_STATE_V2 v2 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
typedef uint8_t PPSMC_Result;
typedef signed char __s8;
typedef short __s16;
typedef __s8 int8_t;
typedef __s16 int16_t;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum amdgpu_dpm_auto_throttle_src {
    AMDGPU_DPM_AUTO_THROTTLE_SRC_THERMAL = 0,
    AMDGPU_DPM_AUTO_THROTTLE_SRC_EXTERNAL = 1
} ;
enum amdgpu_dpm_event_src {
    AMDGPU_DPM_EVENT_SRC_ANALOG = 0,
    AMDGPU_DPM_EVENT_SRC_EXTERNAL = 1,
    AMDGPU_DPM_EVENT_SRC_DIGITAL = 2,
    AMDGPU_DPM_EVENT_SRC_ANALOG_OR_EXTERNAL = 3,
    AMDGPU_DPM_EVENT_SRC_DIGIAL_OR_EXTERNAL = 4
} ;
enum amdgpu_pcie_gen {
    AMDGPU_PCIE_GEN1 = 0,
    AMDGPU_PCIE_GEN2 = 1,
    AMDGPU_PCIE_GEN3 = 2,
    AMDGPU_PCIE_GEN_INVALID = 65535
} ;
struct SMU7_PIDController {
   u32 Ki ;
   int32_t LFWindupUL ;
   int32_t LFWindupLL ;
   u32 StatePrecision ;
   u32 LfPrecision ;
   u32 LfOffset ;
   u32 MaxState ;
   u32 MaxLfFraction ;
   u32 StateShift ;
};
typedef struct SMU7_PIDController SMU7_PIDController;
struct SMU7_Discrete_VoltageLevel {
   u16 Voltage ;
   u16 StdVoltageHiSidd ;
   u16 StdVoltageLoSidd ;
   uint8_t Smio ;
   uint8_t padding ;
};
typedef struct SMU7_Discrete_VoltageLevel SMU7_Discrete_VoltageLevel;
struct SMU7_Discrete_GraphicsLevel {
   u32 Flags ;
   u32 MinVddc ;
   u32 MinVddcPhases ;
   u32 SclkFrequency ;
   uint8_t padding1[2U] ;
   u16 ActivityLevel ;
   u32 CgSpllFuncCntl3 ;
   u32 CgSpllFuncCntl4 ;
   u32 SpllSpreadSpectrum ;
   u32 SpllSpreadSpectrum2 ;
   u32 CcPwrDynRm ;
   u32 CcPwrDynRm1 ;
   uint8_t SclkDid ;
   uint8_t DisplayWatermark ;
   uint8_t EnabledForActivity ;
   uint8_t EnabledForThrottle ;
   uint8_t UpH ;
   uint8_t DownH ;
   uint8_t VoltageDownH ;
   uint8_t PowerThrottle ;
   uint8_t DeepSleepDivId ;
   uint8_t padding[3U] ;
};
typedef struct SMU7_Discrete_GraphicsLevel SMU7_Discrete_GraphicsLevel;
struct SMU7_Discrete_ACPILevel {
   u32 Flags ;
   u32 MinVddc ;
   u32 MinVddcPhases ;
   u32 SclkFrequency ;
   uint8_t SclkDid ;
   uint8_t DisplayWatermark ;
   uint8_t DeepSleepDivId ;
   uint8_t padding ;
   u32 CgSpllFuncCntl ;
   u32 CgSpllFuncCntl2 ;
   u32 CgSpllFuncCntl3 ;
   u32 CgSpllFuncCntl4 ;
   u32 SpllSpreadSpectrum ;
   u32 SpllSpreadSpectrum2 ;
   u32 CcPwrDynRm ;
   u32 CcPwrDynRm1 ;
};
typedef struct SMU7_Discrete_ACPILevel SMU7_Discrete_ACPILevel;
struct SMU7_Discrete_Ulv {
   u32 CcPwrDynRm ;
   u32 CcPwrDynRm1 ;
   u16 VddcOffset ;
   uint8_t VddcOffsetVid ;
   uint8_t VddcPhase ;
   u32 Reserved ;
};
typedef struct SMU7_Discrete_Ulv SMU7_Discrete_Ulv;
struct SMU7_Discrete_MemoryLevel {
   u32 MinVddc ;
   u32 MinVddcPhases ;
   u32 MinVddci ;
   u32 MinMvdd ;
   u32 MclkFrequency ;
   uint8_t EdcReadEnable ;
   uint8_t EdcWriteEnable ;
   uint8_t RttEnable ;
   uint8_t StutterEnable ;
   uint8_t StrobeEnable ;
   uint8_t StrobeRatio ;
   uint8_t EnabledForThrottle ;
   uint8_t EnabledForActivity ;
   uint8_t UpH ;
   uint8_t DownH ;
   uint8_t VoltageDownH ;
   uint8_t padding ;
   u16 ActivityLevel ;
   uint8_t DisplayWatermark ;
   uint8_t padding1 ;
   u32 MpllFuncCntl ;
   u32 MpllFuncCntl_1 ;
   u32 MpllFuncCntl_2 ;
   u32 MpllAdFuncCntl ;
   u32 MpllDqFuncCntl ;
   u32 MclkPwrmgtCntl ;
   u32 DllCntl ;
   u32 MpllSs1 ;
   u32 MpllSs2 ;
};
typedef struct SMU7_Discrete_MemoryLevel SMU7_Discrete_MemoryLevel;
struct SMU7_Discrete_LinkLevel {
   uint8_t PcieGenSpeed ;
   uint8_t PcieLaneCount ;
   uint8_t EnabledForActivity ;
   uint8_t Padding ;
   u32 DownT ;
   u32 UpT ;
   u32 Reserved ;
};
typedef struct SMU7_Discrete_LinkLevel SMU7_Discrete_LinkLevel;
struct SMU7_Discrete_MCArbDramTimingTableEntry {
   u32 McArbDramTiming ;
   u32 McArbDramTiming2 ;
   uint8_t McArbBurstTime ;
   uint8_t padding[3U] ;
};
typedef struct SMU7_Discrete_MCArbDramTimingTableEntry SMU7_Discrete_MCArbDramTimingTableEntry;
struct SMU7_Discrete_MCArbDramTimingTable {
   SMU7_Discrete_MCArbDramTimingTableEntry entries[8U][6U] ;
};
typedef struct SMU7_Discrete_MCArbDramTimingTable SMU7_Discrete_MCArbDramTimingTable;
struct SMU7_Discrete_UvdLevel {
   u32 VclkFrequency ;
   u32 DclkFrequency ;
   u16 MinVddc ;
   uint8_t MinVddcPhases ;
   uint8_t VclkDivider ;
   uint8_t DclkDivider ;
   uint8_t padding[3U] ;
};
typedef struct SMU7_Discrete_UvdLevel SMU7_Discrete_UvdLevel;
struct SMU7_Discrete_ExtClkLevel {
   u32 Frequency ;
   u16 MinVoltage ;
   uint8_t MinPhases ;
   uint8_t Divider ;
};
typedef struct SMU7_Discrete_ExtClkLevel SMU7_Discrete_ExtClkLevel;
struct SMU7_Discrete_DpmTable {
   SMU7_PIDController GraphicsPIDController ;
   SMU7_PIDController MemoryPIDController ;
   SMU7_PIDController LinkPIDController ;
   u32 SystemFlags ;
   u32 SmioMaskVddcVid ;
   u32 SmioMaskVddcPhase ;
   u32 SmioMaskVddciVid ;
   u32 SmioMaskMvddVid ;
   u32 VddcLevelCount ;
   u32 VddciLevelCount ;
   u32 MvddLevelCount ;
   SMU7_Discrete_VoltageLevel VddcLevel[8U] ;
   SMU7_Discrete_VoltageLevel VddciLevel[4U] ;
   SMU7_Discrete_VoltageLevel MvddLevel[4U] ;
   uint8_t GraphicsDpmLevelCount ;
   uint8_t MemoryDpmLevelCount ;
   uint8_t LinkLevelCount ;
   uint8_t UvdLevelCount ;
   uint8_t VceLevelCount ;
   uint8_t AcpLevelCount ;
   uint8_t SamuLevelCount ;
   uint8_t MasterDeepSleepControl ;
   u32 Reserved[5U] ;
   SMU7_Discrete_GraphicsLevel GraphicsLevel[8U] ;
   SMU7_Discrete_MemoryLevel MemoryACPILevel ;
   SMU7_Discrete_MemoryLevel MemoryLevel[6U] ;
   SMU7_Discrete_LinkLevel LinkLevel[8U] ;
   SMU7_Discrete_ACPILevel ACPILevel ;
   SMU7_Discrete_UvdLevel UvdLevel[8U] ;
   SMU7_Discrete_ExtClkLevel VceLevel[8U] ;
   SMU7_Discrete_ExtClkLevel AcpLevel[8U] ;
   SMU7_Discrete_ExtClkLevel SamuLevel[8U] ;
   SMU7_Discrete_Ulv Ulv ;
   u32 SclkStepSize ;
   u32 Smio[32U] ;
   uint8_t UvdBootLevel ;
   uint8_t VceBootLevel ;
   uint8_t AcpBootLevel ;
   uint8_t SamuBootLevel ;
   uint8_t UVDInterval ;
   uint8_t VCEInterval ;
   uint8_t ACPInterval ;
   uint8_t SAMUInterval ;
   uint8_t GraphicsBootLevel ;
   uint8_t GraphicsVoltageChangeEnable ;
   uint8_t GraphicsThermThrottleEnable ;
   uint8_t GraphicsInterval ;
   uint8_t VoltageInterval ;
   uint8_t ThermalInterval ;
   u16 TemperatureLimitHigh ;
   u16 TemperatureLimitLow ;
   uint8_t MemoryBootLevel ;
   uint8_t MemoryVoltageChangeEnable ;
   uint8_t MemoryInterval ;
   uint8_t MemoryThermThrottleEnable ;
   u16 VddcVddciDelta ;
   u16 VoltageResponseTime ;
   u16 PhaseResponseTime ;
   uint8_t PCIeBootLinkLevel ;
   uint8_t PCIeGenInterval ;
   uint8_t DTEInterval ;
   uint8_t DTEMode ;
   uint8_t SVI2Enable ;
   uint8_t VRHotGpio ;
   uint8_t AcDcGpio ;
   uint8_t ThermGpio ;
   u16 PPM_PkgPwrLimit ;
   u16 PPM_TemperatureLimit ;
   u16 DefaultTdp ;
   u16 TargetTdp ;
   u16 FpsHighT ;
   u16 FpsLowT ;
   u16 BAPMTI_R[5U][3U][1U] ;
   u16 BAPMTI_RC[5U][3U][1U] ;
   uint8_t DTEAmbientTempBase ;
   uint8_t DTETjOffset ;
   uint8_t GpuTjMax ;
   uint8_t GpuTjHyst ;
   u16 BootVddc ;
   u16 BootVddci ;
   u16 BootMVdd ;
   u16 padding ;
   u32 BAPM_TEMP_GRADIENT ;
   u32 LowSclkInterruptT ;
};
typedef struct SMU7_Discrete_DpmTable SMU7_Discrete_DpmTable;
struct SMU7_Discrete_MCRegisterAddress {
   u16 s0 ;
   u16 s1 ;
};
typedef struct SMU7_Discrete_MCRegisterAddress SMU7_Discrete_MCRegisterAddress;
struct SMU7_Discrete_MCRegisterSet {
   u32 value[16U] ;
};
typedef struct SMU7_Discrete_MCRegisterSet SMU7_Discrete_MCRegisterSet;
struct SMU7_Discrete_MCRegisters {
   uint8_t last ;
   uint8_t reserved[3U] ;
   SMU7_Discrete_MCRegisterAddress address[16U] ;
   SMU7_Discrete_MCRegisterSet data[6U] ;
};
typedef struct SMU7_Discrete_MCRegisters SMU7_Discrete_MCRegisters;
struct SMU7_Discrete_FanTable {
   u16 FdoMode ;
   int16_t TempMin ;
   int16_t TempMed ;
   int16_t TempMax ;
   int16_t Slope1 ;
   int16_t Slope2 ;
   int16_t FdoMin ;
   int16_t HystUp ;
   int16_t HystDown ;
   int16_t HystSlope ;
   int16_t TempRespLim ;
   int16_t TempCurr ;
   int16_t SlopeCurr ;
   int16_t PwmCurr ;
   u32 RefreshPeriod ;
   int16_t FdoMax ;
   uint8_t TempSrc ;
   int8_t Padding ;
};
typedef struct SMU7_Discrete_FanTable SMU7_Discrete_FanTable;
struct SMU7_Discrete_PmFuses {
   uint8_t BapmVddCVidHiSidd[8U] ;
   uint8_t BapmVddCVidLoSidd[8U] ;
   uint8_t VddCVid[8U] ;
   uint8_t SviLoadLineEn ;
   uint8_t SviLoadLineVddC ;
   uint8_t SviLoadLineTrimVddC ;
   uint8_t SviLoadLineOffsetVddC ;
   u16 TDC_VDDC_PkgLimit ;
   uint8_t TDC_VDDC_ThrottleReleaseLimitPerc ;
   uint8_t TDC_MAWt ;
   uint8_t TdcWaterfallCtl ;
   uint8_t LPMLTemperatureMin ;
   uint8_t LPMLTemperatureMax ;
   uint8_t Reserved ;
   uint8_t BapmVddCVidHiSidd2[8U] ;
   int16_t FuzzyFan_ErrorSetDelta ;
   int16_t FuzzyFan_ErrorRateSetDelta ;
   int16_t FuzzyFan_PwmSetDelta ;
   u16 CalcMeasPowerBlend ;
   uint8_t GnbLPML[16U] ;
   uint8_t GnbLPMLMaxVid ;
   uint8_t GnbLPMLMinVid ;
   uint8_t Reserved1[2U] ;
   u16 BapmVddCBaseLeakageHiSidd ;
   u16 BapmVddCBaseLeakageLoSidd ;
};
typedef struct SMU7_Discrete_PmFuses SMU7_Discrete_PmFuses;
struct ci_pl {
   u32 mclk ;
   u32 sclk ;
   enum amdgpu_pcie_gen pcie_gen ;
   u16 pcie_lane ;
};
struct ci_ps {
   u16 performance_level_count ;
   bool dc_compatible ;
   u32 sclk_t ;
   struct ci_pl performance_levels[2U] ;
};
struct ci_dpm_level {
   bool enabled ;
   u32 value ;
   u32 param1 ;
};
struct ci_single_dpm_table {
   u32 count ;
   struct ci_dpm_level dpm_levels[8U] ;
};
struct ci_dpm_table {
   struct ci_single_dpm_table sclk_table ;
   struct ci_single_dpm_table mclk_table ;
   struct ci_single_dpm_table pcie_speed_table ;
   struct ci_single_dpm_table vddc_table ;
   struct ci_single_dpm_table vddci_table ;
   struct ci_single_dpm_table mvdd_table ;
};
struct ci_mc_reg_entry {
   u32 mclk_max ;
   u32 mc_data[16U] ;
};
struct ci_mc_reg_table {
   u8 last ;
   u8 num_entries ;
   u16 valid_flag ;
   struct ci_mc_reg_entry mc_reg_table_entry[16U] ;
   SMU7_Discrete_MCRegisterAddress mc_reg_address[16U] ;
};
struct ci_ulv_parm {
   bool supported ;
   u32 cg_ulv_parameter ;
   u32 volt_change_delay ;
   struct ci_pl pl ;
};
struct ci_leakage_voltage {
   u16 count ;
   u16 leakage_id[8U] ;
   u16 actual_voltage[8U] ;
};
struct ci_dpm_level_enable_mask {
   u32 uvd_dpm_enable_mask ;
   u32 vce_dpm_enable_mask ;
   u32 acp_dpm_enable_mask ;
   u32 samu_dpm_enable_mask ;
   u32 sclk_dpm_enable_mask ;
   u32 mclk_dpm_enable_mask ;
   u32 pcie_dpm_enable_mask ;
};
struct ci_vbios_boot_state {
   u16 mvdd_bootup_value ;
   u16 vddc_bootup_value ;
   u16 vddci_bootup_value ;
   u32 sclk_bootup_value ;
   u32 mclk_bootup_value ;
   u16 pcie_gen_bootup_value ;
   u16 pcie_lane_bootup_value ;
};
struct ci_clock_registers {
   u32 cg_spll_func_cntl ;
   u32 cg_spll_func_cntl_2 ;
   u32 cg_spll_func_cntl_3 ;
   u32 cg_spll_func_cntl_4 ;
   u32 cg_spll_spread_spectrum ;
   u32 cg_spll_spread_spectrum_2 ;
   u32 dll_cntl ;
   u32 mclk_pwrmgt_cntl ;
   u32 mpll_ad_func_cntl ;
   u32 mpll_dq_func_cntl ;
   u32 mpll_func_cntl ;
   u32 mpll_func_cntl_1 ;
   u32 mpll_func_cntl_2 ;
   u32 mpll_ss1 ;
   u32 mpll_ss2 ;
};
struct ci_thermal_temperature_setting {
   s32 temperature_low ;
   s32 temperature_high ;
   s32 temperature_shutdown ;
};
struct ci_pcie_perf_range {
   u16 max ;
   u16 min ;
};
enum ci_pt_config_reg_type {
    CISLANDS_CONFIGREG_MMR = 0,
    CISLANDS_CONFIGREG_SMC_IND = 1,
    CISLANDS_CONFIGREG_DIDT_IND = 2,
    CISLANDS_CONFIGREG_CACHE = 3,
    CISLANDS_CONFIGREG_MAX = 4
} ;
struct ci_pt_config_reg {
   u32 offset ;
   u32 mask ;
   u32 shift ;
   u32 value ;
   enum ci_pt_config_reg_type type ;
};
struct ci_pt_defaults {
   u8 svi_load_line_en ;
   u8 svi_load_line_vddc ;
   u8 tdc_vddc_throttle_release_limit_perc ;
   u8 tdc_mawt ;
   u8 tdc_waterfall_ctl ;
   u8 dte_ambient_temp_base ;
   u32 display_cac ;
   u32 bapm_temp_gradient ;
   u16 bapmti_r[15U] ;
   u16 bapmti_rc[15U] ;
};
struct ci_power_info {
   struct ci_dpm_table dpm_table ;
   u32 voltage_control ;
   u32 mvdd_control ;
   u32 vddci_control ;
   u32 active_auto_throttle_sources ;
   struct ci_clock_registers clock_registers ;
   u16 acpi_vddc ;
   u16 acpi_vddci ;
   enum amdgpu_pcie_gen force_pcie_gen ;
   enum amdgpu_pcie_gen acpi_pcie_gen ;
   struct ci_leakage_voltage vddc_leakage ;
   struct ci_leakage_voltage vddci_leakage ;
   u16 max_vddc_in_pp_table ;
   u16 min_vddc_in_pp_table ;
   u16 max_vddci_in_pp_table ;
   u16 min_vddci_in_pp_table ;
   u32 mclk_strobe_mode_threshold ;
   u32 mclk_stutter_mode_threshold ;
   u32 mclk_edc_enable_threshold ;
   u32 mclk_edc_wr_enable_threshold ;
   struct ci_vbios_boot_state vbios_boot_state ;
   u32 sram_end ;
   u32 dpm_table_start ;
   u32 soft_regs_start ;
   u32 mc_reg_table_start ;
   u32 fan_table_start ;
   u32 arb_table_start ;
   SMU7_Discrete_DpmTable smc_state_table ;
   SMU7_Discrete_MCRegisters smc_mc_reg_table ;
   SMU7_Discrete_PmFuses smc_powertune_table ;
   struct ci_mc_reg_table mc_reg_table ;
   struct atom_voltage_table vddc_voltage_table ;
   struct atom_voltage_table vddci_voltage_table ;
   struct atom_voltage_table mvdd_voltage_table ;
   struct ci_ulv_parm ulv ;
   u32 power_containment_features ;
   struct ci_pt_defaults const *powertune_defaults ;
   u32 dte_tj_offset ;
   bool vddc_phase_shed_control ;
   struct ci_thermal_temperature_setting thermal_temp_setting ;
   struct ci_dpm_level_enable_mask dpm_level_enable_mask ;
   u32 need_update_smu7_dpm_table ;
   u32 sclk_dpm_key_disabled ;
   u32 mclk_dpm_key_disabled ;
   u32 pcie_dpm_key_disabled ;
   u32 thermal_sclk_dpm_enabled ;
   struct ci_pcie_perf_range pcie_gen_performance ;
   struct ci_pcie_perf_range pcie_lane_performance ;
   struct ci_pcie_perf_range pcie_gen_powersaving ;
   struct ci_pcie_perf_range pcie_lane_powersaving ;
   u32 activity_target[8U] ;
   u32 mclk_activity_target ;
   u32 low_sclk_interrupt_t ;
   u32 last_mclk_dpm_enable_mask ;
   u32 sys_pcie_mask ;
   bool caps_power_containment ;
   bool caps_cac ;
   bool caps_sq_ramping ;
   bool caps_db_ramping ;
   bool caps_td_ramping ;
   bool caps_tcp_ramping ;
   bool caps_fps ;
   bool caps_sclk_ds ;
   bool caps_sclk_ss_support ;
   bool caps_mclk_ss_support ;
   bool caps_uvd_dpm ;
   bool caps_vce_dpm ;
   bool caps_samu_dpm ;
   bool caps_acp_dpm ;
   bool caps_automatic_dc_transition ;
   bool caps_sclk_throttle_low_notification ;
   bool caps_dynamic_ac_timing ;
   bool caps_od_fuzzy_fan_control_support ;
   bool thermal_protection ;
   bool pcie_performance_request ;
   bool dynamic_ss ;
   bool dll_default_on ;
   bool cac_enabled ;
   bool uvd_enabled ;
   bool battery_state ;
   bool pspp_notify_required ;
   bool enable_bapm_feature ;
   bool enable_tdc_limit_feature ;
   bool enable_pkg_pwr_tracking_feature ;
   bool use_pcie_performance_levels ;
   bool use_pcie_powersaving_levels ;
   bool uvd_power_gated ;
   struct amdgpu_ps current_rps ;
   struct ci_ps current_ps ;
   struct amdgpu_ps requested_rps ;
   struct ci_ps requested_ps ;
   bool fan_ctrl_is_in_default_mode ;
   bool fan_is_controlled_by_smc ;
   u32 t_min ;
   u32 fan_ctrl_default_mode ;
};
struct _ATOM_PPLIB_SI_CLOCK_INFO {
   USHORT usEngineClockLow ;
   UCHAR ucEngineClockHigh ;
   USHORT usMemoryClockLow ;
   UCHAR ucMemoryClockHigh ;
   USHORT usVDDC ;
   USHORT usVDDCI ;
   UCHAR ucPCIEGen ;
   UCHAR ucUnused1 ;
   ULONG ulFlags ;
};
struct _ATOM_PPLIB_CI_CLOCK_INFO {
   USHORT usEngineClockLow ;
   UCHAR ucEngineClockHigh ;
   USHORT usMemoryClockLow ;
   UCHAR ucMemoryClockHigh ;
   UCHAR ucPCIEGen ;
   USHORT usPCIELane ;
};
union pplib_clock_info___0 {
   struct _ATOM_PPLIB_R600_CLOCK_INFO r600 ;
   struct _ATOM_PPLIB_RS780_CLOCK_INFO rs780 ;
   struct _ATOM_PPLIB_EVERGREEN_CLOCK_INFO evergreen ;
   struct _ATOM_PPLIB_SUMO_CLOCK_INFO sumo ;
   struct _ATOM_PPLIB_SI_CLOCK_INFO si ;
   struct _ATOM_PPLIB_CI_CLOCK_INFO ci ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hdmi_infoframe_type {
    HDMI_INFOFRAME_TYPE_VENDOR = 129,
    HDMI_INFOFRAME_TYPE_AVI = 130,
    HDMI_INFOFRAME_TYPE_SPD = 131,
    HDMI_INFOFRAME_TYPE_AUDIO = 132
} ;
enum hdmi_colorspace {
    HDMI_COLORSPACE_RGB = 0,
    HDMI_COLORSPACE_YUV422 = 1,
    HDMI_COLORSPACE_YUV444 = 2,
    HDMI_COLORSPACE_YUV420 = 3,
    HDMI_COLORSPACE_RESERVED4 = 4,
    HDMI_COLORSPACE_RESERVED5 = 5,
    HDMI_COLORSPACE_RESERVED6 = 6,
    HDMI_COLORSPACE_IDO_DEFINED = 7
} ;
enum hdmi_scan_mode {
    HDMI_SCAN_MODE_NONE = 0,
    HDMI_SCAN_MODE_OVERSCAN = 1,
    HDMI_SCAN_MODE_UNDERSCAN = 2,
    HDMI_SCAN_MODE_RESERVED = 3
} ;
enum hdmi_colorimetry {
    HDMI_COLORIMETRY_NONE = 0,
    HDMI_COLORIMETRY_ITU_601 = 1,
    HDMI_COLORIMETRY_ITU_709 = 2,
    HDMI_COLORIMETRY_EXTENDED = 3
} ;
enum hdmi_active_aspect {
    HDMI_ACTIVE_ASPECT_16_9_TOP = 2,
    HDMI_ACTIVE_ASPECT_14_9_TOP = 3,
    HDMI_ACTIVE_ASPECT_16_9_CENTER = 4,
    HDMI_ACTIVE_ASPECT_PICTURE = 8,
    HDMI_ACTIVE_ASPECT_4_3 = 9,
    HDMI_ACTIVE_ASPECT_16_9 = 10,
    HDMI_ACTIVE_ASPECT_14_9 = 11,
    HDMI_ACTIVE_ASPECT_4_3_SP_14_9 = 13,
    HDMI_ACTIVE_ASPECT_16_9_SP_14_9 = 14,
    HDMI_ACTIVE_ASPECT_16_9_SP_4_3 = 15
} ;
enum hdmi_extended_colorimetry {
    HDMI_EXTENDED_COLORIMETRY_XV_YCC_601 = 0,
    HDMI_EXTENDED_COLORIMETRY_XV_YCC_709 = 1,
    HDMI_EXTENDED_COLORIMETRY_S_YCC_601 = 2,
    HDMI_EXTENDED_COLORIMETRY_ADOBE_YCC_601 = 3,
    HDMI_EXTENDED_COLORIMETRY_ADOBE_RGB = 4,
    HDMI_EXTENDED_COLORIMETRY_BT2020_CONST_LUM = 5,
    HDMI_EXTENDED_COLORIMETRY_BT2020 = 6,
    HDMI_EXTENDED_COLORIMETRY_RESERVED = 7
} ;
enum hdmi_quantization_range {
    HDMI_QUANTIZATION_RANGE_DEFAULT = 0,
    HDMI_QUANTIZATION_RANGE_LIMITED = 1,
    HDMI_QUANTIZATION_RANGE_FULL = 2,
    HDMI_QUANTIZATION_RANGE_RESERVED = 3
} ;
enum hdmi_nups {
    HDMI_NUPS_UNKNOWN = 0,
    HDMI_NUPS_HORIZONTAL = 1,
    HDMI_NUPS_VERTICAL = 2,
    HDMI_NUPS_BOTH = 3
} ;
enum hdmi_ycc_quantization_range {
    HDMI_YCC_QUANTIZATION_RANGE_LIMITED = 0,
    HDMI_YCC_QUANTIZATION_RANGE_FULL = 1
} ;
enum hdmi_content_type {
    HDMI_CONTENT_TYPE_GRAPHICS = 0,
    HDMI_CONTENT_TYPE_PHOTO = 1,
    HDMI_CONTENT_TYPE_CINEMA = 2,
    HDMI_CONTENT_TYPE_GAME = 3
} ;
struct hdmi_avi_infoframe {
   enum hdmi_infoframe_type type ;
   unsigned char version ;
   unsigned char length ;
   enum hdmi_colorspace colorspace ;
   enum hdmi_scan_mode scan_mode ;
   enum hdmi_colorimetry colorimetry ;
   enum hdmi_picture_aspect picture_aspect ;
   enum hdmi_active_aspect active_aspect ;
   bool itc ;
   enum hdmi_extended_colorimetry extended_colorimetry ;
   enum hdmi_quantization_range quantization_range ;
   enum hdmi_nups nups ;
   unsigned char video_code ;
   enum hdmi_ycc_quantization_range ycc_quantization_range ;
   enum hdmi_content_type content_type ;
   unsigned char pixel_repeat ;
   unsigned short top_bar ;
   unsigned short bottom_bar ;
   unsigned short left_bar ;
   unsigned short right_bar ;
};
struct cea_sad {
   u8 format ;
   u8 channels ;
   u8 freq ;
   u8 byte2 ;
};
enum mode_set_atomic {
    LEAVE_ATOMIC_MODE_SET = 0,
    ENTER_ATOMIC_MODE_SET = 1
} ;
struct drm_crtc_helper_funcs {
   void (*dpms)(struct drm_crtc * , int ) ;
   void (*prepare)(struct drm_crtc * ) ;
   void (*commit)(struct drm_crtc * ) ;
   bool (*mode_fixup)(struct drm_crtc * , struct drm_display_mode const * , struct drm_display_mode * ) ;
   int (*mode_set)(struct drm_crtc * , struct drm_display_mode * , struct drm_display_mode * ,
                   int , int , struct drm_framebuffer * ) ;
   void (*mode_set_nofb)(struct drm_crtc * ) ;
   int (*mode_set_base)(struct drm_crtc * , int , int , struct drm_framebuffer * ) ;
   int (*mode_set_base_atomic)(struct drm_crtc * , struct drm_framebuffer * , int ,
                               int , enum mode_set_atomic ) ;
   void (*load_lut)(struct drm_crtc * ) ;
   void (*disable)(struct drm_crtc * ) ;
   void (*enable)(struct drm_crtc * ) ;
   int (*atomic_check)(struct drm_crtc * , struct drm_crtc_state * ) ;
   void (*atomic_begin)(struct drm_crtc * ) ;
   void (*atomic_flush)(struct drm_crtc * ) ;
};
struct __anonstruct_interrupt_status_offsets_324 {
   u32 reg ;
   u32 vblank ;
   u32 vline ;
   u32 hpd ;
};
struct dce8_wm_params {
   u32 dram_channels ;
   u32 yclk ;
   u32 sclk ;
   u32 disp_clk ;
   u32 src_width ;
   u32 active_time ;
   u32 blank_time ;
   bool interlaced ;
   fixed20_12 vsc ;
   u32 num_heads ;
   u32 bytes_per_pixel ;
   u32 lb_size ;
   u32 vtaps ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_gds_reg_offset {
   u32 mem_base ;
   u32 mem_size ;
   u32 gws ;
   u32 oa ;
};
struct hqd_registers {
   u32 cp_mqd_base_addr ;
   u32 cp_mqd_base_addr_hi ;
   u32 cp_hqd_active ;
   u32 cp_hqd_vmid ;
   u32 cp_hqd_persistent_state ;
   u32 cp_hqd_pipe_priority ;
   u32 cp_hqd_queue_priority ;
   u32 cp_hqd_quantum ;
   u32 cp_hqd_pq_base ;
   u32 cp_hqd_pq_base_hi ;
   u32 cp_hqd_pq_rptr ;
   u32 cp_hqd_pq_rptr_report_addr ;
   u32 cp_hqd_pq_rptr_report_addr_hi ;
   u32 cp_hqd_pq_wptr_poll_addr ;
   u32 cp_hqd_pq_wptr_poll_addr_hi ;
   u32 cp_hqd_pq_doorbell_control ;
   u32 cp_hqd_pq_wptr ;
   u32 cp_hqd_pq_control ;
   u32 cp_hqd_ib_base_addr ;
   u32 cp_hqd_ib_base_addr_hi ;
   u32 cp_hqd_ib_rptr ;
   u32 cp_hqd_ib_control ;
   u32 cp_hqd_iq_timer ;
   u32 cp_hqd_iq_rptr ;
   u32 cp_hqd_dequeue_request ;
   u32 cp_hqd_dma_offload ;
   u32 cp_hqd_sema_cmd ;
   u32 cp_hqd_msg_type ;
   u32 cp_hqd_atomic0_preop_lo ;
   u32 cp_hqd_atomic0_preop_hi ;
   u32 cp_hqd_atomic1_preop_lo ;
   u32 cp_hqd_atomic1_preop_hi ;
   u32 cp_hqd_hq_scheduler0 ;
   u32 cp_hqd_hq_scheduler1 ;
   u32 cp_mqd_control ;
};
struct bonaire_mqd {
   u32 header ;
   u32 dispatch_initiator ;
   u32 dimensions[3U] ;
   u32 start_idx[3U] ;
   u32 num_threads[3U] ;
   u32 pipeline_stat_enable ;
   u32 perf_counter_enable ;
   u32 pgm[2U] ;
   u32 tba[2U] ;
   u32 tma[2U] ;
   u32 pgm_rsrc[2U] ;
   u32 vmid ;
   u32 resource_limits ;
   u32 static_thread_mgmt01[2U] ;
   u32 tmp_ring_size ;
   u32 static_thread_mgmt23[2U] ;
   u32 restart[3U] ;
   u32 thread_trace_enable ;
   u32 reserved1 ;
   u32 user_data[16U] ;
   u32 vgtcs_invoke_count[2U] ;
   struct hqd_registers queue_state ;
   u32 dequeue_cntr ;
   u32 interrupt_queue[64U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
typedef bool ldv_func_ret_type___3;
typedef bool ldv_func_ret_type___4;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
struct i2c_board_info;
enum i2c_slave_event;
enum i2c_slave_event;
struct i2c_board_info {
   char type[20U] ;
   unsigned short flags ;
   unsigned short addr ;
   void *platform_data ;
   struct dev_archdata *archdata ;
   struct device_node *of_node ;
   struct fwnode_handle *fwnode ;
   int irq ;
};
struct _ATOM_PPLIB_FANTABLE {
   UCHAR ucFanTableFormat ;
   UCHAR ucTHyst ;
   USHORT usTMin ;
   USHORT usTMed ;
   USHORT usTHigh ;
   USHORT usPWMMin ;
   USHORT usPWMMed ;
   USHORT usPWMHigh ;
};
typedef struct _ATOM_PPLIB_FANTABLE ATOM_PPLIB_FANTABLE;
struct _ATOM_PPLIB_FANTABLE2 {
   ATOM_PPLIB_FANTABLE basicTable ;
   USHORT usTMax ;
};
typedef struct _ATOM_PPLIB_FANTABLE2 ATOM_PPLIB_FANTABLE2;
struct _ATOM_PPLIB_FANTABLE3 {
   ATOM_PPLIB_FANTABLE2 basicTable2 ;
   UCHAR ucFanControlMode ;
   USHORT usFanPWMMax ;
   USHORT usFanOutputSensitivity ;
};
struct _ATOM_PPLIB_EXTENDEDHEADER {
   USHORT usSize ;
   ULONG ulMaxEngineClock ;
   ULONG ulMaxMemoryClock ;
   USHORT usVCETableOffset ;
   USHORT usUVDTableOffset ;
   USHORT usSAMUTableOffset ;
   USHORT usPPMTableOffset ;
   USHORT usACPTableOffset ;
   USHORT usPowerTuneTableOffset ;
   USHORT usSclkVddgfxTableOffset ;
};
typedef struct _ATOM_PPLIB_EXTENDEDHEADER ATOM_PPLIB_EXTENDEDHEADER;
typedef struct _ATOM_PPLIB_POWERPLAYTABLE3 ATOM_PPLIB_POWERPLAYTABLE3;
struct _ATOM_PPLIB_POWERPLAYTABLE4 {
   ATOM_PPLIB_POWERPLAYTABLE3 basicTable3 ;
   ULONG ulGoldenPPID ;
   ULONG ulGoldenRevision ;
   USHORT usVddcDependencyOnSCLKOffset ;
   USHORT usVddciDependencyOnMCLKOffset ;
   USHORT usVddcDependencyOnMCLKOffset ;
   USHORT usMaxClockVoltageOnDCOffset ;
   USHORT usVddcPhaseShedLimitsTableOffset ;
   USHORT usMvddDependencyOnMCLKOffset ;
};
typedef struct _ATOM_PPLIB_POWERPLAYTABLE4 ATOM_PPLIB_POWERPLAYTABLE4;
struct _ATOM_PPLIB_POWERPLAYTABLE5 {
   ATOM_PPLIB_POWERPLAYTABLE4 basicTable4 ;
   ULONG ulTDPLimit ;
   ULONG ulNearTDPLimit ;
   ULONG ulSQRampingThreshold ;
   USHORT usCACLeakageTableOffset ;
   ULONG ulCACLeakage ;
   USHORT usTDPODLimit ;
   USHORT usLoadLineSlope ;
};
struct _ATOM_PPLIB_Clock_Voltage_Dependency_Record {
   USHORT usClockLow ;
   UCHAR ucClockHigh ;
   USHORT usVoltage ;
};
typedef struct _ATOM_PPLIB_Clock_Voltage_Dependency_Record ATOM_PPLIB_Clock_Voltage_Dependency_Record;
struct _ATOM_PPLIB_Clock_Voltage_Dependency_Table {
   UCHAR ucNumEntries ;
   ATOM_PPLIB_Clock_Voltage_Dependency_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_Clock_Voltage_Dependency_Table ATOM_PPLIB_Clock_Voltage_Dependency_Table;
struct _ATOM_PPLIB_Clock_Voltage_Limit_Record {
   USHORT usSclkLow ;
   UCHAR ucSclkHigh ;
   USHORT usMclkLow ;
   UCHAR ucMclkHigh ;
   USHORT usVddc ;
   USHORT usVddci ;
};
typedef struct _ATOM_PPLIB_Clock_Voltage_Limit_Record ATOM_PPLIB_Clock_Voltage_Limit_Record;
struct _ATOM_PPLIB_Clock_Voltage_Limit_Table {
   UCHAR ucNumEntries ;
   ATOM_PPLIB_Clock_Voltage_Limit_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_Clock_Voltage_Limit_Table ATOM_PPLIB_Clock_Voltage_Limit_Table;
struct __anonstruct____missing_field_name_322 {
   USHORT usVddc ;
   ULONG ulLeakageValue ;
};
struct __anonstruct____missing_field_name_323 {
   USHORT usVddc1 ;
   USHORT usVddc2 ;
   USHORT usVddc3 ;
};
union _ATOM_PPLIB_CAC_Leakage_Record {
   struct __anonstruct____missing_field_name_322 __annonCompField124 ;
   struct __anonstruct____missing_field_name_323 __annonCompField125 ;
};
typedef union _ATOM_PPLIB_CAC_Leakage_Record ATOM_PPLIB_CAC_Leakage_Record;
struct _ATOM_PPLIB_CAC_Leakage_Table {
   UCHAR ucNumEntries ;
   ATOM_PPLIB_CAC_Leakage_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_CAC_Leakage_Table ATOM_PPLIB_CAC_Leakage_Table;
struct _ATOM_PPLIB_PhaseSheddingLimits_Record {
   USHORT usVoltage ;
   USHORT usSclkLow ;
   UCHAR ucSclkHigh ;
   USHORT usMclkLow ;
   UCHAR ucMclkHigh ;
};
typedef struct _ATOM_PPLIB_PhaseSheddingLimits_Record ATOM_PPLIB_PhaseSheddingLimits_Record;
struct _ATOM_PPLIB_PhaseSheddingLimits_Table {
   UCHAR ucNumEntries ;
   ATOM_PPLIB_PhaseSheddingLimits_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_PhaseSheddingLimits_Table ATOM_PPLIB_PhaseSheddingLimits_Table;
struct _VCEClockInfo {
   USHORT usEVClkLow ;
   UCHAR ucEVClkHigh ;
   USHORT usECClkLow ;
   UCHAR ucECClkHigh ;
};
typedef struct _VCEClockInfo VCEClockInfo;
struct _VCEClockInfoArray {
   UCHAR ucNumEntries ;
   VCEClockInfo entries[1U] ;
};
typedef struct _VCEClockInfoArray VCEClockInfoArray;
struct _ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record {
   USHORT usVoltage ;
   UCHAR ucVCEClockInfoIndex ;
};
typedef struct _ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record;
struct _ATOM_PPLIB_VCE_Clock_Voltage_Limit_Table {
   UCHAR numEntries ;
   ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_VCE_Clock_Voltage_Limit_Table ATOM_PPLIB_VCE_Clock_Voltage_Limit_Table;
struct _ATOM_PPLIB_VCE_State_Record {
   UCHAR ucVCEClockInfoIndex ;
   UCHAR ucClockInfoIndex ;
};
typedef struct _ATOM_PPLIB_VCE_State_Record ATOM_PPLIB_VCE_State_Record;
struct _ATOM_PPLIB_VCE_State_Table {
   UCHAR numEntries ;
   ATOM_PPLIB_VCE_State_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_VCE_State_Table ATOM_PPLIB_VCE_State_Table;
struct _UVDClockInfo {
   USHORT usVClkLow ;
   UCHAR ucVClkHigh ;
   USHORT usDClkLow ;
   UCHAR ucDClkHigh ;
};
typedef struct _UVDClockInfo UVDClockInfo;
struct _UVDClockInfoArray {
   UCHAR ucNumEntries ;
   UVDClockInfo entries[1U] ;
};
typedef struct _UVDClockInfoArray UVDClockInfoArray;
struct _ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record {
   USHORT usVoltage ;
   UCHAR ucUVDClockInfoIndex ;
};
typedef struct _ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record;
struct _ATOM_PPLIB_UVD_Clock_Voltage_Limit_Table {
   UCHAR numEntries ;
   ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_UVD_Clock_Voltage_Limit_Table ATOM_PPLIB_UVD_Clock_Voltage_Limit_Table;
struct _ATOM_PPLIB_SAMClk_Voltage_Limit_Record {
   USHORT usVoltage ;
   USHORT usSAMClockLow ;
   UCHAR ucSAMClockHigh ;
};
typedef struct _ATOM_PPLIB_SAMClk_Voltage_Limit_Record ATOM_PPLIB_SAMClk_Voltage_Limit_Record;
struct _ATOM_PPLIB_SAMClk_Voltage_Limit_Table {
   UCHAR numEntries ;
   ATOM_PPLIB_SAMClk_Voltage_Limit_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_SAMClk_Voltage_Limit_Table ATOM_PPLIB_SAMClk_Voltage_Limit_Table;
struct _ATOM_PPLIB_ACPClk_Voltage_Limit_Record {
   USHORT usVoltage ;
   USHORT usACPClockLow ;
   UCHAR ucACPClockHigh ;
};
typedef struct _ATOM_PPLIB_ACPClk_Voltage_Limit_Record ATOM_PPLIB_ACPClk_Voltage_Limit_Record;
struct _ATOM_PPLIB_ACPClk_Voltage_Limit_Table {
   UCHAR numEntries ;
   ATOM_PPLIB_ACPClk_Voltage_Limit_Record entries[1U] ;
};
typedef struct _ATOM_PPLIB_ACPClk_Voltage_Limit_Table ATOM_PPLIB_ACPClk_Voltage_Limit_Table;
struct _ATOM_PowerTune_Table {
   USHORT usTDP ;
   USHORT usConfigurableTDP ;
   USHORT usTDC ;
   USHORT usBatteryPowerLimit ;
   USHORT usSmallPowerLimit ;
   USHORT usLowCACLeakage ;
   USHORT usHighCACLeakage ;
};
typedef struct _ATOM_PowerTune_Table ATOM_PowerTune_Table;
struct _ATOM_PPLIB_POWERTUNE_Table {
   UCHAR revid ;
   ATOM_PowerTune_Table power_tune_table ;
};
typedef struct _ATOM_PPLIB_POWERTUNE_Table ATOM_PPLIB_POWERTUNE_Table;
struct _ATOM_PPLIB_POWERTUNE_Table_V1 {
   UCHAR revid ;
   ATOM_PowerTune_Table power_tune_table ;
   USHORT usMaximumPowerDeliveryLimit ;
   USHORT usReserve[7U] ;
};
typedef struct _ATOM_PPLIB_POWERTUNE_Table_V1 ATOM_PPLIB_POWERTUNE_Table_V1;
struct _ATOM_PPLIB_PPM_Table {
   UCHAR ucRevId ;
   UCHAR ucPpmDesign ;
   USHORT usCpuCoreNumber ;
   ULONG ulPlatformTDP ;
   ULONG ulSmallACPlatformTDP ;
   ULONG ulPlatformTDC ;
   ULONG ulSmallACPlatformTDC ;
   ULONG ulApuTDP ;
   ULONG ulDGpuTDP ;
   ULONG ulDGpuUlvPower ;
   ULONG ulTjmax ;
};
typedef struct _ATOM_PPLIB_PPM_Table ATOM_PPLIB_PPM_Table;
union power_info___0 {
   struct _ATOM_POWERPLAY_INFO info ;
   struct _ATOM_POWERPLAY_INFO_V2 info_2 ;
   struct _ATOM_POWERPLAY_INFO_V3 info_3 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE pplib ;
   struct _ATOM_PPLIB_POWERPLAYTABLE2 pplib2 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE3 pplib3 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE4 pplib4 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE5 pplib5 ;
};
union fan_info {
   struct _ATOM_PPLIB_FANTABLE fan ;
   struct _ATOM_PPLIB_FANTABLE2 fan2 ;
   struct _ATOM_PPLIB_FANTABLE3 fan3 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum cz_scratch_entry {
    CZ_SCRATCH_ENTRY_UCODE_ID_SDMA0 = 0,
    CZ_SCRATCH_ENTRY_UCODE_ID_SDMA1 = 1,
    CZ_SCRATCH_ENTRY_UCODE_ID_CP_CE = 2,
    CZ_SCRATCH_ENTRY_UCODE_ID_CP_PFP = 3,
    CZ_SCRATCH_ENTRY_UCODE_ID_CP_ME = 4,
    CZ_SCRATCH_ENTRY_UCODE_ID_CP_MEC_JT1 = 5,
    CZ_SCRATCH_ENTRY_UCODE_ID_CP_MEC_JT2 = 6,
    CZ_SCRATCH_ENTRY_UCODE_ID_GMCON_RENG = 7,
    CZ_SCRATCH_ENTRY_UCODE_ID_RLC_G = 8,
    CZ_SCRATCH_ENTRY_UCODE_ID_RLC_SCRATCH = 9,
    CZ_SCRATCH_ENTRY_UCODE_ID_RLC_SRM_ARAM = 10,
    CZ_SCRATCH_ENTRY_UCODE_ID_RLC_SRM_DRAM = 11,
    CZ_SCRATCH_ENTRY_UCODE_ID_DMCU_ERAM = 12,
    CZ_SCRATCH_ENTRY_UCODE_ID_DMCU_IRAM = 13,
    CZ_SCRATCH_ENTRY_UCODE_ID_POWER_PROFILING = 14,
    CZ_SCRATCH_ENTRY_DATA_ID_SDMA_HALT = 15,
    CZ_SCRATCH_ENTRY_DATA_ID_SYS_CLOCKGATING = 16,
    CZ_SCRATCH_ENTRY_DATA_ID_SDMA_RING_REGS = 17,
    CZ_SCRATCH_ENTRY_DATA_ID_NONGFX_REINIT = 18,
    CZ_SCRATCH_ENTRY_DATA_ID_SDMA_START = 19,
    CZ_SCRATCH_ENTRY_DATA_ID_IH_REGISTERS = 20,
    CZ_SCRATCH_ENTRY_SMU8_FUSION_CLKTABLE = 21
} ;
struct cz_buffer_entry {
   u32 data_size ;
   u32 mc_addr_low ;
   u32 mc_addr_high ;
   void *kaddr ;
   enum cz_scratch_entry firmware_ID ;
};
struct cz_register_index_data_pair {
   u32 offset ;
   u32 value ;
};
struct cz_ih_meta_data {
   u32 command ;
   struct cz_register_index_data_pair register_index_value_pair[1U] ;
};
struct cz_smu_private_data {
   uint8_t driver_buffer_length ;
   uint8_t scratch_buffer_length ;
   u16 toc_entry_used_count ;
   u16 toc_entry_initialize_index ;
   u16 toc_entry_power_profiling_index ;
   u16 toc_entry_aram ;
   u16 toc_entry_ih_register_restore_task_index ;
   u16 toc_entry_clock_table ;
   u16 ih_register_restore_task_size ;
   u16 smu_buffer_used_bytes ;
   struct cz_buffer_entry toc_buffer ;
   struct cz_buffer_entry smu_buffer ;
   struct cz_buffer_entry driver_buffer[8U] ;
   struct cz_buffer_entry scratch_buffer[11U] ;
};
struct __anonstruct_data_64_t_274 {
   u32 high ;
   u32 low ;
};
typedef struct __anonstruct_data_64_t_274 data_64_t;
struct SMU_Task {
   uint8_t type ;
   uint8_t arg ;
   u16 next ;
   data_64_t addr ;
   u32 size_bytes ;
};
typedef struct SMU_Task SMU_Task;
struct TOC {
   uint8_t JobList[32U] ;
   SMU_Task tasks[1U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct SMU8_Fusion_ClkLevel {
   uint8_t GnbVid ;
   uint8_t GfxVid ;
   uint8_t DfsDid ;
   uint8_t DeepSleepDid ;
   u32 DfsBypass ;
   u32 Frequency ;
};
struct SMU8_Fusion_SclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   u32 SclkValidMask ;
   u32 MaxSclkIndex ;
};
struct SMU8_Fusion_LclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   u32 LclkValidMask ;
   u32 MaxLclkIndex ;
};
struct SMU8_Fusion_EclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   struct SMU8_Fusion_ClkLevel PwrOffLevel ;
   u32 EclkValidMask ;
   u32 MaxEclkIndex ;
};
struct SMU8_Fusion_VclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   struct SMU8_Fusion_ClkLevel PwrOffLevel ;
   u32 VclkValidMask ;
   u32 MaxVclkIndex ;
};
struct SMU8_Fusion_DclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   struct SMU8_Fusion_ClkLevel PwrOffLevel ;
   u32 DclkValidMask ;
   u32 MaxDclkIndex ;
};
struct SMU8_Fusion_AclkBreakdownTable {
   struct SMU8_Fusion_ClkLevel ClkLevel[8U] ;
   struct SMU8_Fusion_ClkLevel DpmOffLevel ;
   struct SMU8_Fusion_ClkLevel PwrOffLevel ;
   u32 AclkValidMask ;
   u32 MaxAclkIndex ;
};
struct SMU8_Fusion_ClkTable {
   struct SMU8_Fusion_SclkBreakdownTable SclkBreakdownTable ;
   struct SMU8_Fusion_LclkBreakdownTable LclkBreakdownTable ;
   struct SMU8_Fusion_EclkBreakdownTable EclkBreakdownTable ;
   struct SMU8_Fusion_VclkBreakdownTable VclkBreakdownTable ;
   struct SMU8_Fusion_DclkBreakdownTable DclkBreakdownTable ;
   struct SMU8_Fusion_AclkBreakdownTable AclkBreakdownTable ;
};
struct cz_dpm_entry {
   u32 soft_min_clk ;
   u32 hard_min_clk ;
   u32 soft_max_clk ;
   u32 hard_max_clk ;
};
struct cz_pl {
   u32 sclk ;
   uint8_t vddc_index ;
   uint8_t ds_divider_index ;
   uint8_t ss_divider_index ;
   uint8_t allow_gnb_slow ;
   uint8_t force_nbp_state ;
   uint8_t display_wm ;
   uint8_t vce_wm ;
};
struct cz_ps {
   struct cz_pl levels[8U] ;
   u32 num_levels ;
   bool need_dfs_bypass ;
   uint8_t dpm0_pg_nb_ps_lo ;
   uint8_t dpm0_pg_nb_ps_hi ;
   uint8_t dpmx_nb_ps_lo ;
   uint8_t dpmx_nb_ps_hi ;
   bool force_high ;
};
struct cz_sys_info {
   u32 bootup_uma_clk ;
   u32 bootup_sclk ;
   u32 dentist_vco_freq ;
   u32 nb_dpm_enable ;
   u32 nbp_memory_clock[2U] ;
   u32 nbp_n_clock[4U] ;
   uint8_t nbp_voltage_index[4U] ;
   u32 display_clock[8U] ;
   u16 bootup_nb_voltage_index ;
   uint8_t htc_tmp_lmt ;
   uint8_t htc_hyst_lmt ;
   u32 uma_channel_number ;
};
struct cz_power_info {
   u32 active_target[8U] ;
   struct cz_sys_info sys_info ;
   struct cz_pl boot_pl ;
   bool disable_nb_ps3_in_battery ;
   bool battery_state ;
   u32 lowest_valid ;
   u32 highest_valid ;
   u16 high_voltage_threshold ;
   u32 sram_end ;
   u32 dpm_table_start ;
   u32 soft_regs_start ;
   uint8_t uvd_level_count ;
   uint8_t vce_level_count ;
   uint8_t acp_level_count ;
   u32 fps_high_threshold ;
   u32 fps_low_threshold ;
   u32 dpm_flags ;
   struct cz_dpm_entry sclk_dpm ;
   struct cz_dpm_entry uvd_dpm ;
   struct cz_dpm_entry vce_dpm ;
   struct cz_dpm_entry acp_dpm ;
   uint8_t uvd_boot_level ;
   uint8_t uvd_interval ;
   uint8_t vce_boot_level ;
   uint8_t vce_interval ;
   uint8_t acp_boot_level ;
   uint8_t acp_interval ;
   uint8_t graphics_boot_level ;
   uint8_t graphics_interval ;
   uint8_t graphics_therm_throttle_enable ;
   uint8_t graphics_voltage_change_enable ;
   uint8_t graphics_clk_slow_enable ;
   uint8_t graphics_clk_slow_divider ;
   u32 low_sclk_interrupt_threshold ;
   bool uvd_power_gated ;
   bool vce_power_gated ;
   bool acp_power_gated ;
   u32 active_process_mask ;
   u32 mgcg_cgtt_local0 ;
   u32 mgcg_cgtt_local1 ;
   u32 clock_slow_down_step ;
   u32 skip_clock_slow_down ;
   bool enable_nb_ps_policy ;
   u32 voting_clients ;
   u32 voltage_drop_threshold ;
   u32 gfx_pg_threshold ;
   u32 max_sclk_level ;
   bool didt_enabled ;
   bool video_start ;
   bool cac_enabled ;
   bool bapm_enabled ;
   bool nb_dpm_enabled_by_driver ;
   bool nb_dpm_enabled ;
   bool auto_thermal_throttling_enabled ;
   bool dpm_enabled ;
   bool need_pptable_upload ;
   bool caps_cac ;
   bool caps_power_containment ;
   bool caps_sq_ramping ;
   bool caps_db_ramping ;
   bool caps_td_ramping ;
   bool caps_tcp_ramping ;
   bool caps_sclk_throttle_low_notification ;
   bool caps_fps ;
   bool caps_uvd_dpm ;
   bool caps_uvd_pg ;
   bool caps_vce_dpm ;
   bool caps_vce_pg ;
   bool caps_acp_dpm ;
   bool caps_acp_pg ;
   bool caps_stable_power_state ;
   bool caps_enable_dfs_bypass ;
   bool caps_sclk_ds ;
   bool caps_voltage_island ;
   struct amdgpu_ps current_rps ;
   struct cz_ps current_ps ;
   struct amdgpu_ps requested_rps ;
   struct cz_ps requested_ps ;
   bool uvd_power_down ;
   bool vce_power_down ;
   bool acp_power_down ;
   bool uvd_dynamic_pg ;
};
struct _ATOM_PPLIB_CZ_CLOCK_INFO {
   UCHAR index ;
   UCHAR rsv[3U] ;
};
union igp_info___1 {
   struct _ATOM_INTEGRATED_SYSTEM_INFO info ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_7 info_7 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_8 info_8 ;
   struct _ATOM_INTEGRATED_SYSTEM_INFO_V1_9 info_9 ;
};
union pplib_clock_info___1 {
   struct _ATOM_PPLIB_EVERGREEN_CLOCK_INFO evergreen ;
   struct _ATOM_PPLIB_SUMO_CLOCK_INFO sumo ;
   struct _ATOM_PPLIB_CZ_CLOCK_INFO carrizo ;
};
union power_info___1 {
   struct _ATOM_PPLIB_POWERPLAYTABLE pplib ;
   struct _ATOM_PPLIB_POWERPLAYTABLE2 pplib2 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE3 pplib3 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE4 pplib4 ;
   struct _ATOM_PPLIB_POWERPLAYTABLE5 pplib5 ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct tonga_smu_private_data {
   uint8_t *header ;
   u32 smu_buffer_addr_high ;
   u32 smu_buffer_addr_low ;
   u32 header_addr_high ;
   u32 header_addr_low ;
};
struct SMU_Entry {
   u16 id ;
   u16 version ;
   u32 image_addr_high ;
   u32 image_addr_low ;
   u32 meta_data_addr_high ;
   u32 meta_data_addr_low ;
   u32 data_size_byte ;
   u16 flags ;
   u16 num_register_entries ;
};
struct SMU_DRAMData_TOC {
   u32 structure_version ;
   u32 num_entries ;
   struct SMU_Entry entry[12U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct iceland_smu_private_data {
   uint8_t *header ;
   uint8_t *mec_image ;
   u32 header_addr_high ;
   u32 header_addr_low ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct __anonstruct_interrupt_status_offsets_324___0 {
   u32 reg ;
   u32 vblank ;
   u32 vline ;
   u32 hpd ;
};
struct dce10_wm_params {
   u32 dram_channels ;
   u32 yclk ;
   u32 sclk ;
   u32 disp_clk ;
   u32 src_width ;
   u32 active_time ;
   u32 blank_time ;
   bool interlaced ;
   fixed20_12 vsc ;
   u32 num_heads ;
   u32 bytes_per_pixel ;
   u32 lb_size ;
   u32 vtaps ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct __anonstruct_interrupt_status_offsets_324___1 {
   u32 reg ;
   u32 vblank ;
   u32 vline ;
   u32 hpd ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct vi_mqd {
   u32 header ;
   u32 compute_dispatch_initiator ;
   u32 compute_dim_x ;
   u32 compute_dim_y ;
   u32 compute_dim_z ;
   u32 compute_start_x ;
   u32 compute_start_y ;
   u32 compute_start_z ;
   u32 compute_num_thread_x ;
   u32 compute_num_thread_y ;
   u32 compute_num_thread_z ;
   u32 compute_pipelinestat_enable ;
   u32 compute_perfcount_enable ;
   u32 compute_pgm_lo ;
   u32 compute_pgm_hi ;
   u32 compute_tba_lo ;
   u32 compute_tba_hi ;
   u32 compute_tma_lo ;
   u32 compute_tma_hi ;
   u32 compute_pgm_rsrc1 ;
   u32 compute_pgm_rsrc2 ;
   u32 compute_vmid ;
   u32 compute_resource_limits ;
   u32 compute_static_thread_mgmt_se0 ;
   u32 compute_static_thread_mgmt_se1 ;
   u32 compute_tmpring_size ;
   u32 compute_static_thread_mgmt_se2 ;
   u32 compute_static_thread_mgmt_se3 ;
   u32 compute_restart_x ;
   u32 compute_restart_y ;
   u32 compute_restart_z ;
   u32 compute_thread_trace_enable ;
   u32 compute_misc_reserved ;
   u32 compute_dispatch_id ;
   u32 compute_threadgroup_id ;
   u32 compute_relaunch ;
   u32 compute_wave_restore_addr_lo ;
   u32 compute_wave_restore_addr_hi ;
   u32 compute_wave_restore_control ;
   u32 reserved9 ;
   u32 reserved10 ;
   u32 reserved11 ;
   u32 reserved12 ;
   u32 reserved13 ;
   u32 reserved14 ;
   u32 reserved15 ;
   u32 reserved16 ;
   u32 reserved17 ;
   u32 reserved18 ;
   u32 reserved19 ;
   u32 reserved20 ;
   u32 reserved21 ;
   u32 reserved22 ;
   u32 reserved23 ;
   u32 reserved24 ;
   u32 reserved25 ;
   u32 reserved26 ;
   u32 reserved27 ;
   u32 reserved28 ;
   u32 reserved29 ;
   u32 reserved30 ;
   u32 reserved31 ;
   u32 reserved32 ;
   u32 reserved33 ;
   u32 reserved34 ;
   u32 compute_user_data_0 ;
   u32 compute_user_data_1 ;
   u32 compute_user_data_2 ;
   u32 compute_user_data_3 ;
   u32 compute_user_data_4 ;
   u32 compute_user_data_5 ;
   u32 compute_user_data_6 ;
   u32 compute_user_data_7 ;
   u32 compute_user_data_8 ;
   u32 compute_user_data_9 ;
   u32 compute_user_data_10 ;
   u32 compute_user_data_11 ;
   u32 compute_user_data_12 ;
   u32 compute_user_data_13 ;
   u32 compute_user_data_14 ;
   u32 compute_user_data_15 ;
   u32 cp_compute_csinvoc_count_lo ;
   u32 cp_compute_csinvoc_count_hi ;
   u32 reserved35 ;
   u32 reserved36 ;
   u32 reserved37 ;
   u32 cp_mqd_query_time_lo ;
   u32 cp_mqd_query_time_hi ;
   u32 cp_mqd_connect_start_time_lo ;
   u32 cp_mqd_connect_start_time_hi ;
   u32 cp_mqd_connect_end_time_lo ;
   u32 cp_mqd_connect_end_time_hi ;
   u32 cp_mqd_connect_end_wf_count ;
   u32 cp_mqd_connect_end_pq_rptr ;
   u32 cp_mqd_connect_end_pq_wptr ;
   u32 cp_mqd_connect_end_ib_rptr ;
   u32 reserved38 ;
   u32 reserved39 ;
   u32 cp_mqd_save_start_time_lo ;
   u32 cp_mqd_save_start_time_hi ;
   u32 cp_mqd_save_end_time_lo ;
   u32 cp_mqd_save_end_time_hi ;
   u32 cp_mqd_restore_start_time_lo ;
   u32 cp_mqd_restore_start_time_hi ;
   u32 cp_mqd_restore_end_time_lo ;
   u32 cp_mqd_restore_end_time_hi ;
   u32 reserved40 ;
   u32 reserved41 ;
   u32 gds_cs_ctxsw_cnt0 ;
   u32 gds_cs_ctxsw_cnt1 ;
   u32 gds_cs_ctxsw_cnt2 ;
   u32 gds_cs_ctxsw_cnt3 ;
   u32 reserved42 ;
   u32 reserved43 ;
   u32 cp_pq_exe_status_lo ;
   u32 cp_pq_exe_status_hi ;
   u32 cp_packet_id_lo ;
   u32 cp_packet_id_hi ;
   u32 cp_packet_exe_status_lo ;
   u32 cp_packet_exe_status_hi ;
   u32 gds_save_base_addr_lo ;
   u32 gds_save_base_addr_hi ;
   u32 gds_save_mask_lo ;
   u32 gds_save_mask_hi ;
   u32 ctx_save_base_addr_lo ;
   u32 ctx_save_base_addr_hi ;
   u32 reserved44 ;
   u32 reserved45 ;
   u32 cp_mqd_base_addr_lo ;
   u32 cp_mqd_base_addr_hi ;
   u32 cp_hqd_active ;
   u32 cp_hqd_vmid ;
   u32 cp_hqd_persistent_state ;
   u32 cp_hqd_pipe_priority ;
   u32 cp_hqd_queue_priority ;
   u32 cp_hqd_quantum ;
   u32 cp_hqd_pq_base_lo ;
   u32 cp_hqd_pq_base_hi ;
   u32 cp_hqd_pq_rptr ;
   u32 cp_hqd_pq_rptr_report_addr_lo ;
   u32 cp_hqd_pq_rptr_report_addr_hi ;
   u32 cp_hqd_pq_wptr_poll_addr ;
   u32 cp_hqd_pq_wptr_poll_addr_hi ;
   u32 cp_hqd_pq_doorbell_control ;
   u32 cp_hqd_pq_wptr ;
   u32 cp_hqd_pq_control ;
   u32 cp_hqd_ib_base_addr_lo ;
   u32 cp_hqd_ib_base_addr_hi ;
   u32 cp_hqd_ib_rptr ;
   u32 cp_hqd_ib_control ;
   u32 cp_hqd_iq_timer ;
   u32 cp_hqd_iq_rptr ;
   u32 cp_hqd_dequeue_request ;
   u32 cp_hqd_dma_offload ;
   u32 cp_hqd_sema_cmd ;
   u32 cp_hqd_msg_type ;
   u32 cp_hqd_atomic0_preop_lo ;
   u32 cp_hqd_atomic0_preop_hi ;
   u32 cp_hqd_atomic1_preop_lo ;
   u32 cp_hqd_atomic1_preop_hi ;
   u32 cp_hqd_hq_status0 ;
   u32 cp_hqd_hq_control0 ;
   u32 cp_mqd_control ;
   u32 cp_hqd_hq_status1 ;
   u32 cp_hqd_hq_control1 ;
   u32 cp_hqd_eop_base_addr_lo ;
   u32 cp_hqd_eop_base_addr_hi ;
   u32 cp_hqd_eop_control ;
   u32 cp_hqd_eop_rptr ;
   u32 cp_hqd_eop_wptr ;
   u32 cp_hqd_eop_done_events ;
   u32 cp_hqd_ctx_save_base_addr_lo ;
   u32 cp_hqd_ctx_save_base_addr_hi ;
   u32 cp_hqd_ctx_save_control ;
   u32 cp_hqd_cntl_stack_offset ;
   u32 cp_hqd_cntl_stack_size ;
   u32 cp_hqd_wg_state_offset ;
   u32 cp_hqd_ctx_save_size ;
   u32 cp_hqd_gds_resource_state ;
   u32 cp_hqd_error ;
   u32 cp_hqd_eop_wptr_mem ;
   u32 cp_hqd_eop_dones ;
   u32 reserved46 ;
   u32 reserved47 ;
   u32 reserved48 ;
   u32 reserved49 ;
   u32 reserved50 ;
   u32 reserved51 ;
   u32 reserved52 ;
   u32 reserved53 ;
   u32 reserved54 ;
   u32 reserved55 ;
   u32 iqtimer_pkt_header ;
   u32 iqtimer_pkt_dw0 ;
   u32 iqtimer_pkt_dw1 ;
   u32 iqtimer_pkt_dw2 ;
   u32 iqtimer_pkt_dw3 ;
   u32 iqtimer_pkt_dw4 ;
   u32 iqtimer_pkt_dw5 ;
   u32 iqtimer_pkt_dw6 ;
   u32 iqtimer_pkt_dw7 ;
   u32 iqtimer_pkt_dw8 ;
   u32 iqtimer_pkt_dw9 ;
   u32 iqtimer_pkt_dw10 ;
   u32 iqtimer_pkt_dw11 ;
   u32 iqtimer_pkt_dw12 ;
   u32 iqtimer_pkt_dw13 ;
   u32 iqtimer_pkt_dw14 ;
   u32 iqtimer_pkt_dw15 ;
   u32 iqtimer_pkt_dw16 ;
   u32 iqtimer_pkt_dw17 ;
   u32 iqtimer_pkt_dw18 ;
   u32 iqtimer_pkt_dw19 ;
   u32 iqtimer_pkt_dw20 ;
   u32 iqtimer_pkt_dw21 ;
   u32 iqtimer_pkt_dw22 ;
   u32 iqtimer_pkt_dw23 ;
   u32 iqtimer_pkt_dw24 ;
   u32 iqtimer_pkt_dw25 ;
   u32 iqtimer_pkt_dw26 ;
   u32 iqtimer_pkt_dw27 ;
   u32 iqtimer_pkt_dw28 ;
   u32 iqtimer_pkt_dw29 ;
   u32 iqtimer_pkt_dw30 ;
   u32 iqtimer_pkt_dw31 ;
   u32 reserved56 ;
   u32 reserved57 ;
   u32 reserved58 ;
   u32 set_resources_header ;
   u32 set_resources_dw1 ;
   u32 set_resources_dw2 ;
   u32 set_resources_dw3 ;
   u32 set_resources_dw4 ;
   u32 set_resources_dw5 ;
   u32 set_resources_dw6 ;
   u32 set_resources_dw7 ;
   u32 reserved59 ;
   u32 reserved60 ;
   u32 reserved61 ;
   u32 reserved62 ;
   u32 reserved63 ;
   u32 reserved64 ;
   u32 reserved65 ;
   u32 reserved66 ;
   u32 reserved67 ;
   u32 reserved68 ;
   u32 reserved69 ;
   u32 reserved70 ;
   u32 reserved71 ;
   u32 reserved72 ;
   u32 reserved73 ;
   u32 reserved74 ;
   u32 reserved75 ;
   u32 reserved76 ;
   u32 reserved77 ;
   u32 reserved78 ;
   u32 reserved_t[256U] ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_uvd_cs_ctx {
   struct amdgpu_cs_parser *parser ;
   unsigned int reg ;
   unsigned int count ;
   unsigned int data0 ;
   unsigned int data1 ;
   unsigned int idx ;
   unsigned int ib_idx ;
   bool has_msg_cmd ;
   unsigned int *buf_sizes ;
};
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
enum i2c_slave_event;
enum i2c_slave_event;
enum hrtimer_restart;
struct bio_vec;
struct iovec {
   void *iov_base ;
   __kernel_size_t iov_len ;
};
struct kvec {
   void *iov_base ;
   size_t iov_len ;
};
union __anonunion____missing_field_name_217 {
   struct iovec const *iov ;
   struct kvec const *kvec ;
   struct bio_vec const *bvec ;
};
struct iov_iter {
   int type ;
   size_t iov_offset ;
   size_t count ;
   union __anonunion____missing_field_name_217 __annonCompField58 ;
   unsigned long nr_segs ;
};
struct bio_vec {
   struct page *bv_page ;
   unsigned int bv_len ;
   unsigned int bv_offset ;
};
typedef s32 compat_long_t;
typedef u32 compat_uptr_t;
struct compat_robust_list {
   compat_uptr_t next ;
};
struct compat_robust_list_head {
   struct compat_robust_list list ;
   compat_long_t futex_offset ;
   compat_uptr_t list_op_pending ;
};
enum hrtimer_restart;
enum vga_switcheroo_client_id {
    VGA_SWITCHEROO_IGD = 0,
    VGA_SWITCHEROO_DIS = 1,
    VGA_SWITCHEROO_MAX_CLIENTS = 2
} ;
struct vga_switcheroo_handler {
   int (*switchto)(enum vga_switcheroo_client_id ) ;
   int (*power_state)(enum vga_switcheroo_client_id , enum vga_switcheroo_state ) ;
   int (*init)(void) ;
   int (*get_client_id)(struct pci_dev * ) ;
};
struct amdgpu_atpx_functions {
   bool px_params ;
   bool power_cntl ;
   bool disp_mux_cntl ;
   bool i2c_mux_cntl ;
   bool switch_start ;
   bool switch_end ;
   bool disp_connectors_mapping ;
   bool disp_detetion_ports ;
};
struct amdgpu_atpx {
   acpi_handle handle ;
   struct amdgpu_atpx_functions functions ;
};
struct amdgpu_atpx_priv {
   bool atpx_detected ;
   acpi_handle dhandle ;
   acpi_handle other_handle ;
   struct amdgpu_atpx atpx ;
};
struct atpx_verify_interface {
   u16 size ;
   u16 version ;
   u32 function_bits ;
};
struct atpx_px_params {
   u16 size ;
   u32 valid_flags ;
   u32 flags ;
};
struct atpx_power_control {
   u16 size ;
   u8 dgpu_state ;
};
struct atpx_mux {
   u16 size ;
   u16 mux ;
};
enum hrtimer_restart;
struct acpi_bus_event {
   struct list_head node ;
   acpi_device_class device_class ;
   acpi_bus_id bus_id ;
   u32 type ;
   u32 data ;
};
enum i2c_slave_event;
enum i2c_slave_event;
enum backlight_update_reason {
    BACKLIGHT_UPDATE_HOTKEY = 0,
    BACKLIGHT_UPDATE_SYSFS = 1
} ;
struct atif_verify_interface {
   u16 size ;
   u16 version ;
   u32 notification_mask ;
   u32 function_bits ;
};
struct atif_system_params {
   u16 size ;
   u32 valid_mask ;
   u32 flags ;
   u8 command_code ;
};
struct atif_sbios_requests {
   u16 size ;
   u32 pending ;
   u8 panel_exp_mode ;
   u8 thermal_gfx ;
   u8 thermal_state ;
   u8 forced_power_gfx ;
   u8 forced_power_state ;
   u8 system_power_src ;
   u8 backlight_level ;
};
struct atcs_verify_interface {
   u16 size ;
   u16 version ;
   u32 function_bits ;
};
struct atcs_pref_req_input {
   u16 size ;
   u16 client_id ;
   u16 valid_flags_mask ;
   u16 flags ;
   u8 req_type ;
   u8 perf_req ;
};
struct atcs_pref_req_output {
   u16 size ;
   u8 ret_val ;
};
enum hrtimer_restart;
struct mmu_notifier_ops;
struct mmu_notifier_mm {
   struct hlist_head list ;
   spinlock_t lock ;
};
struct mmu_notifier_ops {
   void (*release)(struct mmu_notifier * , struct mm_struct * ) ;
   int (*clear_flush_young)(struct mmu_notifier * , struct mm_struct * , unsigned long ,
                            unsigned long ) ;
   int (*test_young)(struct mmu_notifier * , struct mm_struct * , unsigned long ) ;
   void (*change_pte)(struct mmu_notifier * , struct mm_struct * , unsigned long ,
                      pte_t ) ;
   void (*invalidate_page)(struct mmu_notifier * , struct mm_struct * , unsigned long ) ;
   void (*invalidate_range_start)(struct mmu_notifier * , struct mm_struct * , unsigned long ,
                                  unsigned long ) ;
   void (*invalidate_range_end)(struct mmu_notifier * , struct mm_struct * , unsigned long ,
                                unsigned long ) ;
   void (*invalidate_range)(struct mmu_notifier * , struct mm_struct * , unsigned long ,
                            unsigned long ) ;
};
struct mmu_notifier {
   struct hlist_node hlist ;
   struct mmu_notifier_ops const *ops ;
};
enum i2c_slave_event;
enum i2c_slave_event;
struct amdgpu_mn {
   struct amdgpu_device *adev ;
   struct mm_struct *mm ;
   struct mmu_notifier mn ;
   struct work_struct work ;
   struct hlist_node node ;
   struct mutex lock ;
   struct rb_root objects ;
};
struct amdgpu_mn_node {
   struct interval_tree_node it ;
   struct list_head bos ;
};
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern struct module __this_module ;
extern int printk(char const * , ...) ;
extern void *memset(void * , int , size_t ) ;
extern unsigned long volatile jiffies ;
extern bool queue_work_on(int , struct workqueue_struct * , struct work_struct * ) ;
bool ldv_queue_work_on_5(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                         struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_7(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                         struct work_struct *ldv_func_arg3 ) ;
extern bool queue_delayed_work_on(int , struct workqueue_struct * , struct delayed_work * ,
                                  unsigned long ) ;
bool ldv_queue_delayed_work_on_6(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                 struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_9(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                 struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
extern void flush_workqueue(struct workqueue_struct * ) ;
void ldv_flush_workqueue_8(struct workqueue_struct *ldv_func_arg1 ) ;
extern void kfree(void const * ) ;
extern void *__kmalloc(size_t , gfp_t ) ;
__inline static void *kmalloc(size_t size , gfp_t flags )
{
  void *tmp___2 ;
  {
  tmp___2 = __kmalloc(size, flags);
  return (tmp___2);
}
}
__inline static void *kzalloc(size_t size , gfp_t flags )
{
  void *tmp ;
  {
  tmp = kmalloc(size, flags | 32768U);
  return (tmp);
}
}
extern void *malloc(size_t ) ;
extern void *calloc(size_t , size_t ) ;
extern int __VERIFIER_nondet_int(void) ;
extern unsigned long __VERIFIER_nondet_ulong(void) ;
extern void *__VERIFIER_nondet_pointer(void) ;
extern void __VERIFIER_assume(int ) ;
void *ldv_malloc(size_t size )
{
  void *p ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    return ((void *)0);
  } else {
    tmp = malloc(size);
    p = tmp;
    __VERIFIER_assume((unsigned long )p != (unsigned long )((void *)0));
    return (p);
  }
}
}
void *ldv_zalloc(size_t size )
{
  void *p ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    return ((void *)0);
  } else {
    tmp = calloc(1UL, size);
    p = tmp;
    __VERIFIER_assume((unsigned long )p != (unsigned long )((void *)0));
    return (p);
  }
}
}
void *ldv_init_zalloc(size_t size )
{
  void *p ;
  void *tmp ;
  {
  tmp = calloc(1UL, size);
  p = tmp;
  __VERIFIER_assume((unsigned long )p != (unsigned long )((void *)0));
  return (p);
}
}
void *ldv_memset(void *s , int c , size_t n )
{
  void *tmp ;
  {
  tmp = memset(s, c, n);
  return (tmp);
}
}
int ldv_undef_int(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  return (tmp);
}
}
void *ldv_undef_ptr(void)
{
  void *tmp ;
  {
  tmp = __VERIFIER_nondet_pointer();
  return (tmp);
}
}
unsigned long ldv_undef_ulong(void)
{
  unsigned long tmp ;
  {
  tmp = __VERIFIER_nondet_ulong();
  return (tmp);
}
}
__inline static void ldv_stop(void)
{
  {
  LDV_STOP: ;
  goto LDV_STOP;
}
}
__inline static long ldv__builtin_expect(long exp , long c )
{
  {
  return (exp);
}
}
int ldv_state_variable_151 ;
struct work_struct *ldv_work_struct_9_2 ;
int ldv_state_variable_99 ;
int ldv_state_variable_47 ;
int ldv_state_variable_20 ;
struct amdgpu_irq_src *kv_dpm_irq_funcs_group1 ;
struct device *sensor_dev_attr_pwm1_enable_group1 ;
int ldv_state_variable_125 ;
int ldv_state_variable_173 ;
struct trace_event_call *event_class_amdgpu_vm_bo_update_group0 ;
int ldv_state_variable_54 ;
int ldv_state_variable_17 ;
struct drm_encoder *dce_v11_0_ext_helper_funcs_group0 ;
struct drm_gem_object *kms_driver_group0 ;
int ldv_state_variable_160 ;
struct drm_encoder *dce_v10_0_dac_helper_funcs_group0 ;
int ldv_state_variable_66 ;
int ldv_state_variable_19 ;
struct work_struct *ldv_work_struct_4_3 ;
int ldv_state_variable_27 ;
int ldv_state_variable_9 ;
int ldv_state_variable_100 ;
struct drm_connector *amdgpu_connector_dp_helper_funcs_group0 ;
int ldv_state_variable_83 ;
struct trace_event_call *event_class_amdgpu_bo_create_group0 ;
int ldv_work_3_3 ;
struct amdgpu_irq_src *uvd_v5_0_irq_funcs_group1 ;
int ldv_state_variable_55 ;
struct trace_event_call *event_class_amdgpu_vm_set_page_group0 ;
struct amdgpu_device *cik_ih_funcs_group0 ;
int ldv_work_1_3 ;
int ldv_state_variable_145 ;
struct work_struct *ldv_work_struct_3_2 ;
int ldv_state_variable_80 ;
struct work_struct *ldv_work_struct_7_2 ;
int ldv_state_variable_64 ;
int ldv_state_variable_28 ;
struct amdgpu_irq_src *dce_v11_0_hpd_irq_funcs_group1 ;
struct file *amdgpu_ttm_vram_fops_group2 ;
struct amdgpu_device *gfx_v7_0_priv_inst_irq_funcs_group0 ;
struct amdgpu_device *dce_v10_0_hpd_irq_funcs_group0 ;
struct work_struct *ldv_work_struct_6_0 ;
struct fence *amdgpu_fence_ops_group0 ;
struct amdgpu_irq_src *dce_v8_0_hpd_irq_funcs_group1 ;
int ldv_work_8_3 ;
int ldv_state_variable_166 ;
struct amdgpu_device *sdma_v2_4_trap_irq_funcs_group0 ;
void *amdgpu_ttm_gtt_fops_group1 ;
struct pci_dev *amdgpu_switcheroo_ops_group0 ;
int ldv_work_7_1 ;
struct amdgpu_mode_mc_save *dce_v10_0_display_funcs_group1 ;
int ldv_state_variable_78 ;
struct amdgpu_device *cik_sdma_trap_irq_funcs_group0 ;
int ldv_state_variable_76 ;
struct amdgpu_device *dce_v8_0_crtc_irq_funcs_group0 ;
int ldv_work_6_2 ;
int ldv_state_variable_137 ;
struct amdgpu_device *sdma_v3_0_trap_irq_funcs_group0 ;
struct device *sensor_dev_attr_pwm1_group1 ;
int ldv_state_variable_89 ;
struct amdgpu_device *vi_asic_funcs_group0 ;
int ldv_state_variable_124 ;
int ldv_state_variable_8 ;
int ldv_state_variable_169 ;
int ldv_state_variable_46 ;
struct backlight_device *amdgpu_atombios_encoder_backlight_ops_group0 ;
struct trace_event_call *event_class_amdgpu_vm_grab_id_group0 ;
int ldv_work_8_0 ;
int ldv_state_variable_75 ;
int ldv_state_variable_33 ;
struct drm_encoder *dce_v11_0_dig_helper_funcs_group0 ;
int ldv_state_variable_123 ;
int ldv_state_variable_161 ;
int ldv_state_variable_172 ;
int ldv_work_3_0 ;
int ldv_work_10_0 ;
int ldv_state_variable_65 ;
struct drm_connector *amdgpu_connector_lvds_helper_funcs_group0 ;
struct amdgpu_device *gmc_v8_0_irq_funcs_group0 ;
int ldv_state_variable_98 ;
struct trace_event_call *event_class_amdgpu_vm_bo_unmap_group0 ;
struct trace_event_call *event_class_amdgpu_cs_group0 ;
struct amdgpu_irq_src *dce_v10_0_pageflip_irq_funcs_group1 ;
struct trace_event_call *event_class_amdgpu_vm_flush_group0 ;
struct drm_display_mode *dce_v10_0_dac_helper_funcs_group1 ;
struct amdgpu_device *kv_dpm_irq_funcs_group0 ;
int ldv_state_variable_70 ;
struct amdgpu_irq_src *dce_v10_0_crtc_irq_funcs_group1 ;
int ldv_state_variable_142 ;
int ldv_state_variable_158 ;
int ldv_work_6_1 ;
struct work_struct *ldv_work_struct_1_0 ;
int ldv_work_7_0 ;
struct amdgpu_irq_src *dce_v10_0_hpd_irq_funcs_group1 ;
struct drm_crtc *amdgpu_fb_helper_funcs_group0 ;
int ldv_state_variable_63 ;
struct work_struct *ldv_work_struct_7_3 ;
int ldv_state_variable_105 ;
int ldv_state_variable_2 ;
struct amdgpu_ring *uvd_v4_2_ring_funcs_group0 ;
int ldv_work_2_0 ;
struct work_struct *ldv_work_struct_10_0 ;
int ldv_work_4_2 ;
int ldv_state_variable_11 ;
int ldv_state_variable_113 ;
int ldv_work_1_2 ;
struct ttm_bo_device *amdgpu_bo_driver_group2 ;
int ldv_state_variable_18 ;
int ldv_state_variable_150 ;
struct amdgpu_device *tonga_ih_funcs_group0 ;
struct work_struct *ldv_work_struct_5_0 ;
struct work_struct *ldv_work_struct_9_1 ;
struct drm_encoder *dce_v11_0_dac_helper_funcs_group0 ;
struct amdgpu_encoder *dce_v8_0_display_funcs_group2 ;
int ldv_state_variable_90 ;
struct amdgpu_irq_src *gfx_v7_0_priv_inst_irq_funcs_group1 ;
int ldv_state_variable_97 ;
int ldv_state_variable_162 ;
int pci_counter ;
struct amdgpu_device *dce_v8_0_display_funcs_group0 ;
struct amdgpu_irq_src *gmc_v8_0_irq_funcs_group1 ;
int ldv_state_variable_30 ;
struct amdgpu_device *dce_v10_0_pageflip_irq_funcs_group0 ;
int ldv_work_8_1 ;
int ldv_state_variable_0 ;
int ldv_state_variable_81 ;
int ldv_state_variable_102 ;
struct drm_display_mode *dce_v8_0_dig_helper_funcs_group1 ;
int ldv_state_variable_87 ;
int ldv_state_variable_136 ;
int ldv_state_variable_73 ;
void *amdgpu_ttm_vram_fops_group1 ;
int ldv_state_variable_29 ;
int ldv_state_variable_115 ;
struct work_struct *ldv_work_struct_8_1 ;
struct work_struct *ldv_work_struct_2_0 ;
struct amdgpu_device *ci_dpm_irq_funcs_group0 ;
int ldv_state_variable_91 ;
struct work_struct *ldv_work_struct_6_1 ;
int ref_cnt ;
int ldv_state_variable_168 ;
struct work_struct *ldv_work_struct_10_3 ;
struct amdgpu_irq_src *uvd_v6_0_irq_funcs_group1 ;
struct work_struct *ldv_work_struct_3_3 ;
int ldv_state_variable_23 ;
struct drm_crtc *dce_v11_0_crtc_helper_funcs_group0 ;
int ldv_state_variable_143 ;
struct amdgpu_ring *gfx_v7_0_ring_funcs_compute_group0 ;
struct work_struct *ldv_work_struct_1_1 ;
struct amdgpu_device *ci_dpm_funcs_group0 ;
int ldv_state_variable_59 ;
int ldv_state_variable_6 ;
struct amdgpu_device *gmc_v8_0_gart_funcs_group0 ;
struct drm_crtc *dce_v8_0_crtc_funcs_group0 ;
struct drm_encoder *dce_v10_0_dig_helper_funcs_group0 ;
struct work_struct *ldv_work_struct_4_2 ;
struct drm_framebuffer *dce_v8_0_crtc_helper_funcs_group1 ;
struct ttm_buffer_object *amdgpu_bo_driver_group3 ;
int ldv_state_variable_178 ;
struct amdgpu_ring *cik_sdma_buffer_funcs_group0 ;
int ldv_state_variable_38 ;
int ldv_state_variable_157 ;
struct amdgpu_device *gfx_v7_0_eop_irq_funcs_group0 ;
int ldv_state_variable_126 ;
int ldv_state_variable_104 ;
struct device *dev_attr_power_dpm_force_performance_level_group1 ;
struct inode *amdgpu_driver_kms_fops_group1 ;
struct pci_dev *amdgpu_kms_pci_driver_group1 ;
int ldv_state_variable_52 ;
struct amdgpu_device *uvd_v4_2_irq_funcs_group0 ;
struct drm_framebuffer *amdgpu_fb_funcs_group0 ;
struct work_struct *ldv_work_struct_9_0 ;
int ldv_work_10_2 ;
struct drm_encoder *dce_v10_0_ext_helper_funcs_group0 ;
int ldv_state_variable_103 ;
int ldv_state_variable_36 ;
int ldv_state_variable_60 ;
int ldv_state_variable_140 ;
int ldv_state_variable_107 ;
int ldv_state_variable_48 ;
struct amdgpu_device *uvd_v6_0_irq_funcs_group0 ;
int ldv_state_variable_148 ;
struct amdgpu_ring *sdma_v2_4_buffer_funcs_group0 ;
struct trace_event_call *event_class_amdgpu_semaphore_request_group0 ;
int ldv_state_variable_163 ;
struct amdgpu_device *cik_asic_funcs_group0 ;
int ldv_work_3_2 ;
struct drm_minor *kms_driver_group1 ;
int ldv_state_variable_138 ;
struct amdgpu_irq_src *vce_v3_0_irq_funcs_group1 ;
int ldv_state_variable_82 ;
struct work_struct *ldv_work_struct_2_3 ;
struct device_attribute *sensor_dev_attr_pwm1_enable_group0 ;
int ldv_state_variable_49 ;
struct drm_display_mode *dce_v11_0_crtc_helper_funcs_group2 ;
int ldv_state_variable_24 ;
struct drm_encoder *dce_v8_0_dig_helper_funcs_group0 ;
struct device *amdgpu_pm_ops_group1 ;
int ldv_state_variable_1 ;
struct drm_display_mode *dce_v11_0_ext_helper_funcs_group1 ;
struct amdgpu_device *dce_v11_0_hpd_irq_funcs_group0 ;
struct amdgpu_ring *vce_v3_0_ring_funcs_group0 ;
int ldv_state_variable_114 ;
struct amdgpu_irq_src *uvd_v4_2_irq_funcs_group1 ;
int ldv_state_variable_176 ;
struct work_struct *ldv_work_struct_6_2 ;
int ldv_state_variable_16 ;
struct amdgpu_irq_src *dce_v8_0_pageflip_irq_funcs_group1 ;
int ldv_work_6_3 ;
struct work_struct *ldv_work_struct_3_0 ;
void *amdgpu_debugfs_regs_fops_group1 ;
int ldv_state_variable_131 ;
int ldv_state_variable_53 ;
int ldv_state_variable_67 ;
struct work_struct *ldv_work_struct_1_2 ;
struct drm_crtc *dce_v11_0_crtc_funcs_group0 ;
struct drm_crtc *dce_v8_0_crtc_helper_funcs_group0 ;
struct drm_connector *amdgpu_connector_lvds_funcs_group0 ;
struct amdgpu_irq_src *sdma_v2_4_trap_irq_funcs_group1 ;
struct amdgpu_ring *vce_v2_0_ring_funcs_group0 ;
struct work_struct *ldv_work_struct_4_1 ;
int ldv_state_variable_92 ;
int ldv_state_variable_130 ;
int ldv_work_10_3 ;
int ldv_state_variable_156 ;
struct drm_encoder *dce_v8_0_dac_helper_funcs_group0 ;
int ldv_state_variable_179 ;
int ldv_state_variable_35 ;
struct work_struct *ldv_work_struct_3_1 ;
int ldv_state_variable_106 ;
struct fb_var_screeninfo *amdgpufb_ops_group0 ;
int ldv_work_1_1 ;
int ldv_state_variable_111 ;
int ldv_work_9_3 ;
int ldv_state_variable_149 ;
int ldv_state_variable_109 ;
int ldv_state_variable_14 ;
int ldv_state_variable_37 ;
int ldv_state_variable_51 ;
struct work_struct *ldv_work_struct_10_1 ;
int ldv_work_7_2 ;
struct trace_event_call *event_class_amdgpu_fence_request_group0 ;
struct device_attribute *sensor_dev_attr_pwm1_group0 ;
struct amdgpu_ring *cik_sdma_ring_funcs_group0 ;
struct work_struct *ldv_work_struct_2_2 ;
struct work_struct *ldv_work_struct_7_1 ;
int ldv_state_variable_42 ;
struct amdgpu_ring *gfx_v7_0_ring_funcs_gfx_group0 ;
struct amdgpu_device *dce_v10_0_display_funcs_group0 ;
struct work_struct *ldv_work_struct_4_0 ;
struct drm_display_mode *dce_v10_0_crtc_helper_funcs_group2 ;
int ldv_state_variable_7 ;
struct drm_display_mode *dce_v10_0_dig_helper_funcs_group1 ;
int ldv_state_variable_164 ;
int ldv_state_variable_119 ;
struct drm_framebuffer *dce_v10_0_crtc_helper_funcs_group1 ;
struct amdgpu_ring *uvd_v6_0_ring_funcs_group0 ;
struct drm_connector *amdgpu_connector_dvi_funcs_group0 ;
int ldv_state_variable_174 ;
int ldv_work_4_0 ;
struct work_struct *ldv_work_struct_2_1 ;
struct amdgpu_ring *gfx_v8_0_ring_funcs_gfx_group0 ;
struct ttm_tt *amdgpu_backend_func_group0 ;
struct amdgpu_device *dce_v11_0_pageflip_irq_funcs_group0 ;
int ldv_state_variable_26 ;
struct drm_device *kms_driver_group3 ;
struct drm_display_mode *dce_v11_0_dac_helper_funcs_group1 ;
struct work_struct *ldv_work_struct_7_0 ;
int LDV_IN_INTERRUPT = 1;
int ldv_state_variable_58 ;
int ldv_state_variable_155 ;
struct amdgpu_device *gmc_v7_0_irq_funcs_group0 ;
int ldv_work_5_2 ;
int ldv_state_variable_93 ;
int ldv_state_variable_177 ;
struct amdgpu_device *gfx_v8_0_priv_inst_irq_funcs_group0 ;
int ldv_state_variable_31 ;
int ldv_state_variable_96 ;
int ldv_state_variable_141 ;
int ldv_state_variable_68 ;
struct drm_connector *amdgpu_connector_dvi_helper_funcs_group0 ;
int ldv_work_2_1 ;
struct amdgpu_device *cz_dpm_funcs_group0 ;
int ldv_state_variable_15 ;
struct amdgpu_device *vce_v3_0_irq_funcs_group0 ;
struct work_struct *ldv_work_struct_1_3 ;
int ldv_state_variable_74 ;
int ldv_state_variable_21 ;
struct drm_display_mode *dce_v10_0_ext_helper_funcs_group1 ;
struct work_struct *ldv_work_struct_8_0 ;
struct amdgpu_ib *sdma_v2_4_vm_pte_funcs_group0 ;
int ldv_state_variable_146 ;
int ldv_state_variable_69 ;
struct amdgpu_irq_src *cik_sdma_trap_irq_funcs_group1 ;
struct amdgpu_irq_src *dce_v11_0_crtc_irq_funcs_group1 ;
int ldv_state_variable_88 ;
int ldv_state_variable_139 ;
int ldv_state_variable_94 ;
struct drm_framebuffer *dce_v11_0_crtc_helper_funcs_group1 ;
int ldv_state_variable_110 ;
int ldv_work_5_3 ;
int ldv_state_variable_41 ;
int ldv_state_variable_62 ;
int ldv_state_variable_40 ;
int ldv_state_variable_10 ;
int ldv_state_variable_133 ;
struct amdgpu_irq_src *ci_dpm_irq_funcs_group1 ;
int ldv_work_4_1 ;
struct amdgpu_device *gfx_v8_0_priv_reg_irq_funcs_group0 ;
struct amdgpu_irq_src *dce_v11_0_pageflip_irq_funcs_group1 ;
struct drm_display_mode *dce_v8_0_crtc_helper_funcs_group2 ;
int ldv_work_10_1 ;
int ldv_state_variable_25 ;
struct trace_event_call *event_class_amdgpu_bo_list_set_group0 ;
int ldv_state_variable_154 ;
struct amdgpu_irq_src *gfx_v7_0_priv_reg_irq_funcs_group1 ;
struct amdgpu_device *cz_ih_funcs_group0 ;
struct drm_display_mode *dce_v11_0_dig_helper_funcs_group1 ;
struct amdgpu_irq_src *sdma_v3_0_trap_irq_funcs_group1 ;
int ldv_state_variable_79 ;
int ldv_state_variable_127 ;
struct amdgpu_irq_src *gfx_v7_0_eop_irq_funcs_group1 ;
struct amdgpu_ib *sdma_v3_0_vm_pte_funcs_group0 ;
int ldv_work_2_2 ;
int ldv_state_variable_108 ;
int ldv_state_variable_32 ;
struct amdgpu_device *dce_v8_0_hpd_irq_funcs_group0 ;
struct drm_crtc *dce_v10_0_crtc_funcs_group0 ;
struct fb_info *amdgpufb_ops_group1 ;
struct i2c_adapter *amdgpu_atombios_i2c_algo_group0 ;
struct device_attribute *dev_attr_power_dpm_force_performance_level_group0 ;
struct amdgpu_device *dce_v10_0_crtc_irq_funcs_group0 ;
struct amdgpu_device *uvd_v5_0_irq_funcs_group0 ;
int ldv_state_variable_45 ;
int ldv_state_variable_12 ;
int ldv_state_variable_95 ;
int ldv_state_variable_122 ;
int ldv_state_variable_171 ;
int ldv_state_variable_22 ;
int ldv_state_variable_147 ;
struct file *amdgpu_driver_kms_fops_group2 ;
struct device_attribute *dev_attr_power_dpm_state_group0 ;
int ldv_state_variable_61 ;
int ldv_work_9_0 ;
int ldv_work_6_0 ;
struct ttm_tt *amdgpu_bo_driver_group0 ;
struct work_struct *ldv_work_struct_8_3 ;
struct amdgpu_ring *sdma_v3_0_ring_funcs_group0 ;
struct drm_connector *amdgpu_connector_vga_funcs_group0 ;
struct amdgpu_irq_src *gfx_v8_0_eop_irq_funcs_group1 ;
int ldv_state_variable_165 ;
int ldv_state_variable_72 ;
struct mmu_notifier *amdgpu_mn_ops_group0 ;
int ldv_state_variable_132 ;
int ldv_state_variable_120 ;
struct amdgpu_mode_mc_save *dce_v8_0_display_funcs_group1 ;
int ldv_work_5_0 ;
int ldv_state_variable_50 ;
struct amdgpu_ib *cik_sdma_vm_pte_funcs_group0 ;
int ldv_state_variable_84 ;
struct amdgpu_device *kv_dpm_funcs_group0 ;
struct amdgpu_encoder *dce_v11_0_display_funcs_group2 ;
int ldv_state_variable_86 ;
int ldv_state_variable_44 ;
struct amdgpu_ring *uvd_v5_0_ring_funcs_group0 ;
struct amdgpu_irq_src *vce_v2_0_irq_funcs_group1 ;
int ldv_state_variable_116 ;
int ldv_state_variable_128 ;
struct amdgpu_device *gfx_v7_0_priv_reg_irq_funcs_group0 ;
int ldv_state_variable_39 ;
int ldv_state_variable_175 ;
int ldv_state_variable_101 ;
struct work_struct *ldv_work_struct_5_1 ;
int ldv_state_variable_56 ;
struct amdgpu_irq_src *dce_v8_0_crtc_irq_funcs_group1 ;
int ldv_state_variable_112 ;
int ldv_state_variable_3 ;
struct drm_encoder *dce_v8_0_ext_helper_funcs_group0 ;
int ldv_work_1_0 ;
int ldv_state_variable_135 ;
struct drm_connector *amdgpu_connector_vga_helper_funcs_group0 ;
int ldv_state_variable_4 ;
struct work_struct *ldv_work_struct_9_3 ;
struct drm_display_mode *dce_v8_0_ext_helper_funcs_group1 ;
int ldv_state_variable_118 ;
int ldv_work_9_2 ;
struct amdgpu_device *dce_v8_0_pageflip_irq_funcs_group0 ;
int ldv_state_variable_117 ;
struct trace_event_call *event_class_amdgpu_vm_bo_map_group0 ;
struct work_struct *ldv_work_struct_6_3 ;
struct amdgpu_ring *sdma_v2_4_ring_funcs_group0 ;
struct amdgpu_ring *gfx_v8_0_ring_funcs_compute_group0 ;
struct work_struct *ldv_work_struct_5_2 ;
int ldv_work_9_1 ;
struct work_struct *ldv_work_struct_5_3 ;
int ldv_state_variable_5 ;
struct amdgpu_device *iceland_ih_funcs_group0 ;
int ldv_state_variable_13 ;
int ldv_state_variable_170 ;
struct ttm_mem_reg *amdgpu_bo_driver_group1 ;
struct amdgpu_device *dce_v11_0_crtc_irq_funcs_group0 ;
struct drm_device *amdgpu_mode_funcs_group0 ;
int ldv_state_variable_152 ;
struct drm_display_mode *dce_v8_0_dac_helper_funcs_group1 ;
struct mm_struct *amdgpu_mn_ops_group1 ;
int ldv_work_7_3 ;
int ldv_state_variable_153 ;
struct amdgpu_ring *sdma_v3_0_buffer_funcs_group0 ;
struct amdgpu_irq_src *gmc_v7_0_irq_funcs_group1 ;
struct amdgpu_irq_src *gfx_v8_0_priv_reg_irq_funcs_group1 ;
int ldv_state_variable_159 ;
int ldv_state_variable_85 ;
int ldv_state_variable_71 ;
struct work_struct *ldv_work_struct_10_2 ;
struct file *amdgpu_debugfs_regs_fops_group2 ;
struct device *dev_attr_power_dpm_state_group1 ;
struct drm_crtc *dce_v10_0_crtc_helper_funcs_group0 ;
int ldv_state_variable_77 ;
struct work_struct *ldv_work_struct_8_2 ;
int ldv_state_variable_144 ;
int ldv_work_4_3 ;
struct drm_connector *amdgpu_connector_dp_funcs_group0 ;
int ldv_work_3_1 ;
struct drm_connector *amdgpu_connector_edp_funcs_group0 ;
struct drm_file *kms_driver_group2 ;
int ldv_state_variable_43 ;
int ldv_state_variable_121 ;
int ldv_work_5_1 ;
int ldv_state_variable_57 ;
struct amdgpu_device *gmc_v7_0_gart_funcs_group0 ;
struct amdgpu_device *gfx_v8_0_eop_irq_funcs_group0 ;
int ldv_state_variable_134 ;
struct amdgpu_mode_mc_save *dce_v11_0_display_funcs_group1 ;
int ldv_state_variable_167 ;
struct amdgpu_irq_src *gfx_v8_0_priv_inst_irq_funcs_group1 ;
struct amdgpu_device *dce_v11_0_display_funcs_group0 ;
int ldv_state_variable_129 ;
struct file *amdgpu_ttm_gtt_fops_group2 ;
struct amdgpu_device *vce_v2_0_irq_funcs_group0 ;
int ldv_work_8_2 ;
int ldv_state_variable_34 ;
int ldv_work_2_3 ;
struct amdgpu_encoder *dce_v10_0_display_funcs_group2 ;
void ldv_initialize_amdgpu_irq_src_funcs_36(void) ;
void ldv_initialize_trace_event_class_127(void) ;
void ldv_initialize_backlight_ops_122(void) ;
void ldv_initialize_device_attribute_153(void) ;
void ldv_initialize_amdgpu_ring_funcs_32(void) ;
void ldv_initialize_trace_event_class_133(void) ;
void ldv_initialize_ttm_bo_driver_162(void) ;
void ldv_initialize_ttm_backend_func_163(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_112(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_25(void) ;
void ldv_file_operations_160(void) ;
void ldv_initialize_drm_mode_config_funcs_158(void) ;
void ldv_initialize_drm_connector_funcs_168(void) ;
void work_init_1(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_93(void) ;
void ldv_initialize_trace_event_class_129(void) ;
void ldv_file_operations_174(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_40(void) ;
void ldv_initialize_amdgpu_ring_funcs_96(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_98(void) ;
void ldv_initialize_trace_event_class_126(void) ;
void ldv_initialize_amdgpu_dpm_funcs_66(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_92(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_35(void) ;
void ldv_initialize_drm_connector_helper_funcs_171(void) ;
void work_init_10(void) ;
void ldv_initialize_drm_encoder_helper_funcs_58(void) ;
void ldv_initialize_amdgpu_display_funcs_54(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_109(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_75(void) ;
void ldv_initialize_mmu_notifier_ops_11(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_51(void) ;
void ldv_initialize_amdgpu_ring_funcs_81(void) ;
void ldv_initialize_amdgpu_display_funcs_101(void) ;
void ldv_initialize_amdgpu_dpm_funcs_110(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_19(void) ;
void ldv_initialize_amdgpu_gart_funcs_118(void) ;
void ldv_initialize_drm_encoder_helper_funcs_104(void) ;
void ldv_initialize_fb_ops_156(void) ;
void ldv_initialize_drm_crtc_funcs_50(void) ;
void ldv_initialize_amdgpu_ih_funcs_71(void) ;
void ldv_initialize_drm_crtc_helper_funcs_49(void) ;
void ldv_initialize_drm_connector_funcs_166(void) ;
void ldv_dev_pm_ops_179(void) ;
void ldv_initialize_amdgpu_ring_funcs_14(void) ;
void ldv_initialize_fence_ops_164(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_53(void) ;
void ldv_initialize_drm_crtc_helper_funcs_107(void) ;
void ldv_initialize_drm_encoder_helper_funcs_103(void) ;
void ldv_initialize_drm_encoder_helper_funcs_45(void) ;
void ldv_initialize_drm_connector_helper_funcs_169(void) ;
void ldv_initialize_amdgpu_asic_funcs_79(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_42(void) ;
void ldv_initialize_trace_event_class_128(void) ;
void ldv_initialize_vga_switcheroo_client_ops_175(void) ;
void ldv_initialize_trace_event_class_130(void) ;
void ldv_initialize_amdgpu_buffer_funcs_29(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_100(void) ;
void ldv_initialize_drm_connector_funcs_165(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_99(void) ;
void ldv_initialize_amdgpu_ring_funcs_95(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_31(void) ;
void ldv_pci_driver_176(void) ;
void ldv_initialize_amdgpu_vm_pte_funcs_28(void) ;
void ldv_initialize_trace_event_class_123(void) ;
void ldv_initialize_drm_encoder_helper_funcs_46(void) ;
void ldv_initialize_amdgpu_asic_funcs_121(void) ;
void work_init_5(void) ;
void work_init_9(void) ;
void ldv_initialize_amdgpu_ring_funcs_38(void) ;
void ldv_initialize_drm_connector_helper_funcs_167(void) ;
void ldv_initialize_drm_crtc_helper_funcs_60(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_94(void) ;
void ldv_initialize_amdgpu_ring_funcs_84(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_13(void) ;
void ldv_initialize_i2c_algorithm_157(void) ;
void ldv_initialize_drm_framebuffer_funcs_159(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_41(void) ;
void ldv_initialize_amdgpu_ring_funcs_37(void) ;
void ldv_initialize_drm_encoder_helper_funcs_47(void) ;
void ldv_initialize_amdgpu_ih_funcs_115(void) ;
void ldv_file_operations_161(void) ;
void work_init_8(void) ;
void ldv_initialize_amdgpu_ih_funcs_73(void) ;
void ldv_initialize_amdgpu_ring_funcs_20(void) ;
void activate_work_2(struct work_struct *work , int state ) ;
void ldv_file_operations_178(void) ;
void ldv_initialize_drm_fb_helper_funcs_155(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_16(void) ;
void ldv_initialize_drm_crtc_funcs_61(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_89(void) ;
void ldv_initialize_amdgpu_buffer_funcs_87(void) ;
void work_init_4(void) ;
void ldv_initialize_amdgpu_ring_funcs_17(void) ;
void ldv_initialize_drm_encoder_helper_funcs_105(void) ;
void ldv_initialize_drm_connector_helper_funcs_173(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_52(void) ;
void ldv_initialize_sensor_device_attribute_148(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_34(void) ;
void work_init_3(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_117(void) ;
void ldv_initialize_drm_driver_177(void) ;
void ldv_initialize_drm_encoder_helper_funcs_56(void) ;
void ldv_initialize_trace_event_class_125(void) ;
void ldv_initialize_drm_crtc_funcs_108(void) ;
void call_and_disable_all_2(int state ) ;
void ldv_initialize_amdgpu_vm_pte_funcs_86(void) ;
void work_init_7(void) ;
void ldv_initialize_amdgpu_gart_funcs_76(void) ;
void ldv_initialize_drm_connector_funcs_172(void) ;
void ldv_initialize_amdgpu_ring_funcs_90(void) ;
void ldv_initialize_sensor_device_attribute_149(void) ;
void ldv_initialize_trace_event_class_131(void) ;
void ldv_initialize_amdgpu_display_funcs_43(void) ;
void work_init_2(void) ;
void ldv_initialize_drm_connector_funcs_170(void) ;
void work_init_6(void) ;
void ldv_initialize_amdgpu_ih_funcs_69(void) ;
void ldv_initialize_drm_encoder_helper_funcs_57(void) ;
void ldv_initialize_amdgpu_ring_funcs_26(void) ;
void ldv_initialize_amdgpu_dpm_funcs_113(void) ;
void ldv_initialize_amdgpu_vm_pte_funcs_22(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_80(void) ;
void ldv_initialize_trace_event_class_124(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_83(void) ;
void ldv_initialize_trace_event_class_132(void) ;
void ldv_initialize_device_attribute_154(void) ;
void ldv_initialize_amdgpu_buffer_funcs_23(void) ;
__inline static void *dev_get_drvdata(struct device const *dev )
{
  {
  return ((void *)dev->driver_data);
}
}
extern int pci_enable_device(struct pci_dev * ) ;
extern void pci_disable_device(struct pci_dev * ) ;
extern void pci_set_master(struct pci_dev * ) ;
extern void pci_ignore_hotplug(struct pci_dev * ) ;
extern int pci_save_state(struct pci_dev * ) ;
extern void pci_restore_state(struct pci_dev * ) ;
extern int pci_set_power_state(struct pci_dev * , pci_power_t ) ;
__inline static void *pci_get_drvdata(struct pci_dev *pdev )
{
  void *tmp ;
  {
  tmp = dev_get_drvdata((struct device const *)(& pdev->dev));
  return (tmp);
}
}
__inline static struct apertures_struct *alloc_apertures(unsigned int max_num )
{
  struct apertures_struct *a ;
  void *tmp ;
  {
  tmp = kzalloc((unsigned long )max_num * 16UL + 8UL, 208U);
  a = (struct apertures_struct *)tmp;
  if ((unsigned long )a == (unsigned long )((struct apertures_struct *)0)) {
    return ((struct apertures_struct *)0);
  } else {
  }
  a->count = max_num;
  return (a);
}
}
extern int remove_conflicting_framebuffers(struct apertures_struct * , char const * ,
                                           bool ) ;
extern void drm_ut_debug_printk(char const * , char const * , ...) ;
extern void drm_err(char const * , ...) ;
extern long drm_ioctl(struct file * , unsigned int , unsigned long ) ;
extern int drm_open(struct inode * , struct file * ) ;
extern ssize_t drm_read(struct file * , char * , size_t , loff_t * ) ;
extern int drm_release(struct inode * , struct file * ) ;
extern unsigned int drm_poll(struct file * , struct poll_table_struct * ) ;
extern void drm_put_dev(struct drm_device * ) ;
extern unsigned int drm_debug ;
extern int drm_gem_prime_handle_to_fd(struct drm_device * , struct drm_file * , u32 ,
                                      u32 , int * ) ;
extern struct drm_gem_object *drm_gem_prime_import(struct drm_device * , struct dma_buf * ) ;
extern int drm_gem_prime_fd_to_handle(struct drm_device * , struct drm_file * , int ,
                                      u32 * ) ;
extern int drm_pci_init(struct drm_driver * , struct pci_driver * ) ;
extern void drm_pci_exit(struct drm_driver * , struct pci_driver * ) ;
extern int drm_get_pci_dev(struct pci_dev * , struct pci_device_id const * , struct drm_driver * ) ;
extern int drm_pci_set_busid(struct drm_device * , struct drm_master * ) ;
extern int drm_gem_dumb_destroy(struct drm_file * , struct drm_device * , u32 ) ;
long amdgpu_drm_ioctl(struct file *filp , unsigned int cmd , unsigned long arg ) ;
extern bool vgacon_text_force(void) ;
extern int __pm_runtime_suspend(struct device * , int ) ;
extern int __pm_runtime_resume(struct device * , int ) ;
extern void pm_runtime_forbid(struct device * ) ;
__inline static void pm_runtime_mark_last_busy(struct device *dev )
{
  unsigned long __var ;
  {
  __var = 0UL;
  *((unsigned long volatile *)(& dev->power.last_busy)) = jiffies;
  return;
}
}
__inline static int pm_runtime_autosuspend(struct device *dev )
{
  int tmp ;
  {
  tmp = __pm_runtime_suspend(dev, 8);
  return (tmp);
}
}
__inline static int pm_runtime_get_sync(struct device *dev )
{
  int tmp ;
  {
  tmp = __pm_runtime_resume(dev, 4);
  return (tmp);
}
}
__inline static int pm_runtime_put_autosuspend(struct device *dev )
{
  int tmp ;
  {
  tmp = __pm_runtime_suspend(dev, 13);
  return (tmp);
}
}
extern void vga_switcheroo_set_dynamic_switch(struct pci_dev * , enum vga_switcheroo_state ) ;
extern void drm_kms_helper_poll_disable(struct drm_device * ) ;
extern void drm_kms_helper_poll_enable(struct drm_device * ) ;
int amdgpu_get_crtc_scanoutpos(struct drm_device *dev , int crtc , unsigned int flags ,
                               int *vpos , int *hpos , ktime_t *stime , ktime_t *etime ) ;
void amdgpu_irq_preinstall(struct drm_device *dev ) ;
int amdgpu_irq_postinstall(struct drm_device *dev ) ;
void amdgpu_irq_uninstall(struct drm_device *dev ) ;
irqreturn_t amdgpu_irq_handler(int irq , void *arg ) ;
int amdgpu_vram_limit ;
int amdgpu_gart_size ;
int amdgpu_benchmarking ;
int amdgpu_testing ;
int amdgpu_audio ;
int amdgpu_disp_priority ;
int amdgpu_hw_i2c ;
int amdgpu_pcie_gen2 ;
int amdgpu_msi ;
int amdgpu_lockup_timeout ;
int amdgpu_dpm ;
int amdgpu_smc_load_fw ;
int amdgpu_aspm ;
int amdgpu_runtime_pm ;
int amdgpu_hard_reset ;
unsigned int amdgpu_ip_block_mask ;
int amdgpu_bapm ;
int amdgpu_deep_color ;
int amdgpu_vm_size ;
int amdgpu_vm_block_size ;
int amdgpu_mmap(struct file *filp , struct vm_area_struct *vma ) ;
void amdgpu_gem_object_free(struct drm_gem_object *gobj ) ;
int amdgpu_gem_object_open(struct drm_gem_object *obj , struct drm_file *file_priv ) ;
void amdgpu_gem_object_close(struct drm_gem_object *obj , struct drm_file *file_priv ) ;
struct sg_table *amdgpu_gem_prime_get_sg_table(struct drm_gem_object *obj ) ;
struct drm_gem_object *amdgpu_gem_prime_import_sg_table(struct drm_device *dev , struct dma_buf_attachment *attach ,
                                                        struct sg_table *sg ) ;
struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev , struct drm_gem_object *gobj ,
                                        int flags ) ;
int amdgpu_gem_prime_pin(struct drm_gem_object *obj ) ;
void amdgpu_gem_prime_unpin(struct drm_gem_object *obj ) ;
struct reservation_object *amdgpu_gem_prime_res_obj(struct drm_gem_object *obj ) ;
void *amdgpu_gem_prime_vmap(struct drm_gem_object *obj ) ;
void amdgpu_gem_prime_vunmap(struct drm_gem_object *obj , void *vaddr ) ;
int amdgpu_mode_dumb_create(struct drm_file *file_priv , struct drm_device *dev ,
                            struct drm_mode_create_dumb *args ) ;
int amdgpu_mode_dumb_mmap(struct drm_file *filp , struct drm_device *dev , u32 handle ,
                          uint64_t *offset_p ) ;
int amdgpu_debugfs_init(struct drm_minor *minor ) ;
void amdgpu_debugfs_cleanup(struct drm_minor *minor ) ;
bool amdgpu_device_is_px(struct drm_device *dev ) ;
void amdgpu_register_atpx_handler(void) ;
void amdgpu_unregister_atpx_handler(void) ;
struct drm_ioctl_desc const amdgpu_ioctls_kms[18U] ;
int amdgpu_max_kms_ioctl ;
int amdgpu_driver_load_kms(struct drm_device *dev , unsigned long flags ) ;
int amdgpu_driver_unload_kms(struct drm_device *dev ) ;
void amdgpu_driver_lastclose_kms(struct drm_device *dev ) ;
int amdgpu_driver_open_kms(struct drm_device *dev , struct drm_file *file_priv ) ;
void amdgpu_driver_postclose_kms(struct drm_device *dev , struct drm_file *file_priv ) ;
void amdgpu_driver_preclose_kms(struct drm_device *dev , struct drm_file *file_priv ) ;
int amdgpu_suspend_kms(struct drm_device *dev , bool suspend , bool fbcon ) ;
int amdgpu_resume_kms(struct drm_device *dev , bool resume , bool fbcon ) ;
u32 amdgpu_get_vblank_counter_kms(struct drm_device *dev , int crtc ) ;
int amdgpu_enable_vblank_kms(struct drm_device *dev , int crtc ) ;
void amdgpu_disable_vblank_kms(struct drm_device *dev , int crtc ) ;
int amdgpu_get_vblank_timestamp_kms(struct drm_device *dev , int crtc , int *max_error ,
                                    struct timeval *vblank_time , unsigned int flags ) ;
long amdgpu_kms_compat_ioctl(struct file *filp , unsigned int cmd , unsigned long arg ) ;
int amdgpu_vram_limit = 0;
int amdgpu_gart_size = -1;
int amdgpu_benchmarking = 0;
int amdgpu_testing = 0;
int amdgpu_audio = -1;
int amdgpu_disp_priority = 0;
int amdgpu_hw_i2c = 0;
int amdgpu_pcie_gen2 = -1;
int amdgpu_msi = -1;
int amdgpu_lockup_timeout = 10000;
int amdgpu_dpm = -1;
int amdgpu_smc_load_fw = 1;
int amdgpu_aspm = -1;
int amdgpu_runtime_pm = -1;
int amdgpu_hard_reset = 0;
unsigned int amdgpu_ip_block_mask = 4294967295U;
int amdgpu_bapm = -1;
int amdgpu_deep_color = 0;
int amdgpu_vm_size = 8;
int amdgpu_vm_block_size = -1;
int amdgpu_exp_hw_support = 0;
static struct pci_device_id pciidlist[97U] =
  { {4098U, 4868U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4869U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4870U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4871U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4873U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4874U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4875U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4876U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4877U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4878U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4879U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4880U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4881U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4882U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4883U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4885U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4886U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4887U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4888U, 4294967295U, 4294967295U, 0U, 0U, 196609UL},
        {4098U, 4891U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4892U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 4893U, 4294967295U, 4294967295U, 0U, 0U, 131073UL},
        {4098U, 26176U, 4294967295U, 4294967295U, 0U, 0U, 65536UL},
        {4098U, 26177U, 4294967295U, 4294967295U, 0U, 0U, 65536UL},
        {4098U, 26182U, 4294967295U, 4294967295U, 0U, 0U, 65536UL},
        {4098U, 26183U, 4294967295U, 4294967295U, 0U, 0U, 65536UL},
        {4098U, 26185U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26192U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26193U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26200U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26204U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26205U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26207U, 4294967295U, 4294967295U, 0U, 0U, 0UL},
        {4098U, 26528U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26529U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26530U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26536U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26537U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26538U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26544U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26545U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26552U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26553U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26554U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 26558U, 4294967295U, 4294967295U, 0U, 0U, 3UL},
        {4098U, 38960U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38961U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38962U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38963U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38964U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38965U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38966U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38967U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38968U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38969U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38970U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38971U, 4294967295U, 4294967295U, 0U, 0U, 196610UL},
        {4098U, 38972U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38973U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38974U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38975U, 4294967295U, 4294967295U, 0U, 0U, 131074UL},
        {4098U, 38992U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38993U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38994U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38995U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38996U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38997U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38998U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 38999U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39000U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39001U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39002U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39003U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39004U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39005U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39006U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 39007U, 4294967295U, 4294967295U, 0U, 0U, 196612UL},
        {4098U, 26880U, 4294967295U, 4294967295U, 0U, 0U, 5UL},
        {4098U, 26881U, 4294967295U, 4294967295U, 0U, 0U, 5UL},
        {4098U, 26882U, 4294967295U, 4294967295U, 0U, 0U, 5UL},
        {4098U, 26883U, 4294967295U, 4294967295U, 0U, 0U, 5UL},
        {4098U, 26887U, 4294967295U, 4294967295U, 0U, 0U, 5UL},
        {4098U, 26912U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26913U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26920U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26921U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26923U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26927U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26928U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26936U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 26937U, 4294967295U, 4294967295U, 0U, 0U, 6UL},
        {4098U, 39024U, 4294967295U, 4294967295U, 0U, 0U, 131079UL},
        {4098U, 39028U, 4294967295U, 4294967295U, 0U, 0U, 131079UL},
        {4098U, 39029U, 4294967295U, 4294967295U, 0U, 0U, 131079UL},
        {4098U, 39030U, 4294967295U, 4294967295U, 0U, 0U, 131079UL},
        {4098U, 39031U, 4294967295U, 4294967295U, 0U, 0U, 131079UL},
        {0U, 0U, 0U, 0U, 0U, 0U, 0UL}};
struct pci_device_id const __mod_pci__pciidlist_device_table[97U] ;
static struct drm_driver kms_driver ;
static int amdgpu_kick_out_firmware_fb(struct pci_dev *pdev )
{
  struct apertures_struct *ap ;
  bool primary ;
  {
  primary = 0;
  ap = alloc_apertures(1U);
  if ((unsigned long )ap == (unsigned long )((struct apertures_struct *)0)) {
    return (-12);
  } else {
  }
  ap->ranges[0].base = pdev->resource[0].start;
  ap->ranges[0].size = pdev->resource[0].start != 0ULL || pdev->resource[0].end != pdev->resource[0].start ? (pdev->resource[0].end - pdev->resource[0].start) + 1ULL : 0ULL;
  primary = (pdev->resource[6].flags & 2UL) != 0UL;
  remove_conflicting_framebuffers(ap, "amdgpudrmfb", (int )primary);
  kfree((void const *)ap);
  return (0);
}
}
static int amdgpu_pci_probe(struct pci_dev *pdev , struct pci_device_id const *ent )
{
  unsigned long flags ;
  int ret ;
  int tmp ;
  {
  flags = ent->driver_data;
  if ((flags & 524288UL) != 0UL && amdgpu_exp_hw_support == 0) {
    printk("\016[drm] This hardware requires experimental hardware support.\nSee modparam exp_hw_support\n");
    return (-19);
  } else {
  }
  ret = amdgpu_kick_out_firmware_fb(pdev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  tmp = drm_get_pci_dev(pdev, ent, & kms_driver);
  return (tmp);
}
}
static void amdgpu_pci_remove(struct pci_dev *pdev )
{
  struct drm_device *dev ;
  void *tmp ;
  {
  tmp = pci_get_drvdata(pdev);
  dev = (struct drm_device *)tmp;
  drm_put_dev(dev);
  return;
}
}
static int amdgpu_pmops_suspend(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_suspend_kms(drm_dev, 1, 1);
  return (tmp___0);
}
}
static int amdgpu_pmops_resume(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_resume_kms(drm_dev, 1, 1);
  return (tmp___0);
}
}
static int amdgpu_pmops_freeze(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_suspend_kms(drm_dev, 0, 1);
  return (tmp___0);
}
}
static int amdgpu_pmops_thaw(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_resume_kms(drm_dev, 0, 1);
  return (tmp___0);
}
}
static int amdgpu_pmops_runtime_suspend(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int ret ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_device_is_px(drm_dev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    pm_runtime_forbid(dev);
    return (-16);
  } else {
  }
  drm_dev->switch_power_state = 2;
  drm_kms_helper_poll_disable(drm_dev);
  vga_switcheroo_set_dynamic_switch(pdev, 0);
  ret = amdgpu_suspend_kms(drm_dev, 0, 0);
  pci_save_state(pdev);
  pci_disable_device(pdev);
  pci_ignore_hotplug(pdev);
  pci_set_power_state(pdev, 4);
  drm_dev->switch_power_state = 3;
  return (0);
}
}
static int amdgpu_pmops_runtime_resume(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  int ret ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_device_is_px(drm_dev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (-22);
  } else {
  }
  drm_dev->switch_power_state = 2;
  pci_set_power_state(pdev, 0);
  pci_restore_state(pdev);
  ret = pci_enable_device(pdev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pci_set_master(pdev);
  ret = amdgpu_resume_kms(drm_dev, 0, 0);
  drm_kms_helper_poll_enable(drm_dev);
  vga_switcheroo_set_dynamic_switch(pdev, 1);
  drm_dev->switch_power_state = 0;
  return (0);
}
}
static int amdgpu_pmops_runtime_idle(struct device *dev )
{
  struct pci_dev *pdev ;
  struct device const *__mptr ;
  struct drm_device *drm_dev ;
  void *tmp ;
  struct drm_crtc *crtc ;
  bool tmp___0 ;
  int tmp___1 ;
  struct list_head const *__mptr___0 ;
  long tmp___2 ;
  struct list_head const *__mptr___1 ;
  {
  __mptr = (struct device const *)dev;
  pdev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pdev);
  drm_dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_device_is_px(drm_dev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    pm_runtime_forbid(dev);
    return (-16);
  } else {
  }
  __mptr___0 = (struct list_head const *)drm_dev->mode_config.crtc_list.next;
  crtc = (struct drm_crtc *)__mptr___0 + 0xfffffffffffffff0UL;
  goto ldv_44592;
  ldv_44591: ;
  if ((int )crtc->enabled) {
    tmp___2 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("amdgpu_pmops_runtime_idle", "failing to power off - crtc active\n");
    } else {
    }
    return (-16);
  } else {
  }
  __mptr___1 = (struct list_head const *)crtc->head.next;
  crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
  ldv_44592: ;
  if ((unsigned long )(& crtc->head) != (unsigned long )(& drm_dev->mode_config.crtc_list)) {
    goto ldv_44591;
  } else {
  }
  pm_runtime_mark_last_busy(dev);
  pm_runtime_autosuspend(dev);
  return (1);
}
}
long amdgpu_drm_ioctl(struct file *filp , unsigned int cmd , unsigned long arg )
{
  struct drm_file *file_priv ;
  struct drm_device *dev ;
  long ret ;
  int tmp ;
  {
  file_priv = (struct drm_file *)filp->private_data;
  dev = (file_priv->minor)->dev;
  tmp = pm_runtime_get_sync(dev->dev);
  ret = (long )tmp;
  if (ret < 0L) {
    return (ret);
  } else {
  }
  ret = drm_ioctl(filp, cmd, arg);
  pm_runtime_mark_last_busy(dev->dev);
  pm_runtime_put_autosuspend(dev->dev);
  return (ret);
}
}
static struct dev_pm_ops const amdgpu_pm_ops =
     {0, 0, & amdgpu_pmops_suspend, & amdgpu_pmops_resume, & amdgpu_pmops_freeze, & amdgpu_pmops_thaw,
    & amdgpu_pmops_freeze, & amdgpu_pmops_resume, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, & amdgpu_pmops_runtime_suspend, & amdgpu_pmops_runtime_resume, & amdgpu_pmops_runtime_idle};
static struct file_operations const amdgpu_driver_kms_fops =
     {& __this_module, 0, & drm_read, 0, 0, 0, 0, & drm_poll, & amdgpu_drm_ioctl, & amdgpu_kms_compat_ioctl,
    & amdgpu_mmap, 0, & drm_open, 0, & drm_release, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0};
static struct drm_driver kms_driver =
     {& amdgpu_driver_load_kms, 0, & amdgpu_driver_open_kms, & amdgpu_driver_preclose_kms,
    & amdgpu_driver_postclose_kms, & amdgpu_driver_lastclose_kms, & amdgpu_driver_unload_kms,
    0, 0, 0, 0, 0, & drm_pci_set_busid, & amdgpu_get_vblank_counter_kms, & amdgpu_enable_vblank_kms,
    & amdgpu_disable_vblank_kms, 0, & amdgpu_get_crtc_scanoutpos, & amdgpu_get_vblank_timestamp_kms,
    & amdgpu_irq_handler, & amdgpu_irq_preinstall, & amdgpu_irq_postinstall, & amdgpu_irq_uninstall,
    0, 0, 0, 0, & amdgpu_debugfs_init, & amdgpu_debugfs_cleanup, & amdgpu_gem_object_free,
    & amdgpu_gem_object_open, & amdgpu_gem_object_close, & drm_gem_prime_handle_to_fd,
    & drm_gem_prime_fd_to_handle, & amdgpu_gem_prime_export, & drm_gem_prime_import,
    & amdgpu_gem_prime_pin, & amdgpu_gem_prime_unpin, & amdgpu_gem_prime_res_obj,
    & amdgpu_gem_prime_get_sg_table, & amdgpu_gem_prime_import_sg_table, & amdgpu_gem_prime_vmap,
    & amdgpu_gem_prime_vunmap, 0, 0, & amdgpu_mode_dumb_create, & amdgpu_mode_dumb_mmap,
    & drm_gem_dumb_destroy, 0, 3, 0, 0, (char *)"amdgpu", (char *)"AMD GPU", (char *)"20150101",
    53441U, 0, (struct drm_ioctl_desc const *)(& amdgpu_ioctls_kms), 0, & amdgpu_driver_kms_fops,
    {0, 0}};
static struct drm_driver *driver ;
static struct pci_driver *pdriver ;
static struct pci_driver amdgpu_kms_pci_driver =
     {{0, 0}, "amdgpu", (struct pci_device_id const *)(& pciidlist), & amdgpu_pci_probe,
    & amdgpu_pci_remove, 0, 0, 0, 0, 0, 0, 0, {0, 0, 0, 0, (_Bool)0, 0, 0, 0, 0, 0,
                                               0, 0, 0, 0, & amdgpu_pm_ops, 0}, {{{{{{0}},
                                                                                    0U,
                                                                                    0U,
                                                                                    0,
                                                                                    {0,
                                                                                     {0,
                                                                                      0},
                                                                                     0,
                                                                                     0,
                                                                                     0UL}}}},
                                                                                 {0,
                                                                                  0}}};
static int amdgpu_init(void)
{
  bool tmp ;
  int tmp___0 ;
  {
  tmp = vgacon_text_force();
  if ((int )tmp) {
    drm_err("VGACON disables amdgpu kernel modesetting.\n");
    return (-22);
  } else {
  }
  printk("\016[drm] amdgpu kernel modesetting enabled.\n");
  driver = & kms_driver;
  pdriver = & amdgpu_kms_pci_driver;
  driver->driver_features = driver->driver_features | 8192U;
  driver->num_ioctls = amdgpu_max_kms_ioctl;
  amdgpu_register_atpx_handler();
  tmp___0 = drm_pci_init(driver, pdriver);
  return (tmp___0);
}
}
static void amdgpu_exit(void)
{
  {
  drm_pci_exit(driver, pdriver);
  amdgpu_unregister_atpx_handler();
  return;
}
}
int ldv_retval_20 ;
int ldv_retval_42 ;
int ldv_retval_33 ;
extern int ldv_prepare_179(void) ;
int ldv_retval_43 ;
int ldv_retval_35 ;
extern int ldv_resume_noirq_179(void) ;
extern int ldv_thaw_noirq_179(void) ;
extern int ldv_freeze_noirq_179(void) ;
int ldv_retval_22 ;
int ldv_retval_36 ;
int ldv_retval_48 ;
extern int ldv_poweroff_late_179(void) ;
int ldv_retval_37 ;
extern int ldv_shutdown_176(void) ;
int ldv_retval_29 ;
int ldv_retval_32 ;
void ldv_check_final_state(void) ;
int ldv_retval_76 ;
extern int ldv_complete_179(void) ;
int ldv_retval_31 ;
extern int ldv_freeze_late_179(void) ;
int ldv_retval_19 ;
int ldv_retval_38 ;
extern int ldv_thaw_early_179(void) ;
extern int ldv_release_177(void) ;
int ldv_retval_47 ;
extern int ldv_resume_early_179(void) ;
extern int ldv_suspend_late_179(void) ;
int ldv_retval_41 ;
int ldv_retval_40 ;
extern void ldv_initialize(void) ;
int ldv_retval_39 ;
int ldv_retval_46 ;
int ldv_retval_49 ;
extern int ldv_restore_noirq_179(void) ;
int ldv_retval_45 ;
extern int ldv_suspend_noirq_179(void) ;
int ldv_retval_34 ;
extern int ldv_restore_early_179(void) ;
extern int ldv_poweroff_noirq_179(void) ;
int ldv_retval_44 ;
int ldv_retval_30 ;
void ldv_dev_pm_ops_179(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1416UL);
  amdgpu_pm_ops_group1 = (struct device *)tmp;
  return;
}
}
void ldv_pci_driver_176(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(2976UL);
  amdgpu_kms_pci_driver_group1 = (struct pci_dev *)tmp;
  return;
}
}
void ldv_file_operations_178(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(1000UL);
  amdgpu_driver_kms_fops_group1 = (struct inode *)tmp;
  tmp___0 = ldv_init_zalloc(504UL);
  amdgpu_driver_kms_fops_group2 = (struct file *)tmp___0;
  return;
}
}
void ldv_initialize_drm_driver_177(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  {
  tmp = ldv_init_zalloc(240UL);
  kms_driver_group1 = (struct drm_minor *)tmp;
  tmp___0 = ldv_init_zalloc(744UL);
  kms_driver_group2 = (struct drm_file *)tmp___0;
  tmp___1 = ldv_init_zalloc(248UL);
  kms_driver_group0 = (struct drm_gem_object *)tmp___1;
  tmp___2 = ldv_init_zalloc(3320UL);
  kms_driver_group3 = (struct drm_device *)tmp___2;
  return;
}
}
void ldv_main_exported_122(void) ;
void ldv_main_exported_62(void) ;
void ldv_main_exported_124(void) ;
void ldv_main_exported_131(void) ;
void ldv_main_exported_130(void) ;
void ldv_main_exported_127(void) ;
void ldv_main_exported_142(void) ;
void ldv_main_exported_139(void) ;
void ldv_main_exported_129(void) ;
void ldv_main_exported_143(void) ;
void ldv_main_exported_136(void) ;
void ldv_main_exported_144(void) ;
void ldv_main_exported_141(void) ;
void ldv_main_exported_125(void) ;
void ldv_main_exported_133(void) ;
void ldv_main_exported_126(void) ;
void ldv_main_exported_123(void) ;
void ldv_main_exported_128(void) ;
void ldv_main_exported_134(void) ;
void ldv_main_exported_138(void) ;
void ldv_main_exported_135(void) ;
void ldv_main_exported_137(void) ;
void ldv_main_exported_132(void) ;
void ldv_main_exported_140(void) ;
void ldv_main_exported_109(void) ;
void ldv_main_exported_111(void) ;
void ldv_main_exported_110(void) ;
void ldv_main_exported_53(void) ;
void ldv_main_exported_57(void) ;
void ldv_main_exported_61(void) ;
void ldv_main_exported_51(void) ;
void ldv_main_exported_58(void) ;
void ldv_main_exported_59(void) ;
void ldv_main_exported_52(void) ;
void ldv_main_exported_60(void) ;
void ldv_main_exported_56(void) ;
void ldv_main_exported_54(void) ;
void ldv_main_exported_55(void) ;
void ldv_main_exported_84(void) ;
void ldv_main_exported_85(void) ;
void ldv_main_exported_83(void) ;
void ldv_main_exported_77(void) ;
void ldv_main_exported_75(void) ;
void ldv_main_exported_76(void) ;
void ldv_main_exported_157(void) ;
void ldv_main_exported_18(void) ;
void ldv_main_exported_16(void) ;
void ldv_main_exported_17(void) ;
void ldv_main_exported_159(void) ;
void ldv_main_exported_158(void) ;
void ldv_main_exported_65(void) ;
void ldv_main_exported_13(void) ;
void ldv_main_exported_15(void) ;
void ldv_main_exported_14(void) ;
void ldv_main_exported_174(void) ;
void ldv_main_exported_175(void) ;
void ldv_main_exported_69(void) ;
void ldv_main_exported_70(void) ;
void ldv_main_exported_104(void) ;
void ldv_main_exported_102(void) ;
void ldv_main_exported_108(void) ;
void ldv_main_exported_107(void) ;
void ldv_main_exported_98(void) ;
void ldv_main_exported_99(void) ;
void ldv_main_exported_103(void) ;
void ldv_main_exported_106(void) ;
void ldv_main_exported_101(void) ;
void ldv_main_exported_100(void) ;
void ldv_main_exported_105(void) ;
void ldv_main_exported_63(void) ;
void ldv_main_exported_67(void) ;
void ldv_main_exported_66(void) ;
void ldv_main_exported_119(void) ;
void ldv_main_exported_118(void) ;
void ldv_main_exported_117(void) ;
void ldv_main_exported_152(void) ;
void ldv_main_exported_149(void) ;
void ldv_main_exported_147(void) ;
void ldv_main_exported_146(void) ;
void ldv_main_exported_153(void) ;
void ldv_main_exported_154(void) ;
void ldv_main_exported_145(void) ;
void ldv_main_exported_151(void) ;
void ldv_main_exported_148(void) ;
void ldv_main_exported_150(void) ;
void ldv_main_exported_87(void) ;
void ldv_main_exported_90(void) ;
void ldv_main_exported_88(void) ;
void ldv_main_exported_89(void) ;
void ldv_main_exported_91(void) ;
void ldv_main_exported_86(void) ;
void ldv_main_exported_74(void) ;
void ldv_main_exported_73(void) ;
void ldv_main_exported_163(void) ;
void ldv_main_exported_161(void) ;
void ldv_main_exported_162(void) ;
void ldv_main_exported_160(void) ;
void ldv_main_exported_170(void) ;
void ldv_main_exported_165(void) ;
void ldv_main_exported_168(void) ;
void ldv_main_exported_167(void) ;
void ldv_main_exported_166(void) ;
void ldv_main_exported_172(void) ;
void ldv_main_exported_173(void) ;
void ldv_main_exported_169(void) ;
void ldv_main_exported_171(void) ;
void ldv_main_exported_120(void) ;
void ldv_main_exported_121(void) ;
void ldv_main_exported_11(void) ;
void ldv_main_exported_27(void) ;
void ldv_main_exported_25(void) ;
void ldv_main_exported_22(void) ;
void ldv_main_exported_24(void) ;
void ldv_main_exported_26(void) ;
void ldv_main_exported_23(void) ;
void ldv_main_exported_78(void) ;
void ldv_main_exported_79(void) ;
void ldv_main_exported_155(void) ;
void ldv_main_exported_156(void) ;
void ldv_main_exported_68(void) ;
void ldv_main_exported_81(void) ;
void ldv_main_exported_80(void) ;
void ldv_main_exported_82(void) ;
void ldv_main_exported_64(void) ;
void ldv_main_exported_72(void) ;
void ldv_main_exported_71(void) ;
void ldv_main_exported_116(void) ;
void ldv_main_exported_115(void) ;
void ldv_main_exported_33(void) ;
void ldv_main_exported_32(void) ;
void ldv_main_exported_28(void) ;
void ldv_main_exported_30(void) ;
void ldv_main_exported_31(void) ;
void ldv_main_exported_29(void) ;
void ldv_main_exported_50(void) ;
void ldv_main_exported_40(void) ;
void ldv_main_exported_41(void) ;
void ldv_main_exported_48(void) ;
void ldv_main_exported_47(void) ;
void ldv_main_exported_42(void) ;
void ldv_main_exported_49(void) ;
void ldv_main_exported_46(void) ;
void ldv_main_exported_45(void) ;
void ldv_main_exported_43(void) ;
void ldv_main_exported_44(void) ;
void ldv_main_exported_38(void) ;
void ldv_main_exported_35(void) ;
void ldv_main_exported_39(void) ;
void ldv_main_exported_34(void) ;
void ldv_main_exported_36(void) ;
void ldv_main_exported_37(void) ;
void ldv_main_exported_164(void) ;
void ldv_main_exported_12(void) ;
void ldv_main_exported_92(void) ;
void ldv_main_exported_93(void) ;
void ldv_main_exported_95(void) ;
void ldv_main_exported_97(void) ;
void ldv_main_exported_94(void) ;
void ldv_main_exported_96(void) ;
void ldv_main_exported_21(void) ;
void ldv_main_exported_19(void) ;
void ldv_main_exported_20(void) ;
void ldv_main_exported_112(void) ;
void ldv_main_exported_114(void) ;
void ldv_main_exported_113(void) ;
int main(void)
{
  unsigned int ldvarg382 ;
  unsigned long ldvarg377 ;
  struct poll_table_struct *ldvarg380 ;
  void *tmp ;
  size_t ldvarg383 ;
  loff_t *ldvarg385 ;
  void *tmp___0 ;
  struct vm_area_struct *ldvarg379 ;
  void *tmp___1 ;
  char *ldvarg384 ;
  void *tmp___2 ;
  unsigned int ldvarg378 ;
  unsigned long ldvarg381 ;
  struct pci_device_id *ldvarg453 ;
  void *tmp___3 ;
  struct drm_mode_create_dumb *ldvarg988 ;
  void *tmp___4 ;
  int *ldvarg973 ;
  void *tmp___5 ;
  struct dma_buf_attachment *ldvarg992 ;
  void *tmp___6 ;
  int ldvarg980 ;
  u32 ldvarg970 ;
  unsigned long ldvarg969 ;
  int ldvarg964 ;
  unsigned int ldvarg976 ;
  void *ldvarg990 ;
  void *tmp___7 ;
  int ldvarg986 ;
  uint64_t *ldvarg985 ;
  void *tmp___8 ;
  struct dma_buf *ldvarg983 ;
  void *tmp___9 ;
  int ldvarg962 ;
  u32 ldvarg971 ;
  int *ldvarg982 ;
  void *tmp___10 ;
  void *ldvarg963 ;
  void *tmp___11 ;
  int ldvarg989 ;
  unsigned int ldvarg978 ;
  int *ldvarg972 ;
  void *tmp___12 ;
  struct sg_table *ldvarg991 ;
  void *tmp___13 ;
  u32 ldvarg984 ;
  struct drm_master *ldvarg965 ;
  void *tmp___14 ;
  ktime_t *ldvarg977 ;
  void *tmp___15 ;
  ktime_t *ldvarg979 ;
  void *tmp___16 ;
  int ldvarg974 ;
  u32 ldvarg968 ;
  int ldvarg987 ;
  u32 *ldvarg967 ;
  void *tmp___17 ;
  int *ldvarg981 ;
  void *tmp___18 ;
  int ldvarg966 ;
  struct timeval *ldvarg975 ;
  void *tmp___19 ;
  int tmp___20 ;
  int tmp___21 ;
  int tmp___22 ;
  int tmp___23 ;
  int tmp___24 ;
  int tmp___25 ;
  {
  tmp = ldv_init_zalloc(16UL);
  ldvarg380 = (struct poll_table_struct *)tmp;
  tmp___0 = ldv_init_zalloc(8UL);
  ldvarg385 = (loff_t *)tmp___0;
  tmp___1 = ldv_init_zalloc(184UL);
  ldvarg379 = (struct vm_area_struct *)tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg384 = (char *)tmp___2;
  tmp___3 = ldv_init_zalloc(32UL);
  ldvarg453 = (struct pci_device_id *)tmp___3;
  tmp___4 = ldv_init_zalloc(32UL);
  ldvarg988 = (struct drm_mode_create_dumb *)tmp___4;
  tmp___5 = ldv_init_zalloc(4UL);
  ldvarg973 = (int *)tmp___5;
  tmp___6 = __VERIFIER_nondet_pointer();
  ldvarg992 = (struct dma_buf_attachment *)tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg990 = tmp___7;
  tmp___8 = ldv_init_zalloc(8UL);
  ldvarg985 = (uint64_t *)tmp___8;
  tmp___9 = __VERIFIER_nondet_pointer();
  ldvarg983 = (struct dma_buf *)tmp___9;
  tmp___10 = ldv_init_zalloc(4UL);
  ldvarg982 = (int *)tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg963 = tmp___11;
  tmp___12 = ldv_init_zalloc(4UL);
  ldvarg972 = (int *)tmp___12;
  tmp___13 = ldv_init_zalloc(16UL);
  ldvarg991 = (struct sg_table *)tmp___13;
  tmp___14 = ldv_init_zalloc(352UL);
  ldvarg965 = (struct drm_master *)tmp___14;
  tmp___15 = ldv_init_zalloc(8UL);
  ldvarg977 = (ktime_t *)tmp___15;
  tmp___16 = ldv_init_zalloc(8UL);
  ldvarg979 = (ktime_t *)tmp___16;
  tmp___17 = ldv_init_zalloc(4UL);
  ldvarg967 = (u32 *)tmp___17;
  tmp___18 = ldv_init_zalloc(4UL);
  ldvarg981 = (int *)tmp___18;
  tmp___19 = ldv_init_zalloc(16UL);
  ldvarg975 = (struct timeval *)tmp___19;
  ldv_initialize();
  ldv_memset((void *)(& ldvarg382), 0, 4UL);
  ldv_memset((void *)(& ldvarg377), 0, 8UL);
  ldv_memset((void *)(& ldvarg383), 0, 8UL);
  ldv_memset((void *)(& ldvarg378), 0, 4UL);
  ldv_memset((void *)(& ldvarg381), 0, 8UL);
  ldv_memset((void *)(& ldvarg980), 0, 4UL);
  ldv_memset((void *)(& ldvarg970), 0, 4UL);
  ldv_memset((void *)(& ldvarg969), 0, 8UL);
  ldv_memset((void *)(& ldvarg964), 0, 4UL);
  ldv_memset((void *)(& ldvarg976), 0, 4UL);
  ldv_memset((void *)(& ldvarg986), 0, 4UL);
  ldv_memset((void *)(& ldvarg962), 0, 4UL);
  ldv_memset((void *)(& ldvarg971), 0, 4UL);
  ldv_memset((void *)(& ldvarg989), 0, 4UL);
  ldv_memset((void *)(& ldvarg978), 0, 4UL);
  ldv_memset((void *)(& ldvarg984), 0, 4UL);
  ldv_memset((void *)(& ldvarg974), 0, 4UL);
  ldv_memset((void *)(& ldvarg968), 0, 4UL);
  ldv_memset((void *)(& ldvarg987), 0, 4UL);
  ldv_memset((void *)(& ldvarg966), 0, 4UL);
  ldv_state_variable_127 = 0;
  ldv_state_variable_32 = 0;
  ldv_state_variable_90 = 0;
  ldv_state_variable_118 = 0;
  ldv_state_variable_71 = 0;
  ldv_state_variable_102 = 0;
  ldv_state_variable_18 = 0;
  ldv_state_variable_125 = 0;
  ldv_state_variable_16 = 0;
  ldv_state_variable_44 = 0;
  ldv_state_variable_55 = 0;
  ldv_state_variable_84 = 0;
  ldv_state_variable_27 = 0;
  ldv_state_variable_161 = 0;
  ldv_state_variable_95 = 0;
  ldv_state_variable_57 = 0;
  ldv_state_variable_20 = 0;
  ldv_state_variable_163 = 0;
  ldv_state_variable_109 = 0;
  ldv_state_variable_151 = 0;
  ldv_state_variable_89 = 0;
  ldv_state_variable_175 = 0;
  ldv_state_variable_148 = 0;
  ldv_state_variable_31 = 0;
  ldv_state_variable_35 = 0;
  ldv_state_variable_11 = 0;
  ldv_state_variable_78 = 0;
  ldv_state_variable_93 = 0;
  ldv_state_variable_106 = 0;
  ldv_state_variable_157 = 0;
  ldv_state_variable_65 = 0;
  ldv_state_variable_29 = 0;
  ldv_state_variable_138 = 0;
  ldv_state_variable_114 = 0;
  ldv_state_variable_58 = 0;
  ldv_state_variable_153 = 0;
  ldv_state_variable_15 = 0;
  ldv_state_variable_137 = 0;
  ldv_state_variable_81 = 0;
  ldv_state_variable_60 = 0;
  ldv_state_variable_101 = 0;
  ldv_state_variable_73 = 0;
  ldv_state_variable_86 = 0;
  ldv_state_variable_76 = 0;
  ldv_state_variable_62 = 0;
  ldv_state_variable_67 = 0;
  ldv_state_variable_165 = 0;
  ldv_state_variable_139 = 0;
  ldv_state_variable_129 = 0;
  work_init_2();
  ldv_state_variable_2 = 1;
  ldv_state_variable_17 = 0;
  ldv_state_variable_110 = 0;
  ldv_state_variable_82 = 0;
  ldv_state_variable_147 = 0;
  ldv_state_variable_168 = 0;
  ldv_state_variable_135 = 0;
  ldv_state_variable_14 = 0;
  ldv_state_variable_112 = 0;
  ldv_state_variable_69 = 0;
  ldv_state_variable_172 = 0;
  ldv_state_variable_145 = 0;
  ldv_state_variable_49 = 0;
  ldv_state_variable_178 = 0;
  ldv_state_variable_24 = 0;
  ldv_state_variable_140 = 0;
  ldv_state_variable_124 = 0;
  ldv_state_variable_104 = 0;
  ldv_state_variable_131 = 0;
  ldv_state_variable_121 = 0;
  ldv_state_variable_79 = 0;
  ldv_state_variable_154 = 0;
  ref_cnt = 0;
  ldv_state_variable_0 = 1;
  ldv_state_variable_23 = 0;
  ldv_state_variable_96 = 0;
  ldv_state_variable_126 = 0;
  ldv_state_variable_159 = 0;
  ldv_state_variable_160 = 0;
  ldv_state_variable_176 = 0;
  ldv_state_variable_47 = 0;
  work_init_8();
  ldv_state_variable_8 = 1;
  ldv_state_variable_98 = 0;
  ldv_state_variable_37 = 0;
  ldv_state_variable_117 = 0;
  ldv_state_variable_43 = 0;
  work_init_5();
  ldv_state_variable_5 = 1;
  ldv_state_variable_170 = 0;
  ldv_state_variable_33 = 0;
  ldv_state_variable_21 = 0;
  ldv_state_variable_63 = 0;
  work_init_7();
  ldv_state_variable_7 = 1;
  ldv_state_variable_26 = 0;
  ldv_state_variable_80 = 0;
  ldv_state_variable_119 = 0;
  ldv_state_variable_99 = 0;
  ldv_state_variable_179 = 0;
  ldv_state_variable_162 = 0;
  ldv_state_variable_72 = 0;
  ldv_state_variable_74 = 0;
  ldv_state_variable_61 = 0;
  ldv_state_variable_108 = 0;
  ldv_state_variable_115 = 0;
  ldv_state_variable_92 = 0;
  ldv_state_variable_103 = 0;
  work_init_10();
  ldv_state_variable_10 = 1;
  ldv_state_variable_113 = 0;
  ldv_state_variable_152 = 0;
  ldv_state_variable_142 = 0;
  ldv_state_variable_91 = 0;
  ldv_state_variable_167 = 0;
  ldv_state_variable_48 = 0;
  ldv_state_variable_107 = 0;
  ldv_state_variable_87 = 0;
  ldv_state_variable_174 = 0;
  ldv_state_variable_77 = 0;
  ldv_state_variable_133 = 0;
  ldv_state_variable_149 = 0;
  ldv_state_variable_123 = 0;
  ldv_state_variable_50 = 0;
  ldv_state_variable_39 = 0;
  ldv_state_variable_64 = 0;
  ldv_state_variable_97 = 0;
  ldv_state_variable_12 = 0;
  ldv_state_variable_41 = 0;
  ldv_state_variable_52 = 0;
  ldv_state_variable_173 = 0;
  ldv_state_variable_56 = 0;
  ldv_state_variable_45 = 0;
  ldv_state_variable_66 = 0;
  ldv_state_variable_19 = 0;
  ldv_state_variable_54 = 0;
  ldv_state_variable_70 = 0;
  ldv_state_variable_68 = 0;
  ldv_state_variable_166 = 0;
  work_init_1();
  ldv_state_variable_1 = 1;
  ldv_state_variable_136 = 0;
  ldv_state_variable_88 = 0;
  ldv_state_variable_116 = 0;
  ldv_state_variable_144 = 0;
  ldv_state_variable_141 = 0;
  ldv_state_variable_30 = 0;
  ldv_state_variable_100 = 0;
  ldv_state_variable_25 = 0;
  ldv_state_variable_128 = 0;
  ldv_state_variable_28 = 0;
  ldv_state_variable_120 = 0;
  ldv_state_variable_156 = 0;
  ldv_state_variable_134 = 0;
  ldv_state_variable_40 = 0;
  ldv_state_variable_75 = 0;
  ldv_state_variable_83 = 0;
  ldv_state_variable_59 = 0;
  ldv_state_variable_177 = 0;
  ldv_state_variable_150 = 0;
  ldv_state_variable_155 = 0;
  ldv_state_variable_130 = 0;
  ldv_state_variable_53 = 0;
  ldv_state_variable_122 = 0;
  ldv_state_variable_143 = 0;
  ldv_state_variable_158 = 0;
  ldv_state_variable_42 = 0;
  ldv_state_variable_22 = 0;
  ldv_state_variable_46 = 0;
  ldv_state_variable_13 = 0;
  ldv_state_variable_105 = 0;
  work_init_6();
  ldv_state_variable_6 = 1;
  ldv_state_variable_85 = 0;
  ldv_state_variable_36 = 0;
  work_init_3();
  ldv_state_variable_3 = 1;
  ldv_state_variable_94 = 0;
  ldv_state_variable_146 = 0;
  ldv_state_variable_51 = 0;
  work_init_9();
  ldv_state_variable_9 = 1;
  ldv_state_variable_111 = 0;
  ldv_state_variable_38 = 0;
  work_init_4();
  ldv_state_variable_4 = 1;
  ldv_state_variable_34 = 0;
  ldv_state_variable_169 = 0;
  ldv_state_variable_164 = 0;
  ldv_state_variable_132 = 0;
  ldv_state_variable_171 = 0;
  ldv_45350:
  tmp___20 = __VERIFIER_nondet_int();
  switch (tmp___20) {
  case 0: ;
  if (ldv_state_variable_127 != 0) {
    ldv_main_exported_127();
  } else {
  }
  goto ldv_45087;
  case 1: ;
  if (ldv_state_variable_32 != 0) {
    ldv_main_exported_32();
  } else {
  }
  goto ldv_45087;
  case 2: ;
  if (ldv_state_variable_90 != 0) {
    ldv_main_exported_90();
  } else {
  }
  goto ldv_45087;
  case 3: ;
  if (ldv_state_variable_118 != 0) {
    ldv_main_exported_118();
  } else {
  }
  goto ldv_45087;
  case 4: ;
  if (ldv_state_variable_71 != 0) {
    ldv_main_exported_71();
  } else {
  }
  goto ldv_45087;
  case 5: ;
  if (ldv_state_variable_102 != 0) {
    ldv_main_exported_102();
  } else {
  }
  goto ldv_45087;
  case 6: ;
  if (ldv_state_variable_18 != 0) {
    ldv_main_exported_18();
  } else {
  }
  goto ldv_45087;
  case 7: ;
  if (ldv_state_variable_125 != 0) {
    ldv_main_exported_125();
  } else {
  }
  goto ldv_45087;
  case 8: ;
  if (ldv_state_variable_16 != 0) {
    ldv_main_exported_16();
  } else {
  }
  goto ldv_45087;
  case 9: ;
  if (ldv_state_variable_44 != 0) {
    ldv_main_exported_44();
  } else {
  }
  goto ldv_45087;
  case 10: ;
  if (ldv_state_variable_55 != 0) {
    ldv_main_exported_55();
  } else {
  }
  goto ldv_45087;
  case 11: ;
  if (ldv_state_variable_84 != 0) {
    ldv_main_exported_84();
  } else {
  }
  goto ldv_45087;
  case 12: ;
  if (ldv_state_variable_27 != 0) {
    ldv_main_exported_27();
  } else {
  }
  goto ldv_45087;
  case 13: ;
  if (ldv_state_variable_161 != 0) {
    ldv_main_exported_161();
  } else {
  }
  goto ldv_45087;
  case 14: ;
  if (ldv_state_variable_95 != 0) {
    ldv_main_exported_95();
  } else {
  }
  goto ldv_45087;
  case 15: ;
  if (ldv_state_variable_57 != 0) {
    ldv_main_exported_57();
  } else {
  }
  goto ldv_45087;
  case 16: ;
  if (ldv_state_variable_20 != 0) {
    ldv_main_exported_20();
  } else {
  }
  goto ldv_45087;
  case 17: ;
  if (ldv_state_variable_163 != 0) {
    ldv_main_exported_163();
  } else {
  }
  goto ldv_45087;
  case 18: ;
  if (ldv_state_variable_109 != 0) {
    ldv_main_exported_109();
  } else {
  }
  goto ldv_45087;
  case 19: ;
  if (ldv_state_variable_151 != 0) {
    ldv_main_exported_151();
  } else {
  }
  goto ldv_45087;
  case 20: ;
  if (ldv_state_variable_89 != 0) {
    ldv_main_exported_89();
  } else {
  }
  goto ldv_45087;
  case 21: ;
  if (ldv_state_variable_175 != 0) {
    ldv_main_exported_175();
  } else {
  }
  goto ldv_45087;
  case 22: ;
  if (ldv_state_variable_148 != 0) {
    ldv_main_exported_148();
  } else {
  }
  goto ldv_45087;
  case 23: ;
  if (ldv_state_variable_31 != 0) {
    ldv_main_exported_31();
  } else {
  }
  goto ldv_45087;
  case 24: ;
  if (ldv_state_variable_35 != 0) {
    ldv_main_exported_35();
  } else {
  }
  goto ldv_45087;
  case 25: ;
  if (ldv_state_variable_11 != 0) {
    ldv_main_exported_11();
  } else {
  }
  goto ldv_45087;
  case 26: ;
  if (ldv_state_variable_78 != 0) {
    ldv_main_exported_78();
  } else {
  }
  goto ldv_45087;
  case 27: ;
  if (ldv_state_variable_93 != 0) {
    ldv_main_exported_93();
  } else {
  }
  goto ldv_45087;
  case 28: ;
  if (ldv_state_variable_106 != 0) {
    ldv_main_exported_106();
  } else {
  }
  goto ldv_45087;
  case 29: ;
  if (ldv_state_variable_157 != 0) {
    ldv_main_exported_157();
  } else {
  }
  goto ldv_45087;
  case 30: ;
  if (ldv_state_variable_65 != 0) {
    ldv_main_exported_65();
  } else {
  }
  goto ldv_45087;
  case 31: ;
  if (ldv_state_variable_29 != 0) {
    ldv_main_exported_29();
  } else {
  }
  goto ldv_45087;
  case 32: ;
  if (ldv_state_variable_138 != 0) {
    ldv_main_exported_138();
  } else {
  }
  goto ldv_45087;
  case 33: ;
  if (ldv_state_variable_114 != 0) {
    ldv_main_exported_114();
  } else {
  }
  goto ldv_45087;
  case 34: ;
  if (ldv_state_variable_58 != 0) {
    ldv_main_exported_58();
  } else {
  }
  goto ldv_45087;
  case 35: ;
  if (ldv_state_variable_153 != 0) {
    ldv_main_exported_153();
  } else {
  }
  goto ldv_45087;
  case 36: ;
  if (ldv_state_variable_15 != 0) {
    ldv_main_exported_15();
  } else {
  }
  goto ldv_45087;
  case 37: ;
  if (ldv_state_variable_137 != 0) {
    ldv_main_exported_137();
  } else {
  }
  goto ldv_45087;
  case 38: ;
  if (ldv_state_variable_81 != 0) {
    ldv_main_exported_81();
  } else {
  }
  goto ldv_45087;
  case 39: ;
  if (ldv_state_variable_60 != 0) {
    ldv_main_exported_60();
  } else {
  }
  goto ldv_45087;
  case 40: ;
  if (ldv_state_variable_101 != 0) {
    ldv_main_exported_101();
  } else {
  }
  goto ldv_45087;
  case 41: ;
  if (ldv_state_variable_73 != 0) {
    ldv_main_exported_73();
  } else {
  }
  goto ldv_45087;
  case 42: ;
  if (ldv_state_variable_86 != 0) {
    ldv_main_exported_86();
  } else {
  }
  goto ldv_45087;
  case 43: ;
  if (ldv_state_variable_76 != 0) {
    ldv_main_exported_76();
  } else {
  }
  goto ldv_45087;
  case 44: ;
  if (ldv_state_variable_62 != 0) {
    ldv_main_exported_62();
  } else {
  }
  goto ldv_45087;
  case 45: ;
  if (ldv_state_variable_67 != 0) {
    ldv_main_exported_67();
  } else {
  }
  goto ldv_45087;
  case 46: ;
  if (ldv_state_variable_165 != 0) {
    ldv_main_exported_165();
  } else {
  }
  goto ldv_45087;
  case 47: ;
  if (ldv_state_variable_139 != 0) {
    ldv_main_exported_139();
  } else {
  }
  goto ldv_45087;
  case 48: ;
  if (ldv_state_variable_129 != 0) {
    ldv_main_exported_129();
  } else {
  }
  goto ldv_45087;
  case 49: ;
  goto ldv_45087;
  case 50: ;
  if (ldv_state_variable_17 != 0) {
    ldv_main_exported_17();
  } else {
  }
  goto ldv_45087;
  case 51: ;
  if (ldv_state_variable_110 != 0) {
    ldv_main_exported_110();
  } else {
  }
  goto ldv_45087;
  case 52: ;
  if (ldv_state_variable_82 != 0) {
    ldv_main_exported_82();
  } else {
  }
  goto ldv_45087;
  case 53: ;
  if (ldv_state_variable_147 != 0) {
    ldv_main_exported_147();
  } else {
  }
  goto ldv_45087;
  case 54: ;
  if (ldv_state_variable_168 != 0) {
    ldv_main_exported_168();
  } else {
  }
  goto ldv_45087;
  case 55: ;
  if (ldv_state_variable_135 != 0) {
    ldv_main_exported_135();
  } else {
  }
  goto ldv_45087;
  case 56: ;
  if (ldv_state_variable_14 != 0) {
    ldv_main_exported_14();
  } else {
  }
  goto ldv_45087;
  case 57: ;
  if (ldv_state_variable_112 != 0) {
    ldv_main_exported_112();
  } else {
  }
  goto ldv_45087;
  case 58: ;
  if (ldv_state_variable_69 != 0) {
    ldv_main_exported_69();
  } else {
  }
  goto ldv_45087;
  case 59: ;
  if (ldv_state_variable_172 != 0) {
    ldv_main_exported_172();
  } else {
  }
  goto ldv_45087;
  case 60: ;
  if (ldv_state_variable_145 != 0) {
    ldv_main_exported_145();
  } else {
  }
  goto ldv_45087;
  case 61: ;
  if (ldv_state_variable_49 != 0) {
    ldv_main_exported_49();
  } else {
  }
  goto ldv_45087;
  case 62: ;
  if (ldv_state_variable_178 != 0) {
    tmp___21 = __VERIFIER_nondet_int();
    switch (tmp___21) {
    case 0: ;
    if (ldv_state_variable_178 == 2) {
      drm_read(amdgpu_driver_kms_fops_group2, ldvarg384, ldvarg383, ldvarg385);
      ldv_state_variable_178 = 2;
    } else {
    }
    goto ldv_45151;
    case 1: ;
    if (ldv_state_variable_178 == 2) {
      amdgpu_kms_compat_ioctl(amdgpu_driver_kms_fops_group2, ldvarg382, ldvarg381);
      ldv_state_variable_178 = 2;
    } else {
    }
    goto ldv_45151;
    case 2: ;
    if (ldv_state_variable_178 == 1) {
      drm_poll(amdgpu_driver_kms_fops_group2, ldvarg380);
      ldv_state_variable_178 = 1;
    } else {
    }
    if (ldv_state_variable_178 == 2) {
      drm_poll(amdgpu_driver_kms_fops_group2, ldvarg380);
      ldv_state_variable_178 = 2;
    } else {
    }
    goto ldv_45151;
    case 3: ;
    if (ldv_state_variable_178 == 1) {
      ldv_retval_19 = drm_open(amdgpu_driver_kms_fops_group1, amdgpu_driver_kms_fops_group2);
      if (ldv_retval_19 == 0) {
        ldv_state_variable_178 = 2;
        ref_cnt = ref_cnt + 1;
      } else {
      }
    } else {
    }
    goto ldv_45151;
    case 4: ;
    if (ldv_state_variable_178 == 1) {
      amdgpu_mmap(amdgpu_driver_kms_fops_group2, ldvarg379);
      ldv_state_variable_178 = 1;
    } else {
    }
    if (ldv_state_variable_178 == 2) {
      amdgpu_mmap(amdgpu_driver_kms_fops_group2, ldvarg379);
      ldv_state_variable_178 = 2;
    } else {
    }
    goto ldv_45151;
    case 5: ;
    if (ldv_state_variable_178 == 2) {
      drm_release(amdgpu_driver_kms_fops_group1, amdgpu_driver_kms_fops_group2);
      ldv_state_variable_178 = 1;
      ref_cnt = ref_cnt - 1;
    } else {
    }
    goto ldv_45151;
    case 6: ;
    if (ldv_state_variable_178 == 2) {
      amdgpu_drm_ioctl(amdgpu_driver_kms_fops_group2, ldvarg378, ldvarg377);
      ldv_state_variable_178 = 2;
    } else {
    }
    goto ldv_45151;
    default:
    ldv_stop();
    }
    ldv_45151: ;
  } else {
  }
  goto ldv_45087;
  case 63: ;
  if (ldv_state_variable_24 != 0) {
    ldv_main_exported_24();
  } else {
  }
  goto ldv_45087;
  case 64: ;
  if (ldv_state_variable_140 != 0) {
    ldv_main_exported_140();
  } else {
  }
  goto ldv_45087;
  case 65: ;
  if (ldv_state_variable_124 != 0) {
    ldv_main_exported_124();
  } else {
  }
  goto ldv_45087;
  case 66: ;
  if (ldv_state_variable_104 != 0) {
    ldv_main_exported_104();
  } else {
  }
  goto ldv_45087;
  case 67: ;
  if (ldv_state_variable_131 != 0) {
    ldv_main_exported_131();
  } else {
  }
  goto ldv_45087;
  case 68: ;
  if (ldv_state_variable_121 != 0) {
    ldv_main_exported_121();
  } else {
  }
  goto ldv_45087;
  case 69: ;
  if (ldv_state_variable_79 != 0) {
    ldv_main_exported_79();
  } else {
  }
  goto ldv_45087;
  case 70: ;
  if (ldv_state_variable_154 != 0) {
    ldv_main_exported_154();
  } else {
  }
  goto ldv_45087;
  case 71: ;
  if (ldv_state_variable_0 != 0) {
    tmp___22 = __VERIFIER_nondet_int();
    switch (tmp___22) {
    case 0: ;
    if (ldv_state_variable_0 == 2 && ref_cnt == 0) {
      amdgpu_exit();
      ldv_state_variable_0 = 3;
      goto ldv_final;
    } else {
    }
    goto ldv_45170;
    case 1: ;
    if (ldv_state_variable_0 == 1) {
      ldv_retval_20 = amdgpu_init();
      if (ldv_retval_20 != 0) {
        ldv_state_variable_0 = 3;
        goto ldv_final;
      } else {
      }
      if (ldv_retval_20 == 0) {
        ldv_state_variable_0 = 2;
        ldv_state_variable_171 = 1;
        ldv_initialize_drm_connector_helper_funcs_171();
        ldv_state_variable_132 = 1;
        ldv_initialize_trace_event_class_132();
        ldv_state_variable_164 = 1;
        ldv_initialize_fence_ops_164();
        ldv_state_variable_169 = 1;
        ldv_initialize_drm_connector_helper_funcs_169();
        ldv_state_variable_34 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_34();
        ldv_state_variable_38 = 1;
        ldv_initialize_amdgpu_ring_funcs_38();
        ldv_state_variable_111 = 1;
        ldv_state_variable_51 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_51();
        ldv_state_variable_146 = 1;
        ldv_state_variable_94 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_94();
        ldv_state_variable_36 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_36();
        ldv_state_variable_85 = 1;
        ldv_state_variable_105 = 1;
        ldv_initialize_drm_encoder_helper_funcs_105();
        ldv_state_variable_13 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_13();
        ldv_state_variable_46 = 1;
        ldv_initialize_drm_encoder_helper_funcs_46();
        ldv_state_variable_42 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_42();
        ldv_state_variable_22 = 1;
        ldv_initialize_amdgpu_vm_pte_funcs_22();
        ldv_state_variable_158 = 1;
        ldv_initialize_drm_mode_config_funcs_158();
        ldv_state_variable_143 = 1;
        ldv_state_variable_122 = 1;
        ldv_initialize_backlight_ops_122();
        ldv_state_variable_53 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_53();
        ldv_state_variable_130 = 1;
        ldv_initialize_trace_event_class_130();
        ldv_state_variable_155 = 1;
        ldv_initialize_drm_fb_helper_funcs_155();
        ldv_state_variable_150 = 1;
        ldv_state_variable_177 = 1;
        ldv_initialize_drm_driver_177();
        ldv_state_variable_59 = 1;
        ldv_state_variable_134 = 1;
        ldv_state_variable_83 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_83();
        ldv_state_variable_75 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_75();
        ldv_state_variable_40 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_40();
        ldv_state_variable_156 = 1;
        ldv_initialize_fb_ops_156();
        ldv_state_variable_120 = 1;
        ldv_state_variable_28 = 1;
        ldv_initialize_amdgpu_vm_pte_funcs_28();
        ldv_state_variable_128 = 1;
        ldv_initialize_trace_event_class_128();
        ldv_state_variable_25 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_25();
        ldv_state_variable_100 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_100();
        ldv_state_variable_30 = 1;
        ldv_state_variable_141 = 1;
        ldv_state_variable_144 = 1;
        ldv_state_variable_136 = 1;
        ldv_state_variable_116 = 1;
        ldv_state_variable_88 = 1;
        ldv_state_variable_166 = 1;
        ldv_initialize_drm_connector_funcs_166();
        ldv_state_variable_68 = 1;
        ldv_state_variable_70 = 1;
        ldv_state_variable_54 = 1;
        ldv_initialize_amdgpu_display_funcs_54();
        ldv_state_variable_19 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_19();
        ldv_state_variable_66 = 1;
        ldv_initialize_amdgpu_dpm_funcs_66();
        ldv_state_variable_45 = 1;
        ldv_initialize_drm_encoder_helper_funcs_45();
        ldv_state_variable_56 = 1;
        ldv_initialize_drm_encoder_helper_funcs_56();
        ldv_state_variable_173 = 1;
        ldv_initialize_drm_connector_helper_funcs_173();
        ldv_state_variable_52 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_52();
        ldv_state_variable_12 = 1;
        ldv_state_variable_41 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_41();
        ldv_state_variable_97 = 1;
        ldv_state_variable_64 = 1;
        ldv_state_variable_39 = 1;
        ldv_state_variable_50 = 1;
        ldv_initialize_drm_crtc_funcs_50();
        ldv_state_variable_123 = 1;
        ldv_initialize_trace_event_class_123();
        ldv_state_variable_149 = 1;
        ldv_initialize_sensor_device_attribute_149();
        ldv_state_variable_133 = 1;
        ldv_initialize_trace_event_class_133();
        ldv_state_variable_77 = 1;
        ldv_state_variable_174 = 1;
        ldv_file_operations_174();
        ldv_state_variable_87 = 1;
        ldv_initialize_amdgpu_buffer_funcs_87();
        ldv_state_variable_107 = 1;
        ldv_initialize_drm_crtc_helper_funcs_107();
        ldv_state_variable_48 = 1;
        ldv_state_variable_167 = 1;
        ldv_initialize_drm_connector_helper_funcs_167();
        ldv_state_variable_91 = 1;
        ldv_state_variable_142 = 1;
        ldv_state_variable_152 = 1;
        ldv_state_variable_113 = 1;
        ldv_initialize_amdgpu_dpm_funcs_113();
        ldv_state_variable_103 = 1;
        ldv_initialize_drm_encoder_helper_funcs_103();
        ldv_state_variable_92 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_92();
        ldv_state_variable_115 = 1;
        ldv_initialize_amdgpu_ih_funcs_115();
        ldv_state_variable_108 = 1;
        ldv_initialize_drm_crtc_funcs_108();
        ldv_state_variable_61 = 1;
        ldv_initialize_drm_crtc_funcs_61();
        ldv_state_variable_74 = 1;
        ldv_state_variable_72 = 1;
        ldv_state_variable_162 = 1;
        ldv_initialize_ttm_bo_driver_162();
        ldv_state_variable_179 = 1;
        ldv_dev_pm_ops_179();
        ldv_state_variable_99 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_99();
        ldv_state_variable_119 = 1;
        ldv_state_variable_80 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_80();
        ldv_state_variable_26 = 1;
        ldv_initialize_amdgpu_ring_funcs_26();
        ldv_state_variable_63 = 1;
        ldv_state_variable_21 = 1;
        ldv_state_variable_33 = 1;
        ldv_state_variable_170 = 1;
        ldv_initialize_drm_connector_funcs_170();
        ldv_state_variable_43 = 1;
        ldv_initialize_amdgpu_display_funcs_43();
        ldv_state_variable_117 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_117();
        ldv_state_variable_37 = 1;
        ldv_initialize_amdgpu_ring_funcs_37();
        ldv_state_variable_98 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_98();
        ldv_state_variable_47 = 1;
        ldv_initialize_drm_encoder_helper_funcs_47();
        ldv_state_variable_176 = 1;
        ldv_pci_driver_176();
        ldv_state_variable_160 = 1;
        ldv_file_operations_160();
        ldv_state_variable_159 = 1;
        ldv_initialize_drm_framebuffer_funcs_159();
        ldv_state_variable_126 = 1;
        ldv_initialize_trace_event_class_126();
        ldv_state_variable_96 = 1;
        ldv_initialize_amdgpu_ring_funcs_96();
        ldv_state_variable_23 = 1;
        ldv_initialize_amdgpu_buffer_funcs_23();
        ldv_state_variable_154 = 1;
        ldv_initialize_device_attribute_154();
        ldv_state_variable_79 = 1;
        ldv_initialize_amdgpu_asic_funcs_79();
        ldv_state_variable_121 = 1;
        ldv_initialize_amdgpu_asic_funcs_121();
        ldv_state_variable_131 = 1;
        ldv_initialize_trace_event_class_131();
        ldv_state_variable_104 = 1;
        ldv_initialize_drm_encoder_helper_funcs_104();
        ldv_state_variable_124 = 1;
        ldv_initialize_trace_event_class_124();
        ldv_state_variable_140 = 1;
        ldv_state_variable_24 = 1;
        ldv_state_variable_178 = 1;
        ldv_file_operations_178();
        ldv_state_variable_49 = 1;
        ldv_initialize_drm_crtc_helper_funcs_49();
        ldv_state_variable_145 = 1;
        ldv_state_variable_172 = 1;
        ldv_initialize_drm_connector_funcs_172();
        ldv_state_variable_69 = 1;
        ldv_initialize_amdgpu_ih_funcs_69();
        ldv_state_variable_112 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_112();
        ldv_state_variable_14 = 1;
        ldv_initialize_amdgpu_ring_funcs_14();
        ldv_state_variable_135 = 1;
        ldv_state_variable_168 = 1;
        ldv_initialize_drm_connector_funcs_168();
        ldv_state_variable_147 = 1;
        ldv_state_variable_82 = 1;
        ldv_state_variable_110 = 1;
        ldv_initialize_amdgpu_dpm_funcs_110();
        ldv_state_variable_17 = 1;
        ldv_initialize_amdgpu_ring_funcs_17();
        ldv_state_variable_129 = 1;
        ldv_initialize_trace_event_class_129();
        ldv_state_variable_139 = 1;
        ldv_state_variable_165 = 1;
        ldv_initialize_drm_connector_funcs_165();
        ldv_state_variable_67 = 1;
        ldv_state_variable_62 = 1;
        ldv_state_variable_76 = 1;
        ldv_initialize_amdgpu_gart_funcs_76();
        ldv_state_variable_86 = 1;
        ldv_initialize_amdgpu_vm_pte_funcs_86();
        ldv_state_variable_73 = 1;
        ldv_initialize_amdgpu_ih_funcs_73();
        ldv_state_variable_101 = 1;
        ldv_initialize_amdgpu_display_funcs_101();
        ldv_state_variable_60 = 1;
        ldv_initialize_drm_crtc_helper_funcs_60();
        ldv_state_variable_81 = 1;
        ldv_initialize_amdgpu_ring_funcs_81();
        ldv_state_variable_137 = 1;
        ldv_state_variable_15 = 1;
        ldv_state_variable_153 = 1;
        ldv_initialize_device_attribute_153();
        ldv_state_variable_58 = 1;
        ldv_initialize_drm_encoder_helper_funcs_58();
        ldv_state_variable_114 = 1;
        ldv_state_variable_138 = 1;
        ldv_state_variable_29 = 1;
        ldv_initialize_amdgpu_buffer_funcs_29();
        ldv_state_variable_65 = 1;
        ldv_state_variable_157 = 1;
        ldv_initialize_i2c_algorithm_157();
        ldv_state_variable_106 = 1;
        ldv_state_variable_93 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_93();
        ldv_state_variable_78 = 1;
        ldv_state_variable_11 = 1;
        ldv_initialize_mmu_notifier_ops_11();
        ldv_state_variable_35 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_35();
        ldv_state_variable_31 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_31();
        ldv_state_variable_148 = 1;
        ldv_initialize_sensor_device_attribute_148();
        ldv_state_variable_175 = 1;
        ldv_initialize_vga_switcheroo_client_ops_175();
        ldv_state_variable_89 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_89();
        ldv_state_variable_151 = 1;
        ldv_state_variable_109 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_109();
        ldv_state_variable_163 = 1;
        ldv_initialize_ttm_backend_func_163();
        ldv_state_variable_20 = 1;
        ldv_initialize_amdgpu_ring_funcs_20();
        ldv_state_variable_57 = 1;
        ldv_initialize_drm_encoder_helper_funcs_57();
        ldv_state_variable_95 = 1;
        ldv_initialize_amdgpu_ring_funcs_95();
        ldv_state_variable_161 = 1;
        ldv_file_operations_161();
        ldv_state_variable_27 = 1;
        ldv_state_variable_84 = 1;
        ldv_initialize_amdgpu_ring_funcs_84();
        ldv_state_variable_55 = 1;
        ldv_state_variable_44 = 1;
        ldv_state_variable_16 = 1;
        ldv_initialize_amdgpu_irq_src_funcs_16();
        ldv_state_variable_125 = 1;
        ldv_initialize_trace_event_class_125();
        ldv_state_variable_18 = 1;
        ldv_state_variable_102 = 1;
        ldv_state_variable_71 = 1;
        ldv_initialize_amdgpu_ih_funcs_71();
        ldv_state_variable_118 = 1;
        ldv_initialize_amdgpu_gart_funcs_118();
        ldv_state_variable_90 = 1;
        ldv_initialize_amdgpu_ring_funcs_90();
        ldv_state_variable_32 = 1;
        ldv_initialize_amdgpu_ring_funcs_32();
        ldv_state_variable_127 = 1;
        ldv_initialize_trace_event_class_127();
      } else {
      }
    } else {
    }
    goto ldv_45170;
    default:
    ldv_stop();
    }
    ldv_45170: ;
  } else {
  }
  goto ldv_45087;
  case 72: ;
  if (ldv_state_variable_23 != 0) {
    ldv_main_exported_23();
  } else {
  }
  goto ldv_45087;
  case 73: ;
  if (ldv_state_variable_96 != 0) {
    ldv_main_exported_96();
  } else {
  }
  goto ldv_45087;
  case 74: ;
  if (ldv_state_variable_126 != 0) {
    ldv_main_exported_126();
  } else {
  }
  goto ldv_45087;
  case 75: ;
  if (ldv_state_variable_159 != 0) {
    ldv_main_exported_159();
  } else {
  }
  goto ldv_45087;
  case 76: ;
  if (ldv_state_variable_160 != 0) {
    ldv_main_exported_160();
  } else {
  }
  goto ldv_45087;
  case 77: ;
  if (ldv_state_variable_176 != 0) {
    tmp___23 = __VERIFIER_nondet_int();
    switch (tmp___23) {
    case 0: ;
    if (ldv_state_variable_176 == 1) {
      ldv_retval_22 = amdgpu_pci_probe(amdgpu_kms_pci_driver_group1, (struct pci_device_id const *)ldvarg453);
      if (ldv_retval_22 == 0) {
        ldv_state_variable_176 = 2;
        ref_cnt = ref_cnt + 1;
      } else {
      }
    } else {
    }
    goto ldv_45180;
    case 1: ;
    if (ldv_state_variable_176 == 2) {
      amdgpu_pci_remove(amdgpu_kms_pci_driver_group1);
      ldv_state_variable_176 = 1;
    } else {
    }
    goto ldv_45180;
    case 2: ;
    if (ldv_state_variable_176 == 2) {
      ldv_shutdown_176();
      ldv_state_variable_176 = 2;
    } else {
    }
    goto ldv_45180;
    default:
    ldv_stop();
    }
    ldv_45180: ;
  } else {
  }
  goto ldv_45087;
  case 78: ;
  if (ldv_state_variable_47 != 0) {
    ldv_main_exported_47();
  } else {
  }
  goto ldv_45087;
  case 79: ;
  goto ldv_45087;
  case 80: ;
  if (ldv_state_variable_98 != 0) {
    ldv_main_exported_98();
  } else {
  }
  goto ldv_45087;
  case 81: ;
  if (ldv_state_variable_37 != 0) {
    ldv_main_exported_37();
  } else {
  }
  goto ldv_45087;
  case 82: ;
  if (ldv_state_variable_117 != 0) {
    ldv_main_exported_117();
  } else {
  }
  goto ldv_45087;
  case 83: ;
  if (ldv_state_variable_43 != 0) {
    ldv_main_exported_43();
  } else {
  }
  goto ldv_45087;
  case 84: ;
  goto ldv_45087;
  case 85: ;
  if (ldv_state_variable_170 != 0) {
    ldv_main_exported_170();
  } else {
  }
  goto ldv_45087;
  case 86: ;
  if (ldv_state_variable_33 != 0) {
    ldv_main_exported_33();
  } else {
  }
  goto ldv_45087;
  case 87: ;
  if (ldv_state_variable_21 != 0) {
    ldv_main_exported_21();
  } else {
  }
  goto ldv_45087;
  case 88: ;
  if (ldv_state_variable_63 != 0) {
    ldv_main_exported_63();
  } else {
  }
  goto ldv_45087;
  case 89: ;
  goto ldv_45087;
  case 90: ;
  if (ldv_state_variable_26 != 0) {
    ldv_main_exported_26();
  } else {
  }
  goto ldv_45087;
  case 91: ;
  if (ldv_state_variable_80 != 0) {
    ldv_main_exported_80();
  } else {
  }
  goto ldv_45087;
  case 92: ;
  if (ldv_state_variable_119 != 0) {
    ldv_main_exported_119();
  } else {
  }
  goto ldv_45087;
  case 93: ;
  if (ldv_state_variable_99 != 0) {
    ldv_main_exported_99();
  } else {
  }
  goto ldv_45087;
  case 94: ;
  if (ldv_state_variable_179 != 0) {
    tmp___24 = __VERIFIER_nondet_int();
    switch (tmp___24) {
    case 0: ;
    if (ldv_state_variable_179 == 15) {
      ldv_retval_49 = amdgpu_pmops_thaw(amdgpu_pm_ops_group1);
      if (ldv_retval_49 == 0) {
        ldv_state_variable_179 = 16;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 1: ;
    if (ldv_state_variable_179 == 2) {
      amdgpu_pmops_runtime_idle(amdgpu_pm_ops_group1);
      ldv_state_variable_179 = 2;
    } else {
    }
    if (ldv_state_variable_179 == 1) {
      amdgpu_pmops_runtime_idle(amdgpu_pm_ops_group1);
      ldv_state_variable_179 = 1;
    } else {
    }
    goto ldv_45202;
    case 2: ;
    if (ldv_state_variable_179 == 2) {
      ldv_retval_48 = amdgpu_pmops_runtime_resume(amdgpu_pm_ops_group1);
      if (ldv_retval_48 == 0) {
        ldv_state_variable_179 = 1;
        ref_cnt = ref_cnt - 1;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 3: ;
    if (ldv_state_variable_179 == 3) {
      ldv_retval_47 = amdgpu_pmops_suspend(amdgpu_pm_ops_group1);
      if (ldv_retval_47 == 0) {
        ldv_state_variable_179 = 4;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 4: ;
    if (ldv_state_variable_179 == 14) {
      ldv_retval_46 = amdgpu_pmops_resume(amdgpu_pm_ops_group1);
      if (ldv_retval_46 == 0) {
        ldv_state_variable_179 = 16;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 5: ;
    if (ldv_state_variable_179 == 1) {
      ldv_retval_45 = amdgpu_pmops_runtime_suspend(amdgpu_pm_ops_group1);
      if (ldv_retval_45 == 0) {
        ldv_state_variable_179 = 2;
        ref_cnt = ref_cnt + 1;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 6: ;
    if (ldv_state_variable_179 == 3) {
      ldv_retval_44 = amdgpu_pmops_freeze(amdgpu_pm_ops_group1);
      if (ldv_retval_44 == 0) {
        ldv_state_variable_179 = 5;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 7: ;
    if (ldv_state_variable_179 == 3) {
      ldv_retval_43 = amdgpu_pmops_freeze(amdgpu_pm_ops_group1);
      if (ldv_retval_43 == 0) {
        ldv_state_variable_179 = 6;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 8: ;
    if (ldv_state_variable_179 == 13) {
      ldv_retval_42 = amdgpu_pmops_resume(amdgpu_pm_ops_group1);
      if (ldv_retval_42 == 0) {
        ldv_state_variable_179 = 16;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 9: ;
    if (ldv_state_variable_179 == 4) {
      ldv_retval_41 = ldv_suspend_late_179();
      if (ldv_retval_41 == 0) {
        ldv_state_variable_179 = 7;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 10: ;
    if (ldv_state_variable_179 == 10) {
      ldv_retval_40 = ldv_restore_early_179();
      if (ldv_retval_40 == 0) {
        ldv_state_variable_179 = 14;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 11: ;
    if (ldv_state_variable_179 == 7) {
      ldv_retval_39 = ldv_resume_early_179();
      if (ldv_retval_39 == 0) {
        ldv_state_variable_179 = 13;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 12: ;
    if (ldv_state_variable_179 == 12) {
      ldv_retval_38 = ldv_thaw_early_179();
      if (ldv_retval_38 == 0) {
        ldv_state_variable_179 = 15;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 13: ;
    if (ldv_state_variable_179 == 8) {
      ldv_retval_37 = ldv_resume_noirq_179();
      if (ldv_retval_37 == 0) {
        ldv_state_variable_179 = 13;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 14: ;
    if (ldv_state_variable_179 == 6) {
      ldv_retval_36 = ldv_freeze_noirq_179();
      if (ldv_retval_36 == 0) {
        ldv_state_variable_179 = 11;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 15: ;
    if (ldv_state_variable_179 == 1) {
      ldv_retval_35 = ldv_prepare_179();
      if (ldv_retval_35 == 0) {
        ldv_state_variable_179 = 3;
        ref_cnt = ref_cnt + 1;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 16: ;
    if (ldv_state_variable_179 == 6) {
      ldv_retval_34 = ldv_freeze_late_179();
      if (ldv_retval_34 == 0) {
        ldv_state_variable_179 = 12;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 17: ;
    if (ldv_state_variable_179 == 11) {
      ldv_retval_33 = ldv_thaw_noirq_179();
      if (ldv_retval_33 == 0) {
        ldv_state_variable_179 = 15;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 18: ;
    if (ldv_state_variable_179 == 5) {
      ldv_retval_32 = ldv_poweroff_noirq_179();
      if (ldv_retval_32 == 0) {
        ldv_state_variable_179 = 9;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 19: ;
    if (ldv_state_variable_179 == 5) {
      ldv_retval_31 = ldv_poweroff_late_179();
      if (ldv_retval_31 == 0) {
        ldv_state_variable_179 = 10;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 20: ;
    if (ldv_state_variable_179 == 9) {
      ldv_retval_30 = ldv_restore_noirq_179();
      if (ldv_retval_30 == 0) {
        ldv_state_variable_179 = 14;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 21: ;
    if (ldv_state_variable_179 == 4) {
      ldv_retval_29 = ldv_suspend_noirq_179();
      if (ldv_retval_29 == 0) {
        ldv_state_variable_179 = 8;
      } else {
      }
    } else {
    }
    goto ldv_45202;
    case 22: ;
    if (ldv_state_variable_179 == 16) {
      ldv_complete_179();
      ldv_state_variable_179 = 1;
      ref_cnt = ref_cnt - 1;
    } else {
    }
    goto ldv_45202;
    default:
    ldv_stop();
    }
    ldv_45202: ;
  } else {
  }
  goto ldv_45087;
  case 95: ;
  if (ldv_state_variable_162 != 0) {
    ldv_main_exported_162();
  } else {
  }
  goto ldv_45087;
  case 96: ;
  if (ldv_state_variable_72 != 0) {
    ldv_main_exported_72();
  } else {
  }
  goto ldv_45087;
  case 97: ;
  if (ldv_state_variable_74 != 0) {
    ldv_main_exported_74();
  } else {
  }
  goto ldv_45087;
  case 98: ;
  if (ldv_state_variable_61 != 0) {
    ldv_main_exported_61();
  } else {
  }
  goto ldv_45087;
  case 99: ;
  if (ldv_state_variable_108 != 0) {
    ldv_main_exported_108();
  } else {
  }
  goto ldv_45087;
  case 100: ;
  if (ldv_state_variable_115 != 0) {
    ldv_main_exported_115();
  } else {
  }
  goto ldv_45087;
  case 101: ;
  if (ldv_state_variable_92 != 0) {
    ldv_main_exported_92();
  } else {
  }
  goto ldv_45087;
  case 102: ;
  if (ldv_state_variable_103 != 0) {
    ldv_main_exported_103();
  } else {
  }
  goto ldv_45087;
  case 103: ;
  goto ldv_45087;
  case 104: ;
  if (ldv_state_variable_113 != 0) {
    ldv_main_exported_113();
  } else {
  }
  goto ldv_45087;
  case 105: ;
  if (ldv_state_variable_152 != 0) {
    ldv_main_exported_152();
  } else {
  }
  goto ldv_45087;
  case 106: ;
  if (ldv_state_variable_142 != 0) {
    ldv_main_exported_142();
  } else {
  }
  goto ldv_45087;
  case 107: ;
  if (ldv_state_variable_91 != 0) {
    ldv_main_exported_91();
  } else {
  }
  goto ldv_45087;
  case 108: ;
  if (ldv_state_variable_167 != 0) {
    ldv_main_exported_167();
  } else {
  }
  goto ldv_45087;
  case 109: ;
  if (ldv_state_variable_48 != 0) {
    ldv_main_exported_48();
  } else {
  }
  goto ldv_45087;
  case 110: ;
  if (ldv_state_variable_107 != 0) {
    ldv_main_exported_107();
  } else {
  }
  goto ldv_45087;
  case 111: ;
  if (ldv_state_variable_87 != 0) {
    ldv_main_exported_87();
  } else {
  }
  goto ldv_45087;
  case 112: ;
  if (ldv_state_variable_174 != 0) {
    ldv_main_exported_174();
  } else {
  }
  goto ldv_45087;
  case 113: ;
  if (ldv_state_variable_77 != 0) {
    ldv_main_exported_77();
  } else {
  }
  goto ldv_45087;
  case 114: ;
  if (ldv_state_variable_133 != 0) {
    ldv_main_exported_133();
  } else {
  }
  goto ldv_45087;
  case 115: ;
  if (ldv_state_variable_149 != 0) {
    ldv_main_exported_149();
  } else {
  }
  goto ldv_45087;
  case 116: ;
  if (ldv_state_variable_123 != 0) {
    ldv_main_exported_123();
  } else {
  }
  goto ldv_45087;
  case 117: ;
  if (ldv_state_variable_50 != 0) {
    ldv_main_exported_50();
  } else {
  }
  goto ldv_45087;
  case 118: ;
  if (ldv_state_variable_39 != 0) {
    ldv_main_exported_39();
  } else {
  }
  goto ldv_45087;
  case 119: ;
  if (ldv_state_variable_64 != 0) {
    ldv_main_exported_64();
  } else {
  }
  goto ldv_45087;
  case 120: ;
  if (ldv_state_variable_97 != 0) {
    ldv_main_exported_97();
  } else {
  }
  goto ldv_45087;
  case 121: ;
  if (ldv_state_variable_12 != 0) {
    ldv_main_exported_12();
  } else {
  }
  goto ldv_45087;
  case 122: ;
  if (ldv_state_variable_41 != 0) {
    ldv_main_exported_41();
  } else {
  }
  goto ldv_45087;
  case 123: ;
  if (ldv_state_variable_52 != 0) {
    ldv_main_exported_52();
  } else {
  }
  goto ldv_45087;
  case 124: ;
  if (ldv_state_variable_173 != 0) {
    ldv_main_exported_173();
  } else {
  }
  goto ldv_45087;
  case 125: ;
  if (ldv_state_variable_56 != 0) {
    ldv_main_exported_56();
  } else {
  }
  goto ldv_45087;
  case 126: ;
  if (ldv_state_variable_45 != 0) {
    ldv_main_exported_45();
  } else {
  }
  goto ldv_45087;
  case 127: ;
  if (ldv_state_variable_66 != 0) {
    ldv_main_exported_66();
  } else {
  }
  goto ldv_45087;
  case 128: ;
  if (ldv_state_variable_19 != 0) {
    ldv_main_exported_19();
  } else {
  }
  goto ldv_45087;
  case 129: ;
  if (ldv_state_variable_54 != 0) {
    ldv_main_exported_54();
  } else {
  }
  goto ldv_45087;
  case 130: ;
  if (ldv_state_variable_70 != 0) {
    ldv_main_exported_70();
  } else {
  }
  goto ldv_45087;
  case 131: ;
  if (ldv_state_variable_68 != 0) {
    ldv_main_exported_68();
  } else {
  }
  goto ldv_45087;
  case 132: ;
  if (ldv_state_variable_166 != 0) {
    ldv_main_exported_166();
  } else {
  }
  goto ldv_45087;
  case 133: ;
  goto ldv_45087;
  case 134: ;
  if (ldv_state_variable_136 != 0) {
    ldv_main_exported_136();
  } else {
  }
  goto ldv_45087;
  case 135: ;
  if (ldv_state_variable_88 != 0) {
    ldv_main_exported_88();
  } else {
  }
  goto ldv_45087;
  case 136: ;
  if (ldv_state_variable_116 != 0) {
    ldv_main_exported_116();
  } else {
  }
  goto ldv_45087;
  case 137: ;
  if (ldv_state_variable_144 != 0) {
    ldv_main_exported_144();
  } else {
  }
  goto ldv_45087;
  case 138: ;
  if (ldv_state_variable_141 != 0) {
    ldv_main_exported_141();
  } else {
  }
  goto ldv_45087;
  case 139: ;
  if (ldv_state_variable_30 != 0) {
    ldv_main_exported_30();
  } else {
  }
  goto ldv_45087;
  case 140: ;
  if (ldv_state_variable_100 != 0) {
    ldv_main_exported_100();
  } else {
  }
  goto ldv_45087;
  case 141: ;
  if (ldv_state_variable_25 != 0) {
    ldv_main_exported_25();
  } else {
  }
  goto ldv_45087;
  case 142: ;
  if (ldv_state_variable_128 != 0) {
    ldv_main_exported_128();
  } else {
  }
  goto ldv_45087;
  case 143: ;
  if (ldv_state_variable_28 != 0) {
    ldv_main_exported_28();
  } else {
  }
  goto ldv_45087;
  case 144: ;
  if (ldv_state_variable_120 != 0) {
    ldv_main_exported_120();
  } else {
  }
  goto ldv_45087;
  case 145: ;
  if (ldv_state_variable_156 != 0) {
    ldv_main_exported_156();
  } else {
  }
  goto ldv_45087;
  case 146: ;
  if (ldv_state_variable_134 != 0) {
    ldv_main_exported_134();
  } else {
  }
  goto ldv_45087;
  case 147: ;
  if (ldv_state_variable_40 != 0) {
    ldv_main_exported_40();
  } else {
  }
  goto ldv_45087;
  case 148: ;
  if (ldv_state_variable_75 != 0) {
    ldv_main_exported_75();
  } else {
  }
  goto ldv_45087;
  case 149: ;
  if (ldv_state_variable_83 != 0) {
    ldv_main_exported_83();
  } else {
  }
  goto ldv_45087;
  case 150: ;
  if (ldv_state_variable_59 != 0) {
    ldv_main_exported_59();
  } else {
  }
  goto ldv_45087;
  case 151: ;
  if (ldv_state_variable_177 != 0) {
    tmp___25 = __VERIFIER_nondet_int();
    switch (tmp___25) {
    case 0: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_import_sg_table(kms_driver_group3, ldvarg992, ldvarg991);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_import_sg_table(kms_driver_group3, ldvarg992, ldvarg991);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 1: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_unpin(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_unpin(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 2: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_object_close(kms_driver_group0, kms_driver_group2);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_object_close(kms_driver_group0, kms_driver_group2);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 3: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_irq_preinstall(kms_driver_group3);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_irq_preinstall(kms_driver_group3);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 4: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_get_sg_table(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_get_sg_table(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 5: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_irq_uninstall(kms_driver_group3);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_irq_uninstall(kms_driver_group3);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 6: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_vunmap(kms_driver_group0, ldvarg990);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_vunmap(kms_driver_group0, ldvarg990);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 7: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_irq_postinstall(kms_driver_group3);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_irq_postinstall(kms_driver_group3);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 8: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_vmap(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_vmap(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 9: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_export(kms_driver_group3, kms_driver_group0, ldvarg989);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_export(kms_driver_group3, kms_driver_group0, ldvarg989);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 10: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_mode_dumb_create(kms_driver_group2, kms_driver_group3, ldvarg988);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_mode_dumb_create(kms_driver_group2, kms_driver_group3, ldvarg988);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 11: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_enable_vblank_kms(kms_driver_group3, ldvarg987);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_enable_vblank_kms(kms_driver_group3, ldvarg987);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 12: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_debugfs_init(kms_driver_group1);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_debugfs_init(kms_driver_group1);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 13: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_driver_preclose_kms(kms_driver_group3, kms_driver_group2);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_driver_preclose_kms(kms_driver_group3, kms_driver_group2);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 14: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_driver_lastclose_kms(kms_driver_group3);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_driver_lastclose_kms(kms_driver_group3);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 15: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_get_vblank_counter_kms(kms_driver_group3, ldvarg986);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_get_vblank_counter_kms(kms_driver_group3, ldvarg986);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 16: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_mode_dumb_mmap(kms_driver_group2, kms_driver_group3, ldvarg984, ldvarg985);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_mode_dumb_mmap(kms_driver_group2, kms_driver_group3, ldvarg984, ldvarg985);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 17: ;
    if (ldv_state_variable_177 == 2) {
      drm_gem_prime_import(kms_driver_group3, ldvarg983);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      drm_gem_prime_import(kms_driver_group3, ldvarg983);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 18: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_object_open(kms_driver_group0, kms_driver_group2);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_object_open(kms_driver_group0, kms_driver_group2);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 19: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_get_crtc_scanoutpos(kms_driver_group3, ldvarg980, ldvarg978, ldvarg981,
                                 ldvarg982, ldvarg977, ldvarg979);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_get_crtc_scanoutpos(kms_driver_group3, ldvarg980, ldvarg978, ldvarg981,
                                 ldvarg982, ldvarg977, ldvarg979);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 20: ;
    if (ldv_state_variable_177 == 1) {
      ldv_retval_76 = amdgpu_driver_open_kms(kms_driver_group3, kms_driver_group2);
      if (ldv_retval_76 == 0) {
        ldv_state_variable_177 = 2;
        ref_cnt = ref_cnt + 1;
      } else {
      }
    } else {
    }
    goto ldv_45284;
    case 21: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_debugfs_cleanup(kms_driver_group1);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_debugfs_cleanup(kms_driver_group1);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 22: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_pin(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_pin(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 23: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_object_free(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_object_free(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 24: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_gem_prime_res_obj(kms_driver_group0);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_gem_prime_res_obj(kms_driver_group0);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 25: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_get_vblank_timestamp_kms(kms_driver_group3, ldvarg974, ldvarg973, ldvarg975,
                                      ldvarg976);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_get_vblank_timestamp_kms(kms_driver_group3, ldvarg974, ldvarg973, ldvarg975,
                                      ldvarg976);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 26: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_driver_unload_kms(kms_driver_group3);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_driver_unload_kms(kms_driver_group3);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 27: ;
    if (ldv_state_variable_177 == 2) {
      drm_gem_prime_handle_to_fd(kms_driver_group3, kms_driver_group2, ldvarg970,
                                 ldvarg971, ldvarg972);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      drm_gem_prime_handle_to_fd(kms_driver_group3, kms_driver_group2, ldvarg970,
                                 ldvarg971, ldvarg972);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 28: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_driver_load_kms(kms_driver_group3, ldvarg969);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_driver_load_kms(kms_driver_group3, ldvarg969);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 29: ;
    if (ldv_state_variable_177 == 2) {
      drm_gem_dumb_destroy(kms_driver_group2, kms_driver_group3, ldvarg968);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      drm_gem_dumb_destroy(kms_driver_group2, kms_driver_group3, ldvarg968);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 30: ;
    if (ldv_state_variable_177 == 2) {
      drm_gem_prime_fd_to_handle(kms_driver_group3, kms_driver_group2, ldvarg966,
                                 ldvarg967);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      drm_gem_prime_fd_to_handle(kms_driver_group3, kms_driver_group2, ldvarg966,
                                 ldvarg967);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 31: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_driver_postclose_kms(kms_driver_group3, kms_driver_group2);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_driver_postclose_kms(kms_driver_group3, kms_driver_group2);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 32: ;
    if (ldv_state_variable_177 == 2) {
      drm_pci_set_busid(kms_driver_group3, ldvarg965);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      drm_pci_set_busid(kms_driver_group3, ldvarg965);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 33: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_irq_handler(ldvarg964, ldvarg963);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_irq_handler(ldvarg964, ldvarg963);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 34: ;
    if (ldv_state_variable_177 == 2) {
      amdgpu_disable_vblank_kms(kms_driver_group3, ldvarg962);
      ldv_state_variable_177 = 2;
    } else {
    }
    if (ldv_state_variable_177 == 1) {
      amdgpu_disable_vblank_kms(kms_driver_group3, ldvarg962);
      ldv_state_variable_177 = 1;
    } else {
    }
    goto ldv_45284;
    case 35: ;
    if (ldv_state_variable_177 == 2) {
      ldv_release_177();
      ldv_state_variable_177 = 1;
      ref_cnt = ref_cnt - 1;
    } else {
    }
    goto ldv_45284;
    default:
    ldv_stop();
    }
    ldv_45284: ;
  } else {
  }
  goto ldv_45087;
  case 152: ;
  if (ldv_state_variable_150 != 0) {
    ldv_main_exported_150();
  } else {
  }
  goto ldv_45087;
  case 153: ;
  if (ldv_state_variable_155 != 0) {
    ldv_main_exported_155();
  } else {
  }
  goto ldv_45087;
  case 154: ;
  if (ldv_state_variable_130 != 0) {
    ldv_main_exported_130();
  } else {
  }
  goto ldv_45087;
  case 155: ;
  if (ldv_state_variable_53 != 0) {
    ldv_main_exported_53();
  } else {
  }
  goto ldv_45087;
  case 156: ;
  if (ldv_state_variable_122 != 0) {
    ldv_main_exported_122();
  } else {
  }
  goto ldv_45087;
  case 157: ;
  if (ldv_state_variable_143 != 0) {
    ldv_main_exported_143();
  } else {
  }
  goto ldv_45087;
  case 158: ;
  if (ldv_state_variable_158 != 0) {
    ldv_main_exported_158();
  } else {
  }
  goto ldv_45087;
  case 159: ;
  if (ldv_state_variable_42 != 0) {
    ldv_main_exported_42();
  } else {
  }
  goto ldv_45087;
  case 160: ;
  if (ldv_state_variable_22 != 0) {
    ldv_main_exported_22();
  } else {
  }
  goto ldv_45087;
  case 161: ;
  if (ldv_state_variable_46 != 0) {
    ldv_main_exported_46();
  } else {
  }
  goto ldv_45087;
  case 162: ;
  if (ldv_state_variable_13 != 0) {
    ldv_main_exported_13();
  } else {
  }
  goto ldv_45087;
  case 163: ;
  if (ldv_state_variable_105 != 0) {
    ldv_main_exported_105();
  } else {
  }
  goto ldv_45087;
  case 164: ;
  goto ldv_45087;
  case 165: ;
  if (ldv_state_variable_85 != 0) {
    ldv_main_exported_85();
  } else {
  }
  goto ldv_45087;
  case 166: ;
  if (ldv_state_variable_36 != 0) {
    ldv_main_exported_36();
  } else {
  }
  goto ldv_45087;
  case 167: ;
  goto ldv_45087;
  case 168: ;
  if (ldv_state_variable_94 != 0) {
    ldv_main_exported_94();
  } else {
  }
  goto ldv_45087;
  case 169: ;
  if (ldv_state_variable_146 != 0) {
    ldv_main_exported_146();
  } else {
  }
  goto ldv_45087;
  case 170: ;
  if (ldv_state_variable_51 != 0) {
    ldv_main_exported_51();
  } else {
  }
  goto ldv_45087;
  case 171: ;
  goto ldv_45087;
  case 172: ;
  if (ldv_state_variable_111 != 0) {
    ldv_main_exported_111();
  } else {
  }
  goto ldv_45087;
  case 173: ;
  if (ldv_state_variable_38 != 0) {
    ldv_main_exported_38();
  } else {
  }
  goto ldv_45087;
  case 174: ;
  goto ldv_45087;
  case 175: ;
  if (ldv_state_variable_34 != 0) {
    ldv_main_exported_34();
  } else {
  }
  goto ldv_45087;
  case 176: ;
  if (ldv_state_variable_169 != 0) {
    ldv_main_exported_169();
  } else {
  }
  goto ldv_45087;
  case 177: ;
  if (ldv_state_variable_164 != 0) {
    ldv_main_exported_164();
  } else {
  }
  goto ldv_45087;
  case 178: ;
  if (ldv_state_variable_132 != 0) {
    ldv_main_exported_132();
  } else {
  }
  goto ldv_45087;
  case 179: ;
  if (ldv_state_variable_171 != 0) {
    ldv_main_exported_171();
  } else {
  }
  goto ldv_45087;
  default:
  ldv_stop();
  }
  ldv_45087: ;
  goto ldv_45350;
  ldv_final:
  ldv_check_final_state();
  return 0;
}
}
bool ldv_queue_work_on_5(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                         struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_6(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                 struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_7(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                         struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_8(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_9(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                 struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void __set_bit(long nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile ("bts %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr): "memory");
  return;
}
}
__inline static void __clear_bit(long nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile ("btr %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr));
  return;
}
}
__inline static int fls(int x )
{
  int r ;
  {
  __asm__ ("bsrl %1,%0": "=r" (r): "rm" (x), "0" (-1));
  return (r + 1);
}
}
extern unsigned long find_first_zero_bit(unsigned long const * , unsigned long ) ;
__inline static int __ilog2_u32(u32 n )
{
  int tmp ;
  {
  tmp = fls((int )n);
  return (tmp + -1);
}
}
extern void __might_fault(char const * , int ) ;
bool ldv_is_err(void const *ptr ) ;
long ldv_ptr_err(void const *ptr ) ;
extern void warn_slowpath_null(char const * , int const ) ;
__inline static long PTR_ERR(void const *ptr ) ;
__inline static bool IS_ERR(void const *ptr ) ;
__inline static int atomic_read(atomic_t const *v )
{
  int __var ;
  {
  __var = 0;
  return ((int )*((int const volatile *)(& v->counter)));
}
}
__inline static void atomic_set(atomic_t *v , int i )
{
  {
  v->counter = i;
  return;
}
}
__inline static void atomic_inc(atomic_t *v )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; incl %0": "+m" (v->counter));
  return;
}
}
extern void __raw_spin_lock_init(raw_spinlock_t * , char const * , struct lock_class_key * ) ;
extern void _raw_spin_lock(raw_spinlock_t * ) ;
extern unsigned long _raw_spin_lock_irqsave(raw_spinlock_t * ) ;
extern void _raw_spin_unlock(raw_spinlock_t * ) ;
extern void _raw_spin_unlock_irqrestore(raw_spinlock_t * , unsigned long ) ;
__inline static raw_spinlock_t *spinlock_check(spinlock_t *lock )
{
  {
  return (& lock->__annonCompField18.rlock);
}
}
__inline static void spin_lock(spinlock_t *lock )
{
  {
  _raw_spin_lock(& lock->__annonCompField18.rlock);
  return;
}
}
__inline static void spin_unlock(spinlock_t *lock )
{
  {
  _raw_spin_unlock(& lock->__annonCompField18.rlock);
  return;
}
}
__inline static void spin_unlock_irqrestore(spinlock_t *lock , unsigned long flags )
{
  {
  _raw_spin_unlock_irqrestore(& lock->__annonCompField18.rlock, flags);
  return;
}
}
extern void __mutex_init(struct mutex * , char const * , struct lock_class_key * ) ;
extern void mutex_lock_nested(struct mutex * , unsigned int ) ;
extern int mutex_lock_interruptible_nested(struct mutex * , unsigned int ) ;
extern int mutex_trylock(struct mutex * ) ;
extern void __init_rwsem(struct rw_semaphore * , char const * , struct lock_class_key * ) ;
extern void down_write(struct rw_semaphore * ) ;
extern void up_write(struct rw_semaphore * ) ;
bool ldv_queue_work_on_19(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_21(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_20(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_23(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_22(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static unsigned int readl(void const volatile *addr )
{
  unsigned int ret ;
  {
  __asm__ volatile ("movl %1,%0": "=r" (ret): "m" (*((unsigned int volatile *)addr)): "memory");
  return (ret);
}
}
__inline static void writel(unsigned int val , void volatile *addr )
{
  {
  __asm__ volatile ("movl %0,%1": : "r" (val), "m" (*((unsigned int volatile *)addr)): "memory");
  return;
}
}
extern void *ioremap_nocache(resource_size_t , unsigned long ) ;
__inline static void *ioremap(resource_size_t offset , unsigned long size )
{
  void *tmp ;
  {
  tmp = ioremap_nocache(offset, size);
  return (tmp);
}
}
extern void iounmap(void volatile * ) ;
extern unsigned int ioread32(void * ) ;
extern void iowrite32(u32 , void * ) ;
extern void pci_iounmap(struct pci_dev * , void * ) ;
extern void *pci_iomap(struct pci_dev * , int , unsigned long ) ;
extern struct page *alloc_pages_current(gfp_t , unsigned int ) ;
__inline static struct page *alloc_pages(gfp_t gfp_mask , unsigned int order )
{
  struct page *tmp ;
  {
  tmp = alloc_pages_current(gfp_mask, order);
  return (tmp);
}
}
extern void __free_pages(struct page * , unsigned int ) ;
__inline static void *kmalloc_array(size_t n , size_t size , gfp_t flags )
{
  void *tmp ;
  {
  if (size != 0UL && 0xffffffffffffffffUL / size < n) {
    return ((void *)0);
  } else {
  }
  tmp = __kmalloc(n * size, flags);
  return (tmp);
}
}
__inline static void *kcalloc(size_t n , size_t size , gfp_t flags )
{
  void *tmp ;
  {
  tmp = kmalloc_array(n, size, flags | 32768U);
  return (tmp);
}
}
extern void console_lock(void) ;
extern void console_unlock(void) ;
__inline static void i_size_write(struct inode *inode , loff_t i_size )
{
  {
  inode->i_size = i_size;
  return;
}
}
extern loff_t default_llseek(struct file * , loff_t , int ) ;
extern struct dentry *debugfs_create_file(char const * , umode_t , struct dentry * ,
                                          void * , struct file_operations const * ) ;
extern void debugfs_remove(struct dentry * ) ;
extern void dev_err(struct device const * , char const * , ...) ;
extern void dev_warn(struct device const * , char const * , ...) ;
extern void _dev_info(struct device const * , char const * , ...) ;
__inline static void *lowmem_page_address(struct page const *page )
{
  {
  return ((void *)((unsigned long )((unsigned long long )(((long )page + 24189255811072L) / 64L) << 12) + 0xffff880000000000UL));
}
}
__inline static int valid_dma_direction(int dma_direction )
{
  {
  return ((dma_direction == 0 || dma_direction == 1) || dma_direction == 2);
}
}
__inline static void kmemcheck_mark_initialized(void *address , unsigned int n )
{
  {
  return;
}
}
extern void debug_dma_map_page(struct device * , struct page * , size_t , size_t ,
                               int , dma_addr_t , bool ) ;
extern void debug_dma_mapping_error(struct device * , dma_addr_t ) ;
extern void debug_dma_unmap_page(struct device * , dma_addr_t , size_t , int ,
                                 bool ) ;
extern struct dma_map_ops *dma_ops ;
__inline static struct dma_map_ops *get_dma_ops(struct device *dev )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((unsigned long )dev == (unsigned long )((struct device *)0),
                         0L);
  if (tmp != 0L || (unsigned long )dev->archdata.dma_ops == (unsigned long )((struct dma_map_ops *)0)) {
    return (dma_ops);
  } else {
    return (dev->archdata.dma_ops);
  }
}
}
__inline static dma_addr_t dma_map_page(struct device *dev , struct page *page , size_t offset ,
                                        size_t size , enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  void *tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = lowmem_page_address((struct page const *)page);
  kmemcheck_mark_initialized(tmp___0 + offset, (unsigned int )size);
  tmp___1 = valid_dma_direction((int )dir);
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (84), "i" (12UL));
    ldv_29736: ;
    goto ldv_29736;
  } else {
  }
  addr = (*(ops->map_page))(dev, page, offset, size, dir, (struct dma_attrs *)0);
  debug_dma_map_page(dev, page, offset, size, (int )dir, addr, 0);
  return (addr);
}
}
__inline static void dma_unmap_page(struct device *dev , dma_addr_t addr , size_t size ,
                                    enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (96), "i" (12UL));
    ldv_29744: ;
    goto ldv_29744;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, (struct dma_attrs *)0);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 0);
  return;
}
}
__inline static int dma_mapping_error(struct device *dev , dma_addr_t dma_addr )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  debug_dma_mapping_error(dev, dma_addr);
  if ((unsigned long )ops->mapping_error != (unsigned long )((int (*)(struct device * ,
                                                                      dma_addr_t ))0)) {
    tmp___0 = (*(ops->mapping_error))(dev, dma_addr);
    return (tmp___0);
  } else {
  }
  return (dma_addr == 0ULL);
}
}
extern int pci_bus_write_config_dword(struct pci_bus * , unsigned int , int , u32 ) ;
__inline static int pci_write_config_dword(struct pci_dev const *dev , int where ,
                                           u32 val )
{
  int tmp ;
  {
  tmp = pci_bus_write_config_dword(dev->bus, dev->devfn, where, val);
  return (tmp);
}
}
__inline static dma_addr_t pci_map_page(struct pci_dev *hwdev , struct page *page ,
                                        unsigned long offset , size_t size , int direction )
{
  dma_addr_t tmp ;
  {
  tmp = dma_map_page((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                     page, offset, size, (enum dma_data_direction )direction);
  return (tmp);
}
}
__inline static void pci_unmap_page(struct pci_dev *hwdev , dma_addr_t dma_address ,
                                    size_t size , int direction )
{
  {
  dma_unmap_page((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                 dma_address, size, (enum dma_data_direction )direction);
  return;
}
}
__inline static int pci_dma_mapping_error(struct pci_dev *pdev , dma_addr_t dma_addr )
{
  int tmp ;
  {
  tmp = dma_mapping_error(& pdev->dev, dma_addr);
  return (tmp);
}
}
extern int __ww_mutex_lock(struct ww_mutex * , struct ww_acquire_ctx * ) ;
extern int __ww_mutex_lock_interruptible(struct ww_mutex * , struct ww_acquire_ctx * ) ;
__inline static int ww_mutex_lock(struct ww_mutex *lock , struct ww_acquire_ctx *ctx )
{
  int tmp ;
  {
  if ((unsigned long )ctx != (unsigned long )((struct ww_acquire_ctx *)0)) {
    tmp = __ww_mutex_lock(lock, ctx);
    return (tmp);
  } else {
  }
  mutex_lock_nested(& lock->base, 0U);
  return (0);
}
}
__inline static int ww_mutex_lock_interruptible(struct ww_mutex *lock , struct ww_acquire_ctx *ctx )
{
  int tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )ctx != (unsigned long )((struct ww_acquire_ctx *)0)) {
    tmp = __ww_mutex_lock_interruptible(lock, ctx);
    return (tmp);
  } else {
    tmp___0 = mutex_lock_interruptible_nested(& lock->base, 0U);
    return (tmp___0);
  }
}
}
extern void ww_mutex_unlock(struct ww_mutex * ) ;
__inline static int ww_mutex_trylock(struct ww_mutex *lock )
{
  int tmp ;
  {
  tmp = mutex_trylock(& lock->base);
  return (tmp);
}
}
extern void drm_mode_config_init(struct drm_device * ) ;
extern int drm_debugfs_create_files(struct drm_info_list const * , int , struct dentry * ,
                                    struct drm_minor * ) ;
extern int drm_debugfs_remove_files(struct drm_info_list const * , int , struct drm_minor * ) ;
extern void drm_helper_connector_dpms(struct drm_connector * , int ) ;
extern void drm_helper_resume_force_mode(struct drm_device * ) ;
extern int vga_client_register(struct pci_dev * , void * , void (*)(void * , bool ) ,
                               unsigned int (*)(void * , bool ) ) ;
extern void vga_switcheroo_unregister_client(struct pci_dev * ) ;
extern int vga_switcheroo_register_client(struct pci_dev * , struct vga_switcheroo_client_ops const * ,
                                          bool ) ;
extern int vga_switcheroo_init_domain_pm_ops(struct device * , struct dev_pm_domain * ) ;
__inline static void __hash_init(struct hlist_head *ht , unsigned int sz )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_41796;
  ldv_41795:
  (ht + (unsigned long )i)->first = (struct hlist_node *)0;
  i = i + 1U;
  ldv_41796: ;
  if (i < sz) {
    goto ldv_41795;
  } else {
  }
  return;
}
}
extern unsigned int fence_context_alloc(unsigned int ) ;
extern void ttm_bo_add_to_lru(struct ttm_buffer_object * ) ;
extern int ttm_bo_lock_delayed_workqueue(struct ttm_bo_device * ) ;
extern void ttm_bo_unlock_delayed_workqueue(struct ttm_bo_device * , int ) ;
extern void ttm_bo_del_sub_from_lru(struct ttm_buffer_object * ) ;
__inline static int __ttm_bo_reserve(struct ttm_buffer_object *bo , bool interruptible ,
                                     bool no_wait , bool use_ticket , struct ww_acquire_ctx *ticket )
{
  int ret ;
  bool success ;
  int __ret_warn_on ;
  long tmp ;
  long tmp___0 ;
  int tmp___1 ;
  {
  ret = 0;
  if ((int )no_wait) {
    __ret_warn_on = (unsigned long )ticket != (unsigned long )((struct ww_acquire_ctx *)0);
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_null("include/drm/ttm/ttm_bo_driver.h", 787);
    } else {
    }
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      return (-16);
    } else {
    }
    tmp___1 = ww_mutex_trylock(& (bo->resv)->lock);
    success = tmp___1 != 0;
    return ((int )success ? 0 : -16);
  } else {
  }
  if ((int )interruptible) {
    ret = ww_mutex_lock_interruptible(& (bo->resv)->lock, ticket);
  } else {
    ret = ww_mutex_lock(& (bo->resv)->lock, ticket);
  }
  if (ret == -4) {
    return (-512);
  } else {
  }
  return (ret);
}
}
__inline static int ttm_bo_reserve(struct ttm_buffer_object *bo , bool interruptible ,
                                   bool no_wait , bool use_ticket , struct ww_acquire_ctx *ticket )
{
  int ret ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp = atomic_read((atomic_t const *)(& bo->kref.refcount));
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("include/drm/ttm/ttm_bo_driver.h", 855);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  ret = __ttm_bo_reserve(bo, (int )interruptible, (int )no_wait, (int )use_ticket,
                         ticket);
  tmp___1 = ldv__builtin_expect(ret == 0, 1L);
  if (tmp___1 != 0L) {
    ttm_bo_del_sub_from_lru(bo);
  } else {
  }
  return (ret);
}
}
__inline static void __ttm_bo_unreserve(struct ttm_buffer_object *bo )
{
  {
  ww_mutex_unlock(& (bo->resv)->lock);
  return;
}
}
__inline static void ttm_bo_unreserve(struct ttm_buffer_object *bo )
{
  {
  if ((bo->mem.placement & 2097152U) == 0U) {
    spin_lock(& (bo->glob)->lru_lock);
    ttm_bo_add_to_lru(bo);
    spin_unlock(& (bo->glob)->lru_lock);
  } else {
  }
  __ttm_bo_unreserve(bo);
  return;
}
}
int amdgpu_fbdev_init(struct amdgpu_device *adev ) ;
void amdgpu_fbdev_fini(struct amdgpu_device *adev ) ;
void amdgpu_fbdev_set_suspend(struct amdgpu_device *adev , int state ) ;
bool amdgpu_fbdev_robj_is_fb(struct amdgpu_device *adev , struct amdgpu_bo *robj ) ;
int amdgpu_set_clockgating_state(struct amdgpu_device *adev , enum amd_ip_block_type block_type ,
                                 enum amd_clockgating_state state ) ;
int amdgpu_set_powergating_state(struct amdgpu_device *adev , enum amd_ip_block_type block_type ,
                                 enum amd_powergating_state state ) ;
int amdgpu_ip_block_version_cmp(struct amdgpu_device *adev , enum amd_ip_block_type type ,
                                u32 major , u32 minor ) ;
struct amdgpu_ip_block_version const *amdgpu_get_ip_block(struct amdgpu_device *adev ,
                                                            enum amd_ip_block_type type ) ;
bool amdgpu_get_bios(struct amdgpu_device *adev ) ;
int amdgpu_dummy_page_init(struct amdgpu_device *adev ) ;
void amdgpu_dummy_page_fini(struct amdgpu_device *adev ) ;
int amdgpu_fence_driver_init(struct amdgpu_device *adev ) ;
void amdgpu_fence_driver_fini(struct amdgpu_device *adev ) ;
void amdgpu_fence_driver_force_completion(struct amdgpu_device *adev ) ;
int amdgpu_fence_wait_empty(struct amdgpu_ring *ring ) ;
int amdgpu_gem_debugfs_init(struct amdgpu_device *adev ) ;
void amdgpu_doorbell_get_kfd_info(struct amdgpu_device *adev , phys_addr_t *aperture_base ,
                                  size_t *aperture_size , size_t *start_offset ) ;
int amdgpu_ib_pool_init(struct amdgpu_device *adev ) ;
void amdgpu_ib_pool_fini(struct amdgpu_device *adev ) ;
int amdgpu_ib_ring_tests(struct amdgpu_device *adev ) ;
unsigned int amdgpu_ring_backup(struct amdgpu_ring *ring , u32 **data ) ;
int amdgpu_ring_restore(struct amdgpu_ring *ring , unsigned int size , u32 *data ) ;
int amdgpu_wb_get(struct amdgpu_device *adev , u32 *wb ) ;
void amdgpu_wb_free(struct amdgpu_device *adev , u32 wb ) ;
void amdgpu_benchmark(struct amdgpu_device *adev , int test_number ) ;
void amdgpu_test_moves(struct amdgpu_device *adev ) ;
void amdgpu_test_syncing(struct amdgpu_device *adev ) ;
int amdgpu_debugfs_add_files(struct amdgpu_device *adev , struct drm_info_list *files ,
                             unsigned int nfiles ) ;
int amdgpu_device_init(struct amdgpu_device *adev , struct drm_device *ddev , struct pci_dev *pdev ,
                       u32 flags ) ;
void amdgpu_device_fini(struct amdgpu_device *adev ) ;
u32 amdgpu_mm_rreg(struct amdgpu_device *adev , u32 reg , bool always_indirect ) ;
void amdgpu_mm_wreg(struct amdgpu_device *adev , u32 reg , u32 v , bool always_indirect ) ;
u32 amdgpu_io_rreg(struct amdgpu_device *adev , u32 reg ) ;
void amdgpu_io_wreg(struct amdgpu_device *adev , u32 reg , u32 v ) ;
u32 amdgpu_mm_rdoorbell(struct amdgpu_device *adev , u32 index ) ;
void amdgpu_mm_wdoorbell(struct amdgpu_device *adev , u32 index , u32 v ) ;
int amdgpu_gpu_reset(struct amdgpu_device *adev ) ;
void amdgpu_pci_config_reset(struct amdgpu_device *adev ) ;
bool amdgpu_card_posted(struct amdgpu_device *adev ) ;
bool amdgpu_boot_test_post_card(struct amdgpu_device *adev ) ;
void amdgpu_vram_location(struct amdgpu_device *adev , struct amdgpu_mc *mc , u64 base ) ;
void amdgpu_gtt_location(struct amdgpu_device *adev , struct amdgpu_mc *mc ) ;
void amdgpu_program_register_sequence(struct amdgpu_device *adev , u32 const *registers ,
                                      u32 const array_size ) ;
__inline static int amdgpu_bo_reserve(struct amdgpu_bo *bo , bool no_intr )
{
  int r ;
  long tmp ;
  {
  r = ttm_bo_reserve(& bo->tbo, (int )((bool )(! ((int )no_intr != 0))), 0, 0, (struct ww_acquire_ctx *)0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    if (r != -512) {
      dev_err((struct device const *)(bo->adev)->dev, "%p reserve failed\n", bo);
    } else {
    }
    return (r);
  } else {
  }
  return (0);
}
}
__inline static void amdgpu_bo_unreserve(struct amdgpu_bo *bo )
{
  {
  ttm_bo_unreserve(& bo->tbo);
  return;
}
}
int amdgpu_bo_create(struct amdgpu_device *adev , unsigned long size , int byte_align ,
                     bool kernel , u32 domain , u64 flags , struct sg_table *sg ,
                     struct amdgpu_bo **bo_ptr ) ;
int amdgpu_bo_kmap(struct amdgpu_bo *bo , void **ptr ) ;
void amdgpu_bo_kunmap(struct amdgpu_bo *bo ) ;
void amdgpu_bo_unref(struct amdgpu_bo **bo ) ;
int amdgpu_bo_pin(struct amdgpu_bo *bo , u32 domain , u64 *gpu_addr ) ;
int amdgpu_bo_unpin(struct amdgpu_bo *bo ) ;
int amdgpu_bo_evict_vram(struct amdgpu_device *adev ) ;
void amdgpu_i2c_fini(struct amdgpu_device *adev ) ;
struct atom_context *amdgpu_atom_parse(struct card_info *card , void *bios ) ;
int amdgpu_atom_asic_init(struct atom_context *ctx ) ;
int amdgpu_atom_allocate_fb_scratch(struct atom_context *ctx ) ;
void amdgpu_atombios_i2c_init(struct amdgpu_device *adev ) ;
int amdgpu_atombios_get_clock_info(struct amdgpu_device *adev ) ;
void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev ) ;
int cik_set_ip_blocks(struct amdgpu_device *adev ) ;
int vi_set_ip_blocks(struct amdgpu_device *adev ) ;
static int amdgpu_debugfs_regs_init(struct amdgpu_device *adev ) ;
static void amdgpu_debugfs_regs_cleanup(struct amdgpu_device *adev ) ;
static char const *amdgpu_asic_name[9U] =
  { "BONAIRE", "KAVERI", "KABINI", "HAWAII",
        "MULLINS", "TOPAZ", "TONGA", "CARRIZO",
        "LAST"};
bool amdgpu_device_is_px(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((adev->flags & 262144UL) != 0UL) {
    return (1);
  } else {
  }
  return (0);
}
}
u32 amdgpu_mm_rreg(struct amdgpu_device *adev , u32 reg , bool always_indirect )
{
  unsigned int tmp ;
  unsigned long flags ;
  u32 ret ;
  raw_spinlock_t *tmp___0 ;
  {
  if ((resource_size_t )(reg * 4U) < adev->rmmio_size && ! always_indirect) {
    tmp = readl((void const volatile *)adev->rmmio + (unsigned long )(reg * 4U));
    return (tmp);
  } else {
    tmp___0 = spinlock_check(& adev->mmio_idx_lock);
    flags = _raw_spin_lock_irqsave(tmp___0);
    writel(reg * 4U, (void volatile *)adev->rmmio);
    ret = readl((void const volatile *)adev->rmmio + 4U);
    spin_unlock_irqrestore(& adev->mmio_idx_lock, flags);
    return (ret);
  }
}
}
void amdgpu_mm_wreg(struct amdgpu_device *adev , u32 reg , u32 v , bool always_indirect )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  if ((resource_size_t )(reg * 4U) < adev->rmmio_size && ! always_indirect) {
    writel(v, (void volatile *)adev->rmmio + (unsigned long )(reg * 4U));
  } else {
    tmp = spinlock_check(& adev->mmio_idx_lock);
    flags = _raw_spin_lock_irqsave(tmp);
    writel(reg * 4U, (void volatile *)adev->rmmio);
    writel(v, (void volatile *)adev->rmmio + 4U);
    spin_unlock_irqrestore(& adev->mmio_idx_lock, flags);
  }
  return;
}
}
u32 amdgpu_io_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned int tmp ;
  unsigned int tmp___0 ;
  {
  if ((resource_size_t )(reg * 4U) < adev->rio_mem_size) {
    tmp = ioread32(adev->rio_mem + (unsigned long )(reg * 4U));
    return (tmp);
  } else {
    iowrite32(reg * 4U, adev->rio_mem);
    tmp___0 = ioread32(adev->rio_mem + 4UL);
    return (tmp___0);
  }
}
}
void amdgpu_io_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  {
  if ((resource_size_t )(reg * 4U) < adev->rio_mem_size) {
    iowrite32(v, adev->rio_mem + (unsigned long )(reg * 4U));
  } else {
    iowrite32(reg * 4U, adev->rio_mem);
    iowrite32(v, adev->rio_mem + 4UL);
  }
  return;
}
}
u32 amdgpu_mm_rdoorbell(struct amdgpu_device *adev , u32 index )
{
  unsigned int tmp ;
  {
  if (adev->doorbell.num_doorbells > index) {
    tmp = readl((void const volatile *)adev->doorbell.ptr + (unsigned long )index);
    return (tmp);
  } else {
    drm_err("reading beyond doorbell aperture: 0x%08x!\n", index);
    return (0U);
  }
}
}
void amdgpu_mm_wdoorbell(struct amdgpu_device *adev , u32 index , u32 v )
{
  {
  if (adev->doorbell.num_doorbells > index) {
    writel(v, (void volatile *)adev->doorbell.ptr + (unsigned long )index);
  } else {
    drm_err("writing beyond doorbell aperture: 0x%08x!\n", index);
  }
  return;
}
}
static u32 amdgpu_invalid_rreg(struct amdgpu_device *adev , u32 reg )
{
  {
  drm_err("Invalid callback to read register 0x%04X\n", reg);
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c"),
                       "i" (179), "i" (12UL));
  ldv_49977: ;
  goto ldv_49977;
  return (0U);
}
}
static void amdgpu_invalid_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  {
  drm_err("Invalid callback to write register 0x%04X with 0x%08X\n", reg, v);
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c"),
                       "i" (197), "i" (12UL));
  ldv_49983: ;
  goto ldv_49983;
}
}
static u32 amdgpu_block_invalid_rreg(struct amdgpu_device *adev , u32 block , u32 reg )
{
  {
  drm_err("Invalid callback to read register 0x%04X in block 0x%04X\n", reg, block);
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c"),
                       "i" (216), "i" (12UL));
  ldv_49989: ;
  goto ldv_49989;
  return (0U);
}
}
static void amdgpu_block_invalid_wreg(struct amdgpu_device *adev , u32 block , u32 reg ,
                                      u32 v )
{
  {
  drm_err("Invalid block callback to write register 0x%04X in block 0x%04X with 0x%08X\n",
          reg, block, v);
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c"),
                       "i" (237), "i" (12UL));
  ldv_49996: ;
  goto ldv_49996;
}
}
static int amdgpu_vram_scratch_init(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->vram_scratch.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, 4096UL, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & adev->vram_scratch.robj);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_reserve(adev->vram_scratch.robj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->vram_scratch.robj, 4U, & adev->vram_scratch.gpu_addr);
  if (r != 0) {
    amdgpu_bo_unreserve(adev->vram_scratch.robj);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->vram_scratch.robj, (void **)(& adev->vram_scratch.ptr));
  if (r != 0) {
    amdgpu_bo_unpin(adev->vram_scratch.robj);
  } else {
  }
  amdgpu_bo_unreserve(adev->vram_scratch.robj);
  return (r);
}
}
static void amdgpu_vram_scratch_fini(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->vram_scratch.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    return;
  } else {
  }
  r = amdgpu_bo_reserve(adev->vram_scratch.robj, 0);
  tmp = ldv__builtin_expect(r == 0, 1L);
  if (tmp != 0L) {
    amdgpu_bo_kunmap(adev->vram_scratch.robj);
    amdgpu_bo_unpin(adev->vram_scratch.robj);
    amdgpu_bo_unreserve(adev->vram_scratch.robj);
  } else {
  }
  amdgpu_bo_unref(& adev->vram_scratch.robj);
  return;
}
}
void amdgpu_program_register_sequence(struct amdgpu_device *adev , u32 const *registers ,
                                      u32 const array_size )
{
  u32 tmp ;
  u32 reg ;
  u32 and_mask ;
  u32 or_mask ;
  int i ;
  {
  if ((unsigned int )array_size % 3U != 0U) {
    return;
  } else {
  }
  i = 0;
  goto ldv_50016;
  ldv_50015:
  reg = *(registers + (unsigned long )i);
  and_mask = *(registers + ((unsigned long )i + 1UL));
  or_mask = *(registers + ((unsigned long )i + 2UL));
  if (and_mask == 4294967295U) {
    tmp = or_mask;
  } else {
    tmp = amdgpu_mm_rreg(adev, reg, 0);
    tmp = ~ and_mask & tmp;
    tmp = tmp | or_mask;
  }
  amdgpu_mm_wreg(adev, reg, tmp, 0);
  i = i + 3;
  ldv_50016: ;
  if ((unsigned int )i < (unsigned int )array_size) {
    goto ldv_50015;
  } else {
  }
  return;
}
}
void amdgpu_pci_config_reset(struct amdgpu_device *adev )
{
  {
  pci_write_config_dword((struct pci_dev const *)adev->pdev, 124, 970319979U);
  return;
}
}
static int amdgpu_doorbell_init(struct amdgpu_device *adev )
{
  u32 __min1 ;
  u32 __min2 ;
  void *tmp ;
  {
  adev->doorbell.base = (adev->pdev)->resource[2].start;
  adev->doorbell.size = (adev->pdev)->resource[2].start != 0ULL || (adev->pdev)->resource[2].end != (adev->pdev)->resource[2].start ? ((adev->pdev)->resource[2].end - (adev->pdev)->resource[2].start) + 1ULL : 0ULL;
  __min1 = (u32 )(adev->doorbell.size / 4ULL);
  __min2 = 1024U;
  adev->doorbell.num_doorbells = __min1 < __min2 ? __min1 : __min2;
  if (adev->doorbell.num_doorbells == 0U) {
    return (-22);
  } else {
  }
  tmp = ioremap(adev->doorbell.base, (unsigned long )adev->doorbell.num_doorbells * 4UL);
  adev->doorbell.ptr = (u32 *)tmp;
  if ((unsigned long )adev->doorbell.ptr == (unsigned long )((u32 *)0U)) {
    return (-12);
  } else {
  }
  printk("\016[drm] doorbell mmio base: 0x%08X\n", (unsigned int )adev->doorbell.base);
  printk("\016[drm] doorbell mmio size: %u\n", (unsigned int )adev->doorbell.size);
  return (0);
}
}
static void amdgpu_doorbell_fini(struct amdgpu_device *adev )
{
  {
  iounmap((void volatile *)adev->doorbell.ptr);
  adev->doorbell.ptr = (u32 *)0U;
  return;
}
}
void amdgpu_doorbell_get_kfd_info(struct amdgpu_device *adev , phys_addr_t *aperture_base ,
                                  size_t *aperture_size , size_t *start_offset )
{
  {
  if (adev->doorbell.size > (unsigned long long )((unsigned long )adev->doorbell.num_doorbells * 4UL)) {
    *aperture_base = adev->doorbell.base;
    *aperture_size = (size_t )adev->doorbell.size;
    *start_offset = (unsigned long )adev->doorbell.num_doorbells * 4UL;
  } else {
    *aperture_base = 0ULL;
    *aperture_size = 0UL;
    *start_offset = 0UL;
  }
  return;
}
}
static void amdgpu_wb_fini(struct amdgpu_device *adev )
{
  int tmp ;
  {
  if ((unsigned long )adev->wb.wb_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    tmp = amdgpu_bo_reserve(adev->wb.wb_obj, 0);
    if (tmp == 0) {
      amdgpu_bo_kunmap(adev->wb.wb_obj);
      amdgpu_bo_unpin(adev->wb.wb_obj);
      amdgpu_bo_unreserve(adev->wb.wb_obj);
    } else {
    }
    amdgpu_bo_unref(& adev->wb.wb_obj);
    adev->wb.wb = (u32 volatile *)0U;
    adev->wb.wb_obj = (struct amdgpu_bo *)0;
  } else {
  }
  return;
}
}
static int amdgpu_wb_init(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->wb.wb_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, 4096UL, 4096, 1, 2U, 0ULL, (struct sg_table *)0, & adev->wb.wb_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) create WB bo failed\n", r);
      return (r);
    } else {
    }
    r = amdgpu_bo_reserve(adev->wb.wb_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      amdgpu_wb_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(adev->wb.wb_obj, 2U, & adev->wb.gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(adev->wb.wb_obj);
      dev_warn((struct device const *)adev->dev, "(%d) pin WB bo failed\n", r);
      amdgpu_wb_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(adev->wb.wb_obj, (void **)(& adev->wb.wb));
    amdgpu_bo_unreserve(adev->wb.wb_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) map WB bo failed\n", r);
      amdgpu_wb_fini(adev);
      return (r);
    } else {
    }
    adev->wb.num_wb = 1024U;
    memset((void *)(& adev->wb.used), 0, 128UL);
    memset((void *)adev->wb.wb, 0, 4096UL);
  } else {
  }
  return (0);
}
}
int amdgpu_wb_get(struct amdgpu_device *adev , u32 *wb )
{
  unsigned long offset ;
  unsigned long tmp ;
  {
  tmp = find_first_zero_bit((unsigned long const *)(& adev->wb.used), (unsigned long )adev->wb.num_wb);
  offset = tmp;
  if ((unsigned long )adev->wb.num_wb > offset) {
    __set_bit((long )offset, (unsigned long volatile *)(& adev->wb.used));
    *wb = (u32 )offset;
    return (0);
  } else {
    return (-22);
  }
}
}
void amdgpu_wb_free(struct amdgpu_device *adev , u32 wb )
{
  {
  if (adev->wb.num_wb > wb) {
    __clear_bit((long )wb, (unsigned long volatile *)(& adev->wb.used));
  } else {
  }
  return;
}
}
void amdgpu_vram_location(struct amdgpu_device *adev , struct amdgpu_mc *mc , u64 base )
{
  uint64_t limit ;
  {
  limit = (unsigned long long )amdgpu_vram_limit << 20;
  mc->vram_start = base;
  if (mc->mc_vram_size > (adev->mc.mc_mask - base) + 1ULL) {
    dev_warn((struct device const *)adev->dev, "limiting VRAM to PCI aperture size\n");
    mc->real_vram_size = mc->aper_size;
    mc->mc_vram_size = mc->aper_size;
  } else {
  }
  mc->vram_end = (mc->vram_start + mc->mc_vram_size) - 1ULL;
  if (limit != 0ULL && mc->real_vram_size > limit) {
    mc->real_vram_size = limit;
  } else {
  }
  _dev_info((struct device const *)adev->dev, "VRAM: %lluM 0x%016llX - 0x%016llX (%lluM used)\n",
            mc->mc_vram_size >> 20, mc->vram_start, mc->vram_end, mc->real_vram_size >> 20);
  return;
}
}
void amdgpu_gtt_location(struct amdgpu_device *adev , struct amdgpu_mc *mc )
{
  u64 size_af ;
  u64 size_bf ;
  {
  size_af = ((adev->mc.mc_mask - mc->vram_end) + mc->gtt_base_align) & ~ mc->gtt_base_align;
  size_bf = mc->vram_start & ~ mc->gtt_base_align;
  if (size_bf > size_af) {
    if (mc->gtt_size > size_bf) {
      dev_warn((struct device const *)adev->dev, "limiting GTT\n");
      mc->gtt_size = size_bf;
    } else {
    }
    mc->gtt_start = (mc->vram_start & ~ mc->gtt_base_align) - mc->gtt_size;
  } else {
    if (mc->gtt_size > size_af) {
      dev_warn((struct device const *)adev->dev, "limiting GTT\n");
      mc->gtt_size = size_af;
    } else {
    }
    mc->gtt_start = ((mc->vram_end + mc->gtt_base_align) + 1ULL) & ~ mc->gtt_base_align;
  }
  mc->gtt_end = (mc->gtt_start + mc->gtt_size) - 1ULL;
  _dev_info((struct device const *)adev->dev, "GTT: %lluM 0x%016llX - 0x%016llX\n",
            mc->gtt_size >> 20, mc->gtt_start, mc->gtt_end);
  return;
}
}
bool amdgpu_card_posted(struct amdgpu_device *adev )
{
  u32 reg ;
  {
  reg = amdgpu_mm_rreg(adev, 5386U, 0);
  if (reg != 0U) {
    return (1);
  } else {
  }
  return (0);
}
}
bool amdgpu_boot_test_post_card(struct amdgpu_device *adev )
{
  bool tmp ;
  {
  tmp = amdgpu_card_posted(adev);
  if ((int )tmp) {
    return (1);
  } else {
  }
  if ((unsigned long )adev->bios != (unsigned long )((uint8_t *)0U)) {
    printk("\016[drm] GPU not posted. posting now...\n");
    if ((int )adev->is_atom_bios) {
      amdgpu_atom_asic_init(adev->mode_info.atom_context);
    } else {
    }
    return (1);
  } else {
    dev_err((struct device const *)adev->dev, "Card not posted and no BIOS - ignoring\n");
    return (0);
  }
}
}
int amdgpu_dummy_page_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  if ((unsigned long )adev->dummy_page.page != (unsigned long )((struct page *)0)) {
    return (0);
  } else {
  }
  adev->dummy_page.page = alloc_pages(32980U, 0U);
  if ((unsigned long )adev->dummy_page.page == (unsigned long )((struct page *)0)) {
    return (-12);
  } else {
  }
  adev->dummy_page.addr = pci_map_page(adev->pdev, adev->dummy_page.page, 0UL, 4096UL,
                                       0);
  tmp = pci_dma_mapping_error(adev->pdev, adev->dummy_page.addr);
  if (tmp != 0) {
    dev_err((struct device const *)(& (adev->pdev)->dev), "Failed to DMA MAP the dummy page\n");
    __free_pages(adev->dummy_page.page, 0U);
    adev->dummy_page.page = (struct page *)0;
    return (-12);
  } else {
  }
  return (0);
}
}
void amdgpu_dummy_page_fini(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->dummy_page.page == (unsigned long )((struct page *)0)) {
    return;
  } else {
  }
  pci_unmap_page(adev->pdev, adev->dummy_page.addr, 4096UL, 0);
  __free_pages(adev->dummy_page.page, 0U);
  adev->dummy_page.page = (struct page *)0;
  return;
}
}
static u32 cail_pll_read(struct card_info *info , u32 reg )
{
  {
  return (0U);
}
}
static void cail_pll_write(struct card_info *info , u32 reg , u32 val )
{
  {
  return;
}
}
static u32 cail_mc_read(struct card_info *info , u32 reg )
{
  {
  return (0U);
}
}
static void cail_mc_write(struct card_info *info , u32 reg , u32 val )
{
  {
  return;
}
}
static void cail_reg_write(struct card_info *info , u32 reg , u32 val )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)(info->dev)->dev_private;
  amdgpu_mm_wreg(adev, reg, val, 0);
  return;
}
}
static u32 cail_reg_read(struct card_info *info , u32 reg )
{
  struct amdgpu_device *adev ;
  u32 r ;
  {
  adev = (struct amdgpu_device *)(info->dev)->dev_private;
  r = amdgpu_mm_rreg(adev, reg, 0);
  return (r);
}
}
static void cail_ioreg_write(struct card_info *info , u32 reg , u32 val )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)(info->dev)->dev_private;
  amdgpu_io_wreg(adev, reg, val);
  return;
}
}
static u32 cail_ioreg_read(struct card_info *info , u32 reg )
{
  struct amdgpu_device *adev ;
  u32 r ;
  {
  adev = (struct amdgpu_device *)(info->dev)->dev_private;
  r = amdgpu_io_rreg(adev, reg);
  return (r);
}
}
static void amdgpu_atombios_fini(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.atom_context != (unsigned long )((struct atom_context *)0)) {
    kfree((void const *)(adev->mode_info.atom_context)->scratch);
  } else {
  }
  kfree((void const *)adev->mode_info.atom_context);
  adev->mode_info.atom_context = (struct atom_context *)0;
  kfree((void const *)adev->mode_info.atom_card_info);
  adev->mode_info.atom_card_info = (struct card_info *)0;
  return;
}
}
static int amdgpu_atombios_init(struct amdgpu_device *adev )
{
  struct card_info *atom_card_info ;
  void *tmp ;
  struct lock_class_key __key ;
  {
  tmp = kzalloc(72UL, 208U);
  atom_card_info = (struct card_info *)tmp;
  if ((unsigned long )atom_card_info == (unsigned long )((struct card_info *)0)) {
    return (-12);
  } else {
  }
  adev->mode_info.atom_card_info = atom_card_info;
  atom_card_info->dev = adev->ddev;
  atom_card_info->reg_read = & cail_reg_read;
  atom_card_info->reg_write = & cail_reg_write;
  if ((unsigned long )adev->rio_mem != (unsigned long )((void *)0)) {
    atom_card_info->ioreg_read = & cail_ioreg_read;
    atom_card_info->ioreg_write = & cail_ioreg_write;
  } else {
    drm_err("Unable to find PCI I/O BAR; using MMIO for ATOM IIO\n");
    atom_card_info->ioreg_read = & cail_reg_read;
    atom_card_info->ioreg_write = & cail_reg_write;
  }
  atom_card_info->mc_read = & cail_mc_read;
  atom_card_info->mc_write = & cail_mc_write;
  atom_card_info->pll_read = & cail_pll_read;
  atom_card_info->pll_write = & cail_pll_write;
  adev->mode_info.atom_context = amdgpu_atom_parse(atom_card_info, (void *)adev->bios);
  if ((unsigned long )adev->mode_info.atom_context == (unsigned long )((struct atom_context *)0)) {
    amdgpu_atombios_fini(adev);
    return (-12);
  } else {
  }
  __mutex_init(& (adev->mode_info.atom_context)->mutex, "&adev->mode_info.atom_context->mutex",
               & __key);
  amdgpu_atombios_scratch_regs_init(adev);
  amdgpu_atom_allocate_fb_scratch(adev->mode_info.atom_context);
  return (0);
}
}
static unsigned int amdgpu_vga_set_decode(void *cookie , bool state )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)cookie;
  (*((adev->asic_funcs)->set_vga_state))(adev, (int )state);
  if ((int )state) {
    return (15U);
  } else {
    return (12U);
  }
}
}
static bool amdgpu_check_pot_argument(int arg )
{
  {
  return (((arg + -1) & arg) == 0);
}
}
static void amdgpu_check_arguments(struct amdgpu_device *adev )
{
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  unsigned int bits ;
  int tmp___5 ;
  {
  tmp = amdgpu_check_pot_argument(amdgpu_vram_limit);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    dev_warn((struct device const *)adev->dev, "vram limit (%d) must be a power of 2\n",
             amdgpu_vram_limit);
    amdgpu_vram_limit = 0;
  } else {
  }
  if (amdgpu_gart_size != -1) {
    if (amdgpu_gart_size <= 31) {
      dev_warn((struct device const *)adev->dev, "gart size (%d) too small\n", amdgpu_gart_size);
      amdgpu_gart_size = -1;
    } else {
      tmp___1 = amdgpu_check_pot_argument(amdgpu_gart_size);
      if (tmp___1) {
        tmp___2 = 0;
      } else {
        tmp___2 = 1;
      }
      if (tmp___2) {
        dev_warn((struct device const *)adev->dev, "gart size (%d) must be a power of 2\n",
                 amdgpu_gart_size);
        amdgpu_gart_size = -1;
      } else {
      }
    }
  } else {
  }
  tmp___3 = amdgpu_check_pot_argument(amdgpu_vm_size);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
    dev_warn((struct device const *)adev->dev, "VM size (%d) must be a power of 2\n",
             amdgpu_vm_size);
    amdgpu_vm_size = 8;
  } else {
  }
  if (amdgpu_vm_size <= 0) {
    dev_warn((struct device const *)adev->dev, "VM size (%d) too small, min is 1GB\n",
             amdgpu_vm_size);
    amdgpu_vm_size = 8;
  } else {
  }
  if (amdgpu_vm_size > 1024) {
    dev_warn((struct device const *)adev->dev, "VM size (%d) too large, max is 1TB\n",
             amdgpu_vm_size);
    amdgpu_vm_size = 8;
  } else {
  }
  if (amdgpu_vm_block_size == -1) {
    tmp___5 = __ilog2_u32((u32 )amdgpu_vm_size);
    bits = (unsigned int )(tmp___5 + 18);
    if (amdgpu_vm_size <= 8) {
      amdgpu_vm_block_size = (int )(bits - 9U);
    } else {
      amdgpu_vm_block_size = (int )((bits + 3U) / 2U);
    }
  } else
  if (amdgpu_vm_block_size <= 8) {
    dev_warn((struct device const *)adev->dev, "VM page table size (%d) too small\n",
             amdgpu_vm_block_size);
    amdgpu_vm_block_size = 9;
  } else {
  }
  if (amdgpu_vm_block_size > 24 || (unsigned long long )(amdgpu_vm_size * 1024) >> amdgpu_vm_block_size == 0ULL) {
    dev_warn((struct device const *)adev->dev, "VM page table size (%d) too large\n",
             amdgpu_vm_block_size);
    amdgpu_vm_block_size = 9;
  } else {
  }
  return;
}
}
static void amdgpu_switcheroo_set_state(struct pci_dev *pdev , enum vga_switcheroo_state state )
{
  struct drm_device *dev ;
  void *tmp ;
  bool tmp___0 ;
  unsigned int d3_delay ;
  {
  tmp = pci_get_drvdata(pdev);
  dev = (struct drm_device *)tmp;
  tmp___0 = amdgpu_device_is_px(dev);
  if ((int )tmp___0 && (unsigned int )state == 0U) {
    return;
  } else {
  }
  if ((unsigned int )state == 1U) {
    d3_delay = (dev->pdev)->d3_delay;
    printk("\016amdgpu: switched on\n");
    dev->switch_power_state = 2;
    amdgpu_resume_kms(dev, 1, 1);
    (dev->pdev)->d3_delay = d3_delay;
    dev->switch_power_state = 0;
    drm_kms_helper_poll_enable(dev);
  } else {
    printk("\016amdgpu: switched off\n");
    drm_kms_helper_poll_disable(dev);
    dev->switch_power_state = 2;
    amdgpu_suspend_kms(dev, 1, 1);
    dev->switch_power_state = 1;
  }
  return;
}
}
static bool amdgpu_switcheroo_can_switch(struct pci_dev *pdev )
{
  struct drm_device *dev ;
  void *tmp ;
  {
  tmp = pci_get_drvdata(pdev);
  dev = (struct drm_device *)tmp;
  return (dev->open_count == 0);
}
}
static struct vga_switcheroo_client_ops const amdgpu_switcheroo_ops = {& amdgpu_switcheroo_set_state, (void (*)(struct pci_dev * ))0, & amdgpu_switcheroo_can_switch};
int amdgpu_set_clockgating_state(struct amdgpu_device *adev , enum amd_ip_block_type block_type ,
                                 enum amd_clockgating_state state )
{
  int i ;
  int r ;
  {
  r = 0;
  i = 0;
  goto ldv_50158;
  ldv_50157: ;
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == (unsigned int )block_type) {
    r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->set_clockgating_state))((void *)adev,
                                                                                   state);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  i = i + 1;
  ldv_50158: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50157;
  } else {
  }
  return (r);
}
}
int amdgpu_set_powergating_state(struct amdgpu_device *adev , enum amd_ip_block_type block_type ,
                                 enum amd_powergating_state state )
{
  int i ;
  int r ;
  {
  r = 0;
  i = 0;
  goto ldv_50168;
  ldv_50167: ;
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == (unsigned int )block_type) {
    r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->set_powergating_state))((void *)adev,
                                                                                   state);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  i = i + 1;
  ldv_50168: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50167;
  } else {
  }
  return (r);
}
}
struct amdgpu_ip_block_version const *amdgpu_get_ip_block(struct amdgpu_device *adev ,
                                                            enum amd_ip_block_type type )
{
  int i ;
  {
  i = 0;
  goto ldv_50176;
  ldv_50175: ;
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == (unsigned int )type) {
    return (adev->ip_blocks + (unsigned long )i);
  } else {
  }
  i = i + 1;
  ldv_50176: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50175;
  } else {
  }
  return ((struct amdgpu_ip_block_version const *)0);
}
}
int amdgpu_ip_block_version_cmp(struct amdgpu_device *adev , enum amd_ip_block_type type ,
                                u32 major , u32 minor )
{
  struct amdgpu_ip_block_version const *ip_block ;
  {
  ip_block = amdgpu_get_ip_block(adev, type);
  if ((unsigned long )ip_block != (unsigned long )((struct amdgpu_ip_block_version const *)0) && ((unsigned int )ip_block->major > major || ((unsigned int )ip_block->major == major && (unsigned int )ip_block->minor >= minor))) {
    return (0);
  } else {
  }
  return (1);
}
}
static int amdgpu_early_init(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  void *tmp ;
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U: ;
  case 6U: ;
  case 7U: ;
  if ((unsigned int )adev->asic_type == 7U) {
    adev->family = 135U;
  } else {
    adev->family = 130U;
  }
  r = vi_set_ip_blocks(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  goto ldv_50193;
  case 0U: ;
  case 3U: ;
  case 1U: ;
  case 2U: ;
  case 4U: ;
  if ((unsigned int )adev->asic_type == 0U || (unsigned int )adev->asic_type == 3U) {
    adev->family = 120U;
  } else {
    adev->family = 125U;
  }
  r = cik_set_ip_blocks(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  goto ldv_50193;
  default: ;
  return (-22);
  }
  ldv_50193:
  tmp = kcalloc((size_t )adev->num_ip_blocks, 1UL, 208U);
  adev->ip_block_enabled = (bool *)tmp;
  if ((unsigned long )adev->ip_block_enabled == (unsigned long )((bool *)0)) {
    return (-12);
  } else {
  }
  if ((unsigned long )adev->ip_blocks == (unsigned long )((struct amdgpu_ip_block_version const *)0)) {
    drm_err("No IP blocks found!\n");
    return (r);
  } else {
  }
  i = 0;
  goto ldv_50201;
  ldv_50200: ;
  if (((unsigned int )(1 << i) & amdgpu_ip_block_mask) == 0U) {
    drm_err("disabled ip block: %d\n", i);
    *(adev->ip_block_enabled + (unsigned long )i) = 0;
  } else {
    if ((unsigned long )((adev->ip_blocks + (unsigned long )i)->funcs)->early_init != (unsigned long )((int (* )(void * ))0)) {
      r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->early_init))((void *)adev);
      if (r != 0) {
        return (r);
      } else {
      }
    } else {
    }
    *(adev->ip_block_enabled + (unsigned long )i) = 1;
  }
  i = i + 1;
  ldv_50201: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50200;
  } else {
  }
  return (0);
}
}
static int amdgpu_init___0(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  {
  i = 0;
  goto ldv_50210;
  ldv_50209: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50208;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->sw_init))((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == 1U) {
    r = amdgpu_vram_scratch_init(adev);
    if (r != 0) {
      return (r);
    } else {
    }
    r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->hw_init))((void *)adev);
    if (r != 0) {
      return (r);
    } else {
    }
    r = amdgpu_wb_init(adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  ldv_50208:
  i = i + 1;
  ldv_50210: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50209;
  } else {
  }
  i = 0;
  goto ldv_50214;
  ldv_50213: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50212;
  } else {
  }
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == 1U) {
    goto ldv_50212;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->hw_init))((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ldv_50212:
  i = i + 1;
  ldv_50214: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50213;
  } else {
  }
  return (0);
}
}
static int amdgpu_late_init(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  {
  i = 0;
  i = 0;
  goto ldv_50223;
  ldv_50222: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50221;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->set_clockgating_state))((void *)adev,
                                                                                 0);
  if (r != 0) {
    return (r);
  } else {
  }
  if ((unsigned long )((adev->ip_blocks + (unsigned long )i)->funcs)->late_init != (unsigned long )((int (* )(void * ))0)) {
    r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->late_init))((void *)adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  ldv_50221:
  i = i + 1;
  ldv_50223: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50222;
  } else {
  }
  return (0);
}
}
static int amdgpu_fini(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  {
  i = adev->num_ip_blocks + -1;
  goto ldv_50232;
  ldv_50231: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50230;
  } else {
  }
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == 1U) {
    amdgpu_wb_fini(adev);
    amdgpu_vram_scratch_fini(adev);
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->set_clockgating_state))((void *)adev,
                                                                                 1);
  if (r != 0) {
    return (r);
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->hw_fini))((void *)adev);
  ldv_50230:
  i = i - 1;
  ldv_50232: ;
  if (i >= 0) {
    goto ldv_50231;
  } else {
  }
  i = adev->num_ip_blocks + -1;
  goto ldv_50236;
  ldv_50235: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50234;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->sw_fini))((void *)adev);
  *(adev->ip_block_enabled + (unsigned long )i) = 0;
  ldv_50234:
  i = i - 1;
  ldv_50236: ;
  if (i >= 0) {
    goto ldv_50235;
  } else {
  }
  return (0);
}
}
static int amdgpu_suspend(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  {
  i = adev->num_ip_blocks + -1;
  goto ldv_50245;
  ldv_50244: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50243;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->set_clockgating_state))((void *)adev,
                                                                                 1);
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->suspend))((void *)adev);
  ldv_50243:
  i = i - 1;
  ldv_50245: ;
  if (i >= 0) {
    goto ldv_50244;
  } else {
  }
  return (0);
}
}
static int amdgpu_resume(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  {
  i = 0;
  goto ldv_50254;
  ldv_50253: ;
  if (! *(adev->ip_block_enabled + (unsigned long )i)) {
    goto ldv_50252;
  } else {
  }
  r = (*(((adev->ip_blocks + (unsigned long )i)->funcs)->resume))((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ldv_50252:
  i = i + 1;
  ldv_50254: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_50253;
  } else {
  }
  return (0);
}
}
int amdgpu_device_init(struct amdgpu_device *adev , struct drm_device *ddev , struct pci_dev *pdev ,
                       u32 flags )
{
  int r ;
  int i ;
  bool runtime ;
  struct lock_class_key __key ;
  struct lock_class_key __key___0 ;
  struct lock_class_key __key___1 ;
  struct lock_class_key __key___2 ;
  struct lock_class_key __key___3 ;
  struct lock_class_key __key___4 ;
  struct lock_class_key __key___5 ;
  struct lock_class_key __key___6 ;
  struct lock_class_key __key___7 ;
  struct lock_class_key __key___8 ;
  struct lock_class_key __key___9 ;
  struct lock_class_key __key___10 ;
  struct lock_class_key __key___11 ;
  struct lock_class_key __key___12 ;
  bool tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  runtime = 0;
  adev->shutdown = 0;
  adev->dev = & pdev->dev;
  adev->ddev = ddev;
  adev->pdev = pdev;
  adev->flags = (unsigned long )flags;
  adev->asic_type = (enum amdgpu_asic_type )(flags & 65535U);
  adev->is_atom_bios = 0;
  adev->usec_timeout = 100000;
  adev->mc.gtt_size = 536870912ULL;
  adev->accel_working = 0;
  adev->num_rings = 0U;
  adev->mman.buffer_funcs = (struct amdgpu_buffer_funcs const *)0;
  adev->mman.buffer_funcs_ring = (struct amdgpu_ring *)0;
  adev->vm_manager.vm_pte_funcs = (struct amdgpu_vm_pte_funcs const *)0;
  adev->vm_manager.vm_pte_funcs_ring = (struct amdgpu_ring *)0;
  adev->gart.gart_funcs = (struct amdgpu_gart_funcs const *)0;
  adev->fence_context = fence_context_alloc(16U);
  adev->smc_rreg = & amdgpu_invalid_rreg;
  adev->smc_wreg = & amdgpu_invalid_wreg;
  adev->pcie_rreg = & amdgpu_invalid_rreg;
  adev->pcie_wreg = & amdgpu_invalid_wreg;
  adev->uvd_ctx_rreg = & amdgpu_invalid_rreg;
  adev->uvd_ctx_wreg = & amdgpu_invalid_wreg;
  adev->didt_rreg = & amdgpu_invalid_rreg;
  adev->didt_wreg = & amdgpu_invalid_wreg;
  adev->audio_endpt_rreg = & amdgpu_block_invalid_rreg;
  adev->audio_endpt_wreg = & amdgpu_block_invalid_wreg;
  printk("\016[drm] initializing kernel modesetting (%s 0x%04X:0x%04X 0x%04X:0x%04X 0x%02X).\n",
         amdgpu_asic_name[(unsigned int )adev->asic_type], (int )pdev->vendor, (int )pdev->device,
         (int )pdev->subsystem_vendor, (int )pdev->subsystem_device, (int )pdev->revision);
  __mutex_init(& adev->ring_lock, "&adev->ring_lock", & __key);
  atomic_set(& adev->irq.ih.lock, 0);
  __mutex_init(& adev->gem.mutex, "&adev->gem.mutex", & __key___0);
  __mutex_init(& adev->pm.mutex, "&adev->pm.mutex", & __key___1);
  __mutex_init(& adev->gfx.gpu_clock_mutex, "&adev->gfx.gpu_clock_mutex", & __key___2);
  __mutex_init(& adev->srbm_mutex, "&adev->srbm_mutex", & __key___3);
  __mutex_init(& adev->grbm_idx_mutex, "&adev->grbm_idx_mutex", & __key___4);
  __init_rwsem(& adev->exclusive_lock, "&adev->exclusive_lock", & __key___5);
  __mutex_init(& adev->mn_lock, "&adev->mn_lock", & __key___6);
  __hash_init((struct hlist_head *)(& adev->mn_hash), 128U);
  amdgpu_check_arguments(adev);
  spinlock_check(& adev->mmio_idx_lock);
  __raw_spin_lock_init(& adev->mmio_idx_lock.__annonCompField18.rlock, "&(&adev->mmio_idx_lock)->rlock",
                       & __key___7);
  spinlock_check(& adev->smc_idx_lock);
  __raw_spin_lock_init(& adev->smc_idx_lock.__annonCompField18.rlock, "&(&adev->smc_idx_lock)->rlock",
                       & __key___8);
  spinlock_check(& adev->pcie_idx_lock);
  __raw_spin_lock_init(& adev->pcie_idx_lock.__annonCompField18.rlock, "&(&adev->pcie_idx_lock)->rlock",
                       & __key___9);
  spinlock_check(& adev->uvd_ctx_idx_lock);
  __raw_spin_lock_init(& adev->uvd_ctx_idx_lock.__annonCompField18.rlock, "&(&adev->uvd_ctx_idx_lock)->rlock",
                       & __key___10);
  spinlock_check(& adev->didt_idx_lock);
  __raw_spin_lock_init(& adev->didt_idx_lock.__annonCompField18.rlock, "&(&adev->didt_idx_lock)->rlock",
                       & __key___11);
  spinlock_check(& adev->audio_endpt_idx_lock);
  __raw_spin_lock_init(& adev->audio_endpt_idx_lock.__annonCompField18.rlock, "&(&adev->audio_endpt_idx_lock)->rlock",
                       & __key___12);
  adev->rmmio_base = (adev->pdev)->resource[5].start;
  adev->rmmio_size = (adev->pdev)->resource[5].start != 0ULL || (adev->pdev)->resource[5].end != (adev->pdev)->resource[5].start ? ((adev->pdev)->resource[5].end - (adev->pdev)->resource[5].start) + 1ULL : 0ULL;
  adev->rmmio = ioremap(adev->rmmio_base, (unsigned long )adev->rmmio_size);
  if ((unsigned long )adev->rmmio == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  printk("\016[drm] register mmio base: 0x%08X\n", (unsigned int )adev->rmmio_base);
  printk("\016[drm] register mmio size: %u\n", (unsigned int )adev->rmmio_size);
  amdgpu_doorbell_init(adev);
  i = 0;
  goto ldv_50283;
  ldv_50282: ;
  if (((adev->pdev)->resource[i].flags & 256UL) != 0UL) {
    adev->rio_mem_size = (adev->pdev)->resource[i].start != 0ULL || (adev->pdev)->resource[i].end != (adev->pdev)->resource[i].start ? ((adev->pdev)->resource[i].end - (adev->pdev)->resource[i].start) + 1ULL : 0ULL;
    adev->rio_mem = pci_iomap(adev->pdev, i, (unsigned long )adev->rio_mem_size);
    goto ldv_50281;
  } else {
  }
  i = i + 1;
  ldv_50283: ;
  if (i <= 16) {
    goto ldv_50282;
  } else {
  }
  ldv_50281: ;
  if ((unsigned long )adev->rio_mem == (unsigned long )((void *)0)) {
    drm_err("Unable to find PCI I/O BAR\n");
  } else {
  }
  r = amdgpu_early_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  vga_client_register(adev->pdev, (void *)adev, (void (*)(void * , bool ))0, & amdgpu_vga_set_decode);
  if (amdgpu_runtime_pm == 1) {
    runtime = 1;
  } else {
  }
  tmp = amdgpu_device_is_px(ddev);
  if ((int )tmp) {
    runtime = 1;
  } else {
  }
  vga_switcheroo_register_client(adev->pdev, & amdgpu_switcheroo_ops, (int )runtime);
  if ((int )runtime) {
    vga_switcheroo_init_domain_pm_ops(adev->dev, & adev->vga_pm_domain);
  } else {
  }
  tmp___0 = amdgpu_get_bios(adev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (-22);
  } else {
  }
  if (! adev->is_atom_bios) {
    dev_err((struct device const *)adev->dev, "Expecting atombios for GPU\n");
    return (-22);
  } else {
  }
  r = amdgpu_atombios_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp___2 = amdgpu_card_posted(adev);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
      dev_err((struct device const *)adev->dev, "Card not posted and no BIOS - ignoring\n");
      return (-22);
    } else {
    }
    printk("\016[drm] GPU not posted. posting now...\n");
    amdgpu_atom_asic_init(adev->mode_info.atom_context);
  } else {
  }
  r = amdgpu_atombios_get_clock_info(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  amdgpu_atombios_i2c_init(adev);
  r = amdgpu_fence_driver_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  drm_mode_config_init(adev->ddev);
  r = amdgpu_init___0(adev);
  if (r != 0) {
    amdgpu_fini(adev);
    return (r);
  } else {
  }
  adev->accel_working = 1;
  amdgpu_fbdev_init(adev);
  r = amdgpu_ib_pool_init(adev);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "IB initialization failed (%d).\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_ib_ring_tests(adev);
  if (r != 0) {
    drm_err("ib ring test failed (%d).\n", r);
  } else {
  }
  r = amdgpu_gem_debugfs_init(adev);
  if (r != 0) {
    drm_err("registering gem debugfs failed (%d).\n", r);
  } else {
  }
  r = amdgpu_debugfs_regs_init(adev);
  if (r != 0) {
    drm_err("registering register debugfs failed (%d).\n", r);
  } else {
  }
  if (amdgpu_testing & 1) {
    if ((int )adev->accel_working) {
      amdgpu_test_moves(adev);
    } else {
      printk("\016[drm] amdgpu: acceleration disabled, skipping move tests\n");
    }
  } else {
  }
  if ((amdgpu_testing & 2) != 0) {
    if ((int )adev->accel_working) {
      amdgpu_test_syncing(adev);
    } else {
      printk("\016[drm] amdgpu: acceleration disabled, skipping sync tests\n");
    }
  } else {
  }
  if (amdgpu_benchmarking != 0) {
    if ((int )adev->accel_working) {
      amdgpu_benchmark(adev, amdgpu_benchmarking);
    } else {
      printk("\016[drm] amdgpu: acceleration disabled, skipping benchmarks\n");
    }
  } else {
  }
  r = amdgpu_late_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static void amdgpu_debugfs_remove_files(struct amdgpu_device *adev ) ;
void amdgpu_device_fini(struct amdgpu_device *adev )
{
  int r ;
  {
  printk("\016[drm] amdgpu: finishing device.\n");
  adev->shutdown = 1;
  amdgpu_bo_evict_vram(adev);
  amdgpu_ib_pool_fini(adev);
  amdgpu_fence_driver_fini(adev);
  amdgpu_fbdev_fini(adev);
  r = amdgpu_fini(adev);
  kfree((void const *)adev->ip_block_enabled);
  adev->ip_block_enabled = (bool *)0;
  adev->accel_working = 0;
  amdgpu_i2c_fini(adev);
  amdgpu_atombios_fini(adev);
  kfree((void const *)adev->bios);
  adev->bios = (uint8_t *)0U;
  vga_switcheroo_unregister_client(adev->pdev);
  vga_client_register(adev->pdev, (void *)0, (void (*)(void * , bool ))0, (unsigned int (*)(void * ,
                                                                                             bool ))0);
  if ((unsigned long )adev->rio_mem != (unsigned long )((void *)0)) {
    pci_iounmap(adev->pdev, adev->rio_mem);
  } else {
  }
  adev->rio_mem = (void *)0;
  iounmap((void volatile *)adev->rmmio);
  adev->rmmio = (void *)0;
  amdgpu_doorbell_fini(adev);
  amdgpu_debugfs_regs_cleanup(adev);
  amdgpu_debugfs_remove_files(adev);
  return;
}
}
int amdgpu_suspend_kms(struct drm_device *dev , bool suspend , bool fbcon )
{
  struct amdgpu_device *adev ;
  struct drm_crtc *crtc ;
  struct drm_connector *connector ;
  int i ;
  int r ;
  bool force_completion ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct amdgpu_framebuffer *rfb ;
  struct drm_framebuffer const *__mptr___2 ;
  struct amdgpu_bo *robj ;
  struct drm_gem_object const *__mptr___3 ;
  bool tmp ;
  int tmp___0 ;
  struct list_head const *__mptr___4 ;
  struct amdgpu_ring *ring ;
  {
  force_completion = 0;
  if ((unsigned long )dev == (unsigned long )((struct drm_device *)0) || (unsigned long )dev->dev_private == (unsigned long )((void *)0)) {
    return (-19);
  } else {
  }
  adev = (struct amdgpu_device *)dev->dev_private;
  if (dev->switch_power_state == 1) {
    return (0);
  } else {
  }
  drm_kms_helper_poll_disable(dev);
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_50306;
  ldv_50305:
  drm_helper_connector_dpms(connector, 3);
  __mptr___0 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  ldv_50306: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_50305;
  } else {
  }
  __mptr___1 = (struct list_head const *)dev->mode_config.crtc_list.next;
  crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
  goto ldv_50320;
  ldv_50319:
  __mptr___2 = (struct drm_framebuffer const *)(crtc->primary)->fb;
  rfb = (struct amdgpu_framebuffer *)__mptr___2;
  if ((unsigned long )rfb == (unsigned long )((struct amdgpu_framebuffer *)0) || (unsigned long )rfb->obj == (unsigned long )((struct drm_gem_object *)0)) {
    goto ldv_50316;
  } else {
  }
  __mptr___3 = (struct drm_gem_object const *)rfb->obj;
  robj = (struct amdgpu_bo *)__mptr___3 + 0xfffffffffffffbc0UL;
  tmp = amdgpu_fbdev_robj_is_fb(adev, robj);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    r = amdgpu_bo_reserve(robj, 0);
    if (r == 0) {
      amdgpu_bo_unpin(robj);
      amdgpu_bo_unreserve(robj);
    } else {
    }
  } else {
  }
  ldv_50316:
  __mptr___4 = (struct list_head const *)crtc->head.next;
  crtc = (struct drm_crtc *)__mptr___4 + 0xfffffffffffffff0UL;
  ldv_50320: ;
  if ((unsigned long )(& crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
    goto ldv_50319;
  } else {
  }
  amdgpu_bo_evict_vram(adev);
  i = 0;
  goto ldv_50325;
  ldv_50324:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0)) {
    goto ldv_50323;
  } else {
  }
  r = amdgpu_fence_wait_empty(ring);
  if (r != 0) {
    force_completion = 1;
  } else {
  }
  ldv_50323:
  i = i + 1;
  ldv_50325: ;
  if (i <= 15) {
    goto ldv_50324;
  } else {
  }
  if ((int )force_completion) {
    amdgpu_fence_driver_force_completion(adev);
  } else {
  }
  r = amdgpu_suspend(adev);
  amdgpu_bo_evict_vram(adev);
  pci_save_state(dev->pdev);
  if ((int )suspend) {
    pci_disable_device(dev->pdev);
    pci_set_power_state(dev->pdev, 3);
  } else {
  }
  if ((int )fbcon) {
    console_lock();
    amdgpu_fbdev_set_suspend(adev, 1);
    console_unlock();
  } else {
  }
  return (0);
}
}
int amdgpu_resume_kms(struct drm_device *dev , bool resume , bool fbcon )
{
  struct drm_connector *connector ;
  struct amdgpu_device *adev ;
  int r ;
  int tmp ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if (dev->switch_power_state == 1) {
    return (0);
  } else {
  }
  if ((int )fbcon) {
    console_lock();
  } else {
  }
  if ((int )resume) {
    pci_set_power_state(dev->pdev, 0);
    pci_restore_state(dev->pdev);
    tmp = pci_enable_device(dev->pdev);
    if (tmp != 0) {
      if ((int )fbcon) {
        console_unlock();
      } else {
      }
      return (-1);
    } else {
    }
  } else {
  }
  amdgpu_atom_asic_init(adev->mode_info.atom_context);
  r = amdgpu_resume(adev);
  r = amdgpu_ib_ring_tests(adev);
  if (r != 0) {
    drm_err("ib ring test failed (%d).\n", r);
  } else {
  }
  r = amdgpu_late_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if ((int )fbcon) {
    drm_helper_resume_force_mode(dev);
    __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
    connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
    goto ldv_50340;
    ldv_50339:
    drm_helper_connector_dpms(connector, 0);
    __mptr___0 = (struct list_head const *)connector->head.next;
    connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
    ldv_50340: ;
    if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
      goto ldv_50339;
    } else {
    }
  } else {
  }
  drm_kms_helper_poll_enable(dev);
  if ((int )fbcon) {
    amdgpu_fbdev_set_suspend(adev, 0);
    console_unlock();
  } else {
  }
  return (0);
}
}
int amdgpu_gpu_reset(struct amdgpu_device *adev )
{
  unsigned int ring_sizes[16U] ;
  u32 *ring_data[16U] ;
  bool saved ;
  int i ;
  int r ;
  int resched ;
  struct amdgpu_ring *ring ;
  struct amdgpu_ring *ring___0 ;
  {
  saved = 0;
  down_write(& adev->exclusive_lock);
  if (! adev->needs_reset) {
    up_write(& adev->exclusive_lock);
    return (0);
  } else {
  }
  adev->needs_reset = 0;
  atomic_inc(& adev->gpu_reset_counter);
  resched = ttm_bo_lock_delayed_workqueue(& adev->mman.bdev);
  r = amdgpu_suspend(adev);
  i = 0;
  goto ldv_50354;
  ldv_50353:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0)) {
    goto ldv_50352;
  } else {
  }
  ring_sizes[i] = amdgpu_ring_backup(ring, (u32 **)(& ring_data) + (unsigned long )i);
  if (ring_sizes[i] != 0U) {
    saved = 1;
    _dev_info((struct device const *)adev->dev, "Saved %d dwords of commands on ring %d.\n",
              ring_sizes[i], i);
  } else {
  }
  ldv_50352:
  i = i + 1;
  ldv_50354: ;
  if (i <= 15) {
    goto ldv_50353;
  } else {
  }
  retry:
  r = (*((adev->asic_funcs)->reset))(adev);
  if (r == 0) {
    _dev_info((struct device const *)adev->dev, "GPU reset succeeded, trying to resume\n");
    r = amdgpu_resume(adev);
  } else {
  }
  if (r == 0) {
    i = 0;
    goto ldv_50360;
    ldv_50359:
    ring___0 = adev->rings[i];
    if ((unsigned long )ring___0 == (unsigned long )((struct amdgpu_ring *)0)) {
      goto ldv_50358;
    } else {
    }
    amdgpu_ring_restore(ring___0, ring_sizes[i], ring_data[i]);
    ring_sizes[i] = 0U;
    ring_data[i] = (u32 *)0U;
    ldv_50358:
    i = i + 1;
    ldv_50360: ;
    if (i <= 15) {
      goto ldv_50359;
    } else {
    }
    r = amdgpu_ib_ring_tests(adev);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "ib ring test failed (%d).\n", r);
      if ((int )saved) {
        saved = 0;
        r = amdgpu_suspend(adev);
        goto retry;
      } else {
      }
    } else {
    }
  } else {
    amdgpu_fence_driver_force_completion(adev);
    i = 0;
    goto ldv_50363;
    ldv_50362: ;
    if ((unsigned long )adev->rings[i] != (unsigned long )((struct amdgpu_ring *)0)) {
      kfree((void const *)ring_data[i]);
    } else {
    }
    i = i + 1;
    ldv_50363: ;
    if (i <= 15) {
      goto ldv_50362;
    } else {
    }
  }
  drm_helper_resume_force_mode(adev->ddev);
  ttm_bo_unlock_delayed_workqueue(& adev->mman.bdev, resched);
  if (r != 0) {
    _dev_info((struct device const *)adev->dev, "GPU reset failed\n");
  } else {
  }
  up_write(& adev->exclusive_lock);
  return (r);
}
}
int amdgpu_debugfs_add_files(struct amdgpu_device *adev , struct drm_info_list *files ,
                             unsigned int nfiles )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_50372;
  ldv_50371: ;
  if ((unsigned long )adev->debugfs[i].files == (unsigned long )files) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_50372: ;
  if (adev->debugfs_count > i) {
    goto ldv_50371;
  } else {
  }
  i = adev->debugfs_count + 1U;
  if (i > 32U) {
    drm_err("Reached maximum number of debugfs components.\n");
    drm_err("Report so we increase AMDGPU_DEBUGFS_MAX_COMPONENTS.\n");
    return (-22);
  } else {
  }
  adev->debugfs[adev->debugfs_count].files = files;
  adev->debugfs[adev->debugfs_count].num_files = nfiles;
  adev->debugfs_count = i;
  drm_debugfs_create_files((struct drm_info_list const *)files, (int )nfiles, ((adev->ddev)->control)->debugfs_root,
                           (adev->ddev)->control);
  drm_debugfs_create_files((struct drm_info_list const *)files, (int )nfiles, ((adev->ddev)->primary)->debugfs_root,
                           (adev->ddev)->primary);
  return (0);
}
}
static void amdgpu_debugfs_remove_files(struct amdgpu_device *adev )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_50379;
  ldv_50378:
  drm_debugfs_remove_files((struct drm_info_list const *)adev->debugfs[i].files,
                           (int )adev->debugfs[i].num_files, (adev->ddev)->control);
  drm_debugfs_remove_files((struct drm_info_list const *)adev->debugfs[i].files,
                           (int )adev->debugfs[i].num_files, (adev->ddev)->primary);
  i = i + 1U;
  ldv_50379: ;
  if (adev->debugfs_count > i) {
    goto ldv_50378;
  } else {
  }
  return;
}
}
static ssize_t amdgpu_debugfs_regs_read(struct file *f , char *buf , size_t size ,
                                        loff_t *pos )
{
  struct amdgpu_device *adev ;
  ssize_t result ;
  int r ;
  u32 value ;
  int __ret_pu ;
  u32 __pu_val ;
  {
  adev = (struct amdgpu_device *)(f->f_inode)->i_private;
  result = 0L;
  if ((size & 3UL) != 0UL || (*pos & 3LL) != 0LL) {
    return (-22L);
  } else {
  }
  goto ldv_50401;
  ldv_50400: ;
  if ((unsigned long long )*pos > adev->rmmio_size) {
    return (result);
  } else {
  }
  value = amdgpu_mm_rreg(adev, (u32 )(*pos >> 2), 0);
  __might_fault("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c",
                1924);
  __pu_val = value;
  switch (4UL) {
  case 1UL:
  __asm__ volatile ("call __put_user_1": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_50394;
  case 2UL:
  __asm__ volatile ("call __put_user_2": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_50394;
  case 4UL:
  __asm__ volatile ("call __put_user_4": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_50394;
  case 8UL:
  __asm__ volatile ("call __put_user_8": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_50394;
  default:
  __asm__ volatile ("call __put_user_X": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_50394;
  }
  ldv_50394:
  r = __ret_pu;
  if (r != 0) {
    return ((ssize_t )r);
  } else {
  }
  result = result + 4L;
  buf = buf + 4UL;
  *pos = *pos + 4LL;
  size = size - 4UL;
  ldv_50401: ;
  if (size != 0UL) {
    goto ldv_50400;
  } else {
  }
  return (result);
}
}
static ssize_t amdgpu_debugfs_regs_write(struct file *f , char const *buf , size_t size ,
                                         loff_t *pos )
{
  struct amdgpu_device *adev ;
  ssize_t result ;
  int r ;
  u32 value ;
  int __ret_gu ;
  register unsigned long __val_gu ;
  {
  adev = (struct amdgpu_device *)(f->f_inode)->i_private;
  result = 0L;
  if ((size & 3UL) != 0UL || (*pos & 3LL) != 0LL) {
    return (-22L);
  } else {
  }
  goto ldv_50417;
  ldv_50416: ;
  if ((unsigned long long )*pos > adev->rmmio_size) {
    return (result);
  } else {
  }
  __might_fault("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_device.c",
                1953);
  __asm__ volatile ("call __get_user_%P3": "=a" (__ret_gu), "=r" (__val_gu): "0" ((u32 *)buf),
                       "i" (4UL));
  value = (unsigned int )__val_gu;
  r = __ret_gu;
  if (r != 0) {
    return ((ssize_t )r);
  } else {
  }
  amdgpu_mm_wreg(adev, (u32 )(*pos >> 2), value, 0);
  result = result + 4L;
  buf = buf + 4UL;
  *pos = *pos + 4LL;
  size = size - 4UL;
  ldv_50417: ;
  if (size != 0UL) {
    goto ldv_50416;
  } else {
  }
  return (result);
}
}
static struct file_operations const amdgpu_debugfs_regs_fops =
     {& __this_module, & default_llseek, & amdgpu_debugfs_regs_read, & amdgpu_debugfs_regs_write,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
static int amdgpu_debugfs_regs_init(struct amdgpu_device *adev )
{
  struct drm_minor *minor ;
  struct dentry *ent ;
  struct dentry *root ;
  long tmp ;
  bool tmp___0 ;
  {
  minor = (adev->ddev)->primary;
  root = minor->debugfs_root;
  ent = debugfs_create_file("amdgpu_regs", 33060, root, (void *)adev, & amdgpu_debugfs_regs_fops);
  tmp___0 = IS_ERR((void const *)ent);
  if ((int )tmp___0) {
    tmp = PTR_ERR((void const *)ent);
    return ((int )tmp);
  } else {
  }
  i_size_write(ent->d_inode, (loff_t )adev->rmmio_size);
  adev->debugfs_regs = ent;
  return (0);
}
}
static void amdgpu_debugfs_regs_cleanup(struct amdgpu_device *adev )
{
  {
  debugfs_remove(adev->debugfs_regs);
  adev->debugfs_regs = (struct dentry *)0;
  return;
}
}
int amdgpu_debugfs_init(struct drm_minor *minor )
{
  {
  return (0);
}
}
void amdgpu_debugfs_cleanup(struct drm_minor *minor )
{
  {
  return;
}
}
extern int ldv_open_174(void) ;
extern int ldv_release_174(void) ;
int ldv_retval_58 ;
void ldv_file_operations_174(void)
{
  void *tmp ;
  {
  amdgpu_debugfs_regs_fops_group1 = ldv_init_zalloc(1000UL);
  tmp = ldv_init_zalloc(504UL);
  amdgpu_debugfs_regs_fops_group2 = (struct file *)tmp;
  return;
}
}
void ldv_initialize_vga_switcheroo_client_ops_175(void)
{
  void *tmp ;
  {
  tmp = __VERIFIER_nondet_pointer();
  amdgpu_switcheroo_ops_group0 = (struct pci_dev *)tmp;
  return;
}
}
void ldv_main_exported_174(void)
{
  loff_t *ldvarg712 ;
  void *tmp ;
  char *ldvarg711 ;
  void *tmp___0 ;
  size_t ldvarg707 ;
  size_t ldvarg710 ;
  char *ldvarg708 ;
  void *tmp___1 ;
  loff_t *ldvarg709 ;
  void *tmp___2 ;
  int ldvarg705 ;
  loff_t ldvarg706 ;
  int tmp___3 ;
  {
  tmp = ldv_init_zalloc(8UL);
  ldvarg712 = (loff_t *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg711 = (char *)tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg708 = (char *)tmp___1;
  tmp___2 = ldv_init_zalloc(8UL);
  ldvarg709 = (loff_t *)tmp___2;
  ldv_memset((void *)(& ldvarg707), 0, 8UL);
  ldv_memset((void *)(& ldvarg710), 0, 8UL);
  ldv_memset((void *)(& ldvarg705), 0, 4UL);
  ldv_memset((void *)(& ldvarg706), 0, 8UL);
  tmp___3 = __VERIFIER_nondet_int();
  switch (tmp___3) {
  case 0: ;
  if (ldv_state_variable_174 == 2) {
    amdgpu_debugfs_regs_write(amdgpu_debugfs_regs_fops_group2, (char const *)ldvarg711,
                              ldvarg710, ldvarg712);
    ldv_state_variable_174 = 2;
  } else {
  }
  if (ldv_state_variable_174 == 1) {
    amdgpu_debugfs_regs_write(amdgpu_debugfs_regs_fops_group2, (char const *)ldvarg711,
                              ldvarg710, ldvarg712);
    ldv_state_variable_174 = 1;
  } else {
  }
  goto ldv_50458;
  case 1: ;
  if (ldv_state_variable_174 == 2) {
    amdgpu_debugfs_regs_read(amdgpu_debugfs_regs_fops_group2, ldvarg708, ldvarg707,
                             ldvarg709);
    ldv_state_variable_174 = 2;
  } else {
  }
  goto ldv_50458;
  case 2: ;
  if (ldv_state_variable_174 == 2) {
    default_llseek(amdgpu_debugfs_regs_fops_group2, ldvarg706, ldvarg705);
    ldv_state_variable_174 = 2;
  } else {
  }
  goto ldv_50458;
  case 3: ;
  if (ldv_state_variable_174 == 1) {
    ldv_retval_58 = ldv_open_174();
    if (ldv_retval_58 == 0) {
      ldv_state_variable_174 = 2;
      ref_cnt = ref_cnt + 1;
    } else {
    }
  } else {
  }
  goto ldv_50458;
  case 4: ;
  if (ldv_state_variable_174 == 2) {
    ldv_release_174();
    ldv_state_variable_174 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50458;
  default:
  ldv_stop();
  }
  ldv_50458: ;
  return;
}
}
void ldv_main_exported_175(void)
{
  enum vga_switcheroo_state ldvarg112 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg112), 0, 4UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_175 == 1) {
    amdgpu_switcheroo_set_state(amdgpu_switcheroo_ops_group0, ldvarg112);
    ldv_state_variable_175 = 1;
  } else {
  }
  goto ldv_50469;
  case 1: ;
  if (ldv_state_variable_175 == 1) {
    amdgpu_switcheroo_can_switch(amdgpu_switcheroo_ops_group0);
    ldv_state_variable_175 = 1;
  } else {
  }
  goto ldv_50469;
  default:
  ldv_stop();
  }
  ldv_50469: ;
  return;
}
}
__inline static long PTR_ERR(void const *ptr )
{
  long tmp ;
  {
  tmp = ldv_ptr_err(ptr);
  return (tmp);
}
}
__inline static bool IS_ERR(void const *ptr )
{
  bool tmp ;
  {
  tmp = ldv_is_err(ptr);
  return (tmp);
}
}
bool ldv_queue_work_on_19(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_20(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_21(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_22(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_23(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern void __dynamic_dev_dbg(struct _ddebug * , struct device const * , char const *
                              , ...) ;
extern void *memcpy(void * , void const * , size_t ) ;
__inline static long atomic64_read(atomic64_t const *v )
{
  long __var ;
  {
  __var = 0L;
  return ((long )*((long const volatile *)(& v->counter)));
}
}
extern void mutex_destroy(struct mutex * ) ;
bool ldv_queue_work_on_33(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_35(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_34(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_37(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_36(struct workqueue_struct *ldv_func_arg1 ) ;
extern void *idr_get_next(struct idr * , int * ) ;
extern void idr_destroy(struct idr * ) ;
extern void idr_init(struct idr * ) ;
extern unsigned long _copy_to_user(void * , void const * , unsigned int ) ;
extern void __copy_to_user_overflow(void) ;
__inline static unsigned long copy_to_user(void *to , void const *from , unsigned long n )
{
  int sz ;
  unsigned long tmp ;
  long tmp___0 ;
  {
  tmp = __builtin_object_size(from, 0);
  sz = (int )tmp;
  __might_fault("./arch/x86/include/asm/uaccess.h", 732);
  tmp___0 = ldv__builtin_expect((long )(sz < 0 || (unsigned long )sz >= n), 1L);
  if (tmp___0 != 0L) {
    n = _copy_to_user(to, from, (unsigned int )n);
  } else {
    __copy_to_user_overflow();
  }
  return (n);
}
}
extern int drm_calc_vbltimestamp_from_scanoutpos(struct drm_device * , int , int * ,
                                                 struct timeval * , unsigned int ,
                                                 struct drm_crtc const * , struct drm_display_mode const * ) ;
int amdgpu_crtc_idx_to_irq_type(struct amdgpu_device *adev , int crtc ) ;
int amdgpu_irq_get(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type ) ;
int amdgpu_irq_put(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type ) ;
void amdgpu_bo_list_free(struct amdgpu_bo_list *list ) ;
int amdgpu_gem_create_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_bo_list_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_userptr_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_mmap_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_va_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_op_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_cs_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_cs_wait_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_gem_metadata_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
void amdgpu_ctx_fini(struct amdgpu_fpriv *fpriv ) ;
int amdgpu_ctx_ioctl(struct drm_device *dev , void *data , struct drm_file *filp ) ;
int amdgpu_vm_init(struct amdgpu_device *adev , struct amdgpu_vm *vm ) ;
void amdgpu_vm_fini(struct amdgpu_device *adev , struct amdgpu_vm *vm ) ;
int amdgpu_acpi_init(struct amdgpu_device *adev ) ;
void amdgpu_acpi_fini(struct amdgpu_device *adev ) ;
void amdgpu_uvd_free_handles(struct amdgpu_device *adev , struct drm_file *filp ) ;
void amdgpu_vce_free_handles(struct amdgpu_device *adev , struct drm_file *filp ) ;
extern int vga_switcheroo_process_delayed_switch(void) ;
extern int __pm_runtime_set_status(struct device * , unsigned int ) ;
extern void pm_runtime_allow(struct device * ) ;
extern void __pm_runtime_use_autosuspend(struct device * , bool ) ;
extern void pm_runtime_set_autosuspend_delay(struct device * , int ) ;
__inline static int pm_runtime_set_active(struct device *dev )
{
  int tmp ;
  {
  tmp = __pm_runtime_set_status(dev, 0U);
  return (tmp);
}
}
__inline static void pm_runtime_use_autosuspend(struct device *dev )
{
  {
  __pm_runtime_use_autosuspend(dev, 1);
  return;
}
}
bool amdgpu_has_atpx(void) ;
int amdgpu_driver_unload_kms(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0)) {
    return (0);
  } else {
  }
  if ((unsigned long )adev->rmmio == (unsigned long )((void *)0)) {
    goto done_free;
  } else {
  }
  pm_runtime_get_sync(dev->dev);
  amdgpu_acpi_fini(adev);
  amdgpu_device_fini(adev);
  done_free:
  kfree((void const *)adev);
  dev->dev_private = (void *)0;
  return (0);
}
}
int amdgpu_driver_load_kms(struct drm_device *dev , unsigned long flags )
{
  struct amdgpu_device *adev ;
  int r ;
  int acpi_status ;
  void *tmp ;
  bool tmp___0 ;
  struct _ddebug descriptor ;
  long tmp___1 ;
  bool tmp___2 ;
  {
  tmp = kzalloc(23352UL, 208U);
  adev = (struct amdgpu_device *)tmp;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0)) {
    return (-12);
  } else {
  }
  dev->dev_private = (void *)adev;
  if (amdgpu_runtime_pm != 0) {
    tmp___0 = amdgpu_has_atpx();
    if ((int )tmp___0) {
      if ((flags & 131072UL) == 0UL) {
        flags = flags | 262144UL;
      } else {
      }
    } else {
    }
  } else {
  }
  r = amdgpu_device_init(adev, dev, dev->pdev, (u32 )flags);
  if (r != 0) {
    dev_err((struct device const *)(& (dev->pdev)->dev), "Fatal error during GPU init\n");
    goto out;
  } else {
  }
  if (r == 0) {
    acpi_status = amdgpu_acpi_init(adev);
    if (acpi_status != 0) {
      descriptor.modname = "amdgpu";
      descriptor.function = "amdgpu_driver_load_kms";
      descriptor.filename = "/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_kms.c";
      descriptor.format = "Error during ACPI methods call\n";
      descriptor.lineno = 118U;
      descriptor.flags = 0U;
      tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___1 != 0L) {
        __dynamic_dev_dbg(& descriptor, (struct device const *)(& (dev->pdev)->dev),
                          "Error during ACPI methods call\n");
      } else {
      }
    } else {
    }
  } else {
  }
  tmp___2 = amdgpu_device_is_px(dev);
  if ((int )tmp___2) {
    pm_runtime_use_autosuspend(dev->dev);
    pm_runtime_set_autosuspend_delay(dev->dev, 5000);
    pm_runtime_set_active(dev->dev);
    pm_runtime_allow(dev->dev);
    pm_runtime_mark_last_busy(dev->dev);
    pm_runtime_put_autosuspend(dev->dev);
  } else {
  }
  out: ;
  if (r != 0) {
    amdgpu_driver_unload_kms(dev);
  } else {
  }
  return (r);
}
}
static int amdgpu_info_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct amdgpu_device *adev ;
  struct drm_amdgpu_info *info ;
  struct amdgpu_mode_info *minfo ;
  void *out ;
  u32 size ;
  struct drm_crtc *crtc ;
  u32 ui32 ;
  uint64_t ui64 ;
  int i ;
  int found ;
  u32 _min1 ;
  unsigned int _min2 ;
  unsigned long tmp ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  long tmp___0 ;
  u32 _min1___0 ;
  unsigned int _min2___0 ;
  unsigned long tmp___1 ;
  struct drm_amdgpu_info_hw_ip ip ;
  enum amd_ip_block_type type ;
  u32 ring_mask ;
  u32 ib_start_alignment ;
  u32 ib_size_alignment ;
  unsigned long _min1___1 ;
  unsigned long _min2___1 ;
  unsigned long tmp___2 ;
  enum amd_ip_block_type type___0 ;
  u32 count ;
  u32 _min1___2 ;
  unsigned int _min2___2 ;
  unsigned long tmp___3 ;
  u32 _min1___3 ;
  unsigned int _min2___3 ;
  unsigned long tmp___4 ;
  struct drm_amdgpu_info_firmware fw_info ;
  unsigned long _min1___4 ;
  unsigned long _min2___4 ;
  unsigned long tmp___5 ;
  long tmp___6 ;
  u32 _min1___5 ;
  unsigned int _min2___5 ;
  unsigned long tmp___7 ;
  long tmp___8 ;
  u32 _min1___6 ;
  unsigned int _min2___6 ;
  unsigned long tmp___9 ;
  long tmp___10 ;
  u32 _min1___7 ;
  unsigned int _min2___7 ;
  unsigned long tmp___11 ;
  long tmp___12 ;
  u32 _min1___8 ;
  unsigned int _min2___8 ;
  unsigned long tmp___13 ;
  struct drm_amdgpu_info_gds gds_info ;
  unsigned long _min1___9 ;
  unsigned long _min2___9 ;
  unsigned long tmp___14 ;
  struct drm_amdgpu_info_vram_gtt vram_gtt ;
  unsigned long _min1___10 ;
  unsigned long _min2___10 ;
  unsigned long tmp___15 ;
  unsigned int n ;
  unsigned int alloc_size ;
  u32 *regs ;
  unsigned int se_num ;
  unsigned int sh_num ;
  void *tmp___16 ;
  long tmp___17 ;
  int tmp___18 ;
  u32 _min1___11 ;
  unsigned int _min2___11 ;
  unsigned long tmp___19 ;
  struct drm_amdgpu_info_device dev_info ;
  struct amdgpu_cu_info cu_info ;
  u32 tmp___20 ;
  unsigned long _max1 ;
  unsigned long _max2 ;
  unsigned long _min1___12 ;
  unsigned long _min2___12 ;
  unsigned long tmp___21 ;
  long tmp___22 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  info = (struct drm_amdgpu_info *)data;
  minfo = & adev->mode_info;
  out = (void *)info->return_pointer;
  size = info->return_size;
  ui32 = 0U;
  ui64 = 0ULL;
  if (info->return_size == 0U || info->return_pointer == 0ULL) {
    return (-22);
  } else {
  }
  switch (info->query) {
  case 0U:
  ui32 = (u32 )adev->accel_working;
  _min1 = size;
  _min2 = 4U;
  tmp = copy_to_user(out, (void const *)(& ui32), (unsigned long )(_min1 < _min2 ? _min1 : _min2));
  return (tmp != 0UL ? -14 : 0);
  case 1U:
  i = 0;
  found = 0;
  goto ldv_43958;
  ldv_43957:
  crtc = (struct drm_crtc *)minfo->crtcs[i];
  if ((unsigned long )crtc != (unsigned long )((struct drm_crtc *)0) && crtc->base.id == info->__annonCompField78.mode_crtc.id) {
    __mptr = (struct drm_crtc const *)crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
    ui32 = (u32 )amdgpu_crtc->crtc_id;
    found = 1;
    goto ldv_43956;
  } else {
  }
  i = i + 1;
  ldv_43958: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_43957;
  } else {
  }
  ldv_43956: ;
  if (found == 0) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_info_ioctl", "unknown crtc id %d\n", info->__annonCompField78.mode_crtc.id);
    } else {
    }
    return (-22);
  } else {
  }
  _min1___0 = size;
  _min2___0 = 4U;
  tmp___1 = copy_to_user(out, (void const *)(& ui32), (unsigned long )(_min1___0 < _min2___0 ? _min1___0 : _min2___0));
  return (tmp___1 != 0UL ? -14 : 0);
  case 2U:
  ip.hw_ip_version_major = 0U;
  ip.hw_ip_version_minor = 0U;
  ip.capabilities_flags = 0ULL;
  ip.ib_start_alignment = 0U;
  ip.ib_size_alignment = 0U;
  ip.available_rings = 0U;
  ip._pad = 0U;
  ring_mask = 0U;
  ib_start_alignment = 0U;
  ib_size_alignment = 0U;
  if (info->__annonCompField78.query_hw_ip.ip_instance != 0U) {
    return (-22);
  } else {
  }
  switch (info->__annonCompField78.query_hw_ip.type) {
  case 0U:
  type = 5;
  i = 0;
  goto ldv_43971;
  ldv_43970:
  ring_mask = (u32 )((int )adev->gfx.gfx_ring[i].ready << i) | ring_mask;
  i = i + 1;
  ldv_43971: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_43970;
  } else {
  }
  ib_start_alignment = 4096U;
  ib_size_alignment = 8U;
  goto ldv_43973;
  case 1U:
  type = 5;
  i = 0;
  goto ldv_43976;
  ldv_43975:
  ring_mask = (u32 )((int )adev->gfx.compute_ring[i].ready << i) | ring_mask;
  i = i + 1;
  ldv_43976: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_43975;
  } else {
  }
  ib_start_alignment = 4096U;
  ib_size_alignment = 8U;
  goto ldv_43973;
  case 2U:
  type = 6;
  ring_mask = (u32 )adev->sdma[0].ring.ready;
  ring_mask = (u32 )((int )adev->sdma[1].ring.ready << 1) | ring_mask;
  ib_start_alignment = 4096U;
  ib_size_alignment = 1U;
  goto ldv_43973;
  case 3U:
  type = 7;
  ring_mask = (u32 )adev->uvd.ring.ready;
  ib_start_alignment = 4096U;
  ib_size_alignment = 8U;
  goto ldv_43973;
  case 4U:
  type = 8;
  i = 0;
  goto ldv_43982;
  ldv_43981:
  ring_mask = (u32 )((int )adev->vce.ring[i].ready << i) | ring_mask;
  i = i + 1;
  ldv_43982: ;
  if (i <= 1) {
    goto ldv_43981;
  } else {
  }
  ib_start_alignment = 4096U;
  ib_size_alignment = 8U;
  goto ldv_43973;
  default: ;
  return (-22);
  }
  ldv_43973:
  i = 0;
  goto ldv_43987;
  ldv_43986: ;
  if ((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == (unsigned int )type && (int )*(adev->ip_block_enabled + (unsigned long )i)) {
    ip.hw_ip_version_major = (adev->ip_blocks + (unsigned long )i)->major;
    ip.hw_ip_version_minor = (adev->ip_blocks + (unsigned long )i)->minor;
    ip.capabilities_flags = 0ULL;
    ip.available_rings = ring_mask;
    ip.ib_start_alignment = ib_start_alignment;
    ip.ib_size_alignment = ib_size_alignment;
    goto ldv_43985;
  } else {
  }
  i = i + 1;
  ldv_43987: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_43986;
  } else {
  }
  ldv_43985:
  _min1___1 = (unsigned long )size;
  _min2___1 = 32UL;
  tmp___2 = copy_to_user(out, (void const *)(& ip), _min1___1 < _min2___1 ? _min1___1 : _min2___1);
  return (tmp___2 != 0UL ? -14 : 0);
  case 3U:
  count = 0U;
  switch (info->__annonCompField78.query_hw_ip.type) {
  case 0U:
  type___0 = 5;
  goto ldv_43995;
  case 1U:
  type___0 = 5;
  goto ldv_43995;
  case 2U:
  type___0 = 6;
  goto ldv_43995;
  case 3U:
  type___0 = 7;
  goto ldv_43995;
  case 4U:
  type___0 = 8;
  goto ldv_43995;
  default: ;
  return (-22);
  }
  ldv_43995:
  i = 0;
  goto ldv_44002;
  ldv_44001: ;
  if (((unsigned int )(adev->ip_blocks + (unsigned long )i)->type == (unsigned int )type___0 && (int )*(adev->ip_block_enabled + (unsigned long )i)) && count == 0U) {
    count = count + 1U;
  } else {
  }
  i = i + 1;
  ldv_44002: ;
  if (adev->num_ip_blocks > i) {
    goto ldv_44001;
  } else {
  }
  _min1___2 = size;
  _min2___2 = 4U;
  tmp___3 = copy_to_user(out, (void const *)(& count), (unsigned long )(_min1___2 < _min2___2 ? _min1___2 : _min2___2));
  return (tmp___3 != 0UL ? -14 : 0);
  case 5U:
  ui64 = (*((adev->asic_funcs)->get_gpu_clock_counter))(adev);
  _min1___3 = size;
  _min2___3 = 8U;
  tmp___4 = copy_to_user(out, (void const *)(& ui64), (unsigned long )(_min1___3 < _min2___3 ? _min1___3 : _min2___3));
  return (tmp___4 != 0UL ? -14 : 0);
  case 14U: ;
  if (info->__annonCompField78.query_fw.ip_instance != 0U) {
    return (-22);
  } else {
  }
  switch (info->__annonCompField78.query_fw.fw_type) {
  case 1U:
  fw_info.ver = adev->vce.fw_version;
  fw_info.feature = adev->vce.fb_version;
  goto ldv_44014;
  case 2U:
  fw_info.ver = 0U;
  fw_info.feature = 0U;
  goto ldv_44014;
  case 3U:
  fw_info.ver = adev->mc.fw_version;
  fw_info.feature = 0U;
  goto ldv_44014;
  case 4U:
  fw_info.ver = adev->gfx.me_fw_version;
  fw_info.feature = adev->gfx.me_feature_version;
  goto ldv_44014;
  case 5U:
  fw_info.ver = adev->gfx.pfp_fw_version;
  fw_info.feature = adev->gfx.pfp_feature_version;
  goto ldv_44014;
  case 6U:
  fw_info.ver = adev->gfx.ce_fw_version;
  fw_info.feature = adev->gfx.ce_feature_version;
  goto ldv_44014;
  case 7U:
  fw_info.ver = adev->gfx.rlc_fw_version;
  fw_info.feature = 0U;
  goto ldv_44014;
  case 8U: ;
  if (info->__annonCompField78.query_fw.index == 0U) {
    fw_info.ver = adev->gfx.mec_fw_version;
  } else
  if (info->__annonCompField78.query_fw.index == 1U) {
    fw_info.ver = adev->gfx.mec2_fw_version;
  } else {
    return (-22);
  }
  fw_info.feature = 0U;
  goto ldv_44014;
  case 10U:
  fw_info.ver = adev->pm.fw_version;
  fw_info.feature = 0U;
  goto ldv_44014;
  case 11U: ;
  if (info->__annonCompField78.query_fw.index > 1U) {
    return (-22);
  } else {
  }
  fw_info.ver = adev->sdma[info->__annonCompField78.query_fw.index].fw_version;
  fw_info.feature = 0U;
  goto ldv_44014;
  default: ;
  return (-22);
  }
  ldv_44014:
  _min1___4 = (unsigned long )size;
  _min2___4 = 8UL;
  tmp___5 = copy_to_user(out, (void const *)(& fw_info), _min1___4 < _min2___4 ? _min1___4 : _min2___4);
  return (tmp___5 != 0UL ? -14 : 0);
  case 15U:
  tmp___6 = atomic64_read((atomic64_t const *)(& adev->num_bytes_moved));
  ui64 = (uint64_t )tmp___6;
  _min1___5 = size;
  _min2___5 = 8U;
  tmp___7 = copy_to_user(out, (void const *)(& ui64), (unsigned long )(_min1___5 < _min2___5 ? _min1___5 : _min2___5));
  return (tmp___7 != 0UL ? -14 : 0);
  case 16U:
  tmp___8 = atomic64_read((atomic64_t const *)(& adev->vram_usage));
  ui64 = (uint64_t )tmp___8;
  _min1___6 = size;
  _min2___6 = 8U;
  tmp___9 = copy_to_user(out, (void const *)(& ui64), (unsigned long )(_min1___6 < _min2___6 ? _min1___6 : _min2___6));
  return (tmp___9 != 0UL ? -14 : 0);
  case 23U:
  tmp___10 = atomic64_read((atomic64_t const *)(& adev->vram_vis_usage));
  ui64 = (uint64_t )tmp___10;
  _min1___7 = size;
  _min2___7 = 8U;
  tmp___11 = copy_to_user(out, (void const *)(& ui64), (unsigned long )(_min1___7 < _min2___7 ? _min1___7 : _min2___7));
  return (tmp___11 != 0UL ? -14 : 0);
  case 17U:
  tmp___12 = atomic64_read((atomic64_t const *)(& adev->gtt_usage));
  ui64 = (uint64_t )tmp___12;
  _min1___8 = size;
  _min2___8 = 8U;
  tmp___13 = copy_to_user(out, (void const *)(& ui64), (unsigned long )(_min1___8 < _min2___8 ? _min1___8 : _min2___8));
  return (tmp___13 != 0UL ? -14 : 0);
  case 19U:
  memset((void *)(& gds_info), 0, 32UL);
  gds_info.gds_gfx_partition_size = adev->gds.mem.gfx_partition_size >> 2;
  gds_info.compute_partition_size = adev->gds.mem.cs_partition_size >> 2;
  gds_info.gds_total_size = adev->gds.mem.total_size >> 2;
  gds_info.gws_per_gfx_partition = adev->gds.gws.gfx_partition_size >> 12;
  gds_info.gws_per_compute_partition = adev->gds.gws.cs_partition_size >> 12;
  gds_info.oa_per_gfx_partition = adev->gds.oa.gfx_partition_size >> 12;
  gds_info.oa_per_compute_partition = adev->gds.oa.cs_partition_size >> 12;
  _min1___9 = (unsigned long )size;
  _min2___9 = 32UL;
  tmp___14 = copy_to_user(out, (void const *)(& gds_info), _min1___9 < _min2___9 ? _min1___9 : _min2___9);
  return (tmp___14 != 0UL ? -14 : 0);
  case 20U:
  vram_gtt.vram_size = adev->mc.real_vram_size;
  vram_gtt.vram_cpu_accessible_size = adev->mc.visible_vram_size;
  vram_gtt.vram_cpu_accessible_size = vram_gtt.vram_cpu_accessible_size - adev->vram_pin_size;
  vram_gtt.gtt_size = adev->mc.gtt_size;
  vram_gtt.gtt_size = vram_gtt.gtt_size - adev->gart_pin_size;
  _min1___10 = (unsigned long )size;
  _min2___10 = 24UL;
  tmp___15 = copy_to_user(out, (void const *)(& vram_gtt), _min1___10 < _min2___10 ? _min1___10 : _min2___10);
  return (tmp___15 != 0UL ? -14 : 0);
  case 21U:
  alloc_size = info->__annonCompField78.read_mmr_reg.count * 4U;
  se_num = info->__annonCompField78.read_mmr_reg.instance & 255U;
  sh_num = (info->__annonCompField78.read_mmr_reg.instance >> 8) & 255U;
  if (se_num == 255U) {
    se_num = 4294967295U;
  } else {
  }
  if (sh_num == 255U) {
    sh_num = 4294967295U;
  } else {
  }
  tmp___16 = kmalloc((size_t )alloc_size, 208U);
  regs = (u32 *)tmp___16;
  if ((unsigned long )regs == (unsigned long )((u32 *)0U)) {
    return (-12);
  } else {
  }
  i = 0;
  goto ldv_44061;
  ldv_44060:
  tmp___18 = (*((adev->asic_funcs)->read_register))(adev, se_num, sh_num, info->__annonCompField78.read_mmr_reg.dword_offset + (u32 )i,
                                                    regs + (unsigned long )i);
  if (tmp___18 != 0) {
    tmp___17 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___17 != 0L) {
      drm_ut_debug_printk("amdgpu_info_ioctl", "unallowed offset %#x\n", info->__annonCompField78.read_mmr_reg.dword_offset + (u32 )i);
    } else {
    }
    kfree((void const *)regs);
    return (-14);
  } else {
  }
  i = i + 1;
  ldv_44061: ;
  if ((u32 )i < info->__annonCompField78.read_mmr_reg.count) {
    goto ldv_44060;
  } else {
  }
  _min1___11 = size;
  _min2___11 = alloc_size;
  tmp___19 = copy_to_user(out, (void const *)regs, (unsigned long )(_min1___11 < _min2___11 ? _min1___11 : _min2___11));
  n = (unsigned int )tmp___19;
  kfree((void const *)regs);
  return (n != 0U ? -14 : 0);
  case 22U:
  dev_info.device_id = (u32 )(dev->pdev)->device;
  dev_info.chip_rev = adev->rev_id;
  dev_info.external_rev = adev->external_rev_id;
  dev_info.pci_rev = (u32 )(dev->pdev)->revision;
  dev_info.family = adev->family;
  dev_info.num_shader_engines = adev->gfx.config.max_shader_engines;
  dev_info.num_shader_arrays_per_engine = adev->gfx.config.max_sh_per_se;
  tmp___20 = (*((adev->asic_funcs)->get_xclk))(adev);
  dev_info.gpu_counter_freq = tmp___20 * 10U;
  if ((int )adev->pm.dpm_enabled) {
    dev_info.max_engine_clock = (uint64_t )(adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.sclk * 10U);
    dev_info.max_memory_clock = (uint64_t )(adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.mclk * 10U);
  } else {
    dev_info.max_engine_clock = (uint64_t )(adev->pm.default_sclk * 10U);
    dev_info.max_memory_clock = (uint64_t )(adev->pm.default_mclk * 10U);
  }
  dev_info.enabled_rb_pipes_mask = adev->gfx.config.backend_enable_mask;
  dev_info.num_rb_pipes = adev->gfx.config.max_backends_per_se * adev->gfx.config.max_shader_engines;
  dev_info.num_hw_gfx_contexts = adev->gfx.config.max_hw_contexts;
  dev_info._pad = 0U;
  dev_info.ids_flags = 0ULL;
  if ((adev->flags & 131072UL) != 0UL) {
    dev_info.ids_flags = dev_info.ids_flags | 1ULL;
  } else {
  }
  dev_info.virtual_address_offset = 8388608ULL;
  dev_info.virtual_address_max = (unsigned long long )adev->vm_manager.max_pfn * 4096ULL;
  _max1 = 4096UL;
  _max2 = 65536UL;
  dev_info.virtual_address_alignment = (u32 )(_max1 > _max2 ? _max1 : _max2);
  dev_info.pte_fragment_size = 65536U;
  dev_info.gart_page_size = 4096U;
  (*((adev->asic_funcs)->get_cu_info))(adev, & cu_info);
  dev_info.cu_active_number = cu_info.number;
  dev_info.cu_ao_mask = cu_info.ao_cu_mask;
  dev_info.ce_ram_size = adev->gfx.ce_ram_size;
  memcpy((void *)(& dev_info.cu_bitmap), (void const *)(& cu_info.bitmap), 64UL);
  dev_info.vram_type = adev->mc.vram_type;
  dev_info.vram_bit_width = adev->mc.vram_width;
  _min1___12 = (unsigned long )size;
  _min2___12 = 184UL;
  tmp___21 = copy_to_user(out, (void const *)(& dev_info), _min1___12 < _min2___12 ? _min1___12 : _min2___12);
  return (tmp___21 != 0UL ? -14 : 0);
  default:
  tmp___22 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___22 != 0L) {
    drm_ut_debug_printk("amdgpu_info_ioctl", "Invalid request %d\n", info->query);
  } else {
  }
  return (-22);
  }
  return (0);
}
}
void amdgpu_driver_lastclose_kms(struct drm_device *dev )
{
  {
  vga_switcheroo_process_delayed_switch();
  return;
}
}
int amdgpu_driver_open_kms(struct drm_device *dev , struct drm_file *file_priv )
{
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  int r ;
  void *tmp ;
  long tmp___0 ;
  struct lock_class_key __key ;
  struct lock_class_key __key___0 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  file_priv->driver_priv = (void *)0;
  r = pm_runtime_get_sync(dev->dev);
  if (r < 0) {
    return (r);
  } else {
  }
  tmp = kzalloc(1360UL, 208U);
  fpriv = (struct amdgpu_fpriv *)tmp;
  tmp___0 = ldv__builtin_expect((unsigned long )fpriv == (unsigned long )((struct amdgpu_fpriv *)0),
                             0L);
  if (tmp___0 != 0L) {
    return (-12);
  } else {
  }
  r = amdgpu_vm_init(adev, & fpriv->vm);
  if (r != 0) {
    goto error_free;
  } else {
  }
  __mutex_init(& fpriv->bo_list_lock, "&fpriv->bo_list_lock", & __key);
  idr_init(& fpriv->bo_list_handles);
  __mutex_init(& fpriv->ctx_mgr.lock, "&fpriv->ctx_mgr.lock", & __key___0);
  idr_init(& fpriv->ctx_mgr.ctx_handles);
  fpriv->ctx_mgr.adev = adev;
  file_priv->driver_priv = (void *)fpriv;
  pm_runtime_mark_last_busy(dev->dev);
  pm_runtime_put_autosuspend(dev->dev);
  return (0);
  error_free:
  kfree((void const *)fpriv);
  return (r);
}
}
void amdgpu_driver_postclose_kms(struct drm_device *dev , struct drm_file *file_priv )
{
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_bo_list *list ;
  int handle ;
  void *tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  fpriv = (struct amdgpu_fpriv *)file_priv->driver_priv;
  if ((unsigned long )fpriv == (unsigned long )((struct amdgpu_fpriv *)0)) {
    return;
  } else {
  }
  amdgpu_vm_fini(adev, & fpriv->vm);
  handle = 0;
  goto ldv_44098;
  ldv_44097:
  amdgpu_bo_list_free(list);
  handle = handle + 1;
  ldv_44098:
  tmp = idr_get_next(& fpriv->bo_list_handles, & handle);
  list = (struct amdgpu_bo_list *)tmp;
  if ((unsigned long )list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    goto ldv_44097;
  } else {
  }
  idr_destroy(& fpriv->bo_list_handles);
  mutex_destroy(& fpriv->bo_list_lock);
  amdgpu_ctx_fini(fpriv);
  kfree((void const *)fpriv);
  file_priv->driver_priv = (void *)0;
  return;
}
}
void amdgpu_driver_preclose_kms(struct drm_device *dev , struct drm_file *file_priv )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_uvd_free_handles(adev, file_priv);
  amdgpu_vce_free_handles(adev, file_priv);
  return;
}
}
u32 amdgpu_get_vblank_counter_kms(struct drm_device *dev , int crtc )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if (crtc < 0 || adev->mode_info.num_crtc <= crtc) {
    drm_err("Invalid crtc %d\n", crtc);
    return (4294967274U);
  } else {
  }
  tmp = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, crtc);
  return (tmp);
}
}
int amdgpu_enable_vblank_kms(struct drm_device *dev , int crtc )
{
  struct amdgpu_device *adev ;
  int idx ;
  int tmp ;
  int tmp___0 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_crtc_idx_to_irq_type(adev, crtc);
  idx = tmp;
  tmp___0 = amdgpu_irq_get(adev, & adev->crtc_irq, (unsigned int )idx);
  return (tmp___0);
}
}
void amdgpu_disable_vblank_kms(struct drm_device *dev , int crtc )
{
  struct amdgpu_device *adev ;
  int idx ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_crtc_idx_to_irq_type(adev, crtc);
  idx = tmp;
  amdgpu_irq_put(adev, & adev->crtc_irq, (unsigned int )idx);
  return;
}
}
int amdgpu_get_vblank_timestamp_kms(struct drm_device *dev , int crtc , int *max_error ,
                                    struct timeval *vblank_time , unsigned int flags )
{
  struct drm_crtc *drmcrtc ;
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if (crtc < 0 || (unsigned int )crtc >= dev->num_crtcs) {
    drm_err("Invalid crtc %d\n", crtc);
    return (-22);
  } else {
  }
  drmcrtc = & (adev->mode_info.crtcs[crtc])->base;
  tmp = drm_calc_vbltimestamp_from_scanoutpos(dev, crtc, max_error, vblank_time, flags,
                                              (struct drm_crtc const *)drmcrtc,
                                              (struct drm_display_mode const *)(& drmcrtc->hwmode));
  return (tmp);
}
}
struct drm_ioctl_desc const amdgpu_ioctls_kms[18U] =
  { {3223348288U, 49, & amdgpu_gem_create_ioctl, "AMDGPU_GEM_CREATE"},
        {3221775425U, 49, & amdgpu_gem_mmap_ioctl, "AMDGPU_GEM_MMAP"},
        {3222299714U, 49, & amdgpu_ctx_ioctl, "AMDGPU_CTX"},
        {3222824003U, 49, & amdgpu_bo_list_ioctl, "AMDGPU_BO_LIST"},
        {3222824004U, 49, & amdgpu_cs_ioctl, "AMDGPU_CS"},
        {1075864645U, 49, & amdgpu_info_ioctl, "AMDGPU_INFO"},
        {3240125510U, 49, & amdgpu_gem_metadata_ioctl, "AMDGPU_GEM_METADATA"},
        {3222299719U, 49, & amdgpu_gem_wait_idle_ioctl, "AMDGPU_GEM_WAIT_IDLE"},
        {1076388936U, 49, & amdgpu_gem_va_ioctl, "AMDGPU_GEM_VA"},
        {3223348297U, 49, & amdgpu_cs_wait_ioctl, "AMDGPU_WAIT_CS"},
        {0U, 0, 0, 0},
        {0U, 0, 0, 0},
        {0U, 0, 0, 0},
        {0U, 0, 0, 0},
        {0U, 0, 0, 0},
        {0U, 0, 0, 0},
        {3222299728U, 49, & amdgpu_gem_op_ioctl, "AMDGPU_GEM_OP"},
        {3222824017U, 49, & amdgpu_gem_userptr_ioctl, "AMDGPU_GEM_USERPTR"}};
int amdgpu_max_kms_ioctl = 18;
bool ldv_queue_work_on_33(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_34(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_35(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_36(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_37(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
extern int sprintf(char * , char const * , ...) ;
bool ldv_queue_work_on_47(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_49(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_48(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_51(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_50(struct workqueue_struct *ldv_func_arg1 ) ;
void amdgpu_link_encoder_connector(struct drm_device *dev ) ;
struct amdgpu_gpio_rec amdgpu_atombios_lookup_gpio(struct amdgpu_device *adev , u8 id ) ;
struct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev ,
                                                          uint8_t id ) ;
bool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev ) ;
bool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev , struct amdgpu_atom_ss *ss ,
                                      int id , u32 clock ) ;
int amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev , u8 clock_type ,
                                       u32 clock , bool strobe_mode , struct atom_clock_dividers *dividers ) ;
int amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev , u32 clock ,
                                            bool strobe_mode , struct atom_mpll_param *mpll_param ) ;
u32 amdgpu_atombios_get_engine_clock(struct amdgpu_device *adev ) ;
u32 amdgpu_atombios_get_memory_clock(struct amdgpu_device *adev ) ;
void amdgpu_atombios_set_engine_clock(struct amdgpu_device *adev , u32 eng_clock ) ;
void amdgpu_atombios_set_memory_clock(struct amdgpu_device *adev , u32 mem_clock ) ;
void amdgpu_atombios_set_voltage(struct amdgpu_device *adev , u16 voltage_level ,
                                 u8 voltage_type ) ;
void amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev , u32 eng_clock ,
                                             u32 mem_clock ) ;
int amdgpu_atombios_get_leakage_id_from_vbios(struct amdgpu_device *adev , u16 *leakage_id ) ;
int amdgpu_atombios_get_leakage_vddc_based_on_leakage_params(struct amdgpu_device *adev ,
                                                             u16 *vddc , u16 *vddci ,
                                                             u16 virtual_voltage_id ,
                                                             u16 vbios_voltage_id ) ;
int amdgpu_atombios_get_voltage_evv(struct amdgpu_device *adev , u16 virtual_voltage_id ,
                                    u16 *voltage ) ;
bool amdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev , u8 voltage_type ,
                                     u8 voltage_mode ) ;
int amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev , u8 voltage_type ,
                                      u8 voltage_mode , struct atom_voltage_table *voltage_table ) ;
int amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev , u8 module_index ,
                                      struct atom_mc_reg_table *reg_table ) ;
void amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev , bool lock ) ;
void amdgpu_atombios_scratch_regs_save(struct amdgpu_device *adev ) ;
void amdgpu_atombios_scratch_regs_restore(struct amdgpu_device *adev ) ;
void amdgpu_atombios_copy_swap(u8 *dst , u8 *src , u8 num_bytes , bool to_le ) ;
struct amdgpu_i2c_chan *amdgpu_i2c_create(struct drm_device *dev , struct amdgpu_i2c_bus_rec *rec ,
                                          char const *name ) ;
int amdgpu_atom_execute_table(struct atom_context *ctx , int index , u32 *params ) ;
bool amdgpu_atom_parse_data_header(struct atom_context *ctx , int index , u16 *size ,
                                   uint8_t *frev , uint8_t *crev , u16 *data_start ) ;
bool amdgpu_atom_parse_cmd_header(struct atom_context *ctx , int index , uint8_t *frev ,
                                  uint8_t *crev ) ;
static void amdgpu_atombios_lookup_i2c_gpio_quirks(struct amdgpu_device *adev , ATOM_GPIO_I2C_ASSIGMENT *gpio ,
                                                   u8 index )
{
  {
  return;
}
}
static struct amdgpu_i2c_bus_rec amdgpu_atombios_get_bus_rec_for_i2c_gpio(ATOM_GPIO_I2C_ASSIGMENT *gpio )
{
  struct amdgpu_i2c_bus_rec i2c ;
  {
  memset((void *)(& i2c), 0, 76UL);
  i2c.mask_clk_reg = (u32 )gpio->usClkMaskRegisterIndex;
  i2c.mask_data_reg = (u32 )gpio->usDataMaskRegisterIndex;
  i2c.en_clk_reg = (u32 )gpio->usClkEnRegisterIndex;
  i2c.en_data_reg = (u32 )gpio->usDataEnRegisterIndex;
  i2c.y_clk_reg = (u32 )gpio->usClkY_RegisterIndex;
  i2c.y_data_reg = (u32 )gpio->usDataY_RegisterIndex;
  i2c.a_clk_reg = (u32 )gpio->usClkA_RegisterIndex;
  i2c.a_data_reg = (u32 )gpio->usDataA_RegisterIndex;
  i2c.mask_clk_mask = (u32 )(1 << (int )gpio->ucClkMaskShift);
  i2c.mask_data_mask = (u32 )(1 << (int )gpio->ucDataMaskShift);
  i2c.en_clk_mask = (u32 )(1 << (int )gpio->ucClkEnShift);
  i2c.en_data_mask = (u32 )(1 << (int )gpio->ucDataEnShift);
  i2c.y_clk_mask = (u32 )(1 << (int )gpio->ucClkY_Shift);
  i2c.y_data_mask = (u32 )(1 << (int )gpio->ucDataY_Shift);
  i2c.a_clk_mask = (u32 )(1 << (int )gpio->ucClkA_Shift);
  i2c.a_data_mask = (u32 )(1 << (int )gpio->ucDataA_Shift);
  if ((unsigned int )*((unsigned char *)gpio + 16UL) != 0U) {
    i2c.hw_capable = 1;
  } else {
    i2c.hw_capable = 0;
  }
  if ((unsigned int )gpio->sucI2cId.ucAccess == 160U) {
    i2c.mm_i2c = 1;
  } else {
    i2c.mm_i2c = 0;
  }
  i2c.i2c_id = gpio->sucI2cId.ucAccess;
  if (i2c.mask_clk_reg != 0U) {
    i2c.valid = 1;
  } else {
    i2c.valid = 0;
  }
  return (i2c);
}
}
struct amdgpu_i2c_bus_rec amdgpu_atombios_lookup_i2c_gpio(struct amdgpu_device *adev ,
                                                          uint8_t id )
{
  struct atom_context *ctx ;
  ATOM_GPIO_I2C_ASSIGMENT *gpio ;
  struct amdgpu_i2c_bus_rec i2c ;
  int index ;
  struct _ATOM_GPIO_I2C_INFO *i2c_info ;
  u16 data_offset ;
  u16 size ;
  int i ;
  int num_indices ;
  bool tmp ;
  {
  ctx = adev->mode_info.atom_context;
  index = 10;
  memset((void *)(& i2c), 0, 76UL);
  i2c.valid = 0;
  tmp = amdgpu_atom_parse_data_header(ctx, index, & size, (uint8_t *)0U, (uint8_t *)0U,
                                      & data_offset);
  if ((int )tmp) {
    i2c_info = (struct _ATOM_GPIO_I2C_INFO *)ctx->bios + (unsigned long )data_offset;
    num_indices = (int )(((unsigned long )size - 4UL) / 27UL);
    gpio = (ATOM_GPIO_I2C_ASSIGMENT *)(& i2c_info->asGPIO_Info);
    i = 0;
    goto ldv_48007;
    ldv_48006:
    amdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, (int )((u8 )i));
    if ((int )gpio->sucI2cId.ucAccess == (int )id) {
      i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);
      goto ldv_48005;
    } else {
    }
    gpio = gpio + 27U;
    i = i + 1;
    ldv_48007: ;
    if (i < num_indices) {
      goto ldv_48006;
    } else {
    }
    ldv_48005: ;
  } else {
  }
  return (i2c);
}
}
void amdgpu_atombios_i2c_init(struct amdgpu_device *adev )
{
  struct atom_context *ctx ;
  ATOM_GPIO_I2C_ASSIGMENT *gpio ;
  struct amdgpu_i2c_bus_rec i2c ;
  int index ;
  struct _ATOM_GPIO_I2C_INFO *i2c_info ;
  u16 data_offset ;
  u16 size ;
  int i ;
  int num_indices ;
  char stmp[32U] ;
  bool tmp ;
  {
  ctx = adev->mode_info.atom_context;
  index = 10;
  tmp = amdgpu_atom_parse_data_header(ctx, index, & size, (uint8_t *)0U, (uint8_t *)0U,
                                      & data_offset);
  if ((int )tmp) {
    i2c_info = (struct _ATOM_GPIO_I2C_INFO *)ctx->bios + (unsigned long )data_offset;
    num_indices = (int )(((unsigned long )size - 4UL) / 27UL);
    gpio = (ATOM_GPIO_I2C_ASSIGMENT *)(& i2c_info->asGPIO_Info);
    i = 0;
    goto ldv_48022;
    ldv_48021:
    amdgpu_atombios_lookup_i2c_gpio_quirks(adev, gpio, (int )((u8 )i));
    i2c = amdgpu_atombios_get_bus_rec_for_i2c_gpio(gpio);
    if ((int )i2c.valid) {
      sprintf((char *)(& stmp), "0x%x", (int )i2c.i2c_id);
      adev->i2c_bus[i] = amdgpu_i2c_create(adev->ddev, & i2c, (char const *)(& stmp));
    } else {
    }
    gpio = gpio + 27U;
    i = i + 1;
    ldv_48022: ;
    if (i < num_indices) {
      goto ldv_48021;
    } else {
    }
  } else {
  }
  return;
}
}
struct amdgpu_gpio_rec amdgpu_atombios_lookup_gpio(struct amdgpu_device *adev , u8 id )
{
  struct atom_context *ctx ;
  struct amdgpu_gpio_rec gpio ;
  int index ;
  struct _ATOM_GPIO_PIN_LUT *gpio_info ;
  ATOM_GPIO_PIN_ASSIGNMENT *pin ;
  u16 data_offset ;
  u16 size ;
  int i ;
  int num_indices ;
  bool tmp ;
  {
  ctx = adev->mode_info.atom_context;
  index = 12;
  memset((void *)(& gpio), 0, 16UL);
  gpio.valid = 0;
  tmp = amdgpu_atom_parse_data_header(ctx, index, & size, (uint8_t *)0U, (uint8_t *)0U,
                                      & data_offset);
  if ((int )tmp) {
    gpio_info = (struct _ATOM_GPIO_PIN_LUT *)ctx->bios + (unsigned long )data_offset;
    num_indices = (int )(((unsigned long )size - 4UL) / 4UL);
    pin = (ATOM_GPIO_PIN_ASSIGNMENT *)(& gpio_info->asGPIO_Pin);
    i = 0;
    goto ldv_48039;
    ldv_48038: ;
    if ((int )pin->ucGPIO_ID == (int )id) {
      gpio.id = pin->ucGPIO_ID;
      gpio.reg = (u32 )pin->usGpioPin_AIndex;
      gpio.shift = (u32 )pin->ucGpioPinBitShift;
      gpio.mask = (u32 )(1 << (int )pin->ucGpioPinBitShift);
      gpio.valid = 1;
      goto ldv_48037;
    } else {
    }
    pin = pin + 4U;
    i = i + 1;
    ldv_48039: ;
    if (i < num_indices) {
      goto ldv_48038;
    } else {
    }
    ldv_48037: ;
  } else {
  }
  return (gpio);
}
}
static struct amdgpu_hpd amdgpu_atombios_get_hpd_info_from_gpio(struct amdgpu_device *adev ,
                                                                struct amdgpu_gpio_rec *gpio )
{
  struct amdgpu_hpd hpd ;
  u32 reg ;
  {
  memset((void *)(& hpd), 0, 24UL);
  reg = (*((adev->mode_info.funcs)->hpd_get_gpio_reg))(adev);
  hpd.gpio = *gpio;
  if (gpio->reg == reg) {
    switch (gpio->mask) {
    case 1U:
    hpd.hpd = 0;
    goto ldv_48047;
    case 256U:
    hpd.hpd = 1;
    goto ldv_48047;
    case 65536U:
    hpd.hpd = 2;
    goto ldv_48047;
    case 16777216U:
    hpd.hpd = 3;
    goto ldv_48047;
    case 67108864U:
    hpd.hpd = 4;
    goto ldv_48047;
    case 268435456U:
    hpd.hpd = 5;
    goto ldv_48047;
    default:
    hpd.hpd = 255;
    goto ldv_48047;
    }
    ldv_48047: ;
  } else {
    hpd.hpd = 255;
  }
  return (hpd);
}
}
static bool amdgpu_atombios_apply_quirks(struct amdgpu_device *adev , u32 supported_device ,
                                         int *connector_type , struct amdgpu_i2c_bus_rec *i2c_bus ,
                                         u16 *line_mux , struct amdgpu_hpd *hpd )
{
  {
  return (1);
}
}
static int const object_connector_convert[22U] =
  { 0, 2, 2, 3,
        3, 1, 5, 6,
        0, 0, 9, 0,
        11, 12, 7, 9,
        0, 0, 0, 10,
        14, 0};
bool amdgpu_atombios_get_connector_info_from_object_table(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  struct atom_context *ctx ;
  int index ;
  u16 size ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  ATOM_OBJECT_TABLE *con_obj ;
  ATOM_OBJECT_TABLE *enc_obj ;
  ATOM_OBJECT_TABLE *router_obj ;
  ATOM_DISPLAY_OBJECT_PATH_TABLE *path_obj ;
  ATOM_OBJECT_HEADER *obj_header ;
  int i ;
  int j ;
  int k ;
  int path_size ;
  int device_support ;
  int connector_type ;
  u16 conn_id ;
  u16 connector_object_id ;
  struct amdgpu_i2c_bus_rec ddc_bus ;
  struct amdgpu_router router ;
  struct amdgpu_gpio_rec gpio ;
  struct amdgpu_hpd hpd ;
  bool tmp ;
  int tmp___0 ;
  uint8_t *addr ;
  ATOM_DISPLAY_OBJECT_PATH *path ;
  uint8_t con_obj_id ;
  uint8_t con_obj_num ;
  uint8_t con_obj_type ;
  uint8_t grph_obj_id ;
  uint8_t grph_obj_num ;
  uint8_t grph_obj_type ;
  u16 encoder_obj ;
  ATOM_COMMON_RECORD_HEADER *record ;
  ATOM_ENCODER_CAP_RECORD *cap_record ;
  u16 caps ;
  u16 router_obj_id ;
  ATOM_COMMON_RECORD_HEADER *record___0 ;
  ATOM_I2C_RECORD *i2c_record ;
  ATOM_I2C_ID_CONFIG_ACCESS *i2c_config ;
  ATOM_ROUTER_DDC_PATH_SELECT_RECORD *ddc_path ;
  ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *cd_path ;
  ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *router_src_dst_table ;
  u8 *num_dst_objs ;
  u16 *dst_objs ;
  int enum_id ;
  ATOM_COMMON_RECORD_HEADER *record___1 ;
  ATOM_I2C_RECORD *i2c_record___0 ;
  ATOM_HPD_INT_RECORD *hpd_record ;
  ATOM_I2C_ID_CONFIG_ACCESS *i2c_config___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  {
  mode_info = & adev->mode_info;
  ctx = mode_info->atom_context;
  index = 22;
  tmp = amdgpu_atom_parse_data_header(ctx, index, & size, & frev, & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (0);
  } else {
  }
  if ((unsigned int )crev <= 1U) {
    return (0);
  } else {
  }
  obj_header = (ATOM_OBJECT_HEADER *)ctx->bios + (unsigned long )data_offset;
  path_obj = (ATOM_DISPLAY_OBJECT_PATH_TABLE *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )obj_header->usDisplayPathTableOffset));
  con_obj = (ATOM_OBJECT_TABLE *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )obj_header->usConnectorObjectTableOffset));
  enc_obj = (ATOM_OBJECT_TABLE *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )obj_header->usEncoderObjectTableOffset));
  router_obj = (ATOM_OBJECT_TABLE *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )obj_header->usRouterObjectTableOffset));
  device_support = (int )obj_header->usDeviceSupport;
  path_size = 0;
  i = 0;
  goto ldv_48151;
  ldv_48150:
  addr = (uint8_t *)(& path_obj->asDispPath);
  addr = addr + (unsigned long )path_size;
  path = (ATOM_DISPLAY_OBJECT_PATH *)addr;
  path_size = (int )path->usSize + path_size;
  if (((int )path->usDeviceTag & device_support) != 0) {
    con_obj_id = (uint8_t )path->usConnObjectId;
    con_obj_num = (uint8_t )(((int )path->usConnObjectId & 1792) >> 8);
    con_obj_type = (uint8_t )(((int )path->usConnObjectId & 28672) >> 12);
    connector_type = object_connector_convert[(int )con_obj_id];
    connector_object_id = (u16 )con_obj_id;
    if (connector_type == 0) {
      goto ldv_48095;
    } else {
    }
    router.ddc_valid = 0;
    router.cd_valid = 0;
    j = 0;
    goto ldv_48135;
    ldv_48134:
    grph_obj_id = (uint8_t )path->usGraphicObjIds[j];
    grph_obj_num = (uint8_t )(((int )path->usGraphicObjIds[j] & 1792) >> 8);
    grph_obj_type = (uint8_t )(((int )path->usGraphicObjIds[j] & 28672) >> 12);
    if ((unsigned int )grph_obj_type == 2U) {
      k = 0;
      goto ldv_48109;
      ldv_48108:
      encoder_obj = enc_obj->asObjects[k].usObjectID;
      if ((int )path->usGraphicObjIds[j] == (int )encoder_obj) {
        record = (ATOM_COMMON_RECORD_HEADER *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )enc_obj->asObjects[k].usRecordOffset));
        caps = 0U;
        goto ldv_48106;
        ldv_48105: ;
        switch ((int )record->ucRecordType) {
        case 20:
        cap_record = (ATOM_ENCODER_CAP_RECORD *)record;
        caps = cap_record->__annonCompField110.usEncoderCap;
        goto ldv_48104;
        }
        ldv_48104:
        record = record + (unsigned long )record->ucRecordSize;
        ldv_48106: ;
        if (((unsigned int )record->ucRecordSize != 0U && (unsigned int )record->ucRecordType != 0U) && (unsigned int )record->ucRecordType <= 20U) {
          goto ldv_48105;
        } else {
        }
        (*((adev->mode_info.funcs)->add_encoder))(adev, (u32 )encoder_obj, (u32 )path->usDeviceTag,
                                                  (int )caps);
      } else {
      }
      k = k + 1;
      ldv_48109: ;
      if ((int )enc_obj->ucNumberOfObjects > k) {
        goto ldv_48108;
      } else {
      }
    } else
    if ((unsigned int )grph_obj_type == 4U) {
      k = 0;
      goto ldv_48132;
      ldv_48131:
      router_obj_id = router_obj->asObjects[k].usObjectID;
      if ((int )path->usGraphicObjIds[j] == (int )router_obj_id) {
        record___0 = (ATOM_COMMON_RECORD_HEADER *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )router_obj->asObjects[k].usRecordOffset));
        router_src_dst_table = (ATOM_SRC_DST_TABLE_FOR_ONE_OBJECT *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )router_obj->asObjects[k].usSrcDstTableOffset));
        num_dst_objs = (u8 *)router_src_dst_table + ((unsigned long )((int )router_src_dst_table->ucNumberOfSrc * 2) + 1UL);
        dst_objs = (u16 *)num_dst_objs + 1U;
        router.router_id = (u32 )router_obj_id;
        enum_id = 0;
        goto ldv_48123;
        ldv_48122: ;
        if ((int )path->usConnObjectId == (int )*(dst_objs + (unsigned long )enum_id)) {
          goto ldv_48121;
        } else {
        }
        enum_id = enum_id + 1;
        ldv_48123: ;
        if ((int )*num_dst_objs > enum_id) {
          goto ldv_48122;
        } else {
        }
        ldv_48121: ;
        goto ldv_48129;
        ldv_48128: ;
        switch ((int )record___0->ucRecordType) {
        case 1:
        i2c_record = (ATOM_I2C_RECORD *)record___0;
        i2c_config = (ATOM_I2C_ID_CONFIG_ACCESS *)(& i2c_record->sucI2cId);
        router.i2c_info = amdgpu_atombios_lookup_i2c_gpio(adev, (int )i2c_config->ucAccess);
        router.i2c_addr = (u8 )((int )i2c_record->ucI2CAddr >> 1);
        goto ldv_48125;
        case 14:
        ddc_path = (ATOM_ROUTER_DDC_PATH_SELECT_RECORD *)record___0;
        router.ddc_valid = 1;
        router.ddc_mux_type = ddc_path->ucMuxType;
        router.ddc_mux_control_pin = ddc_path->ucMuxControlPin;
        router.ddc_mux_state = ddc_path->ucMuxState[enum_id];
        goto ldv_48125;
        case 15:
        cd_path = (ATOM_ROUTER_DATA_CLOCK_PATH_SELECT_RECORD *)record___0;
        router.cd_valid = 1;
        router.cd_mux_type = cd_path->ucMuxType;
        router.cd_mux_control_pin = cd_path->ucMuxControlPin;
        router.cd_mux_state = cd_path->ucMuxState[enum_id];
        goto ldv_48125;
        }
        ldv_48125:
        record___0 = record___0 + (unsigned long )record___0->ucRecordSize;
        ldv_48129: ;
        if (((unsigned int )record___0->ucRecordSize != 0U && (unsigned int )record___0->ucRecordType != 0U) && (unsigned int )record___0->ucRecordType <= 20U) {
          goto ldv_48128;
        } else {
        }
      } else {
      }
      k = k + 1;
      ldv_48132: ;
      if ((int )router_obj->ucNumberOfObjects > k) {
        goto ldv_48131;
      } else {
      }
    } else {
    }
    j = j + 1;
    ldv_48135: ;
    if (((int )path->usSize + -8) / 2 > j) {
      goto ldv_48134;
    } else {
    }
    ddc_bus.valid = 0;
    hpd.hpd = 255;
    if (((long )path->usDeviceTag & 260L) == 0L) {
      j = 0;
      goto ldv_48149;
      ldv_48148: ;
      if ((int )path->usConnObjectId == (int )con_obj->asObjects[j].usObjectID) {
        record___1 = (ATOM_COMMON_RECORD_HEADER *)(ctx->bios + ((unsigned long )data_offset + (unsigned long )con_obj->asObjects[j].usRecordOffset));
        goto ldv_48145;
        ldv_48144: ;
        switch ((int )record___1->ucRecordType) {
        case 1:
        i2c_record___0 = (ATOM_I2C_RECORD *)record___1;
        i2c_config___0 = (ATOM_I2C_ID_CONFIG_ACCESS *)(& i2c_record___0->sucI2cId);
        ddc_bus = amdgpu_atombios_lookup_i2c_gpio(adev, (int )i2c_config___0->ucAccess);
        goto ldv_48142;
        case 2:
        hpd_record = (ATOM_HPD_INT_RECORD *)record___1;
        gpio = amdgpu_atombios_lookup_gpio(adev, (int )hpd_record->ucHPDIntGPIOID);
        hpd = amdgpu_atombios_get_hpd_info_from_gpio(adev, & gpio);
        hpd.plugged_state = hpd_record->ucPlugged_PinState;
        goto ldv_48142;
        }
        ldv_48142:
        record___1 = record___1 + (unsigned long )record___1->ucRecordSize;
        ldv_48145: ;
        if (((unsigned int )record___1->ucRecordSize != 0U && (unsigned int )record___1->ucRecordType != 0U) && (unsigned int )record___1->ucRecordType <= 20U) {
          goto ldv_48144;
        } else {
        }
        goto ldv_48147;
      } else {
      }
      j = j + 1;
      ldv_48149: ;
      if ((int )con_obj->ucNumberOfObjects > j) {
        goto ldv_48148;
      } else {
      }
      ldv_48147: ;
    } else {
    }
    ddc_bus.hpd = hpd.hpd;
    conn_id = path->usConnObjectId;
    tmp___1 = amdgpu_atombios_apply_quirks(adev, (u32 )path->usDeviceTag, & connector_type,
                                           & ddc_bus, & conn_id, & hpd);
    if (tmp___1) {
      tmp___2 = 0;
    } else {
      tmp___2 = 1;
    }
    if (tmp___2) {
      goto ldv_48095;
    } else {
    }
    (*((adev->mode_info.funcs)->add_connector))(adev, (u32 )conn_id, (u32 )path->usDeviceTag,
                                                connector_type, & ddc_bus, (int )connector_object_id,
                                                & hpd, & router);
  } else {
  }
  ldv_48095:
  i = i + 1;
  ldv_48151: ;
  if ((int )path_obj->ucNumOfDispPath > i) {
    goto ldv_48150;
  } else {
  }
  amdgpu_link_encoder_connector(adev->ddev);
  return (1);
}
}
int amdgpu_atombios_get_clock_info(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  u16 data_offset ;
  int ret ;
  int i ;
  struct amdgpu_pll *ppll ;
  struct amdgpu_pll *spll ;
  struct amdgpu_pll *mpll ;
  union firmware_info *firmware_info ;
  bool tmp ;
  {
  mode_info = & adev->mode_info;
  index = 4;
  ret = -22;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if ((int )tmp) {
    ppll = (struct amdgpu_pll *)(& adev->clock.ppll);
    spll = & adev->clock.spll;
    mpll = & adev->clock.mpll;
    firmware_info = (union firmware_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    ppll->reference_freq = (u32 )firmware_info->info.usReferenceClock;
    ppll->reference_div = 0U;
    if ((unsigned int )crev <= 1U) {
      ppll->pll_out_min = (u32 )firmware_info->info.usMinPixelClockPLL_Output;
    } else {
      ppll->pll_out_min = firmware_info->info_12.ulMinPixelClockPLL_Output;
    }
    ppll->pll_out_max = firmware_info->info.ulMaxPixelClockPLL_Output;
    if ((unsigned int )crev > 3U) {
      ppll->lcd_pll_out_min = (u32 )((int )firmware_info->info_14.usLcdMinPixelClockPLL_Output * 100);
      if (ppll->lcd_pll_out_min == 0U) {
        ppll->lcd_pll_out_min = ppll->pll_out_min;
      } else {
      }
      ppll->lcd_pll_out_max = (u32 )((int )firmware_info->info_14.usLcdMaxPixelClockPLL_Output * 100);
      if (ppll->lcd_pll_out_max == 0U) {
        ppll->lcd_pll_out_max = ppll->pll_out_max;
      } else {
      }
    } else {
      ppll->lcd_pll_out_min = ppll->pll_out_min;
      ppll->lcd_pll_out_max = ppll->pll_out_max;
    }
    if (ppll->pll_out_min == 0U) {
      ppll->pll_out_min = 64800U;
    } else {
    }
    ppll->pll_in_min = (u32 )firmware_info->info.usMinPixelClockPLL_Input;
    ppll->pll_in_max = (u32 )firmware_info->info.usMaxPixelClockPLL_Input;
    ppll->min_post_div = 2U;
    ppll->max_post_div = 127U;
    ppll->min_frac_feedback_div = 0U;
    ppll->max_frac_feedback_div = 9U;
    ppll->min_ref_div = 2U;
    ppll->max_ref_div = 1023U;
    ppll->min_feedback_div = 4U;
    ppll->max_feedback_div = 4095U;
    ppll->best_vco = 0U;
    i = 1;
    goto ldv_48175;
    ldv_48174:
    adev->clock.ppll[i] = *ppll;
    i = i + 1;
    ldv_48175: ;
    if (i <= 2) {
      goto ldv_48174;
    } else {
    }
    spll->reference_freq = (u32 )firmware_info->info_21.usCoreReferenceClock;
    spll->reference_div = 0U;
    spll->pll_out_min = (u32 )firmware_info->info.usMinEngineClockPLL_Output;
    spll->pll_out_max = firmware_info->info.ulMaxEngineClockPLL_Output;
    if (spll->pll_out_min == 0U) {
      spll->pll_out_min = 64800U;
    } else {
    }
    spll->pll_in_min = (u32 )firmware_info->info.usMinEngineClockPLL_Input;
    spll->pll_in_max = (u32 )firmware_info->info.usMaxEngineClockPLL_Input;
    spll->min_post_div = 1U;
    spll->max_post_div = 1U;
    spll->min_ref_div = 2U;
    spll->max_ref_div = 255U;
    spll->min_feedback_div = 4U;
    spll->max_feedback_div = 255U;
    spll->best_vco = 0U;
    mpll->reference_freq = (u32 )firmware_info->info_21.usMemoryReferenceClock;
    mpll->reference_div = 0U;
    mpll->pll_out_min = (u32 )firmware_info->info.usMinMemoryClockPLL_Output;
    mpll->pll_out_max = firmware_info->info.ulMaxMemoryClockPLL_Output;
    if (mpll->pll_out_min == 0U) {
      mpll->pll_out_min = 64800U;
    } else {
    }
    mpll->pll_in_min = (u32 )firmware_info->info.usMinMemoryClockPLL_Input;
    mpll->pll_in_max = (u32 )firmware_info->info.usMaxMemoryClockPLL_Input;
    adev->clock.default_sclk = firmware_info->info.ulDefaultEngineClock;
    adev->clock.default_mclk = firmware_info->info.ulDefaultMemoryClock;
    mpll->min_post_div = 1U;
    mpll->max_post_div = 1U;
    mpll->min_ref_div = 2U;
    mpll->max_ref_div = 255U;
    mpll->min_feedback_div = 4U;
    mpll->max_feedback_div = 255U;
    mpll->best_vco = 0U;
    adev->clock.default_dispclk = firmware_info->info_21.ulDefaultDispEngineClkFreq;
    if (adev->clock.default_dispclk == 0U) {
      adev->clock.default_dispclk = 54000U;
    } else {
    }
    adev->clock.dp_extclk = (u32 )firmware_info->info_21.usUniphyDPModeExtClkFreq;
    adev->clock.current_dispclk = adev->clock.default_dispclk;
    adev->clock.max_pixel_clock = (u32 )firmware_info->info.usMaxPixelClock;
    if (adev->clock.max_pixel_clock == 0U) {
      adev->clock.max_pixel_clock = 40000U;
    } else {
    }
    adev->mode_info.firmware_flags = firmware_info->info.usFirmwareCapability.susAccess;
    ret = 0;
  } else {
  }
  adev->pm.current_sclk = adev->clock.default_sclk;
  adev->pm.current_mclk = adev->clock.default_mclk;
  return (ret);
}
}
static void amdgpu_atombios_get_igp_ss_overrides(struct amdgpu_device *adev , struct amdgpu_atom_ss *ss ,
                                                 int id )
{
  struct amdgpu_mode_info *mode_info ;
  int index ;
  u16 data_offset ;
  u16 size ;
  union igp_info *igp_info ;
  u8 frev ;
  u8 crev ;
  u16 percentage ;
  u16 rate ;
  bool tmp ;
  {
  mode_info = & adev->mode_info;
  index = 30;
  percentage = 0U;
  rate = 0U;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, & size, & frev,
                                      & crev, & data_offset);
  if ((int )tmp) {
    igp_info = (union igp_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    switch ((int )crev) {
    case 6: ;
    switch (id) {
    case 4:
    percentage = igp_info->info_6.usDVISSPercentage;
    rate = igp_info->info_6.usDVISSpreadRateIn10Hz;
    goto ldv_48200;
    case 5:
    percentage = igp_info->info_6.usHDMISSPercentage;
    rate = igp_info->info_6.usHDMISSpreadRateIn10Hz;
    goto ldv_48200;
    case 6:
    percentage = igp_info->info_6.usLvdsSSPercentage;
    rate = igp_info->info_6.usLvdsSSpreadRateIn10Hz;
    goto ldv_48200;
    }
    ldv_48200: ;
    goto ldv_48203;
    case 7: ;
    switch (id) {
    case 4:
    percentage = igp_info->info_7.usDVISSPercentage;
    rate = igp_info->info_7.usDVISSpreadRateIn10Hz;
    goto ldv_48206;
    case 5:
    percentage = igp_info->info_7.usHDMISSPercentage;
    rate = igp_info->info_7.usHDMISSpreadRateIn10Hz;
    goto ldv_48206;
    case 6:
    percentage = igp_info->info_7.usLvdsSSPercentage;
    rate = igp_info->info_7.usLvdsSSpreadRateIn10Hz;
    goto ldv_48206;
    }
    ldv_48206: ;
    goto ldv_48203;
    case 8: ;
    switch (id) {
    case 4:
    percentage = igp_info->info_8.usDVISSPercentage;
    rate = igp_info->info_8.usDVISSpreadRateIn10Hz;
    goto ldv_48211;
    case 5:
    percentage = igp_info->info_8.usHDMISSPercentage;
    rate = igp_info->info_8.usHDMISSpreadRateIn10Hz;
    goto ldv_48211;
    case 6:
    percentage = igp_info->info_8.usLvdsSSPercentage;
    rate = igp_info->info_8.usLvdsSSpreadRateIn10Hz;
    goto ldv_48211;
    }
    ldv_48211: ;
    goto ldv_48203;
    case 9: ;
    switch (id) {
    case 4:
    percentage = igp_info->info_9.usDVISSPercentage;
    rate = igp_info->info_9.usDVISSpreadRateIn10Hz;
    goto ldv_48216;
    case 5:
    percentage = igp_info->info_9.usHDMISSPercentage;
    rate = igp_info->info_9.usHDMISSpreadRateIn10Hz;
    goto ldv_48216;
    case 6:
    percentage = igp_info->info_9.usLvdsSSPercentage;
    rate = igp_info->info_9.usLvdsSSpreadRateIn10Hz;
    goto ldv_48216;
    }
    ldv_48216: ;
    goto ldv_48203;
    default:
    drm_err("Unsupported IGP table: %d %d\n", (int )frev, (int )crev);
    goto ldv_48203;
    }
    ldv_48203: ;
    if ((unsigned int )percentage != 0U) {
      ss->percentage = percentage;
    } else {
    }
    if ((unsigned int )rate != 0U) {
      ss->rate = rate;
    } else {
    }
  } else {
  }
  return;
}
}
bool amdgpu_atombios_get_asic_ss_info(struct amdgpu_device *adev , struct amdgpu_atom_ss *ss ,
                                      int id , u32 clock )
{
  struct amdgpu_mode_info *mode_info ;
  int index ;
  u16 data_offset ;
  u16 size ;
  union asic_ss_info *ss_info ;
  union asic_ss_assignment *ss_assign ;
  uint8_t frev ;
  uint8_t crev ;
  int i ;
  int num_indices ;
  bool tmp ;
  {
  mode_info = & adev->mode_info;
  index = 26;
  if (id == 1) {
    if (((int )adev->mode_info.firmware_flags & 8) == 0) {
      return (0);
    } else {
    }
  } else {
  }
  if (id == 2) {
    if (((int )adev->mode_info.firmware_flags & 16) == 0) {
      return (0);
    } else {
    }
  } else {
  }
  memset((void *)ss, 0, 16UL);
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, & size, & frev,
                                      & crev, & data_offset);
  if ((int )tmp) {
    ss_info = (union asic_ss_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    switch ((int )frev) {
    case 1:
    num_indices = (int )(((unsigned long )size - 4UL) / 12UL);
    ss_assign = (union asic_ss_assignment *)(& ss_info->info.asSpreadSpectrum);
    i = 0;
    goto ldv_48246;
    ldv_48245: ;
    if ((int )ss_assign->v1.ucClockIndication == id && ss_assign->v1.ulTargetClockRange >= clock) {
      ss->percentage = ss_assign->v1.usSpreadSpectrumPercentage;
      ss->type = ss_assign->v1.ucSpreadSpectrumMode;
      ss->rate = ss_assign->v1.usSpreadRateInKhz;
      ss->percentage_divider = 100U;
      return (1);
    } else {
    }
    ss_assign = ss_assign + 12U;
    i = i + 1;
    ldv_48246: ;
    if (i < num_indices) {
      goto ldv_48245;
    } else {
    }
    goto ldv_48248;
    case 2:
    num_indices = (int )(((unsigned long )size - 4UL) / 12UL);
    ss_assign = (union asic_ss_assignment *)(& ss_info->info_2.asSpreadSpectrum);
    i = 0;
    goto ldv_48251;
    ldv_48250: ;
    if ((int )ss_assign->v2.ucClockIndication == id && ss_assign->v2.ulTargetClockRange >= clock) {
      ss->percentage = ss_assign->v2.usSpreadSpectrumPercentage;
      ss->type = ss_assign->v2.ucSpreadSpectrumMode;
      ss->rate = ss_assign->v2.usSpreadRateIn10Hz;
      ss->percentage_divider = 100U;
      if ((unsigned int )crev == 2U && (id == 2 || id == 1)) {
        ss->rate = (u16 )((unsigned int )ss->rate / 100U);
      } else {
      }
      return (1);
    } else {
    }
    ss_assign = ss_assign + 12U;
    i = i + 1;
    ldv_48251: ;
    if (i < num_indices) {
      goto ldv_48250;
    } else {
    }
    goto ldv_48248;
    case 3:
    num_indices = (int )(((unsigned long )size - 4UL) / 12UL);
    ss_assign = (union asic_ss_assignment *)(& ss_info->info_3.asSpreadSpectrum);
    i = 0;
    goto ldv_48255;
    ldv_48254: ;
    if ((int )ss_assign->v3.ucClockIndication == id && ss_assign->v3.ulTargetClockRange >= clock) {
      ss->percentage = ss_assign->v3.usSpreadSpectrumPercentage;
      ss->type = ss_assign->v3.ucSpreadSpectrumMode;
      ss->rate = ss_assign->v3.usSpreadRateIn10Hz;
      if (((int )ss_assign->v3.ucSpreadSpectrumMode & 16) != 0) {
        ss->percentage_divider = 1000U;
      } else {
        ss->percentage_divider = 100U;
      }
      if (id == 2 || id == 1) {
        ss->rate = (u16 )((unsigned int )ss->rate / 100U);
      } else {
      }
      if ((adev->flags & 131072UL) != 0UL) {
        amdgpu_atombios_get_igp_ss_overrides(adev, ss, id);
      } else {
      }
      return (1);
    } else {
    }
    ss_assign = ss_assign + 12U;
    i = i + 1;
    ldv_48255: ;
    if (i < num_indices) {
      goto ldv_48254;
    } else {
    }
    goto ldv_48248;
    default:
    drm_err("Unsupported ASIC_InternalSS_Info table: %d %d\n", (int )frev, (int )crev);
    goto ldv_48248;
    }
    ldv_48248: ;
  } else {
  }
  return (0);
}
}
int amdgpu_atombios_get_clock_dividers(struct amdgpu_device *adev , u8 clock_type ,
                                       u32 clock , bool strobe_mode , struct atom_clock_dividers *dividers )
{
  union get_clock_dividers args ;
  int index ;
  u8 frev ;
  u8 crev ;
  bool tmp ;
  int tmp___0 ;
  u32 tmp___1 ;
  {
  index = 60;
  memset((void *)(& args), 0, 12UL);
  memset((void *)dividers, 0, 32UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  switch ((int )crev) {
  case 4:
  args.v4.ulClock = clock;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  tmp___1 = (u32 )args.v4.ucPostDiv;
  dividers->post_div = tmp___1;
  dividers->post_divider = tmp___1;
  dividers->real_clock = args.v4.ulClock;
  goto ldv_48278;
  case 6:
  args.v6_in.ulClock.ulComputeClockFlag = clock_type;
  args.v6_in.ulClock.ulClockFreq = clock;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  dividers->__annonCompField82.__annonCompField81.whole_fb_div = args.v6_out.ulFbDiv.usFbDiv;
  dividers->__annonCompField82.__annonCompField81.frac_fb_div = args.v6_out.ulFbDiv.usFbDivFrac;
  dividers->ref_div = (u32 )args.v6_out.ucPllRefDiv;
  dividers->post_div = (u32 )args.v6_out.ucPllPostDiv;
  dividers->flags = (u32 )args.v6_out.ucPllCntlFlag;
  dividers->real_clock = args.v6_out.ulClock.ulClock;
  dividers->post_divider = (u32 )args.v6_out.ulClock.ucPostDiv;
  goto ldv_48278;
  default: ;
  return (-22);
  }
  ldv_48278: ;
  return (0);
}
}
int amdgpu_atombios_get_memory_pll_dividers(struct amdgpu_device *adev , u32 clock ,
                                            bool strobe_mode , struct atom_mpll_param *mpll_param )
{
  COMPUTE_MEMORY_CLOCK_PARAM_PARAMETERS_V2_1 args ;
  int index ;
  u8 frev ;
  u8 crev ;
  bool tmp ;
  int tmp___0 ;
  {
  index = 70;
  memset((void *)(& args), 0, 8UL);
  memset((void *)mpll_param, 0, 32UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  switch ((int )frev) {
  case 2: ;
  switch ((int )crev) {
  case 1:
  args.__annonCompField88.ulClock = clock;
  args.__annonCompField89.ucInputFlag = 0U;
  if ((int )strobe_mode) {
    args.__annonCompField89.ucInputFlag = (UCHAR )((unsigned int )args.__annonCompField89.ucInputFlag | 1U);
  } else {
  }
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  mpll_param->__annonCompField84.__annonCompField83.clkfrac = args.__annonCompField88.ulFbDiv.usFbDivFrac;
  mpll_param->__annonCompField84.__annonCompField83.clkf = args.__annonCompField88.ulFbDiv.usFbDiv;
  mpll_param->post_div = (u32 )args.ucPostDiv;
  mpll_param->dll_speed = (u32 )args.ucDllSpeed;
  mpll_param->bwcntl = (u32 )args.ucBWCntl;
  mpll_param->vco_mode = (u32 )args.__annonCompField89.ucPllCntlFlag & 3U;
  mpll_param->yclk_sel = ((int )args.__annonCompField89.ucPllCntlFlag & 4) != 0;
  mpll_param->qdr = ((int )args.__annonCompField89.ucPllCntlFlag & 8) != 0;
  mpll_param->half_rate = ((int )args.__annonCompField89.ucPllCntlFlag & 16) != 0;
  goto ldv_48293;
  default: ;
  return (-22);
  }
  ldv_48293: ;
  goto ldv_48295;
  default: ;
  return (-22);
  }
  ldv_48295: ;
  return (0);
}
}
u32 amdgpu_atombios_get_engine_clock(struct amdgpu_device *adev )
{
  GET_ENGINE_CLOCK_PARAMETERS args ;
  int index ;
  {
  index = 48;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return (args.ulReturnEngineClock);
}
}
u32 amdgpu_atombios_get_memory_clock(struct amdgpu_device *adev )
{
  GET_MEMORY_CLOCK_PARAMETERS args ;
  int index ;
  {
  index = 47;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return (args.ulReturnMemoryClock);
}
}
void amdgpu_atombios_set_engine_clock(struct amdgpu_device *adev , u32 eng_clock )
{
  SET_ENGINE_CLOCK_PS_ALLOCATION args ;
  int index ;
  {
  index = 10;
  args.ulTargetEngineClock = eng_clock;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_set_memory_clock(struct amdgpu_device *adev , u32 mem_clock )
{
  SET_MEMORY_CLOCK_PS_ALLOCATION args ;
  int index ;
  {
  index = 11;
  if ((adev->flags & 131072UL) != 0UL) {
    return;
  } else {
  }
  args.ulTargetMemoryClock = mem_clock;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_set_engine_dram_timings(struct amdgpu_device *adev , u32 eng_clock ,
                                             u32 mem_clock )
{
  SET_ENGINE_CLOCK_PS_ALLOCATION args ;
  int index ;
  u32 tmp ;
  {
  index = 63;
  memset((void *)(& args), 0, 12UL);
  tmp = eng_clock & 16777215U;
  tmp = tmp | 33554432U;
  args.ulTargetEngineClock = tmp;
  if (mem_clock != 0U) {
    args.sReserved.ulClock = mem_clock & 16777215U;
  } else {
  }
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_set_voltage(struct amdgpu_device *adev , u16 voltage_level ,
                                 u8 voltage_type )
{
  union set_voltage args ;
  int index ;
  u8 frev ;
  u8 crev ;
  u8 volt_index ;
  bool tmp ;
  int tmp___0 ;
  {
  index = 67;
  volt_index = (u8 )voltage_level;
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  if ((unsigned int )voltage_level == 65281U) {
    return;
  } else {
  }
  switch ((int )crev) {
  case 1:
  args.v1.ucVoltageType = voltage_type;
  args.v1.ucVoltageMode = 1U;
  args.v1.ucVoltageIndex = volt_index;
  goto ldv_48343;
  case 2:
  args.v2.ucVoltageType = voltage_type;
  args.v2.ucVoltageMode = 0U;
  args.v2.usVoltageLevel = voltage_level;
  goto ldv_48343;
  case 3:
  args.v3.ucVoltageType = voltage_type;
  args.v3.ucVoltageMode = 0U;
  args.v3.usVoltageLevel = voltage_level;
  goto ldv_48343;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48343:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
int amdgpu_atombios_get_leakage_id_from_vbios(struct amdgpu_device *adev , u16 *leakage_id )
{
  union set_voltage args ;
  int index ;
  u8 frev ;
  u8 crev ;
  bool tmp ;
  int tmp___0 ;
  {
  index = 67;
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  switch ((int )crev) {
  case 3: ;
  case 4:
  args.v3.ucVoltageType = 0U;
  args.v3.ucVoltageMode = 8U;
  args.v3.usVoltageLevel = 0U;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  *leakage_id = args.v3.usVoltageLevel;
  goto ldv_48357;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  return (-22);
  }
  ldv_48357: ;
  return (0);
}
}
int amdgpu_atombios_get_leakage_vddc_based_on_leakage_params(struct amdgpu_device *adev ,
                                                             u16 *vddc , u16 *vddci ,
                                                             u16 virtual_voltage_id ,
                                                             u16 vbios_voltage_id )
{
  int index ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  u16 size ;
  int i ;
  int j ;
  ATOM_ASIC_PROFILING_INFO_V2_1 *profile ;
  u16 *leakage_bin ;
  u16 *vddc_id_buf ;
  u16 *vddc_buf ;
  u16 *vddci_id_buf ;
  u16 *vddci_buf ;
  bool tmp ;
  int tmp___0 ;
  {
  index = 31;
  *vddc = 0U;
  *vddci = 0U;
  tmp = amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, & size,
                                      & frev, & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  profile = (ATOM_ASIC_PROFILING_INFO_V2_1 *)(adev->mode_info.atom_context)->bios + (unsigned long )data_offset;
  switch ((int )frev) {
  case 1: ;
  return (-22);
  case 2: ;
  switch ((int )crev) {
  case 1: ;
  if ((unsigned int )size <= 16U) {
    return (-22);
  } else {
  }
  leakage_bin = (u16 *)((adev->mode_info.atom_context)->bios + ((unsigned long )data_offset + (unsigned long )profile->usLeakageBinArrayOffset));
  vddc_id_buf = (u16 *)((adev->mode_info.atom_context)->bios + ((unsigned long )data_offset + (unsigned long )profile->usElbVDDC_IdArrayOffset));
  vddc_buf = (u16 *)((adev->mode_info.atom_context)->bios + ((unsigned long )data_offset + (unsigned long )profile->usElbVDDC_LevelArrayOffset));
  vddci_id_buf = (u16 *)((adev->mode_info.atom_context)->bios + ((unsigned long )data_offset + (unsigned long )profile->usElbVDDCI_IdArrayOffset));
  vddci_buf = (u16 *)((adev->mode_info.atom_context)->bios + ((unsigned long )data_offset + (unsigned long )profile->usElbVDDCI_LevelArrayOffset));
  if ((unsigned int )profile->ucElbVDDC_Num != 0U) {
    i = 0;
    goto ldv_48387;
    ldv_48386: ;
    if ((int )*(vddc_id_buf + (unsigned long )i) == (int )virtual_voltage_id) {
      j = 0;
      goto ldv_48384;
      ldv_48383: ;
      if ((int )*(leakage_bin + (unsigned long )j) >= (int )vbios_voltage_id) {
        *vddc = *(vddc_buf + (unsigned long )((int )profile->ucElbVDDC_Num * j + i));
        goto ldv_48382;
      } else {
      }
      j = j + 1;
      ldv_48384: ;
      if ((int )profile->ucLeakageBinNum > j) {
        goto ldv_48383;
      } else {
      }
      ldv_48382: ;
      goto ldv_48385;
    } else {
    }
    i = i + 1;
    ldv_48387: ;
    if ((int )profile->ucElbVDDC_Num > i) {
      goto ldv_48386;
    } else {
    }
    ldv_48385: ;
  } else {
  }
  if ((unsigned int )profile->ucElbVDDCI_Num != 0U) {
    i = 0;
    goto ldv_48393;
    ldv_48392: ;
    if ((int )*(vddci_id_buf + (unsigned long )i) == (int )virtual_voltage_id) {
      j = 0;
      goto ldv_48390;
      ldv_48389: ;
      if ((int )*(leakage_bin + (unsigned long )j) >= (int )vbios_voltage_id) {
        *vddci = *(vddci_buf + (unsigned long )((int )profile->ucElbVDDCI_Num * j + i));
        goto ldv_48388;
      } else {
      }
      j = j + 1;
      ldv_48390: ;
      if ((int )profile->ucLeakageBinNum > j) {
        goto ldv_48389;
      } else {
      }
      ldv_48388: ;
      goto ldv_48391;
    } else {
    }
    i = i + 1;
    ldv_48393: ;
    if ((int )profile->ucElbVDDCI_Num > i) {
      goto ldv_48392;
    } else {
    }
    ldv_48391: ;
  } else {
  }
  goto ldv_48394;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  return (-22);
  }
  ldv_48394: ;
  goto ldv_48396;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  return (-22);
  }
  ldv_48396: ;
  return (0);
}
}
int amdgpu_atombios_get_voltage_evv(struct amdgpu_device *adev , u16 virtual_voltage_id ,
                                    u16 *voltage )
{
  int index ;
  u32 entry_id ;
  u32 count ;
  union get_voltage_info args ;
  {
  index = 80;
  count = adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.count;
  entry_id = 0U;
  goto ldv_48412;
  ldv_48411: ;
  if ((int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries + (unsigned long )entry_id)->v == (int )virtual_voltage_id) {
    goto ldv_48410;
  } else {
  }
  entry_id = entry_id + 1U;
  ldv_48412: ;
  if (entry_id < count) {
    goto ldv_48411;
  } else {
  }
  ldv_48410: ;
  if (entry_id >= count) {
    return (-22);
  } else {
  }
  args.in.ucVoltageType = 1U;
  args.in.ucVoltageMode = 9U;
  args.in.usVoltageLevel = virtual_voltage_id;
  args.in.ulSCLKFreq = (adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries + (unsigned long )entry_id)->clk;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  *voltage = args.evv_out.usVoltageLevel;
  return (0);
}
}
static ATOM_VOLTAGE_OBJECT_V3 *amdgpu_atombios_lookup_voltage_object_v3(ATOM_VOLTAGE_OBJECT_INFO_V3_1 *v3 ,
                                                                        u8 voltage_type ,
                                                                        u8 voltage_mode )
{
  u32 size ;
  u32 offset ;
  u8 *start ;
  ATOM_VOLTAGE_OBJECT_V3 *vo ;
  {
  size = (u32 )v3->sHeader.usStructureSize;
  offset = 4U;
  start = (u8 *)v3;
  goto ldv_48431;
  ldv_48430:
  vo = (ATOM_VOLTAGE_OBJECT_V3 *)start + (unsigned long )offset;
  if ((int )vo->asGpioVoltageObj.sHeader.ucVoltageType == (int )voltage_type && (int )vo->asGpioVoltageObj.sHeader.ucVoltageMode == (int )voltage_mode) {
    return (vo);
  } else {
  }
  offset = (u32 )vo->asGpioVoltageObj.sHeader.usSize + offset;
  ldv_48431: ;
  if (offset < size) {
    goto ldv_48430;
  } else {
  }
  return ((ATOM_VOLTAGE_OBJECT_V3 *)0);
}
}
bool amdgpu_atombios_is_voltage_gpio(struct amdgpu_device *adev , u8 voltage_type ,
                                     u8 voltage_mode )
{
  int index ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  u16 size ;
  union voltage_object_info *voltage_info ;
  ATOM_VOLTAGE_OBJECT_V3 *tmp ;
  bool tmp___0 ;
  {
  index = 32;
  tmp___0 = amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, & size,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___0) {
    voltage_info = (union voltage_object_info *)(adev->mode_info.atom_context)->bios + (unsigned long )data_offset;
    switch ((int )frev) {
    case 3: ;
    switch ((int )crev) {
    case 1:
    tmp = amdgpu_atombios_lookup_voltage_object_v3(& voltage_info->v3, (int )voltage_type,
                                                   (int )voltage_mode);
    if ((unsigned long )tmp != (unsigned long )((ATOM_VOLTAGE_OBJECT_V3 *)0)) {
      return (1);
    } else {
    }
    goto ldv_48446;
    default:
    drm_err("unknown voltage object table\n");
    return (0);
    }
    ldv_48446: ;
    goto ldv_48448;
    default:
    drm_err("unknown voltage object table\n");
    return (0);
    }
    ldv_48448: ;
  } else {
  }
  return (0);
}
}
int amdgpu_atombios_get_voltage_table(struct amdgpu_device *adev , u8 voltage_type ,
                                      u8 voltage_mode , struct atom_voltage_table *voltage_table )
{
  int index ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  u16 size ;
  int i ;
  union voltage_object_info *voltage_info ;
  union voltage_object *voltage_object ;
  ATOM_VOLTAGE_OBJECT_V3 *tmp ;
  ATOM_GPIO_VOLTAGE_OBJECT_V3 *gpio ;
  VOLTAGE_LUT_ENTRY_V2 *lut ;
  bool tmp___0 ;
  {
  index = 32;
  voltage_object = (union voltage_object *)0;
  tmp___0 = amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, & size,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___0) {
    voltage_info = (union voltage_object_info *)(adev->mode_info.atom_context)->bios + (unsigned long )data_offset;
    switch ((int )frev) {
    case 3: ;
    switch ((int )crev) {
    case 1:
    tmp = amdgpu_atombios_lookup_voltage_object_v3(& voltage_info->v3, (int )voltage_type,
                                                   (int )voltage_mode);
    voltage_object = (union voltage_object *)tmp;
    if ((unsigned long )voltage_object != (unsigned long )((union voltage_object *)0)) {
      gpio = & voltage_object->v3.asGpioVoltageObj;
      if ((unsigned int )gpio->ucGpioEntryNum > 32U) {
        return (-22);
      } else {
      }
      lut = (VOLTAGE_LUT_ENTRY_V2 *)(& gpio->asVolGpioLut);
      i = 0;
      goto ldv_48469;
      ldv_48468:
      voltage_table->entries[i].value = lut->usVoltageValue;
      voltage_table->entries[i].smio_low = lut->ulVoltageId;
      lut = lut + 6U;
      i = i + 1;
      ldv_48469: ;
      if ((int )gpio->ucGpioEntryNum > i) {
        goto ldv_48468;
      } else {
      }
      voltage_table->mask_low = gpio->ulGpioMaskVal;
      voltage_table->count = (u32 )gpio->ucGpioEntryNum;
      voltage_table->phase_delay = (u32 )gpio->ucPhaseDelay;
      return (0);
    } else {
    }
    goto ldv_48471;
    default:
    drm_err("unknown voltage object table\n");
    return (-22);
    }
    ldv_48471: ;
    goto ldv_48473;
    default:
    drm_err("unknown voltage object table\n");
    return (-22);
    }
    ldv_48473: ;
  } else {
  }
  return (-22);
}
}
int amdgpu_atombios_init_mc_reg_table(struct amdgpu_device *adev , u8 module_index ,
                                      struct atom_mc_reg_table *reg_table )
{
  int index ;
  u8 frev ;
  u8 crev ;
  u8 num_entries ;
  u8 t_mem_id ;
  u8 num_ranges ;
  u32 i ;
  u32 j ;
  u16 data_offset ;
  u16 size ;
  union vram_info *vram_info ;
  ATOM_INIT_REG_BLOCK *reg_block ;
  ATOM_MEMORY_SETTING_DATA_BLOCK *reg_data ;
  ATOM_INIT_REG_INDEX_FORMAT *format ;
  bool tmp ;
  {
  index = 28;
  num_ranges = 0U;
  i = 0U;
  memset((void *)reg_table, 0, 2772UL);
  tmp = amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, & size,
                                      & frev, & crev, & data_offset);
  if ((int )tmp) {
    vram_info = (union vram_info *)(adev->mode_info.atom_context)->bios + (unsigned long )data_offset;
    switch ((int )frev) {
    case 1:
    drm_err("old table version %d, %d\n", (int )frev, (int )crev);
    return (-22);
    case 2: ;
    switch ((int )crev) {
    case 1: ;
    if ((int )vram_info->v2_1.ucNumOfVRAMModule > (int )module_index) {
      reg_block = (ATOM_INIT_REG_BLOCK *)vram_info + (unsigned long )vram_info->v2_1.usMemClkPatchTblOffset;
      reg_data = (ATOM_MEMORY_SETTING_DATA_BLOCK *)reg_block + ((unsigned long )reg_block->usRegIndexTblSize + 4UL);
      format = (ATOM_INIT_REG_INDEX_FORMAT *)(& reg_block->asRegIndexBuf);
      num_entries = (unsigned int )((u8 )((unsigned int )reg_block->usRegIndexTblSize / 3U)) + 255U;
      if ((unsigned int )num_entries > 32U) {
        return (-22);
      } else {
      }
      goto ldv_48503;
      ldv_48502: ;
      if ((int )((signed char )format->ucPreRegDataLength) < 0) {
        goto ldv_48501;
      } else {
      }
      reg_table->mc_reg_address[i].s1 = format->usRegIndex;
      reg_table->mc_reg_address[i].pre_reg_data = format->ucPreRegDataLength;
      i = i + 1U;
      format = format + 3U;
      ldv_48503: ;
      if ((u32 )num_entries > i) {
        goto ldv_48502;
      } else {
      }
      ldv_48501:
      reg_table->last = (u8 )i;
      goto ldv_48508;
      ldv_48507:
      t_mem_id = (unsigned char )(*((u32 *)reg_data) >> 24);
      if ((int )module_index == (int )t_mem_id) {
        reg_table->mc_reg_table_entry[(int )num_ranges].mclk_max = *((u32 *)reg_data) & 16777215U;
        i = 0U;
        j = 1U;
        goto ldv_48505;
        ldv_48504: ;
        if (((int )reg_table->mc_reg_address[i].pre_reg_data & 15) == 4) {
          reg_table->mc_reg_table_entry[(int )num_ranges].mc_data[i] = *((u32 *)reg_data + (unsigned long )j);
          j = j + 1U;
        } else
        if (((int )reg_table->mc_reg_address[i].pre_reg_data & 15) == 0) {
          reg_table->mc_reg_table_entry[(int )num_ranges].mc_data[i] = reg_table->mc_reg_table_entry[(int )num_ranges].mc_data[i - 1U];
        } else {
        }
        i = i + 1U;
        ldv_48505: ;
        if ((u32 )reg_table->last > i) {
          goto ldv_48504;
        } else {
        }
        num_ranges = (u8 )((int )num_ranges + 1);
      } else {
      }
      reg_data = reg_data + (unsigned long )reg_block->usRegDataBlkSize;
      ldv_48508: ;
      if (*((u32 *)reg_data) != 0U && (unsigned int )num_ranges <= 19U) {
        goto ldv_48507;
      } else {
      }
      if (*((u32 *)reg_data) != 0U) {
        return (-22);
      } else {
      }
      reg_table->num_entries = num_ranges;
    } else {
      return (-22);
    }
    goto ldv_48510;
    default:
    drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
    return (-22);
    }
    ldv_48510: ;
    goto ldv_48512;
    default:
    drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
    return (-22);
    }
    ldv_48512: ;
    return (0);
  } else {
  }
  return (-22);
}
}
void amdgpu_atombios_scratch_regs_lock(struct amdgpu_device *adev , bool lock )
{
  u32 bios_6_scratch ;
  {
  bios_6_scratch = amdgpu_mm_rreg(adev, 1487U, 0);
  if ((int )lock) {
    bios_6_scratch = bios_6_scratch | 256U;
    bios_6_scratch = bios_6_scratch & 4294967279U;
  } else {
    bios_6_scratch = bios_6_scratch & 4294967039U;
    bios_6_scratch = bios_6_scratch | 16U;
  }
  amdgpu_mm_wreg(adev, 1487U, bios_6_scratch, 0);
  return;
}
}
void amdgpu_atombios_scratch_regs_init(struct amdgpu_device *adev )
{
  u32 bios_2_scratch ;
  u32 bios_6_scratch ;
  {
  bios_2_scratch = amdgpu_mm_rreg(adev, 1483U, 0);
  bios_6_scratch = amdgpu_mm_rreg(adev, 1487U, 0);
  bios_2_scratch = bios_2_scratch & 3758096383U;
  bios_6_scratch = bios_6_scratch | 536870912U;
  bios_2_scratch = bios_2_scratch & 4294901759U;
  amdgpu_mm_wreg(adev, 1483U, bios_2_scratch, 0);
  amdgpu_mm_wreg(adev, 1487U, bios_6_scratch, 0);
  return;
}
}
void amdgpu_atombios_scratch_regs_save(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_48529;
  ldv_48528:
  adev->bios_scratch[i] = amdgpu_mm_rreg(adev, (u32 )(i + 1481), 0);
  i = i + 1;
  ldv_48529: ;
  if (i <= 7) {
    goto ldv_48528;
  } else {
  }
  return;
}
}
void amdgpu_atombios_scratch_regs_restore(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_48536;
  ldv_48535:
  amdgpu_mm_wreg(adev, (u32 )(i + 1481), adev->bios_scratch[i], 0);
  i = i + 1;
  ldv_48536: ;
  if (i <= 7) {
    goto ldv_48535;
  } else {
  }
  return;
}
}
void amdgpu_atombios_copy_swap(u8 *dst , u8 *src , u8 num_bytes , bool to_le )
{
  {
  memcpy((void *)dst, (void const *)src, (size_t )num_bytes);
  return;
}
}
bool ldv_queue_work_on_47(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_48(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_49(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_50(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_51(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_61(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_63(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_62(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_65(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_64(struct workqueue_struct *ldv_func_arg1 ) ;
struct drm_connector *amdgpu_get_connector_for_encoder(struct drm_encoder *encoder ) ;
bool amdgpu_dig_monitor_is_duallink(struct drm_encoder *encoder , u32 pixel_clock ) ;
u16 amdgpu_encoder_get_dp_bridge_encoder_id(struct drm_encoder *encoder ) ;
int amdgpu_atombios_encoder_get_encoder_mode(struct drm_encoder *encoder ) ;
void amdgpu_pll_compute(struct amdgpu_pll *pll , u32 freq , u32 *dot_clock_p , u32 *fb_div_p ,
                        u32 *frac_fb_div_p , u32 *ref_div_p , u32 *post_div_p ) ;
int amdgpu_connector_get_monitor_bpc(struct drm_connector *connector ) ;
void amdgpu_atombios_crtc_overscan_setup(struct drm_crtc *crtc , struct drm_display_mode *mode ,
                                         struct drm_display_mode *adjusted_mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  SET_CRTC_OVERSCAN_PARAMETERS args ;
  int index ;
  int a1 ;
  int a2 ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  index = 40;
  memset((void *)(& args), 0, 12UL);
  args.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  switch ((unsigned int )amdgpu_crtc->rmx_type) {
  case 2U:
  args.usOverscanTop = (unsigned short )((adjusted_mode->crtc_vdisplay - mode->crtc_vdisplay) / 2);
  args.usOverscanBottom = (unsigned short )((adjusted_mode->crtc_vdisplay - mode->crtc_vdisplay) / 2);
  args.usOverscanLeft = (unsigned short )((adjusted_mode->crtc_hdisplay - mode->crtc_hdisplay) / 2);
  args.usOverscanRight = (unsigned short )((adjusted_mode->crtc_hdisplay - mode->crtc_hdisplay) / 2);
  goto ldv_48010;
  case 3U:
  a1 = mode->crtc_vdisplay * adjusted_mode->crtc_hdisplay;
  a2 = adjusted_mode->crtc_vdisplay * mode->crtc_hdisplay;
  if (a1 > a2) {
    args.usOverscanLeft = (unsigned short )((adjusted_mode->crtc_hdisplay - a2 / mode->crtc_vdisplay) / 2);
    args.usOverscanRight = (unsigned short )((adjusted_mode->crtc_hdisplay - a2 / mode->crtc_vdisplay) / 2);
  } else
  if (a2 > a1) {
    args.usOverscanTop = (unsigned short )((adjusted_mode->crtc_vdisplay - a1 / mode->crtc_hdisplay) / 2);
    args.usOverscanBottom = (unsigned short )((adjusted_mode->crtc_vdisplay - a1 / mode->crtc_hdisplay) / 2);
  } else {
  }
  goto ldv_48010;
  case 1U: ;
  default:
  args.usOverscanRight = (unsigned short )amdgpu_crtc->h_border;
  args.usOverscanLeft = (unsigned short )amdgpu_crtc->h_border;
  args.usOverscanBottom = (unsigned short )amdgpu_crtc->v_border;
  args.usOverscanTop = (unsigned short )amdgpu_crtc->v_border;
  goto ldv_48010;
  }
  ldv_48010:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_scaler_setup(struct drm_crtc *crtc )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  ENABLE_SCALER_PARAMETERS args ;
  int index ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  index = 33;
  memset((void *)(& args), 0, 4UL);
  args.ucScaler = (UCHAR )amdgpu_crtc->crtc_id;
  switch ((unsigned int )amdgpu_crtc->rmx_type) {
  case 1U:
  args.ucEnable = 2U;
  goto ldv_48025;
  case 2U:
  args.ucEnable = 1U;
  goto ldv_48025;
  case 3U:
  args.ucEnable = 2U;
  goto ldv_48025;
  default:
  args.ucEnable = 0U;
  goto ldv_48025;
  }
  ldv_48025:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_lock(struct drm_crtc *crtc , int lock )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int index ;
  ENABLE_CRTC_PARAMETERS args ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 44;
  memset((void *)(& args), 0, 4UL);
  args.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  args.ucEnable = (UCHAR )lock;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_enable(struct drm_crtc *crtc , int state )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int index ;
  ENABLE_CRTC_PARAMETERS args ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 35;
  memset((void *)(& args), 0, 4UL);
  args.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  args.ucEnable = (UCHAR )state;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_blank(struct drm_crtc *crtc , int state )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int index ;
  BLANK_CRTC_PARAMETERS args ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 34;
  memset((void *)(& args), 0, 8UL);
  args.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  args.ucBlanking = (UCHAR )state;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_powergate(struct drm_crtc *crtc , int state )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int index ;
  ENABLE_DISP_POWER_GATING_PARAMETERS_V2_1 args ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 13;
  memset((void *)(& args), 0, 4UL);
  args.ucDispPipeId = (UCHAR )amdgpu_crtc->crtc_id;
  args.ucEnable = (UCHAR )state;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_powergate_init(struct amdgpu_device *adev )
{
  int index ;
  ENABLE_DISP_POWER_GATING_PARAMETERS_V2_1 args ;
  {
  index = 13;
  memset((void *)(& args), 0, 4UL);
  args.ucEnable = 7U;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_crtc_set_dtd_timing(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  SET_CRTC_USING_DTD_TIMING_PARAMETERS args ;
  int index ;
  u16 misc ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 49;
  misc = 0U;
  memset((void *)(& args), 0, 24UL);
  args.usH_Size = (unsigned int )((unsigned short )mode->crtc_hdisplay) - (unsigned int )((unsigned short )amdgpu_crtc->h_border) * 2U;
  args.usH_Blanking_Time = (unsigned int )((int )((unsigned short )mode->crtc_hblank_end) - (int )((unsigned short )mode->crtc_hdisplay)) + (unsigned int )((unsigned short )amdgpu_crtc->h_border) * 2U;
  args.usV_Size = (unsigned int )((unsigned short )mode->crtc_vdisplay) - (unsigned int )((unsigned short )amdgpu_crtc->v_border) * 2U;
  args.usV_Blanking_Time = (unsigned int )((int )((unsigned short )mode->crtc_vblank_end) - (int )((unsigned short )mode->crtc_vdisplay)) + (unsigned int )((unsigned short )amdgpu_crtc->v_border) * 2U;
  args.usH_SyncOffset = ((int )((unsigned short )mode->crtc_hsync_start) - (int )((unsigned short )mode->crtc_hdisplay)) + (int )((unsigned short )amdgpu_crtc->h_border);
  args.usH_SyncWidth = (int )((unsigned short )mode->crtc_hsync_end) - (int )((unsigned short )mode->crtc_hsync_start);
  args.usV_SyncOffset = ((int )((unsigned short )mode->crtc_vsync_start) - (int )((unsigned short )mode->crtc_vdisplay)) + (int )((unsigned short )amdgpu_crtc->v_border);
  args.usV_SyncWidth = (int )((unsigned short )mode->crtc_vsync_end) - (int )((unsigned short )mode->crtc_vsync_start);
  args.ucH_Border = amdgpu_crtc->h_border;
  args.ucV_Border = amdgpu_crtc->v_border;
  if ((mode->flags & 8U) != 0U) {
    misc = (u16 )((unsigned int )misc | 4U);
  } else {
  }
  if ((mode->flags & 2U) != 0U) {
    misc = (u16 )((unsigned int )misc | 2U);
  } else {
  }
  if ((mode->flags & 64U) != 0U) {
    misc = (u16 )((unsigned int )misc | 64U);
  } else {
  }
  if ((mode->flags & 16U) != 0U) {
    misc = (u16 )((unsigned int )misc | 128U);
  } else {
  }
  if ((mode->flags & 32U) != 0U) {
    misc = (u16 )((unsigned int )misc | 256U);
  } else {
  }
  args.susModeMiscInfo.usAccess = misc;
  args.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
static void amdgpu_atombios_crtc_program_ss(struct amdgpu_device *adev , int enable ,
                                            int pll_id , int crtc_id , struct amdgpu_atom_ss *ss )
{
  unsigned int i ;
  int index ;
  union atom_enable_ss args ;
  {
  index = 65;
  if (enable != 0) {
    if ((unsigned int )ss->percentage == 0U) {
      return;
    } else {
    }
    if (((int )ss->type & 2) != 0) {
      return;
    } else {
    }
  } else {
    i = 0U;
    goto ldv_48105;
    ldv_48104: ;
    if ((((unsigned long )adev->mode_info.crtcs[i] != (unsigned long )((struct amdgpu_crtc *)0) && (int )(adev->mode_info.crtcs[i])->enabled) && (unsigned int )crtc_id != i) && (u32 )pll_id == (adev->mode_info.crtcs[i])->pll_id) {
      return;
    } else {
    }
    i = i + 1U;
    ldv_48105: ;
    if ((unsigned int )adev->mode_info.num_crtc > i) {
      goto ldv_48104;
    } else {
    }
  }
  memset((void *)(& args), 0, 8UL);
  args.v3.usSpreadSpectrumAmountFrac = 0U;
  args.v3.ucSpreadSpectrumType = (unsigned int )ss->type & 1U;
  switch (pll_id) {
  case 0:
  args.v3.ucSpreadSpectrumType = args.v3.ucSpreadSpectrumType;
  goto ldv_48108;
  case 1:
  args.v3.ucSpreadSpectrumType = (UCHAR )((unsigned int )args.v3.ucSpreadSpectrumType | 4U);
  goto ldv_48108;
  case 2:
  args.v3.ucSpreadSpectrumType = (UCHAR )((unsigned int )args.v3.ucSpreadSpectrumType | 8U);
  goto ldv_48108;
  case 255: ;
  return;
  }
  ldv_48108:
  args.v3.usSpreadSpectrumAmount = ss->amount;
  args.v3.usSpreadSpectrumStep = ss->step;
  args.v3.ucEnable = (UCHAR )enable;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
static u32 amdgpu_atombios_crtc_adjust_pll(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  u32 adjusted_clock ;
  int encoder_mode ;
  int tmp___0 ;
  u32 dp_clock ;
  u32 clock ;
  int bpc ;
  bool is_duallink ;
  bool tmp___1 ;
  union adjust_pixel_clock args ;
  u8 frev ;
  u8 crev ;
  int index ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  u16 tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  struct amdgpu_encoder_atom_dig *dig ;
  u16 tmp___5 ;
  u16 tmp___6 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  encoder = amdgpu_crtc->encoder;
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  adjusted_clock = (u32 )mode->clock;
  tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  encoder_mode = tmp___0;
  dp_clock = (u32 )mode->clock;
  clock = (u32 )mode->clock;
  bpc = amdgpu_crtc->bpc;
  tmp___1 = amdgpu_dig_monitor_is_duallink(encoder, (u32 )mode->clock);
  is_duallink = tmp___1;
  amdgpu_crtc->pll_flags = 1024U;
  if (((long )amdgpu_encoder->devices & 3818L) != 0L) {
    goto _L;
  } else {
    tmp___2 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    if ((unsigned int )tmp___2 != 0U) {
      _L:
      if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
        __mptr___1 = (struct drm_connector const *)connector;
        amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
        dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
        dp_clock = (u32 )dig_connector->dp_clock;
      } else {
      }
    } else {
    }
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    if ((int )amdgpu_crtc->ss_enabled) {
      if ((unsigned int )amdgpu_crtc->ss.refdiv != 0U) {
        amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 4U;
        amdgpu_crtc->pll_reference_div = (u32 )amdgpu_crtc->ss.refdiv;
        amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 1024U;
      } else {
      }
    } else {
    }
  } else {
  }
  if (amdgpu_encoder->encoder_id == 20U) {
    adjusted_clock = (u32 )(mode->clock * 2);
  } else {
  }
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 2048U;
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 8192U;
  } else {
  }
  if (encoder_mode == 3) {
    switch (bpc) {
    case 8: ;
    default: ;
    goto ldv_48145;
    case 10:
    clock = (clock * 5U) / 4U;
    goto ldv_48145;
    case 12:
    clock = (clock * 3U) / 2U;
    goto ldv_48145;
    case 16:
    clock = clock * 2U;
    goto ldv_48145;
    }
    ldv_48145: ;
  } else {
  }
  index = 17;
  tmp___3 = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                         & crev);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
    return (adjusted_clock);
  } else {
  }
  memset((void *)(& args), 0, 8UL);
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1: ;
  case 2:
  args.v1.usPixelClock = (unsigned short )(clock / 10U);
  args.v1.ucTransmitterID = (UCHAR )amdgpu_encoder->encoder_id;
  args.v1.ucEncodeMode = (UCHAR )encoder_mode;
  if ((int )amdgpu_crtc->ss_enabled && (unsigned int )amdgpu_crtc->ss.percentage != 0U) {
    args.v1.__annonCompField101.ucConfig = (UCHAR )((unsigned int )args.v1.__annonCompField101.ucConfig | 16U);
  } else {
  }
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  adjusted_clock = (u32 )((int )args.v1.usPixelClock * 10);
  goto ldv_48152;
  case 3:
  args.v3.__annonCompField102.sInput.usPixelClock = (unsigned short )(clock / 10U);
  args.v3.__annonCompField102.sInput.ucTransmitterID = (UCHAR )amdgpu_encoder->encoder_id;
  args.v3.__annonCompField102.sInput.ucEncodeMode = (UCHAR )encoder_mode;
  args.v3.__annonCompField102.sInput.ucDispPllConfig = 0U;
  if ((int )amdgpu_crtc->ss_enabled && (unsigned int )amdgpu_crtc->ss.percentage != 0U) {
    args.v3.__annonCompField102.sInput.ucDispPllConfig = (UCHAR )((unsigned int )args.v3.__annonCompField102.sInput.ucDispPllConfig | 16U);
  } else {
  }
  if (encoder_mode == 0 || encoder_mode == 5) {
    args.v3.__annonCompField102.sInput.ucDispPllConfig = (UCHAR )((unsigned int )args.v3.__annonCompField102.sInput.ucDispPllConfig | 32U);
    args.v3.__annonCompField102.sInput.usPixelClock = (unsigned short )(dp_clock / 10U);
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    if ((int )dig->coherent_mode) {
      args.v3.__annonCompField102.sInput.ucDispPllConfig = (UCHAR )((unsigned int )args.v3.__annonCompField102.sInput.ucDispPllConfig | 32U);
    } else {
    }
    if ((int )is_duallink) {
      args.v3.__annonCompField102.sInput.ucDispPllConfig = (UCHAR )((unsigned int )args.v3.__annonCompField102.sInput.ucDispPllConfig | 64U);
    } else {
    }
  } else {
  }
  tmp___6 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
  if ((unsigned int )tmp___6 != 0U) {
    tmp___5 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    args.v3.__annonCompField102.sInput.ucExtTransmitterID = (UCHAR )tmp___5;
  } else {
    args.v3.__annonCompField102.sInput.ucExtTransmitterID = 0U;
  }
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  adjusted_clock = args.v3.__annonCompField102.sOutput.ulDispPllFreq * 10U;
  if ((unsigned int )args.v3.__annonCompField102.sOutput.ucRefDiv != 0U) {
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 1024U;
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 4U;
    amdgpu_crtc->pll_reference_div = (u32 )args.v3.__annonCompField102.sOutput.ucRefDiv;
  } else {
  }
  if ((unsigned int )args.v3.__annonCompField102.sOutput.ucPostDiv != 0U) {
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 1024U;
    amdgpu_crtc->pll_flags = amdgpu_crtc->pll_flags | 4096U;
    amdgpu_crtc->pll_post_div = (u32 )args.v3.__annonCompField102.sOutput.ucPostDiv;
  } else {
  }
  goto ldv_48152;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return (adjusted_clock);
  }
  ldv_48152: ;
  goto ldv_48156;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return (adjusted_clock);
  }
  ldv_48156: ;
  return (adjusted_clock);
}
}
void amdgpu_atombios_crtc_set_disp_eng_pll(struct amdgpu_device *adev , u32 dispclk )
{
  u8 frev ;
  u8 crev ;
  int index ;
  union set_pixel_clock args ;
  bool tmp ;
  int tmp___0 ;
  {
  memset((void *)(& args), 0, 20UL);
  index = 12;
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 5:
  args.v5.ucCRTC = 255U;
  args.v5.usPixelClock = (unsigned short )dispclk;
  args.v5.ucPpll = 2U;
  goto ldv_48175;
  case 6:
  args.v6.__annonCompField100.ulDispEngClkFreq = dispclk;
  args.v6.ucPpll = 8U;
  goto ldv_48175;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48175: ;
  goto ldv_48178;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48178:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
static bool is_pixel_clock_source_from_pll(u32 encoder_mode , int pll_id )
{
  {
  if (encoder_mode == 0U || encoder_mode == 5U) {
    if (pll_id <= 7) {
      return (1);
    } else {
      return (0);
    }
  } else {
    return (1);
  }
}
}
void amdgpu_atombios_crtc_program_pll(struct drm_crtc *crtc , u32 crtc_id , int pll_id ,
                                      u32 encoder_mode , u32 encoder_id , u32 clock ,
                                      u32 ref_div , u32 fb_div , u32 frac_fb_div ,
                                      u32 post_div , int bpc , bool ss_enabled , struct amdgpu_atom_ss *ss )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u8 frev ;
  u8 crev ;
  int index ;
  union set_pixel_clock args ;
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 12;
  memset((void *)(& args), 0, 20UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1: ;
  if (clock == 0U) {
    return;
  } else {
  }
  args.v1.usPixelClock = (unsigned short )(clock / 10U);
  args.v1.usRefDiv = (unsigned short )ref_div;
  args.v1.usFbDiv = (unsigned short )fb_div;
  args.v1.ucFracFbDiv = (UCHAR )frac_fb_div;
  args.v1.ucPostDiv = (UCHAR )post_div;
  args.v1.ucPpll = (UCHAR )pll_id;
  args.v1.ucCRTC = (UCHAR )crtc_id;
  args.v1.ucRefDivSrc = 1U;
  goto ldv_48207;
  case 2:
  args.v2.usPixelClock = (unsigned short )(clock / 10U);
  args.v2.usRefDiv = (unsigned short )ref_div;
  args.v2.usFbDiv = (unsigned short )fb_div;
  args.v2.ucFracFbDiv = (UCHAR )frac_fb_div;
  args.v2.ucPostDiv = (UCHAR )post_div;
  args.v2.ucPpll = (UCHAR )pll_id;
  args.v2.ucCRTC = (UCHAR )crtc_id;
  args.v2.ucRefDivSrc = 1U;
  goto ldv_48207;
  case 3:
  args.v3.usPixelClock = (unsigned short )(clock / 10U);
  args.v3.usRefDiv = (unsigned short )ref_div;
  args.v3.usFbDiv = (unsigned short )fb_div;
  args.v3.ucFracFbDiv = (UCHAR )frac_fb_div;
  args.v3.ucPostDiv = (UCHAR )post_div;
  args.v3.ucPpll = (UCHAR )pll_id;
  if (crtc_id == 1U) {
    args.v3.ucMiscInfo = 4U;
  } else {
    args.v3.ucMiscInfo = 0U;
  }
  if ((int )ss_enabled && ((int )ss->type & 2) != 0) {
    args.v3.ucMiscInfo = (UCHAR )((unsigned int )args.v3.ucMiscInfo | 16U);
  } else {
  }
  args.v3.ucTransmitterId = (UCHAR )encoder_id;
  args.v3.__annonCompField98.ucEncoderMode = (UCHAR )encoder_mode;
  goto ldv_48207;
  case 5:
  args.v5.ucCRTC = (UCHAR )crtc_id;
  args.v5.usPixelClock = (unsigned short )(clock / 10U);
  args.v5.ucRefDiv = (UCHAR )ref_div;
  args.v5.usFbDiv = (unsigned short )fb_div;
  args.v5.ulFbDivDecFrac = frac_fb_div * 100000U;
  args.v5.ucPostDiv = (UCHAR )post_div;
  args.v5.ucMiscInfo = 0U;
  if (((int )ss_enabled && ((int )ss->type & 2) != 0) && pll_id <= 7) {
    args.v5.ucMiscInfo = (UCHAR )((unsigned int )args.v5.ucMiscInfo | 16U);
  } else {
  }
  if (encoder_mode == 3U) {
    switch (bpc) {
    case 8: ;
    default:
    args.v5.ucMiscInfo = args.v5.ucMiscInfo;
    goto ldv_48213;
    case 10:
    args.v5.ucMiscInfo = (UCHAR )((unsigned int )args.v5.ucMiscInfo | 8U);
    goto ldv_48213;
    case 12:
    args.v5.ucMiscInfo = (UCHAR )((unsigned int )args.v5.ucMiscInfo | 4U);
    goto ldv_48213;
    }
    ldv_48213: ;
  } else {
  }
  args.v5.ucTransmitterID = (UCHAR )encoder_id;
  args.v5.ucEncoderMode = (UCHAR )encoder_mode;
  args.v5.ucPpll = (UCHAR )pll_id;
  goto ldv_48207;
  case 6:
  args.v6.__annonCompField100.ulDispEngClkFreq = (crtc_id << 24) | clock / 10U;
  args.v6.ucRefDiv = (UCHAR )ref_div;
  args.v6.usFbDiv = (unsigned short )fb_div;
  args.v6.ulFbDivDecFrac = frac_fb_div * 100000U;
  args.v6.ucPostDiv = (UCHAR )post_div;
  args.v6.ucMiscInfo = 0U;
  if (((int )ss_enabled && ((int )ss->type & 2) != 0) && pll_id <= 7) {
    tmp___1 = is_pixel_clock_source_from_pll(encoder_mode, pll_id);
    if (tmp___1) {
      tmp___2 = 0;
    } else {
      tmp___2 = 1;
    }
    if (tmp___2) {
      args.v6.ucMiscInfo = (UCHAR )((unsigned int )args.v6.ucMiscInfo | 16U);
    } else {
    }
  } else {
  }
  if (encoder_mode == 3U) {
    switch (bpc) {
    case 8: ;
    default:
    args.v6.ucMiscInfo = args.v6.ucMiscInfo;
    goto ldv_48219;
    case 10:
    args.v6.ucMiscInfo = (UCHAR )((unsigned int )args.v6.ucMiscInfo | 4U);
    goto ldv_48219;
    case 12:
    args.v6.ucMiscInfo = (UCHAR )((unsigned int )args.v6.ucMiscInfo | 8U);
    goto ldv_48219;
    case 16:
    args.v6.ucMiscInfo = (UCHAR )((unsigned int )args.v6.ucMiscInfo | 12U);
    goto ldv_48219;
    }
    ldv_48219: ;
  } else {
  }
  args.v6.ucTransmitterID = (UCHAR )encoder_id;
  args.v6.ucEncoderMode = (UCHAR )encoder_mode;
  args.v6.ucPpll = (UCHAR )pll_id;
  goto ldv_48207;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48207: ;
  goto ldv_48224;
  default:
  drm_err("Unknown table version %d %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48224:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
int amdgpu_atombios_crtc_prepare_pll(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  int encoder_mode ;
  int tmp ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  int dp_clock ;
  u16 tmp___1 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr___0 = (struct drm_encoder const *)amdgpu_crtc->encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  tmp = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
  encoder_mode = tmp;
  amdgpu_crtc->bpc = 8;
  amdgpu_crtc->ss_enabled = 0;
  if (((long )amdgpu_encoder->active_device & 3818L) != 0L) {
    goto _L;
  } else {
    tmp___1 = amdgpu_encoder_get_dp_bridge_encoder_id(amdgpu_crtc->encoder);
    if ((unsigned int )tmp___1 != 0U) {
      _L:
      dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
      tmp___0 = amdgpu_get_connector_for_encoder(amdgpu_crtc->encoder);
      connector = tmp___0;
      __mptr___1 = (struct drm_connector const *)connector;
      amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
      dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
      amdgpu_connector->pixelclock_for_modeset = (unsigned int )mode->clock;
      amdgpu_crtc->bpc = amdgpu_connector_get_monitor_bpc(connector);
      switch (encoder_mode) {
      case 5: ;
      case 0:
      dp_clock = dig_connector->dp_clock / 10;
      amdgpu_crtc->ss_enabled = amdgpu_atombios_get_asic_ss_info(adev, & amdgpu_crtc->ss,
                                                                 7, (u32 )dp_clock);
      goto ldv_48248;
      case 1:
      amdgpu_crtc->ss_enabled = amdgpu_atombios_get_asic_ss_info(adev, & amdgpu_crtc->ss,
                                                                 (int )dig->lcd_ss_id,
                                                                 (u32 )(mode->clock / 10));
      goto ldv_48248;
      case 2:
      amdgpu_crtc->ss_enabled = amdgpu_atombios_get_asic_ss_info(adev, & amdgpu_crtc->ss,
                                                                 4, (u32 )(mode->clock / 10));
      goto ldv_48248;
      case 3:
      amdgpu_crtc->ss_enabled = amdgpu_atombios_get_asic_ss_info(adev, & amdgpu_crtc->ss,
                                                                 5, (u32 )(mode->clock / 10));
      goto ldv_48248;
      default: ;
      goto ldv_48248;
      }
      ldv_48248: ;
    } else {
    }
  }
  amdgpu_crtc->adjusted_clock = amdgpu_atombios_crtc_adjust_pll(crtc, mode);
  return (0);
}
}
void amdgpu_atombios_crtc_set_pll(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  u32 pll_clock ;
  u32 clock ;
  u32 ref_div ;
  u32 fb_div ;
  u32 frac_fb_div ;
  u32 post_div ;
  struct amdgpu_pll *pll ;
  int encoder_mode ;
  int tmp ;
  u32 step_size ;
  u32 amount ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr___0 = (struct drm_encoder const *)amdgpu_crtc->encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  pll_clock = (u32 )mode->clock;
  clock = (u32 )mode->clock;
  ref_div = 0U;
  fb_div = 0U;
  frac_fb_div = 0U;
  post_div = 0U;
  tmp = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
  encoder_mode = tmp;
  if (encoder_mode == 3 && amdgpu_crtc->bpc > 8) {
    clock = amdgpu_crtc->adjusted_clock;
  } else {
  }
  switch (amdgpu_crtc->pll_id) {
  case 0U:
  pll = (struct amdgpu_pll *)(& adev->clock.ppll);
  goto ldv_48274;
  case 1U:
  pll = (struct amdgpu_pll *)(& adev->clock.ppll) + 1UL;
  goto ldv_48274;
  case 2U: ;
  case 255U: ;
  default:
  pll = (struct amdgpu_pll *)(& adev->clock.ppll) + 2UL;
  goto ldv_48274;
  }
  ldv_48274:
  pll->flags = amdgpu_crtc->pll_flags;
  pll->reference_div = amdgpu_crtc->pll_reference_div;
  pll->post_div = amdgpu_crtc->pll_post_div;
  amdgpu_pll_compute(pll, amdgpu_crtc->adjusted_clock, & pll_clock, & fb_div, & frac_fb_div,
                     & ref_div, & post_div);
  amdgpu_atombios_crtc_program_ss(adev, 0, (int )amdgpu_crtc->pll_id, amdgpu_crtc->crtc_id,
                                  & amdgpu_crtc->ss);
  amdgpu_atombios_crtc_program_pll(crtc, (u32 )amdgpu_crtc->crtc_id, (int )amdgpu_crtc->pll_id,
                                   (u32 )encoder_mode, amdgpu_encoder->encoder_id,
                                   clock, ref_div, fb_div, frac_fb_div, post_div,
                                   amdgpu_crtc->bpc, (int )amdgpu_crtc->ss_enabled,
                                   & amdgpu_crtc->ss);
  if ((int )amdgpu_crtc->ss_enabled) {
    amount = ((fb_div * 10U + frac_fb_div) * (u32 )amdgpu_crtc->ss.percentage) / ((unsigned int )amdgpu_crtc->ss.percentage_divider * 100U);
    amdgpu_crtc->ss.amount = (unsigned int )((u16 )(amount / 10U)) & 255U;
    amdgpu_crtc->ss.amount = (unsigned int )amdgpu_crtc->ss.amount | ((unsigned int )(((int )((u16 )amount) - (int )((u16 )(amount / 10U))) << 8U) & 3840U);
    if ((int )amdgpu_crtc->ss.type & 1) {
      step_size = (((amount * ref_div) * (u32 )amdgpu_crtc->ss.rate) * 8192U) / ((pll->reference_freq * 3125U) / 100U);
    } else {
      step_size = (((amount * ref_div) * (u32 )amdgpu_crtc->ss.rate) * 4096U) / ((pll->reference_freq * 3125U) / 100U);
    }
    amdgpu_crtc->ss.step = (u16 )step_size;
    amdgpu_atombios_crtc_program_ss(adev, 1, (int )amdgpu_crtc->pll_id, amdgpu_crtc->crtc_id,
                                    & amdgpu_crtc->ss);
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_61(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_62(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_63(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_64(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_65(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static int list_empty(struct list_head const *head )
{
  {
  return ((unsigned long )((struct list_head const *)head->next) == (unsigned long )head);
}
}
extern void *kmemdup(void const * , size_t , gfp_t ) ;
bool ldv_queue_work_on_75(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_77(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_76(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_79(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_78(struct workqueue_struct *ldv_func_arg1 ) ;
extern void drm_mode_probed_add(struct drm_connector * , struct drm_display_mode * ) ;
extern struct drm_display_mode *drm_cvt_mode(struct drm_device * , int , int , int ,
                                             bool , bool , bool ) ;
extern void drm_mode_set_name(struct drm_display_mode * ) ;
extern void drm_mode_set_crtcinfo(struct drm_display_mode * , int ) ;
extern struct drm_display_mode *drm_mode_duplicate(struct drm_device * , struct drm_display_mode const * ) ;
extern int drm_connector_init(struct drm_device * , struct drm_connector * , struct drm_connector_funcs const * ,
                              int ) ;
extern int drm_connector_register(struct drm_connector * ) ;
extern void drm_connector_unregister(struct drm_connector * ) ;
extern void drm_connector_cleanup(struct drm_connector * ) ;
extern struct edid *drm_get_edid(struct drm_connector * , struct i2c_adapter * ) ;
extern int drm_add_edid_modes(struct drm_connector * , struct edid * ) ;
extern int drm_mode_connector_update_edid_property(struct drm_connector * , struct edid const * ) ;
extern void drm_object_attach_property(struct drm_mode_object * , struct drm_property * ,
                                       uint64_t ) ;
extern struct drm_mode_object *drm_mode_object_find(struct drm_device * , u32 , u32 ) ;
extern bool drm_detect_hdmi_monitor(struct edid * ) ;
__inline static struct drm_encoder *drm_encoder_find(struct drm_device *dev , u32 id )
{
  struct drm_mode_object *mo ;
  struct drm_mode_object const *__mptr ;
  struct drm_encoder *tmp ;
  {
  mo = drm_mode_object_find(dev, id, 3772834016U);
  if ((unsigned long )mo != (unsigned long )((struct drm_mode_object *)0)) {
    __mptr = (struct drm_mode_object const *)mo;
    tmp = (struct drm_encoder *)__mptr + 0xffffffffffffffe8UL;
  } else {
    tmp = (struct drm_encoder *)0;
  }
  return (tmp);
}
}
extern void drm_edid_to_eld(struct drm_connector * , struct edid * ) ;
extern bool drm_crtc_helper_set_mode(struct drm_crtc * , struct drm_display_mode * ,
                                     int , int , struct drm_framebuffer * ) ;
__inline static void drm_connector_helper_add(struct drm_connector *connector , struct drm_connector_helper_funcs const *funcs )
{
  {
  connector->helper_private = (void const *)funcs;
  return;
}
}
extern int drm_helper_probe_single_connector_modes(struct drm_connector * , u32 ,
                                                   u32 ) ;
extern void drm_dp_aux_unregister(struct drm_dp_aux * ) ;
bool amdgpu_ddc_probe(struct amdgpu_connector *amdgpu_connector , bool use_aux ) ;
bool amdgpu_atombios_encoder_set_edp_panel_power(struct drm_connector *connector ,
                                                 int action ) ;
void amdgpu_atombios_encoder_setup_ext_encoder_ddc(struct drm_encoder *encoder ) ;
void amdgpu_atombios_encoder_set_bios_scratch_regs(struct drm_connector *connector ,
                                                   struct drm_encoder *encoder , bool connected ) ;
void amdgpu_atombios_dp_aux_init(struct amdgpu_connector *amdgpu_connector ) ;
u8 amdgpu_atombios_dp_get_sinktype(struct amdgpu_connector *amdgpu_connector ) ;
int amdgpu_atombios_dp_get_dpcd(struct amdgpu_connector *amdgpu_connector ) ;
int amdgpu_atombios_dp_mode_valid_helper(struct drm_connector *connector , struct drm_display_mode *mode ) ;
bool amdgpu_atombios_dp_needs_link_train(struct amdgpu_connector *amdgpu_connector ) ;
struct edid *amdgpu_connector_edid(struct drm_connector *connector ) ;
void amdgpu_connector_hotplug(struct drm_connector *connector ) ;
u16 amdgpu_connector_encoder_get_dp_bridge_encoder_id(struct drm_connector *connector ) ;
bool amdgpu_connector_is_dp12_capable(struct drm_connector *connector ) ;
void amdgpu_connector_add(struct amdgpu_device *adev , u32 connector_id , u32 supported_device ,
                          int connector_type , struct amdgpu_i2c_bus_rec *i2c_bus ,
                          u16 connector_object_id , struct amdgpu_hpd *hpd , struct amdgpu_router *router ) ;
struct amdgpu_i2c_chan *amdgpu_i2c_lookup(struct amdgpu_device *adev , struct amdgpu_i2c_bus_rec *i2c_bus ) ;
void amdgpu_i2c_router_select_ddc_port(struct amdgpu_connector *amdgpu_connector ) ;
void amdgpu_connector_hotplug(struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  int saved_dpms ;
  bool tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned int )amdgpu_connector->hpd.hpd == 255U) {
    return;
  } else {
  }
  (*((adev->mode_info.funcs)->hpd_set_polarity))(adev, amdgpu_connector->hpd.hpd);
  if (connector->dpms != 0) {
    return;
  } else {
  }
  if (connector->connector_type == 10) {
    dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
    if ((unsigned int )dig_connector->dp_sink_type != 19U) {
      return;
    } else {
    }
    dig_connector->dp_sink_type = amdgpu_atombios_dp_get_sinktype(amdgpu_connector);
    if ((unsigned int )dig_connector->dp_sink_type == 19U) {
      saved_dpms = connector->dpms;
      tmp___0 = (*((adev->mode_info.funcs)->hpd_sense))(adev, amdgpu_connector->hpd.hpd);
      if (tmp___0) {
        tmp___1 = 0;
      } else {
        tmp___1 = 1;
      }
      if (tmp___1) {
        drm_helper_connector_dpms(connector, 3);
      } else {
        tmp = amdgpu_atombios_dp_needs_link_train(amdgpu_connector);
        if ((int )tmp) {
          connector->dpms = 3;
          drm_helper_connector_dpms(connector, 0);
        } else {
        }
      }
      connector->dpms = saved_dpms;
    } else {
    }
  } else {
  }
  return;
}
}
static void amdgpu_connector_property_change_mode(struct drm_encoder *encoder )
{
  struct drm_crtc *crtc ;
  {
  crtc = encoder->crtc;
  if ((unsigned long )crtc != (unsigned long )((struct drm_crtc *)0) && (int )crtc->enabled) {
    drm_crtc_helper_set_mode(crtc, & crtc->mode, crtc->x, crtc->y, (crtc->primary)->fb);
  } else {
  }
  return;
}
}
int amdgpu_connector_get_monitor_bpc(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  int bpc ;
  unsigned int mode_clock ;
  unsigned int max_tmds_clock ;
  struct edid *tmp ;
  bool tmp___0 ;
  struct edid *tmp___1 ;
  bool tmp___2 ;
  struct edid *tmp___3 ;
  bool tmp___4 ;
  struct drm_connector_helper_funcs const *connector_funcs ;
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp___5 ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct amdgpu_encoder_atom_dig *dig ;
  long tmp___6 ;
  long tmp___7 ;
  long tmp___8 ;
  long tmp___9 ;
  long tmp___10 ;
  struct edid *tmp___11 ;
  bool tmp___12 ;
  long tmp___13 ;
  long tmp___14 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  bpc = 8;
  switch (connector->connector_type) {
  case 2: ;
  case 12: ;
  if ((int )amdgpu_connector->use_digital) {
    tmp = amdgpu_connector_edid(connector);
    tmp___0 = drm_detect_hdmi_monitor(tmp);
    if ((int )tmp___0) {
      if (connector->display_info.bpc != 0U) {
        bpc = (int )connector->display_info.bpc;
      } else {
      }
    } else {
    }
  } else {
  }
  goto ldv_48164;
  case 3: ;
  case 11:
  tmp___1 = amdgpu_connector_edid(connector);
  tmp___2 = drm_detect_hdmi_monitor(tmp___1);
  if ((int )tmp___2) {
    if (connector->display_info.bpc != 0U) {
      bpc = (int )connector->display_info.bpc;
    } else {
    }
  } else {
  }
  goto ldv_48164;
  case 10:
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dp_sink_type == 19U || (unsigned int )dig_connector->dp_sink_type == 20U) {
    goto _L;
  } else {
    tmp___3 = amdgpu_connector_edid(connector);
    tmp___4 = drm_detect_hdmi_monitor(tmp___3);
    if ((int )tmp___4) {
      _L:
      if (connector->display_info.bpc != 0U) {
        bpc = (int )connector->display_info.bpc;
      } else {
      }
    } else {
    }
  }
  goto ldv_48164;
  case 14: ;
  case 7: ;
  if (connector->display_info.bpc != 0U) {
    bpc = (int )connector->display_info.bpc;
  } else {
    connector_funcs = (struct drm_connector_helper_funcs const *)connector->helper_private;
    tmp___5 = (*(connector_funcs->best_encoder))(connector);
    encoder = tmp___5;
    __mptr___0 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    if ((dig->lcd_misc & 16U) != 0U) {
      bpc = 6;
    } else
    if ((dig->lcd_misc & 32U) != 0U) {
      bpc = 8;
    } else {
    }
  }
  goto ldv_48164;
  }
  ldv_48164:
  tmp___11 = amdgpu_connector_edid(connector);
  tmp___12 = drm_detect_hdmi_monitor(tmp___11);
  if ((int )tmp___12) {
    if (bpc > 12) {
      tmp___6 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
      if (tmp___6 != 0L) {
        drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: HDMI deep color %d bpc unsupported. Using 12 bpc.\n",
                            connector->name, bpc);
      } else {
      }
      bpc = 12;
    } else {
    }
    if (connector->max_tmds_clock > 0) {
      mode_clock = amdgpu_connector->pixelclock_for_modeset;
      max_tmds_clock = (unsigned int )(connector->max_tmds_clock * 1000);
      tmp___7 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
      if (tmp___7 != 0L) {
        drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: hdmi mode dotclock %d kHz, max tmds input clock %d kHz.\n",
                            connector->name, mode_clock, max_tmds_clock);
      } else {
      }
      if (bpc == 12 && (mode_clock * 3U) / 2U > max_tmds_clock) {
        if (((int )connector->display_info.edid_hdmi_dc_modes & 16) != 0 && (mode_clock * 5U) / 4U <= max_tmds_clock) {
          bpc = 10;
        } else {
          bpc = 8;
        }
        tmp___8 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
        if (tmp___8 != 0L) {
          drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: HDMI deep color 12 bpc exceeds max tmds clock. Using %d bpc.\n",
                              connector->name, bpc);
        } else {
        }
      } else {
      }
      if (bpc == 10 && (mode_clock * 5U) / 4U > max_tmds_clock) {
        bpc = 8;
        tmp___9 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
        if (tmp___9 != 0L) {
          drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: HDMI deep color 10 bpc exceeds max tmds clock. Using %d bpc.\n",
                              connector->name, bpc);
        } else {
        }
      } else
      if (bpc > 8) {
        tmp___10 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
        if (tmp___10 != 0L) {
          drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: Required max tmds clock for HDMI deep color missing. Using 8 bpc.\n",
                              connector->name);
        } else {
        }
        bpc = 8;
      } else {
      }
    } else {
    }
  } else {
  }
  if (amdgpu_deep_color == 0 && bpc > 8) {
    tmp___13 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___13 != 0L) {
      drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: Deep color disabled. Set amdgpu module param deep_color=1 to enable.\n",
                          connector->name);
    } else {
    }
    bpc = 8;
  } else {
  }
  tmp___14 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___14 != 0L) {
    drm_ut_debug_printk("amdgpu_connector_get_monitor_bpc", "%s: Display bpc=%d, returned bpc=%d\n",
                        connector->name, connector->display_info.bpc, bpc);
  } else {
  }
  return (bpc);
}
}
static void amdgpu_connector_update_scratch_regs(struct drm_connector *connector ,
                                                 enum drm_connector_status status )
{
  struct drm_encoder *best_encoder ;
  struct drm_encoder *encoder ;
  struct drm_connector_helper_funcs const *connector_funcs ;
  bool connected ;
  int i ;
  {
  best_encoder = (struct drm_encoder *)0;
  encoder = (struct drm_encoder *)0;
  connector_funcs = (struct drm_connector_helper_funcs const *)connector->helper_private;
  best_encoder = (*(connector_funcs->best_encoder))(connector);
  i = 0;
  goto ldv_48189;
  ldv_48188: ;
  if (connector->encoder_ids[i] == 0U) {
    goto ldv_48186;
  } else {
  }
  encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    goto ldv_48187;
  } else {
  }
  if ((unsigned long )encoder == (unsigned long )best_encoder && (unsigned int )status == 1U) {
    connected = 1;
  } else {
    connected = 0;
  }
  amdgpu_atombios_encoder_set_bios_scratch_regs(connector, encoder, (int )connected);
  ldv_48187:
  i = i + 1;
  ldv_48189: ;
  if (i <= 2) {
    goto ldv_48188;
  } else {
  }
  ldv_48186: ;
  return;
}
}
static struct drm_encoder *amdgpu_connector_find_encoder(struct drm_connector *connector ,
                                                         int encoder_type )
{
  struct drm_encoder *encoder ;
  int i ;
  {
  i = 0;
  goto ldv_48199;
  ldv_48198: ;
  if (connector->encoder_ids[i] == 0U) {
    goto ldv_48196;
  } else {
  }
  encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    goto ldv_48197;
  } else {
  }
  if (encoder->encoder_type == encoder_type) {
    return (encoder);
  } else {
  }
  ldv_48197:
  i = i + 1;
  ldv_48199: ;
  if (i <= 2) {
    goto ldv_48198;
  } else {
  }
  ldv_48196: ;
  return ((struct drm_encoder *)0);
}
}
struct edid *amdgpu_connector_edid(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_property_blob *edid_blob ;
  struct edid *edid ;
  void *tmp ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  edid_blob = connector->edid_blob_ptr;
  if ((unsigned long )amdgpu_connector->edid != (unsigned long )((struct edid *)0)) {
    return (amdgpu_connector->edid);
  } else
  if ((unsigned long )edid_blob != (unsigned long )((struct drm_property_blob *)0)) {
    tmp = kmemdup((void const *)(& edid_blob->data), edid_blob->length, 208U);
    edid = (struct edid *)tmp;
    if ((unsigned long )edid != (unsigned long )((struct edid *)0)) {
      amdgpu_connector->edid = edid;
    } else {
    }
  } else {
  }
  return (amdgpu_connector->edid);
}
}
static struct edid *amdgpu_connector_get_hardcoded_edid(struct amdgpu_device *adev )
{
  struct edid *edid ;
  void *tmp ;
  {
  if ((unsigned long )adev->mode_info.bios_hardcoded_edid != (unsigned long )((struct edid *)0)) {
    tmp = kmalloc((size_t )adev->mode_info.bios_hardcoded_edid_size, 208U);
    edid = (struct edid *)tmp;
    if ((unsigned long )edid != (unsigned long )((struct edid *)0)) {
      memcpy((void *)edid, (void const *)adev->mode_info.bios_hardcoded_edid,
               (size_t )adev->mode_info.bios_hardcoded_edid_size);
      return (edid);
    } else {
    }
  } else {
  }
  return ((struct edid *)0);
}
}
static void amdgpu_connector_get_edid(struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig ;
  u16 tmp ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->edid != (unsigned long )((struct edid *)0)) {
    return;
  } else {
  }
  if ((int )amdgpu_connector->router.ddc_valid) {
    amdgpu_i2c_router_select_ddc_port(amdgpu_connector);
  } else {
  }
  tmp = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
  if ((unsigned int )tmp != 0U && (int )(amdgpu_connector->ddc_bus)->has_aux) {
    amdgpu_connector->edid = drm_get_edid(connector, & (amdgpu_connector->ddc_bus)->aux.ddc);
  } else
  if (connector->connector_type == 10 || connector->connector_type == 14) {
    dig = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
    if (((unsigned int )dig->dp_sink_type == 19U || (unsigned int )dig->dp_sink_type == 20U) && (int )(amdgpu_connector->ddc_bus)->has_aux) {
      amdgpu_connector->edid = drm_get_edid(connector, & (amdgpu_connector->ddc_bus)->aux.ddc);
    } else
    if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
      amdgpu_connector->edid = drm_get_edid(connector, & (amdgpu_connector->ddc_bus)->adapter);
    } else {
    }
  } else
  if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    amdgpu_connector->edid = drm_get_edid(connector, & (amdgpu_connector->ddc_bus)->adapter);
  } else {
  }
  if ((unsigned long )amdgpu_connector->edid == (unsigned long )((struct edid *)0)) {
    if (connector->connector_type == 7 || connector->connector_type == 14) {
      amdgpu_connector->edid = amdgpu_connector_get_hardcoded_edid(adev);
    } else {
    }
  } else {
  }
  return;
}
}
static void amdgpu_connector_free_edid(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->edid != (unsigned long )((struct edid *)0)) {
    kfree((void const *)amdgpu_connector->edid);
    amdgpu_connector->edid = (struct edid *)0;
  } else {
  }
  return;
}
}
static int amdgpu_connector_ddc_get_modes(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  int ret ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->edid != (unsigned long )((struct edid *)0)) {
    drm_mode_connector_update_edid_property(connector, (struct edid const *)amdgpu_connector->edid);
    ret = drm_add_edid_modes(connector, amdgpu_connector->edid);
    drm_edid_to_eld(connector, amdgpu_connector->edid);
    return (ret);
  } else {
  }
  drm_mode_connector_update_edid_property(connector, (struct edid const *)0);
  return (0);
}
}
static struct drm_encoder *amdgpu_connector_best_single_encoder(struct drm_connector *connector )
{
  int enc_id ;
  struct drm_encoder *tmp ;
  {
  enc_id = (int )connector->encoder_ids[0];
  if (enc_id != 0) {
    tmp = drm_encoder_find(connector->dev, (u32 )enc_id);
    return (tmp);
  } else {
  }
  return ((struct drm_encoder *)0);
}
}
static void amdgpu_get_native_mode(struct drm_connector *connector )
{
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *preferred_mode ;
  struct list_head const *__mptr___0 ;
  int tmp___0 ;
  {
  tmp = amdgpu_connector_best_single_encoder(connector);
  encoder = tmp;
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    return;
  } else {
  }
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp___0 = list_empty((struct list_head const *)(& connector->probed_modes));
  if (tmp___0 == 0) {
    __mptr___0 = (struct list_head const *)connector->probed_modes.next;
    preferred_mode = (struct drm_display_mode *)__mptr___0;
    amdgpu_encoder->native_mode = *preferred_mode;
  } else {
    amdgpu_encoder->native_mode.clock = 0;
  }
  return;
}
}
static struct drm_display_mode *amdgpu_connector_lcd_native_mode(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *mode ;
  struct drm_display_mode *native_mode ;
  long tmp ;
  long tmp___0 ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  mode = (struct drm_display_mode *)0;
  native_mode = & amdgpu_encoder->native_mode;
  if ((native_mode->hdisplay != 0 && native_mode->vdisplay != 0) && native_mode->clock != 0) {
    mode = drm_mode_duplicate(dev, (struct drm_display_mode const *)native_mode);
    mode->type = 72U;
    drm_mode_set_name(mode);
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_connector_lcd_native_mode", "Adding native panel mode %s\n",
                          (char *)(& mode->name));
    } else {
    }
  } else
  if (native_mode->hdisplay != 0 && native_mode->vdisplay != 0) {
    mode = drm_cvt_mode(dev, native_mode->hdisplay, native_mode->vdisplay, 60, 1,
                        0, 0);
    mode->type = 72U;
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_connector_lcd_native_mode", "Adding cvt approximation of native panel mode %s\n",
                          (char *)(& mode->name));
    } else {
    }
  } else {
  }
  return (mode);
}
}
static void amdgpu_connector_add_common_modes(struct drm_encoder *encoder , struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *mode ;
  struct drm_display_mode *native_mode ;
  int i ;
  struct mode_size common_modes[17U] ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  mode = (struct drm_display_mode *)0;
  native_mode = & amdgpu_encoder->native_mode;
  common_modes[0].w = 640;
  common_modes[0].h = 480;
  common_modes[1].w = 720;
  common_modes[1].h = 480;
  common_modes[2].w = 800;
  common_modes[2].h = 600;
  common_modes[3].w = 848;
  common_modes[3].h = 480;
  common_modes[4].w = 1024;
  common_modes[4].h = 768;
  common_modes[5].w = 1152;
  common_modes[5].h = 768;
  common_modes[6].w = 1280;
  common_modes[6].h = 720;
  common_modes[7].w = 1280;
  common_modes[7].h = 800;
  common_modes[8].w = 1280;
  common_modes[8].h = 854;
  common_modes[9].w = 1280;
  common_modes[9].h = 960;
  common_modes[10].w = 1280;
  common_modes[10].h = 1024;
  common_modes[11].w = 1440;
  common_modes[11].h = 900;
  common_modes[12].w = 1400;
  common_modes[12].h = 1050;
  common_modes[13].w = 1680;
  common_modes[13].h = 1050;
  common_modes[14].w = 1600;
  common_modes[14].h = 1200;
  common_modes[15].w = 1920;
  common_modes[15].h = 1080;
  common_modes[16].w = 1920;
  common_modes[16].h = 1200;
  i = 0;
  goto ldv_48275;
  ldv_48274: ;
  if (((long )amdgpu_encoder->devices & 4L) != 0L) {
    if (common_modes[i].w > 1024 || common_modes[i].h > 768) {
      goto ldv_48273;
    } else {
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    if ((common_modes[i].w > native_mode->hdisplay || common_modes[i].h > native_mode->vdisplay) || (common_modes[i].w == native_mode->hdisplay && common_modes[i].h == native_mode->vdisplay)) {
      goto ldv_48273;
    } else {
    }
  } else {
  }
  if (common_modes[i].w <= 319 || common_modes[i].h <= 199) {
    goto ldv_48273;
  } else {
  }
  mode = drm_cvt_mode(dev, common_modes[i].w, common_modes[i].h, 60, 0, 0, 0);
  drm_mode_probed_add(connector, mode);
  ldv_48273:
  i = i + 1;
  ldv_48275: ;
  if (i <= 16) {
    goto ldv_48274;
  } else {
  }
  return;
}
}
static int amdgpu_connector_set_property(struct drm_connector *connector , struct drm_property *property ,
                                         uint64_t val )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct amdgpu_encoder_atom_dig *dig ;
  bool new_coherent_mode ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct drm_encoder const *__mptr___1 ;
  struct amdgpu_connector *amdgpu_connector___0 ;
  struct drm_connector const *__mptr___2 ;
  struct drm_encoder const *__mptr___3 ;
  struct drm_encoder const *__mptr___4 ;
  struct drm_encoder const *__mptr___5 ;
  struct drm_encoder const *__mptr___6 ;
  struct amdgpu_connector *amdgpu_connector___1 ;
  struct drm_connector const *__mptr___7 ;
  enum amdgpu_rmx_type rmx_type ;
  struct drm_encoder const *__mptr___8 ;
  struct drm_connector_helper_funcs const *connector_funcs ;
  struct drm_encoder const *__mptr___9 ;
  struct drm_encoder *tmp ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((unsigned long )adev->mode_info.coherent_mode_property == (unsigned long )property) {
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
    if ((unsigned long )amdgpu_encoder->enc_priv == (unsigned long )((void *)0)) {
      return (0);
    } else {
    }
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    new_coherent_mode = val != 0ULL;
    if ((int )dig->coherent_mode != (int )new_coherent_mode) {
      dig->coherent_mode = new_coherent_mode;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.audio_property == (unsigned long )property) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr___1 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___1;
    if ((uint64_t )amdgpu_connector->audio != val) {
      amdgpu_connector->audio = (enum amdgpu_connector_audio )val;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.dither_property == (unsigned long )property) {
    __mptr___2 = (struct drm_connector const *)connector;
    amdgpu_connector___0 = (struct amdgpu_connector *)__mptr___2;
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr___3 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___3;
    if ((uint64_t )amdgpu_connector___0->dither != val) {
      amdgpu_connector___0->dither = (enum amdgpu_connector_dither )val;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.underscan_property == (unsigned long )property) {
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr___4 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___4;
    if ((uint64_t )amdgpu_encoder->underscan_type != val) {
      amdgpu_encoder->underscan_type = (enum amdgpu_underscan_type )val;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.underscan_hborder_property == (unsigned long )property) {
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr___5 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___5;
    if ((uint64_t )amdgpu_encoder->underscan_hborder != val) {
      amdgpu_encoder->underscan_hborder = (u32 )val;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.underscan_vborder_property == (unsigned long )property) {
    encoder = amdgpu_connector_find_encoder(connector, 2);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    __mptr___6 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___6;
    if ((uint64_t )amdgpu_encoder->underscan_vborder != val) {
      amdgpu_encoder->underscan_vborder = (u32 )val;
      amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->mode_info.load_detect_property == (unsigned long )property) {
    __mptr___7 = (struct drm_connector const *)connector;
    amdgpu_connector___1 = (struct amdgpu_connector *)__mptr___7;
    if (val == 0ULL) {
      amdgpu_connector___1->dac_load_detect = 0;
    } else {
      amdgpu_connector___1->dac_load_detect = 1;
    }
  } else {
  }
  if ((unsigned long )dev->mode_config.scaling_mode_property == (unsigned long )property) {
    if ((unsigned long )connector->encoder != (unsigned long )((struct drm_encoder *)0)) {
      __mptr___8 = (struct drm_encoder const *)connector->encoder;
      amdgpu_encoder = (struct amdgpu_encoder *)__mptr___8;
    } else {
      connector_funcs = (struct drm_connector_helper_funcs const *)connector->helper_private;
      tmp = (*(connector_funcs->best_encoder))(connector);
      __mptr___9 = (struct drm_encoder const *)tmp;
      amdgpu_encoder = (struct amdgpu_encoder *)__mptr___9;
    }
    switch (val) {
    default: ;
    case 0ULL:
    rmx_type = 0;
    goto ldv_48317;
    case 2ULL:
    rmx_type = 2;
    goto ldv_48317;
    case 3ULL:
    rmx_type = 3;
    goto ldv_48317;
    case 1ULL:
    rmx_type = 1;
    goto ldv_48317;
    }
    ldv_48317: ;
    if ((unsigned int )amdgpu_encoder->rmx_type == (unsigned int )rmx_type) {
      return (0);
    } else {
    }
    if ((unsigned int )rmx_type != 0U && amdgpu_encoder->native_mode.clock == 0) {
      return (0);
    } else {
    }
    amdgpu_encoder->rmx_type = rmx_type;
    amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
  } else {
  }
  return (0);
}
}
static void amdgpu_connector_fixup_lcd_native_mode(struct drm_encoder *encoder , struct drm_connector *connector )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *native_mode ;
  struct drm_display_mode *t ;
  struct drm_display_mode *mode ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  struct list_head const *__mptr___4 ;
  long tmp ;
  struct list_head const *__mptr___5 ;
  long tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  native_mode = & amdgpu_encoder->native_mode;
  __mptr___0 = (struct list_head const *)connector->probed_modes.next;
  mode = (struct drm_display_mode *)__mptr___0;
  __mptr___1 = (struct list_head const *)mode->head.next;
  t = (struct drm_display_mode *)__mptr___1;
  goto ldv_48338;
  ldv_48337: ;
  if ((mode->type & 8U) != 0U) {
    if (mode->hdisplay != native_mode->hdisplay || mode->vdisplay != native_mode->vdisplay) {
      memcpy((void *)native_mode, (void const *)mode, 208UL);
    } else {
    }
  } else {
  }
  mode = t;
  __mptr___2 = (struct list_head const *)t->head.next;
  t = (struct drm_display_mode *)__mptr___2;
  ldv_48338: ;
  if ((unsigned long )(& mode->head) != (unsigned long )(& connector->probed_modes)) {
    goto ldv_48337;
  } else {
  }
  if (native_mode->clock == 0) {
    __mptr___3 = (struct list_head const *)connector->probed_modes.next;
    mode = (struct drm_display_mode *)__mptr___3;
    __mptr___4 = (struct list_head const *)mode->head.next;
    t = (struct drm_display_mode *)__mptr___4;
    goto ldv_48349;
    ldv_48348: ;
    if (mode->hdisplay == native_mode->hdisplay && mode->vdisplay == native_mode->vdisplay) {
      *native_mode = *mode;
      drm_mode_set_crtcinfo(native_mode, 1);
      tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp != 0L) {
        drm_ut_debug_printk("amdgpu_connector_fixup_lcd_native_mode", "Determined LVDS native mode details from EDID\n");
      } else {
      }
      goto ldv_48347;
    } else {
    }
    mode = t;
    __mptr___5 = (struct list_head const *)t->head.next;
    t = (struct drm_display_mode *)__mptr___5;
    ldv_48349: ;
    if ((unsigned long )(& mode->head) != (unsigned long )(& connector->probed_modes)) {
      goto ldv_48348;
    } else {
    }
    ldv_48347: ;
  } else {
  }
  if (native_mode->clock == 0) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_connector_fixup_lcd_native_mode", "No LVDS native mode details, disabling RMX\n");
    } else {
    }
    amdgpu_encoder->rmx_type = 0;
  } else {
  }
  return;
}
}
static int amdgpu_connector_lvds_get_modes(struct drm_connector *connector )
{
  struct drm_encoder *encoder ;
  int ret ;
  struct drm_display_mode *mode ;
  {
  ret = 0;
  amdgpu_connector_get_edid(connector);
  ret = amdgpu_connector_ddc_get_modes(connector);
  if (ret > 0) {
    encoder = amdgpu_connector_best_single_encoder(connector);
    if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
      amdgpu_connector_fixup_lcd_native_mode(encoder, connector);
      amdgpu_connector_add_common_modes(encoder, connector);
    } else {
    }
    return (ret);
  } else {
  }
  encoder = amdgpu_connector_best_single_encoder(connector);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    return (0);
  } else {
  }
  mode = amdgpu_connector_lcd_native_mode(encoder);
  if ((unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    ret = 1;
    drm_mode_probed_add(connector, mode);
    connector->display_info.width_mm = (unsigned int )mode->width_mm;
    connector->display_info.height_mm = (unsigned int )mode->height_mm;
    amdgpu_connector_add_common_modes(encoder, connector);
  } else {
  }
  return (ret);
}
}
static int amdgpu_connector_lvds_mode_valid(struct drm_connector *connector , struct drm_display_mode *mode )
{
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *native_mode ;
  {
  tmp = amdgpu_connector_best_single_encoder(connector);
  encoder = tmp;
  if (mode->hdisplay <= 319 || mode->vdisplay <= 239) {
    return (29);
  } else {
  }
  if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
    __mptr = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
    native_mode = & amdgpu_encoder->native_mode;
    if (mode->hdisplay > native_mode->hdisplay || mode->vdisplay > native_mode->vdisplay) {
      return (29);
    } else {
    }
    if ((unsigned int )amdgpu_encoder->rmx_type == 0U) {
      if (mode->hdisplay != native_mode->hdisplay || mode->vdisplay != native_mode->vdisplay) {
        return (29);
      } else {
      }
    } else {
    }
  } else {
  }
  return (0);
}
}
static enum drm_connector_status amdgpu_connector_lvds_detect(struct drm_connector *connector ,
                                                              bool force )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  enum drm_connector_status ret ;
  int r ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_display_mode *native_mode ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  tmp = amdgpu_connector_best_single_encoder(connector);
  encoder = tmp;
  ret = 2;
  r = pm_runtime_get_sync((connector->dev)->dev);
  if (r < 0) {
    return (2);
  } else {
  }
  if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
    __mptr___0 = (struct drm_encoder const *)encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
    native_mode = & amdgpu_encoder->native_mode;
    if (native_mode->hdisplay > 319 && native_mode->vdisplay > 239) {
      ret = 1;
    } else {
    }
  } else {
  }
  amdgpu_connector_get_edid(connector);
  if ((unsigned long )amdgpu_connector->edid != (unsigned long )((struct edid *)0)) {
    ret = 1;
  } else {
  }
  amdgpu_connector_update_scratch_regs(connector, ret);
  pm_runtime_mark_last_busy((connector->dev)->dev);
  pm_runtime_put_autosuspend((connector->dev)->dev);
  return (ret);
}
}
static void amdgpu_connector_destroy(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((int )(amdgpu_connector->ddc_bus)->has_aux) {
    drm_dp_aux_unregister(& (amdgpu_connector->ddc_bus)->aux);
  } else {
  }
  amdgpu_connector_free_edid(connector);
  kfree((void const *)amdgpu_connector->con_priv);
  drm_connector_unregister(connector);
  drm_connector_cleanup(connector);
  kfree((void const *)connector);
  return;
}
}
static int amdgpu_connector_set_lcd_property(struct drm_connector *connector , struct drm_property *property ,
                                             uint64_t value )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  enum amdgpu_rmx_type rmx_type ;
  long tmp ;
  struct drm_encoder const *__mptr ;
  struct drm_connector_helper_funcs const *connector_funcs ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_encoder *tmp___0 ;
  {
  dev = connector->dev;
  tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_connector_set_lcd_property", "\n");
  } else {
  }
  if ((unsigned long )dev->mode_config.scaling_mode_property != (unsigned long )property) {
    return (0);
  } else {
  }
  if ((unsigned long )connector->encoder != (unsigned long )((struct drm_encoder *)0)) {
    __mptr = (struct drm_encoder const *)connector->encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  } else {
    connector_funcs = (struct drm_connector_helper_funcs const *)connector->helper_private;
    tmp___0 = (*(connector_funcs->best_encoder))(connector);
    __mptr___0 = (struct drm_encoder const *)tmp___0;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  }
  switch (value) {
  case 0ULL:
  rmx_type = 0;
  goto ldv_48400;
  case 2ULL:
  rmx_type = 2;
  goto ldv_48400;
  case 3ULL:
  rmx_type = 3;
  goto ldv_48400;
  default: ;
  case 1ULL:
  rmx_type = 1;
  goto ldv_48400;
  }
  ldv_48400: ;
  if ((unsigned int )amdgpu_encoder->rmx_type == (unsigned int )rmx_type) {
    return (0);
  } else {
  }
  amdgpu_encoder->rmx_type = rmx_type;
  amdgpu_connector_property_change_mode(& amdgpu_encoder->base);
  return (0);
}
}
static struct drm_connector_helper_funcs const amdgpu_connector_lvds_helper_funcs = {& amdgpu_connector_lvds_get_modes,
    (enum drm_mode_status (*)(struct drm_connector * , struct drm_display_mode * ))(& amdgpu_connector_lvds_mode_valid),
    & amdgpu_connector_best_single_encoder};
static struct drm_connector_funcs const amdgpu_connector_lvds_funcs =
     {& drm_helper_connector_dpms, 0, 0, 0, & amdgpu_connector_lvds_detect, & drm_helper_probe_single_connector_modes,
    & amdgpu_connector_set_lcd_property, & amdgpu_connector_destroy, 0, 0, 0, 0, 0};
static int amdgpu_connector_vga_get_modes(struct drm_connector *connector )
{
  int ret ;
  {
  amdgpu_connector_get_edid(connector);
  ret = amdgpu_connector_ddc_get_modes(connector);
  return (ret);
}
}
static int amdgpu_connector_vga_mode_valid(struct drm_connector *connector , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((u32 )(mode->clock / 10) > adev->clock.max_pixel_clock) {
    return (15);
  } else {
  }
  return (0);
}
}
static enum drm_connector_status amdgpu_connector_vga_detect(struct drm_connector *connector ,
                                                             bool force )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_encoder *encoder ;
  struct drm_encoder_helper_funcs const *encoder_funcs ;
  bool dret ;
  enum drm_connector_status ret ;
  int r ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  dret = 0;
  ret = 2;
  r = pm_runtime_get_sync((connector->dev)->dev);
  if (r < 0) {
    return (2);
  } else {
  }
  encoder = amdgpu_connector_best_single_encoder(connector);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    ret = 2;
  } else {
  }
  if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    dret = amdgpu_ddc_probe(amdgpu_connector, 0);
  } else {
  }
  if ((int )dret) {
    amdgpu_connector->detected_by_load = 0;
    amdgpu_connector_free_edid(connector);
    amdgpu_connector_get_edid(connector);
    if ((unsigned long )amdgpu_connector->edid == (unsigned long )((struct edid *)0)) {
      drm_err("%s: probed a monitor but no|invalid EDID\n", connector->name);
      ret = 1;
    } else {
      amdgpu_connector->use_digital = (int )((signed char )(amdgpu_connector->edid)->input) < 0;
      if ((int )amdgpu_connector->use_digital && (int )amdgpu_connector->shared_ddc) {
        amdgpu_connector_free_edid(connector);
        ret = 2;
      } else {
        ret = 1;
      }
    }
  } else {
    if (! force) {
      if ((int )amdgpu_connector->detected_by_load) {
        ret = connector->status;
      } else {
      }
      goto out;
    } else {
    }
    if ((int )amdgpu_connector->dac_load_detect && (unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
      encoder_funcs = (struct drm_encoder_helper_funcs const *)encoder->helper_private;
      ret = (*(encoder_funcs->detect))(encoder, connector);
      if ((unsigned int )ret != 2U) {
        amdgpu_connector->detected_by_load = 1;
      } else {
      }
    } else {
    }
  }
  amdgpu_connector_update_scratch_regs(connector, ret);
  out:
  pm_runtime_mark_last_busy((connector->dev)->dev);
  pm_runtime_put_autosuspend((connector->dev)->dev);
  return (ret);
}
}
static struct drm_connector_helper_funcs const amdgpu_connector_vga_helper_funcs = {& amdgpu_connector_vga_get_modes,
    (enum drm_mode_status (*)(struct drm_connector * , struct drm_display_mode * ))(& amdgpu_connector_vga_mode_valid),
    & amdgpu_connector_best_single_encoder};
static struct drm_connector_funcs const amdgpu_connector_vga_funcs =
     {& drm_helper_connector_dpms, 0, 0, 0, & amdgpu_connector_vga_detect, & drm_helper_probe_single_connector_modes,
    & amdgpu_connector_set_property, & amdgpu_connector_destroy, 0, 0, 0, 0, 0};
static bool amdgpu_connector_check_hpd_status_unchanged(struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  enum drm_connector_status status ;
  bool tmp ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned int )amdgpu_connector->hpd.hpd != 255U) {
    tmp = (*((adev->mode_info.funcs)->hpd_sense))(adev, amdgpu_connector->hpd.hpd);
    if ((int )tmp) {
      status = 1;
    } else {
      status = 2;
    }
    if ((unsigned int )connector->status == (unsigned int )status) {
      return (1);
    } else {
    }
  } else {
  }
  return (0);
}
}
static enum drm_connector_status amdgpu_connector_dvi_detect(struct drm_connector *connector ,
                                                             bool force )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_encoder *encoder ;
  struct drm_encoder_helper_funcs const *encoder_funcs ;
  int i ;
  int r ;
  enum drm_connector_status ret ;
  bool dret ;
  bool broken_edid ;
  bool tmp ;
  struct drm_connector *list_connector ;
  struct amdgpu_connector *list_amdgpu_connector ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  bool tmp___0 ;
  int tmp___1 ;
  struct list_head const *__mptr___2 ;
  enum drm_connector_status lret ;
  long tmp___2 ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  encoder = (struct drm_encoder *)0;
  ret = 2;
  dret = 0;
  broken_edid = 0;
  r = pm_runtime_get_sync((connector->dev)->dev);
  if (r < 0) {
    return (2);
  } else {
  }
  if (! force) {
    tmp = amdgpu_connector_check_hpd_status_unchanged(connector);
    if ((int )tmp) {
      ret = connector->status;
      goto exit;
    } else {
    }
  } else {
  }
  if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    dret = amdgpu_ddc_probe(amdgpu_connector, 0);
  } else {
  }
  if ((int )dret) {
    amdgpu_connector->detected_by_load = 0;
    amdgpu_connector_free_edid(connector);
    amdgpu_connector_get_edid(connector);
    if ((unsigned long )amdgpu_connector->edid == (unsigned long )((struct edid *)0)) {
      drm_err("%s: probed a monitor but no|invalid EDID\n", connector->name);
      ret = 1;
      broken_edid = 1;
    } else {
      amdgpu_connector->use_digital = (int )((signed char )(amdgpu_connector->edid)->input) < 0;
      if (! amdgpu_connector->use_digital && (int )amdgpu_connector->shared_ddc) {
        amdgpu_connector_free_edid(connector);
        ret = 2;
      } else {
        ret = 1;
      }
      if ((int )amdgpu_connector->shared_ddc && (unsigned int )ret == 1U) {
        __mptr___0 = (struct list_head const *)dev->mode_config.connector_list.next;
        list_connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
        goto ldv_48468;
        ldv_48467: ;
        if ((unsigned long )connector == (unsigned long )list_connector) {
          goto ldv_48464;
        } else {
        }
        __mptr___1 = (struct drm_connector const *)list_connector;
        list_amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
        if ((int )list_amdgpu_connector->shared_ddc && (int )(list_amdgpu_connector->ddc_bus)->rec.i2c_id == (int )(amdgpu_connector->ddc_bus)->rec.i2c_id) {
          if (list_connector->connector_type != 1) {
            tmp___0 = (*((adev->mode_info.funcs)->hpd_sense))(adev, amdgpu_connector->hpd.hpd);
            if (tmp___0) {
              tmp___1 = 0;
            } else {
              tmp___1 = 1;
            }
            if (tmp___1) {
              amdgpu_connector_free_edid(connector);
              ret = 2;
            } else {
            }
          } else {
          }
        } else {
        }
        ldv_48464:
        __mptr___2 = (struct list_head const *)list_connector->head.next;
        list_connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
        ldv_48468: ;
        if ((unsigned long )(& list_connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
          goto ldv_48467;
        } else {
        }
      } else {
      }
    }
  } else {
  }
  if ((unsigned int )ret == 1U && (int )amdgpu_connector->use_digital) {
    goto out;
  } else {
  }
  if (connector->connector_type == 3 || connector->connector_type == 11) {
    goto out;
  } else {
  }
  if (! force) {
    if ((int )amdgpu_connector->detected_by_load) {
      ret = connector->status;
    } else {
    }
    goto out;
  } else {
  }
  if ((int )amdgpu_connector->dac_load_detect) {
    i = 0;
    goto ldv_48476;
    ldv_48475: ;
    if (connector->encoder_ids[i] == 0U) {
      goto ldv_48471;
    } else {
    }
    encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      goto ldv_48472;
    } else {
    }
    if (encoder->encoder_type != 1 && encoder->encoder_type != 4) {
      goto ldv_48472;
    } else {
    }
    encoder_funcs = (struct drm_encoder_helper_funcs const *)encoder->helper_private;
    if ((unsigned long )encoder_funcs->detect != (unsigned long )((enum drm_connector_status (* )(struct drm_encoder * ,
                                                                                                             struct drm_connector * ))0)) {
      if (! broken_edid) {
        if ((unsigned int )ret != 1U) {
          ret = (*(encoder_funcs->detect))(encoder, connector);
          if ((unsigned int )ret == 1U) {
            amdgpu_connector->use_digital = 0;
          } else {
          }
          if ((unsigned int )ret != 2U) {
            amdgpu_connector->detected_by_load = 1;
          } else {
          }
        } else {
        }
      } else {
        amdgpu_connector->use_digital = 1;
        lret = (*(encoder_funcs->detect))(encoder, connector);
        tmp___2 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
        if (tmp___2 != 0L) {
          drm_ut_debug_printk("amdgpu_connector_dvi_detect", "load_detect %x returned: %x\n",
                              encoder->encoder_type, (unsigned int )lret);
        } else {
        }
        if ((unsigned int )lret == 1U) {
          amdgpu_connector->use_digital = 0;
        } else {
        }
      }
      goto ldv_48471;
    } else {
    }
    ldv_48472:
    i = i + 1;
    ldv_48476: ;
    if (i <= 2) {
      goto ldv_48475;
    } else {
    }
    ldv_48471: ;
  } else {
  }
  out:
  amdgpu_connector_update_scratch_regs(connector, ret);
  exit:
  pm_runtime_mark_last_busy((connector->dev)->dev);
  pm_runtime_put_autosuspend((connector->dev)->dev);
  return (ret);
}
}
static struct drm_encoder *amdgpu_connector_dvi_encoder(struct drm_connector *connector )
{
  int enc_id ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_encoder *encoder ;
  int i ;
  struct drm_encoder *tmp ;
  {
  enc_id = (int )connector->encoder_ids[0];
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  i = 0;
  goto ldv_48489;
  ldv_48488: ;
  if (connector->encoder_ids[i] == 0U) {
    goto ldv_48486;
  } else {
  }
  encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    goto ldv_48487;
  } else {
  }
  if ((int )amdgpu_connector->use_digital) {
    if (encoder->encoder_type == 2) {
      return (encoder);
    } else {
    }
  } else
  if (encoder->encoder_type == 1 || encoder->encoder_type == 4) {
    return (encoder);
  } else {
  }
  ldv_48487:
  i = i + 1;
  ldv_48489: ;
  if (i <= 2) {
    goto ldv_48488;
  } else {
  }
  ldv_48486: ;
  if (enc_id != 0) {
    tmp = drm_encoder_find(connector->dev, (u32 )enc_id);
    return (tmp);
  } else {
  }
  return ((struct drm_encoder *)0);
}
}
static void amdgpu_connector_dvi_force(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned int )connector->force == 2U) {
    amdgpu_connector->use_digital = 0;
  } else {
  }
  if ((unsigned int )connector->force == 3U) {
    amdgpu_connector->use_digital = 1;
  } else {
  }
  return;
}
}
static int amdgpu_connector_dvi_mode_valid(struct drm_connector *connector , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct edid *tmp ;
  bool tmp___0 ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((int )amdgpu_connector->use_digital && mode->clock > 165000) {
    if (((unsigned int )amdgpu_connector->connector_object_id == 2U || (unsigned int )amdgpu_connector->connector_object_id == 4U) || (unsigned int )amdgpu_connector->connector_object_id == 13U) {
      return (0);
    } else {
      tmp = amdgpu_connector_edid(connector);
      tmp___0 = drm_detect_hdmi_monitor(tmp);
      if ((int )tmp___0) {
        if (mode->clock > 340000) {
          return (15);
        } else {
          return (0);
        }
      } else {
        return (15);
      }
    }
  } else {
  }
  if ((u32 )(mode->clock / 10) > adev->clock.max_pixel_clock) {
    return (15);
  } else {
  }
  return (0);
}
}
static struct drm_connector_helper_funcs const amdgpu_connector_dvi_helper_funcs = {& amdgpu_connector_vga_get_modes,
    (enum drm_mode_status (*)(struct drm_connector * , struct drm_display_mode * ))(& amdgpu_connector_dvi_mode_valid),
    & amdgpu_connector_dvi_encoder};
static struct drm_connector_funcs const amdgpu_connector_dvi_funcs =
     {& drm_helper_connector_dpms, 0, 0, 0, & amdgpu_connector_dvi_detect, & drm_helper_probe_single_connector_modes,
    & amdgpu_connector_set_property, & amdgpu_connector_destroy, & amdgpu_connector_dvi_force,
    0, 0, 0, 0};
static int amdgpu_connector_dp_get_modes(struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *amdgpu_dig_connector ;
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  int ret ;
  struct drm_display_mode *mode ;
  u16 tmp___0 ;
  u16 tmp___1 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  tmp = amdgpu_connector_best_single_encoder(connector);
  encoder = tmp;
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    if (connector->connector_type == 14) {
      if (! amdgpu_dig_connector->edp_on) {
        amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
      } else {
      }
      amdgpu_connector_get_edid(connector);
      ret = amdgpu_connector_ddc_get_modes(connector);
      if (! amdgpu_dig_connector->edp_on) {
        amdgpu_atombios_encoder_set_edp_panel_power(connector, 13);
      } else {
      }
    } else {
      tmp___0 = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
      if ((unsigned int )tmp___0 != 0U) {
        if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
          amdgpu_atombios_encoder_setup_ext_encoder_ddc(encoder);
        } else {
        }
      } else {
      }
      amdgpu_connector_get_edid(connector);
      ret = amdgpu_connector_ddc_get_modes(connector);
    }
    if (ret > 0) {
      if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
        amdgpu_connector_fixup_lcd_native_mode(encoder, connector);
        amdgpu_connector_add_common_modes(encoder, connector);
      } else {
      }
      return (ret);
    } else {
    }
    if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
      return (0);
    } else {
    }
    mode = amdgpu_connector_lcd_native_mode(encoder);
    if ((unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
      ret = 1;
      drm_mode_probed_add(connector, mode);
      connector->display_info.width_mm = (unsigned int )mode->width_mm;
      connector->display_info.height_mm = (unsigned int )mode->height_mm;
      amdgpu_connector_add_common_modes(encoder, connector);
    } else {
    }
  } else {
    tmp___1 = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
    if ((unsigned int )tmp___1 != 0U) {
      if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
        amdgpu_atombios_encoder_setup_ext_encoder_ddc(encoder);
      } else {
      }
    } else {
    }
    amdgpu_connector_get_edid(connector);
    ret = amdgpu_connector_ddc_get_modes(connector);
    amdgpu_get_native_mode(connector);
  }
  return (ret);
}
}
u16 amdgpu_connector_encoder_get_dp_bridge_encoder_id(struct drm_connector *connector )
{
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  int i ;
  struct drm_encoder const *__mptr ;
  {
  i = 0;
  goto ldv_48532;
  ldv_48531: ;
  if (connector->encoder_ids[i] == 0U) {
    goto ldv_48523;
  } else {
  }
  encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    goto ldv_48524;
  } else {
  }
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  switch (amdgpu_encoder->encoder_id) {
  case 35U: ;
  case 34U: ;
  return ((u16 )amdgpu_encoder->encoder_id);
  default: ;
  goto ldv_48530;
  }
  ldv_48530: ;
  ldv_48524:
  i = i + 1;
  ldv_48532: ;
  if (i <= 2) {
    goto ldv_48531;
  } else {
  }
  ldv_48523: ;
  return (0U);
}
}
static bool amdgpu_connector_encoder_is_hbr2(struct drm_connector *connector )
{
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  int i ;
  bool found ;
  struct drm_encoder const *__mptr ;
  {
  found = 0;
  i = 0;
  goto ldv_48545;
  ldv_48544: ;
  if (connector->encoder_ids[i] == 0U) {
    goto ldv_48540;
  } else {
  }
  encoder = drm_encoder_find(connector->dev, connector->encoder_ids[i]);
  if ((unsigned long )encoder == (unsigned long )((struct drm_encoder *)0)) {
    goto ldv_48541;
  } else {
  }
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if ((int )amdgpu_encoder->caps & 1) {
    found = 1;
  } else {
  }
  ldv_48541:
  i = i + 1;
  ldv_48545: ;
  if (i <= 2) {
    goto ldv_48544;
  } else {
  }
  ldv_48540: ;
  return (found);
}
}
bool amdgpu_connector_is_dp12_capable(struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  bool tmp ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if (adev->clock.default_dispclk > 53899U) {
    tmp = amdgpu_connector_encoder_is_hbr2(connector);
    if ((int )tmp) {
      return (1);
    } else {
    }
  } else {
  }
  return (0);
}
}
static enum drm_connector_status amdgpu_connector_dp_detect(struct drm_connector *connector ,
                                                            bool force )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  enum drm_connector_status ret ;
  struct amdgpu_connector_atom_dig *amdgpu_dig_connector ;
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  int r ;
  bool tmp___0 ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_display_mode *native_mode ;
  int tmp___1 ;
  struct drm_encoder_helper_funcs const *encoder_funcs ;
  bool tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  bool tmp___5 ;
  u16 tmp___6 ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  ret = 2;
  amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  tmp = amdgpu_connector_best_single_encoder(connector);
  encoder = tmp;
  r = pm_runtime_get_sync((connector->dev)->dev);
  if (r < 0) {
    return (2);
  } else {
  }
  if (! force) {
    tmp___0 = amdgpu_connector_check_hpd_status_unchanged(connector);
    if ((int )tmp___0) {
      ret = connector->status;
      goto out;
    } else {
    }
  } else {
  }
  amdgpu_connector_free_edid(connector);
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
      __mptr___0 = (struct drm_encoder const *)encoder;
      amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
      native_mode = & amdgpu_encoder->native_mode;
      if (native_mode->hdisplay > 319 && native_mode->vdisplay > 239) {
        ret = 1;
      } else {
      }
    } else {
    }
    amdgpu_dig_connector->dp_sink_type = 19U;
    if (! amdgpu_dig_connector->edp_on) {
      amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
    } else {
    }
    tmp___1 = amdgpu_atombios_dp_get_dpcd(amdgpu_connector);
    if (tmp___1 == 0) {
      ret = 1;
    } else {
    }
    if (! amdgpu_dig_connector->edp_on) {
      amdgpu_atombios_encoder_set_edp_panel_power(connector, 13);
    } else {
    }
  } else {
    tmp___6 = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
    if ((unsigned int )tmp___6 != 0U) {
      amdgpu_dig_connector->dp_sink_type = 19U;
      amdgpu_atombios_dp_get_dpcd(amdgpu_connector);
      if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
        amdgpu_atombios_encoder_setup_ext_encoder_ddc(encoder);
        tmp___2 = amdgpu_ddc_probe(amdgpu_connector, 1);
        if ((int )tmp___2) {
          ret = 1;
        } else
        if ((int )amdgpu_connector->dac_load_detect) {
          encoder_funcs = (struct drm_encoder_helper_funcs const *)encoder->helper_private;
          ret = (*(encoder_funcs->detect))(encoder, connector);
        } else {
        }
      } else {
      }
    } else {
      amdgpu_dig_connector->dp_sink_type = amdgpu_atombios_dp_get_sinktype(amdgpu_connector);
      tmp___5 = (*((adev->mode_info.funcs)->hpd_sense))(adev, amdgpu_connector->hpd.hpd);
      if ((int )tmp___5) {
        ret = 1;
        if ((unsigned int )amdgpu_dig_connector->dp_sink_type == 19U) {
          amdgpu_atombios_dp_get_dpcd(amdgpu_connector);
        } else {
        }
      } else
      if ((unsigned int )amdgpu_dig_connector->dp_sink_type == 19U) {
        tmp___3 = amdgpu_atombios_dp_get_dpcd(amdgpu_connector);
        if (tmp___3 == 0) {
          ret = 1;
        } else {
        }
      } else {
        tmp___4 = amdgpu_ddc_probe(amdgpu_connector, 0);
        if ((int )tmp___4) {
          ret = 1;
        } else {
        }
      }
    }
  }
  amdgpu_connector_update_scratch_regs(connector, ret);
  out:
  pm_runtime_mark_last_busy((connector->dev)->dev);
  pm_runtime_put_autosuspend((connector->dev)->dev);
  return (ret);
}
}
static int amdgpu_connector_dp_mode_valid(struct drm_connector *connector , struct drm_display_mode *mode )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *amdgpu_dig_connector ;
  struct drm_encoder *encoder ;
  struct drm_encoder *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_display_mode *native_mode ;
  int tmp___0 ;
  struct edid *tmp___1 ;
  bool tmp___2 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    tmp = amdgpu_connector_best_single_encoder(connector);
    encoder = tmp;
    if (mode->hdisplay <= 319 || mode->vdisplay <= 239) {
      return (29);
    } else {
    }
    if ((unsigned long )encoder != (unsigned long )((struct drm_encoder *)0)) {
      __mptr___0 = (struct drm_encoder const *)encoder;
      amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
      native_mode = & amdgpu_encoder->native_mode;
      if (mode->hdisplay > native_mode->hdisplay || mode->vdisplay > native_mode->vdisplay) {
        return (29);
      } else {
      }
      if ((unsigned int )amdgpu_encoder->rmx_type == 0U) {
        if (mode->hdisplay != native_mode->hdisplay || mode->vdisplay != native_mode->vdisplay) {
          return (29);
        } else {
        }
      } else {
      }
    } else {
    }
    return (0);
  } else
  if ((unsigned int )amdgpu_dig_connector->dp_sink_type == 19U || (unsigned int )amdgpu_dig_connector->dp_sink_type == 20U) {
    tmp___0 = amdgpu_atombios_dp_mode_valid_helper(connector, mode);
    return (tmp___0);
  } else {
    tmp___1 = amdgpu_connector_edid(connector);
    tmp___2 = drm_detect_hdmi_monitor(tmp___1);
    if ((int )tmp___2) {
      if (mode->clock > 340000) {
        return (15);
      } else {
      }
    } else
    if (mode->clock > 165000) {
      return (15);
    } else {
    }
  }
  return (0);
}
}
static struct drm_connector_helper_funcs const amdgpu_connector_dp_helper_funcs = {& amdgpu_connector_dp_get_modes,
    (enum drm_mode_status (*)(struct drm_connector * , struct drm_display_mode * ))(& amdgpu_connector_dp_mode_valid),
    & amdgpu_connector_dvi_encoder};
static struct drm_connector_funcs const amdgpu_connector_dp_funcs =
     {& drm_helper_connector_dpms, 0, 0, 0, & amdgpu_connector_dp_detect, & drm_helper_probe_single_connector_modes,
    & amdgpu_connector_set_property, & amdgpu_connector_destroy, & amdgpu_connector_dvi_force,
    0, 0, 0, 0};
static struct drm_connector_funcs const amdgpu_connector_edp_funcs =
     {& drm_helper_connector_dpms, 0, 0, 0, & amdgpu_connector_dp_detect, & drm_helper_probe_single_connector_modes,
    & amdgpu_connector_set_lcd_property, & amdgpu_connector_destroy, & amdgpu_connector_dvi_force,
    0, 0, 0, 0};
void amdgpu_connector_add(struct amdgpu_device *adev , u32 connector_id , u32 supported_device ,
                          int connector_type , struct amdgpu_i2c_bus_rec *i2c_bus ,
                          u16 connector_object_id , struct amdgpu_hpd *hpd , struct amdgpu_router *router )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct amdgpu_connector_atom_dig *amdgpu_dig_connector ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  u32 subpixel_order ;
  bool shared_ddc ;
  bool is_dp_bridge ;
  bool has_aux ;
  struct list_head const *__mptr ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct drm_encoder const *__mptr___3 ;
  struct list_head const *__mptr___4 ;
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  void *tmp___3 ;
  void *tmp___4 ;
  void *tmp___5 ;
  {
  dev = adev->ddev;
  subpixel_order = 5U;
  shared_ddc = 0;
  is_dp_bridge = 0;
  has_aux = 0;
  if (connector_type == 0) {
    return;
  } else {
  }
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_48613;
  ldv_48612:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if (amdgpu_connector->connector_id == connector_id) {
    amdgpu_connector->devices = amdgpu_connector->devices | supported_device;
    return;
  } else {
  }
  if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0) && (int )i2c_bus->valid) {
    if ((int )(amdgpu_connector->ddc_bus)->rec.i2c_id == (int )i2c_bus->i2c_id) {
      amdgpu_connector->shared_ddc = 1;
      shared_ddc = 1;
    } else {
    }
    if (((unsigned long )amdgpu_connector->router_bus != (unsigned long )((struct amdgpu_i2c_chan *)0) && (int )router->ddc_valid) && amdgpu_connector->router.router_id == router->router_id) {
      amdgpu_connector->shared_ddc = 0;
      shared_ddc = 0;
    } else {
    }
  } else {
  }
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_48613: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_48612;
  } else {
  }
  __mptr___2 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___2 + 0xfffffffffffffff8UL;
  goto ldv_48626;
  ldv_48625:
  __mptr___3 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___3;
  if ((amdgpu_encoder->devices & supported_device) != 0U) {
    switch (amdgpu_encoder->encoder_id) {
    case 35U: ;
    case 34U:
    is_dp_bridge = 1;
    goto ldv_48623;
    default: ;
    goto ldv_48623;
    }
    ldv_48623: ;
  } else {
  }
  __mptr___4 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___4 + 0xfffffffffffffff8UL;
  ldv_48626: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_48625;
  } else {
  }
  tmp = kzalloc(1120UL, 208U);
  amdgpu_connector = (struct amdgpu_connector *)tmp;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    return;
  } else {
  }
  connector = & amdgpu_connector->base;
  amdgpu_connector->connector_id = connector_id;
  amdgpu_connector->devices = supported_device;
  amdgpu_connector->shared_ddc = shared_ddc;
  amdgpu_connector->connector_object_id = connector_object_id;
  amdgpu_connector->hpd = *hpd;
  amdgpu_connector->router = *router;
  if ((int )router->ddc_valid || (int )router->cd_valid) {
    amdgpu_connector->router_bus = amdgpu_i2c_lookup(adev, & router->i2c_info);
    if ((unsigned long )amdgpu_connector->router_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
      drm_err("Failed to assign router i2c bus! Check dmesg for i2c errors.\n");
    } else {
    }
  } else {
  }
  if ((int )is_dp_bridge) {
    tmp___0 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___0;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        has_aux = 1;
      } else {
        drm_err("DP: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      }
    } else {
    }
    switch (connector_type) {
    case 1: ;
    case 4: ;
    default:
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_dp_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dp_helper_funcs);
    connector->interlace_allowed = 1;
    connector->doublescan_allowed = 1;
    amdgpu_connector->dac_load_detect = 1;
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.load_detect_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    goto ldv_48632;
    case 2: ;
    case 3: ;
    case 11: ;
    case 12: ;
    case 10:
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_dp_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dp_helper_funcs);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_hborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_vborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.dither_property,
                               0ULL);
    if (amdgpu_audio != 0) {
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.audio_property,
                                 2ULL);
    } else {
    }
    subpixel_order = 1U;
    connector->interlace_allowed = 1;
    if (connector_type == 12) {
      connector->doublescan_allowed = 1;
    } else {
      connector->doublescan_allowed = 0;
    }
    if (connector_type == 2) {
      amdgpu_connector->dac_load_detect = 1;
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.load_detect_property,
                                 1ULL);
    } else {
    }
    goto ldv_48632;
    case 7: ;
    case 14:
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_edp_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dp_helper_funcs);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               1ULL);
    subpixel_order = 1U;
    connector->interlace_allowed = 0;
    connector->doublescan_allowed = 0;
    goto ldv_48632;
    }
    ldv_48632: ;
  } else {
    switch (connector_type) {
    case 1:
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_vga_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_vga_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        drm_err("VGA: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      } else {
      }
    } else {
    }
    amdgpu_connector->dac_load_detect = 1;
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.load_detect_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    amdgpu_connector->hpd.hpd = 255;
    connector->polled = 2U;
    connector->interlace_allowed = 1;
    connector->doublescan_allowed = 1;
    goto ldv_48641;
    case 4:
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_vga_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_vga_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        drm_err("DVIA: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      } else {
      }
    } else {
    }
    amdgpu_connector->dac_load_detect = 1;
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.load_detect_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    amdgpu_connector->hpd.hpd = 255;
    connector->interlace_allowed = 1;
    connector->doublescan_allowed = 1;
    goto ldv_48641;
    case 2: ;
    case 3:
    tmp___1 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___1;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_dvi_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dvi_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        drm_err("DVI: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      } else {
      }
    } else {
    }
    subpixel_order = 1U;
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.coherent_mode_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_hborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_vborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    if (amdgpu_audio != 0) {
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.audio_property,
                                 2ULL);
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.dither_property,
                               0ULL);
    if (connector_type == 2) {
      amdgpu_connector->dac_load_detect = 1;
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.load_detect_property,
                                 1ULL);
    } else {
    }
    connector->interlace_allowed = 1;
    if (connector_type == 2) {
      connector->doublescan_allowed = 1;
    } else {
      connector->doublescan_allowed = 0;
    }
    goto ldv_48641;
    case 11: ;
    case 12:
    tmp___2 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___2;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_dvi_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dvi_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        drm_err("HDMI: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      } else {
      }
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.coherent_mode_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_hborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_vborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    if (amdgpu_audio != 0) {
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.audio_property,
                                 2ULL);
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.dither_property,
                               0ULL);
    subpixel_order = 1U;
    connector->interlace_allowed = 1;
    if (connector_type == 12) {
      connector->doublescan_allowed = 1;
    } else {
      connector->doublescan_allowed = 0;
    }
    goto ldv_48641;
    case 10:
    tmp___3 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___3;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_dp_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dp_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        has_aux = 1;
      } else {
        drm_err("DP: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      }
    } else {
    }
    subpixel_order = 1U;
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.coherent_mode_property,
                               1ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_hborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.underscan_vborder_property,
                               0ULL);
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               0ULL);
    if (amdgpu_audio != 0) {
      drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.audio_property,
                                 2ULL);
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, adev->mode_info.dither_property,
                               0ULL);
    connector->interlace_allowed = 1;
    connector->doublescan_allowed = 0;
    goto ldv_48641;
    case 14:
    tmp___4 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___4;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_edp_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_dp_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        has_aux = 1;
      } else {
        drm_err("DP: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      }
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               1ULL);
    subpixel_order = 1U;
    connector->interlace_allowed = 0;
    connector->doublescan_allowed = 0;
    goto ldv_48641;
    case 7:
    tmp___5 = kzalloc(28UL, 208U);
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)tmp___5;
    if ((unsigned long )amdgpu_dig_connector == (unsigned long )((struct amdgpu_connector_atom_dig *)0)) {
      goto failed;
    } else {
    }
    amdgpu_connector->con_priv = (void *)amdgpu_dig_connector;
    drm_connector_init(dev, & amdgpu_connector->base, & amdgpu_connector_lvds_funcs,
                       connector_type);
    drm_connector_helper_add(& amdgpu_connector->base, & amdgpu_connector_lvds_helper_funcs);
    if ((int )i2c_bus->valid) {
      amdgpu_connector->ddc_bus = amdgpu_i2c_lookup(adev, i2c_bus);
      if ((unsigned long )amdgpu_connector->ddc_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        drm_err("LVDS: Failed to assign ddc bus! Check dmesg for i2c errors.\n");
      } else {
      }
    } else {
    }
    drm_object_attach_property(& amdgpu_connector->base.base, dev->mode_config.scaling_mode_property,
                               1ULL);
    subpixel_order = 1U;
    connector->interlace_allowed = 0;
    connector->doublescan_allowed = 0;
    goto ldv_48641;
    }
    ldv_48641: ;
  }
  if ((unsigned int )amdgpu_connector->hpd.hpd == 255U) {
    if ((int )i2c_bus->valid) {
      connector->polled = 2U;
    } else {
    }
  } else {
    connector->polled = 1U;
  }
  connector->display_info.subpixel_order = (enum subpixel_order )subpixel_order;
  drm_connector_register(connector);
  if ((int )has_aux) {
    amdgpu_atombios_dp_aux_init(amdgpu_connector);
  } else {
  }
  return;
  failed:
  drm_connector_cleanup(connector);
  kfree((void const *)connector);
  return;
}
}
extern int ldv_probe_170(void) ;
extern int ldv_probe_172(void) ;
extern int ldv_probe_165(void) ;
extern int ldv_probe_168(void) ;
extern int ldv_probe_166(void) ;
void ldv_initialize_drm_connector_funcs_168(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_dvi_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_helper_funcs_171(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_vga_helper_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_funcs_166(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_dp_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_helper_funcs_169(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_dvi_helper_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_funcs_165(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_edp_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_helper_funcs_167(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_dp_helper_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_helper_funcs_173(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_lvds_helper_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_funcs_172(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_lvds_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_initialize_drm_connector_funcs_170(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(936UL);
  amdgpu_connector_vga_funcs_group0 = (struct drm_connector *)tmp;
  return;
}
}
void ldv_main_exported_170(void)
{
  u32 ldvarg500 ;
  u32 ldvarg499 ;
  bool ldvarg503 ;
  struct drm_property *ldvarg502 ;
  void *tmp ;
  uint64_t ldvarg501 ;
  int ldvarg504 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(104UL);
  ldvarg502 = (struct drm_property *)tmp;
  ldv_memset((void *)(& ldvarg500), 0, 4UL);
  ldv_memset((void *)(& ldvarg499), 0, 4UL);
  ldv_memset((void *)(& ldvarg503), 0, 1UL);
  ldv_memset((void *)(& ldvarg501), 0, 8UL);
  ldv_memset((void *)(& ldvarg504), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_170 == 1) {
    drm_helper_connector_dpms(amdgpu_connector_vga_funcs_group0, ldvarg504);
    ldv_state_variable_170 = 1;
  } else {
  }
  if (ldv_state_variable_170 == 2) {
    drm_helper_connector_dpms(amdgpu_connector_vga_funcs_group0, ldvarg504);
    ldv_state_variable_170 = 2;
  } else {
  }
  goto ldv_48697;
  case 1: ;
  if (ldv_state_variable_170 == 1) {
    amdgpu_connector_vga_detect(amdgpu_connector_vga_funcs_group0, (int )ldvarg503);
    ldv_state_variable_170 = 1;
  } else {
  }
  if (ldv_state_variable_170 == 2) {
    amdgpu_connector_vga_detect(amdgpu_connector_vga_funcs_group0, (int )ldvarg503);
    ldv_state_variable_170 = 2;
  } else {
  }
  goto ldv_48697;
  case 2: ;
  if (ldv_state_variable_170 == 1) {
    amdgpu_connector_set_property(amdgpu_connector_vga_funcs_group0, ldvarg502, ldvarg501);
    ldv_state_variable_170 = 1;
  } else {
  }
  if (ldv_state_variable_170 == 2) {
    amdgpu_connector_set_property(amdgpu_connector_vga_funcs_group0, ldvarg502, ldvarg501);
    ldv_state_variable_170 = 2;
  } else {
  }
  goto ldv_48697;
  case 3: ;
  if (ldv_state_variable_170 == 2) {
    amdgpu_connector_destroy(amdgpu_connector_vga_funcs_group0);
    ldv_state_variable_170 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48697;
  case 4: ;
  if (ldv_state_variable_170 == 1) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_vga_funcs_group0, ldvarg500,
                                            ldvarg499);
    ldv_state_variable_170 = 1;
  } else {
  }
  if (ldv_state_variable_170 == 2) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_vga_funcs_group0, ldvarg500,
                                            ldvarg499);
    ldv_state_variable_170 = 2;
  } else {
  }
  goto ldv_48697;
  case 5: ;
  if (ldv_state_variable_170 == 1) {
    ldv_probe_170();
    ldv_state_variable_170 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48697;
  default:
  ldv_stop();
  }
  ldv_48697: ;
  return;
}
}
void ldv_main_exported_165(void)
{
  struct drm_property *ldvarg294 ;
  void *tmp ;
  u32 ldvarg291 ;
  u32 ldvarg292 ;
  int ldvarg296 ;
  bool ldvarg295 ;
  uint64_t ldvarg293 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(104UL);
  ldvarg294 = (struct drm_property *)tmp;
  ldv_memset((void *)(& ldvarg291), 0, 4UL);
  ldv_memset((void *)(& ldvarg292), 0, 4UL);
  ldv_memset((void *)(& ldvarg296), 0, 4UL);
  ldv_memset((void *)(& ldvarg295), 0, 1UL);
  ldv_memset((void *)(& ldvarg293), 0, 8UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_165 == 1) {
    drm_helper_connector_dpms(amdgpu_connector_edp_funcs_group0, ldvarg296);
    ldv_state_variable_165 = 1;
  } else {
  }
  if (ldv_state_variable_165 == 2) {
    drm_helper_connector_dpms(amdgpu_connector_edp_funcs_group0, ldvarg296);
    ldv_state_variable_165 = 2;
  } else {
  }
  goto ldv_48714;
  case 1: ;
  if (ldv_state_variable_165 == 1) {
    amdgpu_connector_dvi_force(amdgpu_connector_edp_funcs_group0);
    ldv_state_variable_165 = 1;
  } else {
  }
  if (ldv_state_variable_165 == 2) {
    amdgpu_connector_dvi_force(amdgpu_connector_edp_funcs_group0);
    ldv_state_variable_165 = 2;
  } else {
  }
  goto ldv_48714;
  case 2: ;
  if (ldv_state_variable_165 == 1) {
    amdgpu_connector_dp_detect(amdgpu_connector_edp_funcs_group0, (int )ldvarg295);
    ldv_state_variable_165 = 1;
  } else {
  }
  if (ldv_state_variable_165 == 2) {
    amdgpu_connector_dp_detect(amdgpu_connector_edp_funcs_group0, (int )ldvarg295);
    ldv_state_variable_165 = 2;
  } else {
  }
  goto ldv_48714;
  case 3: ;
  if (ldv_state_variable_165 == 1) {
    amdgpu_connector_set_lcd_property(amdgpu_connector_edp_funcs_group0, ldvarg294,
                                      ldvarg293);
    ldv_state_variable_165 = 1;
  } else {
  }
  if (ldv_state_variable_165 == 2) {
    amdgpu_connector_set_lcd_property(amdgpu_connector_edp_funcs_group0, ldvarg294,
                                      ldvarg293);
    ldv_state_variable_165 = 2;
  } else {
  }
  goto ldv_48714;
  case 4: ;
  if (ldv_state_variable_165 == 2) {
    amdgpu_connector_destroy(amdgpu_connector_edp_funcs_group0);
    ldv_state_variable_165 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48714;
  case 5: ;
  if (ldv_state_variable_165 == 1) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_edp_funcs_group0, ldvarg292,
                                            ldvarg291);
    ldv_state_variable_165 = 1;
  } else {
  }
  if (ldv_state_variable_165 == 2) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_edp_funcs_group0, ldvarg292,
                                            ldvarg291);
    ldv_state_variable_165 = 2;
  } else {
  }
  goto ldv_48714;
  case 6: ;
  if (ldv_state_variable_165 == 1) {
    ldv_probe_165();
    ldv_state_variable_165 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48714;
  default:
  ldv_stop();
  }
  ldv_48714: ;
  return;
}
}
void ldv_main_exported_168(void)
{
  int ldvarg342 ;
  uint64_t ldvarg339 ;
  struct drm_property *ldvarg340 ;
  void *tmp ;
  u32 ldvarg337 ;
  u32 ldvarg338 ;
  bool ldvarg341 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(104UL);
  ldvarg340 = (struct drm_property *)tmp;
  ldv_memset((void *)(& ldvarg342), 0, 4UL);
  ldv_memset((void *)(& ldvarg339), 0, 8UL);
  ldv_memset((void *)(& ldvarg337), 0, 4UL);
  ldv_memset((void *)(& ldvarg338), 0, 4UL);
  ldv_memset((void *)(& ldvarg341), 0, 1UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_168 == 1) {
    drm_helper_connector_dpms(amdgpu_connector_dvi_funcs_group0, ldvarg342);
    ldv_state_variable_168 = 1;
  } else {
  }
  if (ldv_state_variable_168 == 2) {
    drm_helper_connector_dpms(amdgpu_connector_dvi_funcs_group0, ldvarg342);
    ldv_state_variable_168 = 2;
  } else {
  }
  goto ldv_48732;
  case 1: ;
  if (ldv_state_variable_168 == 1) {
    amdgpu_connector_dvi_force(amdgpu_connector_dvi_funcs_group0);
    ldv_state_variable_168 = 1;
  } else {
  }
  if (ldv_state_variable_168 == 2) {
    amdgpu_connector_dvi_force(amdgpu_connector_dvi_funcs_group0);
    ldv_state_variable_168 = 2;
  } else {
  }
  goto ldv_48732;
  case 2: ;
  if (ldv_state_variable_168 == 1) {
    amdgpu_connector_dvi_detect(amdgpu_connector_dvi_funcs_group0, (int )ldvarg341);
    ldv_state_variable_168 = 1;
  } else {
  }
  if (ldv_state_variable_168 == 2) {
    amdgpu_connector_dvi_detect(amdgpu_connector_dvi_funcs_group0, (int )ldvarg341);
    ldv_state_variable_168 = 2;
  } else {
  }
  goto ldv_48732;
  case 3: ;
  if (ldv_state_variable_168 == 1) {
    amdgpu_connector_set_property(amdgpu_connector_dvi_funcs_group0, ldvarg340, ldvarg339);
    ldv_state_variable_168 = 1;
  } else {
  }
  if (ldv_state_variable_168 == 2) {
    amdgpu_connector_set_property(amdgpu_connector_dvi_funcs_group0, ldvarg340, ldvarg339);
    ldv_state_variable_168 = 2;
  } else {
  }
  goto ldv_48732;
  case 4: ;
  if (ldv_state_variable_168 == 2) {
    amdgpu_connector_destroy(amdgpu_connector_dvi_funcs_group0);
    ldv_state_variable_168 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48732;
  case 5: ;
  if (ldv_state_variable_168 == 1) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_dvi_funcs_group0, ldvarg338,
                                            ldvarg337);
    ldv_state_variable_168 = 1;
  } else {
  }
  if (ldv_state_variable_168 == 2) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_dvi_funcs_group0, ldvarg338,
                                            ldvarg337);
    ldv_state_variable_168 = 2;
  } else {
  }
  goto ldv_48732;
  case 6: ;
  if (ldv_state_variable_168 == 1) {
    ldv_probe_168();
    ldv_state_variable_168 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48732;
  default:
  ldv_stop();
  }
  ldv_48732: ;
  return;
}
}
void ldv_main_exported_167(void)
{
  struct drm_display_mode *ldvarg673 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg673 = (struct drm_display_mode *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_167 == 1) {
    amdgpu_connector_dp_get_modes(amdgpu_connector_dp_helper_funcs_group0);
    ldv_state_variable_167 = 1;
  } else {
  }
  goto ldv_48745;
  case 1: ;
  if (ldv_state_variable_167 == 1) {
    amdgpu_connector_dp_mode_valid(amdgpu_connector_dp_helper_funcs_group0, ldvarg673);
    ldv_state_variable_167 = 1;
  } else {
  }
  goto ldv_48745;
  case 2: ;
  if (ldv_state_variable_167 == 1) {
    amdgpu_connector_dvi_encoder(amdgpu_connector_dp_helper_funcs_group0);
    ldv_state_variable_167 = 1;
  } else {
  }
  goto ldv_48745;
  default:
  ldv_stop();
  }
  ldv_48745: ;
  return;
}
}
void ldv_main_exported_166(void)
{
  bool ldvarg862 ;
  u32 ldvarg859 ;
  uint64_t ldvarg860 ;
  int ldvarg863 ;
  struct drm_property *ldvarg861 ;
  void *tmp ;
  u32 ldvarg858 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(104UL);
  ldvarg861 = (struct drm_property *)tmp;
  ldv_memset((void *)(& ldvarg862), 0, 1UL);
  ldv_memset((void *)(& ldvarg859), 0, 4UL);
  ldv_memset((void *)(& ldvarg860), 0, 8UL);
  ldv_memset((void *)(& ldvarg863), 0, 4UL);
  ldv_memset((void *)(& ldvarg858), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_166 == 2) {
    drm_helper_connector_dpms(amdgpu_connector_dp_funcs_group0, ldvarg863);
    ldv_state_variable_166 = 2;
  } else {
  }
  if (ldv_state_variable_166 == 1) {
    drm_helper_connector_dpms(amdgpu_connector_dp_funcs_group0, ldvarg863);
    ldv_state_variable_166 = 1;
  } else {
  }
  goto ldv_48759;
  case 1: ;
  if (ldv_state_variable_166 == 2) {
    amdgpu_connector_dvi_force(amdgpu_connector_dp_funcs_group0);
    ldv_state_variable_166 = 2;
  } else {
  }
  if (ldv_state_variable_166 == 1) {
    amdgpu_connector_dvi_force(amdgpu_connector_dp_funcs_group0);
    ldv_state_variable_166 = 1;
  } else {
  }
  goto ldv_48759;
  case 2: ;
  if (ldv_state_variable_166 == 2) {
    amdgpu_connector_dp_detect(amdgpu_connector_dp_funcs_group0, (int )ldvarg862);
    ldv_state_variable_166 = 2;
  } else {
  }
  if (ldv_state_variable_166 == 1) {
    amdgpu_connector_dp_detect(amdgpu_connector_dp_funcs_group0, (int )ldvarg862);
    ldv_state_variable_166 = 1;
  } else {
  }
  goto ldv_48759;
  case 3: ;
  if (ldv_state_variable_166 == 2) {
    amdgpu_connector_set_property(amdgpu_connector_dp_funcs_group0, ldvarg861, ldvarg860);
    ldv_state_variable_166 = 2;
  } else {
  }
  if (ldv_state_variable_166 == 1) {
    amdgpu_connector_set_property(amdgpu_connector_dp_funcs_group0, ldvarg861, ldvarg860);
    ldv_state_variable_166 = 1;
  } else {
  }
  goto ldv_48759;
  case 4: ;
  if (ldv_state_variable_166 == 2) {
    amdgpu_connector_destroy(amdgpu_connector_dp_funcs_group0);
    ldv_state_variable_166 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48759;
  case 5: ;
  if (ldv_state_variable_166 == 2) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_dp_funcs_group0, ldvarg859,
                                            ldvarg858);
    ldv_state_variable_166 = 2;
  } else {
  }
  if (ldv_state_variable_166 == 1) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_dp_funcs_group0, ldvarg859,
                                            ldvarg858);
    ldv_state_variable_166 = 1;
  } else {
  }
  goto ldv_48759;
  case 6: ;
  if (ldv_state_variable_166 == 1) {
    ldv_probe_166();
    ldv_state_variable_166 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48759;
  default:
  ldv_stop();
  }
  ldv_48759: ;
  return;
}
}
void ldv_main_exported_172(void)
{
  struct drm_property *ldvarg361 ;
  void *tmp ;
  bool ldvarg362 ;
  u32 ldvarg359 ;
  int ldvarg363 ;
  u32 ldvarg358 ;
  uint64_t ldvarg360 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(104UL);
  ldvarg361 = (struct drm_property *)tmp;
  ldv_memset((void *)(& ldvarg362), 0, 1UL);
  ldv_memset((void *)(& ldvarg359), 0, 4UL);
  ldv_memset((void *)(& ldvarg363), 0, 4UL);
  ldv_memset((void *)(& ldvarg358), 0, 4UL);
  ldv_memset((void *)(& ldvarg360), 0, 8UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_172 == 1) {
    drm_helper_connector_dpms(amdgpu_connector_lvds_funcs_group0, ldvarg363);
    ldv_state_variable_172 = 1;
  } else {
  }
  if (ldv_state_variable_172 == 2) {
    drm_helper_connector_dpms(amdgpu_connector_lvds_funcs_group0, ldvarg363);
    ldv_state_variable_172 = 2;
  } else {
  }
  goto ldv_48777;
  case 1: ;
  if (ldv_state_variable_172 == 1) {
    amdgpu_connector_lvds_detect(amdgpu_connector_lvds_funcs_group0, (int )ldvarg362);
    ldv_state_variable_172 = 1;
  } else {
  }
  if (ldv_state_variable_172 == 2) {
    amdgpu_connector_lvds_detect(amdgpu_connector_lvds_funcs_group0, (int )ldvarg362);
    ldv_state_variable_172 = 2;
  } else {
  }
  goto ldv_48777;
  case 2: ;
  if (ldv_state_variable_172 == 1) {
    amdgpu_connector_set_lcd_property(amdgpu_connector_lvds_funcs_group0, ldvarg361,
                                      ldvarg360);
    ldv_state_variable_172 = 1;
  } else {
  }
  if (ldv_state_variable_172 == 2) {
    amdgpu_connector_set_lcd_property(amdgpu_connector_lvds_funcs_group0, ldvarg361,
                                      ldvarg360);
    ldv_state_variable_172 = 2;
  } else {
  }
  goto ldv_48777;
  case 3: ;
  if (ldv_state_variable_172 == 2) {
    amdgpu_connector_destroy(amdgpu_connector_lvds_funcs_group0);
    ldv_state_variable_172 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48777;
  case 4: ;
  if (ldv_state_variable_172 == 1) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_lvds_funcs_group0, ldvarg359,
                                            ldvarg358);
    ldv_state_variable_172 = 1;
  } else {
  }
  if (ldv_state_variable_172 == 2) {
    drm_helper_probe_single_connector_modes(amdgpu_connector_lvds_funcs_group0, ldvarg359,
                                            ldvarg358);
    ldv_state_variable_172 = 2;
  } else {
  }
  goto ldv_48777;
  case 5: ;
  if (ldv_state_variable_172 == 1) {
    ldv_probe_172();
    ldv_state_variable_172 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48777;
  default:
  ldv_stop();
  }
  ldv_48777: ;
  return;
}
}
void ldv_main_exported_173(void)
{
  struct drm_display_mode *ldvarg801 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg801 = (struct drm_display_mode *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_173 == 1) {
    amdgpu_connector_lvds_get_modes(amdgpu_connector_lvds_helper_funcs_group0);
    ldv_state_variable_173 = 1;
  } else {
  }
  goto ldv_48789;
  case 1: ;
  if (ldv_state_variable_173 == 1) {
    amdgpu_connector_lvds_mode_valid(amdgpu_connector_lvds_helper_funcs_group0, ldvarg801);
    ldv_state_variable_173 = 1;
  } else {
  }
  goto ldv_48789;
  case 2: ;
  if (ldv_state_variable_173 == 1) {
    amdgpu_connector_best_single_encoder(amdgpu_connector_lvds_helper_funcs_group0);
    ldv_state_variable_173 = 1;
  } else {
  }
  goto ldv_48789;
  default:
  ldv_stop();
  }
  ldv_48789: ;
  return;
}
}
void ldv_main_exported_169(void)
{
  struct drm_display_mode *ldvarg1103 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg1103 = (struct drm_display_mode *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_169 == 1) {
    amdgpu_connector_vga_get_modes(amdgpu_connector_dvi_helper_funcs_group0);
    ldv_state_variable_169 = 1;
  } else {
  }
  goto ldv_48798;
  case 1: ;
  if (ldv_state_variable_169 == 1) {
    amdgpu_connector_dvi_mode_valid(amdgpu_connector_dvi_helper_funcs_group0, ldvarg1103);
    ldv_state_variable_169 = 1;
  } else {
  }
  goto ldv_48798;
  case 2: ;
  if (ldv_state_variable_169 == 1) {
    amdgpu_connector_dvi_encoder(amdgpu_connector_dvi_helper_funcs_group0);
    ldv_state_variable_169 = 1;
  } else {
  }
  goto ldv_48798;
  default:
  ldv_stop();
  }
  ldv_48798: ;
  return;
}
}
void ldv_main_exported_171(void)
{
  struct drm_display_mode *ldvarg1108 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg1108 = (struct drm_display_mode *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_171 == 1) {
    amdgpu_connector_vga_get_modes(amdgpu_connector_vga_helper_funcs_group0);
    ldv_state_variable_171 = 1;
  } else {
  }
  goto ldv_48807;
  case 1: ;
  if (ldv_state_variable_171 == 1) {
    amdgpu_connector_vga_mode_valid(amdgpu_connector_vga_helper_funcs_group0, ldvarg1108);
    ldv_state_variable_171 = 1;
  } else {
  }
  goto ldv_48807;
  case 2: ;
  if (ldv_state_variable_171 == 1) {
    amdgpu_connector_best_single_encoder(amdgpu_connector_vga_helper_funcs_group0);
    ldv_state_variable_171 = 1;
  } else {
  }
  goto ldv_48807;
  default:
  ldv_stop();
  }
  ldv_48807: ;
  return;
}
}
bool ldv_queue_work_on_75(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_76(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_77(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_78(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_79(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern struct pv_irq_ops pv_irq_ops ;
__inline static __u32 __le32_to_cpup(__le32 const *p )
{
  {
  return ((__u32 )*p);
}
}
extern void __bad_percpu_size(void) ;
extern void __bad_size_call_parameter(void) ;
extern size_t strlen(char const * ) ;
extern int strncmp(char const * , char const * , __kernel_size_t ) ;
__inline static unsigned long arch_local_save_flags(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static int arch_irqs_disabled_flags(unsigned long flags )
{
  {
  return ((flags & 512UL) == 0UL);
}
}
extern int __preempt_count ;
__inline static int preempt_count(void)
{
  int pfo_ret__ ;
  {
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%1,%0": "=q" (pfo_ret__): "m" (__preempt_count));
  goto ldv_6002;
  case 2UL:
  __asm__ ("movw %%gs:%1,%0": "=r" (pfo_ret__): "m" (__preempt_count));
  goto ldv_6002;
  case 4UL:
  __asm__ ("movl %%gs:%1,%0": "=r" (pfo_ret__): "m" (__preempt_count));
  goto ldv_6002;
  case 8UL:
  __asm__ ("movq %%gs:%1,%0": "=r" (pfo_ret__): "m" (__preempt_count));
  goto ldv_6002;
  default:
  __bad_percpu_size();
  }
  ldv_6002: ;
  return (pfo_ret__ & 2147483647);
}
}
extern void mutex_unlock(struct mutex * ) ;
extern unsigned int jiffies_to_msecs(unsigned long const ) ;
bool ldv_queue_work_on_89(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_91(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_90(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_93(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_92(struct workqueue_struct *ldv_func_arg1 ) ;
extern int cpu_number ;
__inline static u32 get_unaligned_le32(void const *p )
{
  __u32 tmp ;
  {
  tmp = __le32_to_cpup((__le32 const *)p);
  return (tmp);
}
}
extern atomic_t kgdb_active ;
extern void __udelay(unsigned long ) ;
extern void __const_udelay(unsigned long ) ;
extern void msleep(unsigned int ) ;
__inline static bool drm_can_sleep(void)
{
  int tmp ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  int tmp___0 ;
  unsigned long _flags ;
  int tmp___1 ;
  {
  tmp = preempt_count();
  if (tmp != 0) {
    return (0);
  } else {
    __vpp_verify = (void const *)0;
    switch (4UL) {
    case 1UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%1,%0": "=q" (pfo_ret__): "m" (cpu_number));
    goto ldv_39391;
    case 2UL:
    __asm__ ("movw %%gs:%1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_39391;
    case 4UL:
    __asm__ ("movl %%gs:%1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_39391;
    case 8UL:
    __asm__ ("movq %%gs:%1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_39391;
    default:
    __bad_percpu_size();
    }
    ldv_39391:
    pscr_ret__ = pfo_ret__;
    goto ldv_39397;
    case 2UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_39401;
    case 2UL:
    __asm__ ("movw %%gs:%1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_39401;
    case 4UL:
    __asm__ ("movl %%gs:%1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_39401;
    case 8UL:
    __asm__ ("movq %%gs:%1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_39401;
    default:
    __bad_percpu_size();
    }
    ldv_39401:
    pscr_ret__ = pfo_ret_____0;
    goto ldv_39397;
    case 4UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_39410;
    case 2UL:
    __asm__ ("movw %%gs:%1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_39410;
    case 4UL:
    __asm__ ("movl %%gs:%1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_39410;
    case 8UL:
    __asm__ ("movq %%gs:%1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_39410;
    default:
    __bad_percpu_size();
    }
    ldv_39410:
    pscr_ret__ = pfo_ret_____1;
    goto ldv_39397;
    case 8UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_39419;
    case 2UL:
    __asm__ ("movw %%gs:%1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_39419;
    case 4UL:
    __asm__ ("movl %%gs:%1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_39419;
    case 8UL:
    __asm__ ("movq %%gs:%1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_39419;
    default:
    __bad_percpu_size();
    }
    ldv_39419:
    pscr_ret__ = pfo_ret_____2;
    goto ldv_39397;
    default:
    __bad_size_call_parameter();
    goto ldv_39397;
    }
    ldv_39397:
    tmp___0 = atomic_read((atomic_t const *)(& kgdb_active));
    if (pscr_ret__ == tmp___0) {
      return (0);
    } else {
      _flags = arch_local_save_flags();
      tmp___1 = arch_irqs_disabled_flags(_flags);
      if (tmp___1 != 0) {
        return (0);
      } else {
      }
    }
  }
  return (1);
}
}
int amdgpu_atom_debug ;
void amdgpu_atom_destroy(struct atom_context *ctx ) ;
static char *atom_op_names[123U] =
  { (char *)"RESERVED", (char *)"MOVE_REG", (char *)"MOVE_PS", (char *)"MOVE_WS",
        (char *)"MOVE_FB", (char *)"MOVE_PLL", (char *)"MOVE_MC", (char *)"AND_REG",
        (char *)"AND_PS", (char *)"AND_WS", (char *)"AND_FB", (char *)"AND_PLL",
        (char *)"AND_MC", (char *)"OR_REG", (char *)"OR_PS", (char *)"OR_WS",
        (char *)"OR_FB", (char *)"OR_PLL", (char *)"OR_MC", (char *)"SHIFT_LEFT_REG",
        (char *)"SHIFT_LEFT_PS", (char *)"SHIFT_LEFT_WS", (char *)"SHIFT_LEFT_FB", (char *)"SHIFT_LEFT_PLL",
        (char *)"SHIFT_LEFT_MC", (char *)"SHIFT_RIGHT_REG", (char *)"SHIFT_RIGHT_PS", (char *)"SHIFT_RIGHT_WS",
        (char *)"SHIFT_RIGHT_FB", (char *)"SHIFT_RIGHT_PLL", (char *)"SHIFT_RIGHT_MC", (char *)"MUL_REG",
        (char *)"MUL_PS", (char *)"MUL_WS", (char *)"MUL_FB", (char *)"MUL_PLL",
        (char *)"MUL_MC", (char *)"DIV_REG", (char *)"DIV_PS", (char *)"DIV_WS",
        (char *)"DIV_FB", (char *)"DIV_PLL", (char *)"DIV_MC", (char *)"ADD_REG",
        (char *)"ADD_PS", (char *)"ADD_WS", (char *)"ADD_FB", (char *)"ADD_PLL",
        (char *)"ADD_MC", (char *)"SUB_REG", (char *)"SUB_PS", (char *)"SUB_WS",
        (char *)"SUB_FB", (char *)"SUB_PLL", (char *)"SUB_MC", (char *)"SET_ATI_PORT",
        (char *)"SET_PCI_PORT", (char *)"SET_SYS_IO_PORT", (char *)"SET_REG_BLOCK", (char *)"SET_FB_BASE",
        (char *)"COMPARE_REG", (char *)"COMPARE_PS", (char *)"COMPARE_WS", (char *)"COMPARE_FB",
        (char *)"COMPARE_PLL", (char *)"COMPARE_MC", (char *)"SWITCH", (char *)"JUMP",
        (char *)"JUMP_EQUAL", (char *)"JUMP_BELOW", (char *)"JUMP_ABOVE", (char *)"JUMP_BELOW_OR_EQUAL",
        (char *)"JUMP_ABOVE_OR_EQUAL", (char *)"JUMP_NOT_EQUAL", (char *)"TEST_REG", (char *)"TEST_PS",
        (char *)"TEST_WS", (char *)"TEST_FB", (char *)"TEST_PLL", (char *)"TEST_MC",
        (char *)"DELAY_MILLISEC", (char *)"DELAY_MICROSEC", (char *)"CALL_TABLE", (char *)"REPEAT",
        (char *)"CLEAR_REG", (char *)"CLEAR_PS", (char *)"CLEAR_WS", (char *)"CLEAR_FB",
        (char *)"CLEAR_PLL", (char *)"CLEAR_MC", (char *)"NOP", (char *)"EOT",
        (char *)"MASK_REG", (char *)"MASK_PS", (char *)"MASK_WS", (char *)"MASK_FB",
        (char *)"MASK_PLL", (char *)"MASK_MC", (char *)"POST_CARD", (char *)"BEEP",
        (char *)"SAVE_REG", (char *)"RESTORE_REG", (char *)"SET_DATA_BLOCK", (char *)"XOR_REG",
        (char *)"XOR_PS", (char *)"XOR_WS", (char *)"XOR_FB", (char *)"XOR_PLL",
        (char *)"XOR_MC", (char *)"SHL_REG", (char *)"SHL_PS", (char *)"SHL_WS",
        (char *)"SHL_FB", (char *)"SHL_PLL", (char *)"SHL_MC", (char *)"SHR_REG",
        (char *)"SHR_PS", (char *)"SHR_WS", (char *)"SHR_FB", (char *)"SHR_PLL",
        (char *)"SHR_MC", (char *)"DEBUG", (char *)"CTB_DS"};
static char *atom_table_names[74U] =
  { (char *)"ASIC_Init", (char *)"GetDisplaySurfaceSize", (char *)"ASIC_RegistersInit", (char *)"VRAM_BlockVenderDetection",
        (char *)"SetClocksRatio", (char *)"MemoryControllerInit", (char *)"GPIO_PinInit", (char *)"MemoryParamAdjust",
        (char *)"DVOEncoderControl", (char *)"GPIOPinControl", (char *)"SetEngineClock", (char *)"SetMemoryClock",
        (char *)"SetPixelClock", (char *)"DynamicClockGating", (char *)"ResetMemoryDLL", (char *)"ResetMemoryDevice",
        (char *)"MemoryPLLInit", (char *)"EnableMemorySelfRefresh", (char *)"AdjustMemoryController", (char *)"EnableASIC_StaticPwrMgt",
        (char *)"ASIC_StaticPwrMgtStatusChange", (char *)"DAC_LoadDetection", (char *)"TMDS2EncoderControl", (char *)"LCD1OutputControl",
        (char *)"DAC1EncoderControl", (char *)"DAC2EncoderControl", (char *)"DVOOutputControl", (char *)"CV1OutputControl",
        (char *)"SetCRTC_DPM_State", (char *)"TVEncoderControl", (char *)"TMDS1EncoderControl", (char *)"LVDSEncoderControl",
        (char *)"TV1OutputControl", (char *)"EnableScaler", (char *)"BlankCRTC", (char *)"EnableCRTC",
        (char *)"GetPixelClock", (char *)"EnableVGA_Render", (char *)"EnableVGA_Access", (char *)"SetCRTC_Timing",
        (char *)"SetCRTC_OverScan", (char *)"SetCRTC_Replication", (char *)"SelectCRTC_Source", (char *)"EnableGraphSurfaces",
        (char *)"UpdateCRTC_DoubleBufferRegisters", (char *)"LUT_AutoFill", (char *)"EnableHW_IconCursor", (char *)"GetMemoryClock",
        (char *)"GetEngineClock", (char *)"SetCRTC_UsingDTDTiming", (char *)"TVBootUpStdPinDetection", (char *)"DFP2OutputControl",
        (char *)"VRAM_BlockDetectionByStrap", (char *)"MemoryCleanUp", (char *)"ReadEDIDFromHWAssistedI2C", (char *)"WriteOneByteToHWAssistedI2C",
        (char *)"ReadHWAssistedI2CStatus", (char *)"SpeedFanControl", (char *)"PowerConnectorDetection", (char *)"MC_Synchronization",
        (char *)"ComputeMemoryEnginePLL", (char *)"MemoryRefreshConversion", (char *)"VRAM_GetCurrentInfoBlock", (char *)"DynamicMemorySettings",
        (char *)"MemoryTraining", (char *)"EnableLVDS_SS", (char *)"DFP1OutputControl", (char *)"SetVoltage",
        (char *)"CRT1OutputControl", (char *)"CRT2OutputControl", (char *)"SetupHWAssistedI2CStatus", (char *)"ClockSource",
        (char *)"MemoryDeviceInit", (char *)"EnableYUV"};
static char *atom_io_names[5U] = { (char *)"MM", (char *)"PLL", (char *)"MC", (char *)"PCIE",
        (char *)"PCIE PORT"};
__inline static uint8_t get_u8(void *bios , int ptr )
{
  {
  return (*((unsigned char *)bios + (unsigned long )ptr));
}
}
__inline static u16 get_u16(void *bios , int ptr )
{
  uint8_t tmp ;
  uint8_t tmp___0 ;
  {
  tmp = get_u8(bios, ptr);
  tmp___0 = get_u8(bios, ptr + 1);
  return ((u16 )((int )((short )tmp) | (int )((short )((int )tmp___0 << 8))));
}
}
__inline static u32 get_u32(void *bios , int ptr )
{
  u16 tmp ;
  u16 tmp___0 ;
  {
  tmp = get_u16(bios, ptr);
  tmp___0 = get_u16(bios, ptr + 2);
  return ((unsigned int )tmp | ((unsigned int )tmp___0 << 16));
}
}
int amdgpu_atom_debug = 0;
static int amdgpu_atom_execute_table_locked(struct atom_context *ctx , int index ,
                                            u32 *params ) ;
static u32 atom_arg_mask[8U] =
  { 4294967295U, 65535U, 16776960U, 4294901760U,
        255U, 65280U, 16711680U, 4278190080U};
static int atom_arg_shift[8U] =
  { 0, 0, 8, 16,
        0, 8, 16, 24};
static int atom_dst_to_src[8U][4U] =
  { { 0, 0, 0, 0},
   { 1, 2, 3, 0},
   { 1, 2, 3, 0},
   { 1, 2, 3, 0},
   { 4, 5, 6, 7},
   { 4, 5, 6, 7},
   { 4, 5, 6, 7},
   { 4, 5, 6, 7}};
static int atom_def_dst[8U] =
  { 0, 0, 1, 2,
        0, 1, 2, 3};
static int debug_depth = 0;
static void debug_print_spaces(int n )
{
  int tmp ;
  {
  goto ldv_47839;
  ldv_47838:
  printk("   ");
  ldv_47839:
  tmp = n;
  n = n - 1;
  if (tmp != 0) {
    goto ldv_47838;
  } else {
  }
  return;
}
}
static u32 atom_iio_execute(struct atom_context *ctx , int base , u32 index , u32 data )
{
  u32 temp ;
  uint8_t tmp ;
  u16 tmp___0 ;
  u16 tmp___1 ;
  uint8_t tmp___2 ;
  uint8_t tmp___3 ;
  uint8_t tmp___4 ;
  uint8_t tmp___5 ;
  uint8_t tmp___6 ;
  uint8_t tmp___7 ;
  uint8_t tmp___8 ;
  uint8_t tmp___9 ;
  uint8_t tmp___10 ;
  uint8_t tmp___11 ;
  uint8_t tmp___12 ;
  uint8_t tmp___13 ;
  uint8_t tmp___14 ;
  uint8_t tmp___15 ;
  uint8_t tmp___16 ;
  uint8_t tmp___17 ;
  uint8_t tmp___18 ;
  uint8_t tmp___19 ;
  uint8_t tmp___20 ;
  {
  temp = 3452816845U;
  ldv_47859:
  tmp = get_u8(ctx->bios, base);
  switch ((int )tmp) {
  case 0:
  base = base + 1;
  goto ldv_47849;
  case 2:
  tmp___0 = get_u16(ctx->bios, base + 1);
  temp = (*((ctx->card)->ioreg_read))(ctx->card, (u32 )tmp___0);
  base = base + 3;
  goto ldv_47849;
  case 3:
  tmp___1 = get_u16(ctx->bios, base + 1);
  (*((ctx->card)->ioreg_write))(ctx->card, (u32 )tmp___1, temp);
  base = base + 3;
  goto ldv_47849;
  case 4:
  tmp___2 = get_u8(ctx->bios, base + 1);
  tmp___3 = get_u8(ctx->bios, base + 2);
  temp = ~ ((4294967295U >> (32 - (int )tmp___2)) << (int )tmp___3) & temp;
  base = base + 3;
  goto ldv_47849;
  case 5:
  tmp___4 = get_u8(ctx->bios, base + 1);
  tmp___5 = get_u8(ctx->bios, base + 2);
  temp = ((4294967295U >> (32 - (int )tmp___4)) << (int )tmp___5) | temp;
  base = base + 3;
  goto ldv_47849;
  case 6:
  tmp___6 = get_u8(ctx->bios, base + 1);
  tmp___7 = get_u8(ctx->bios, base + 3);
  temp = ~ ((4294967295U >> (32 - (int )tmp___6)) << (int )tmp___7) & temp;
  tmp___8 = get_u8(ctx->bios, base + 2);
  tmp___9 = get_u8(ctx->bios, base + 1);
  tmp___10 = get_u8(ctx->bios, base + 3);
  temp = (((index >> (int )tmp___8) & (4294967295U >> (32 - (int )tmp___9))) << (int )tmp___10) | temp;
  base = base + 4;
  goto ldv_47849;
  case 8:
  tmp___11 = get_u8(ctx->bios, base + 1);
  tmp___12 = get_u8(ctx->bios, base + 3);
  temp = ~ ((4294967295U >> (32 - (int )tmp___11)) << (int )tmp___12) & temp;
  tmp___13 = get_u8(ctx->bios, base + 2);
  tmp___14 = get_u8(ctx->bios, base + 1);
  tmp___15 = get_u8(ctx->bios, base + 3);
  temp = (((data >> (int )tmp___13) & (4294967295U >> (32 - (int )tmp___14))) << (int )tmp___15) | temp;
  base = base + 4;
  goto ldv_47849;
  case 7:
  tmp___16 = get_u8(ctx->bios, base + 1);
  tmp___17 = get_u8(ctx->bios, base + 3);
  temp = ~ ((4294967295U >> (32 - (int )tmp___16)) << (int )tmp___17) & temp;
  tmp___18 = get_u8(ctx->bios, base + 2);
  tmp___19 = get_u8(ctx->bios, base + 1);
  tmp___20 = get_u8(ctx->bios, base + 3);
  temp = (((unsigned int )((int )ctx->io_attr >> (int )tmp___18) & (4294967295U >> (32 - (int )tmp___19))) << (int )tmp___20) | temp;
  base = base + 4;
  goto ldv_47849;
  case 9: ;
  return (temp);
  default:
  printk("\016Unknown IIO opcode.\n");
  return (0U);
  }
  ldv_47849: ;
  goto ldv_47859;
}
}
static u32 atom_get_src_int(atom_exec_context *ctx , uint8_t attr , int *ptr , u32 *saved ,
                            int print )
{
  u32 idx ;
  u32 val ;
  u32 align ;
  u32 arg ;
  struct atom_context *gctx ;
  u16 tmp ;
  uint8_t tmp___0 ;
  uint8_t tmp___1 ;
  u16 tmp___2 ;
  uint8_t tmp___3 ;
  u16 tmp___4 ;
  uint8_t tmp___5 ;
  uint8_t tmp___6 ;
  uint8_t tmp___7 ;
  {
  val = 3452816845U;
  gctx = ctx->ctx;
  arg = (u32 )attr & 7U;
  align = (u32 )((int )attr >> 3) & 7U;
  switch (arg) {
  case 0U:
  tmp = get_u16((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp;
  *ptr = *ptr + 2;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017REG[0x%04X]", idx);
    } else {
    }
  } else {
  }
  idx = (u32 )gctx->reg_block + idx;
  switch (gctx->io_mode) {
  case 0:
  val = (*((gctx->card)->reg_read))(gctx->card, idx);
  goto ldv_47874;
  case 1:
  printk("\016PCI registers are not implemented.\n");
  return (0U);
  case 2:
  printk("\016SYSIO registers are not implemented.\n");
  return (0U);
  default: ;
  if ((gctx->io_mode & 128) == 0) {
    printk("\016Bad IO mode.\n");
    return (0U);
  } else {
  }
  if ((unsigned int )*(gctx->iio + ((unsigned long )gctx->io_mode & 127UL)) == 0U) {
    printk("\016Undefined indirect IO read method %d.\n", gctx->io_mode & 127);
    return (0U);
  } else {
  }
  val = atom_iio_execute(gctx, (int )*(gctx->iio + ((unsigned long )gctx->io_mode & 127UL)),
                         idx, 0U);
  }
  ldv_47874: ;
  goto ldv_47878;
  case 1U:
  tmp___0 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___0;
  *ptr = *ptr + 1;
  val = get_unaligned_le32((void const *)ctx->ps + (unsigned long )idx);
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017PS[0x%02X,0x%04X]", idx, val);
    } else {
    }
  } else {
  }
  goto ldv_47878;
  case 2U:
  tmp___1 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___1;
  *ptr = *ptr + 1;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017WS[0x%02X]", idx);
    } else {
    }
  } else {
  }
  switch (idx) {
  case 64U:
  val = gctx->divmul[0];
  goto ldv_47882;
  case 65U:
  val = gctx->divmul[1];
  goto ldv_47882;
  case 66U:
  val = (u32 )gctx->data_block;
  goto ldv_47882;
  case 67U:
  val = (u32 )gctx->shift;
  goto ldv_47882;
  case 68U:
  val = (u32 )(1 << (int )gctx->shift);
  goto ldv_47882;
  case 69U:
  val = (u32 )(~ (1 << (int )gctx->shift));
  goto ldv_47882;
  case 70U:
  val = gctx->fb_base;
  goto ldv_47882;
  case 71U:
  val = (u32 )gctx->io_attr;
  goto ldv_47882;
  case 72U:
  val = (u32 )gctx->reg_block;
  goto ldv_47882;
  default:
  val = *(ctx->ws + (unsigned long )idx);
  }
  ldv_47882: ;
  goto ldv_47878;
  case 4U:
  tmp___2 = get_u16((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___2;
  *ptr = *ptr + 2;
  if (print != 0) {
    if ((unsigned int )gctx->data_block != 0U) {
      if (amdgpu_atom_debug != 0) {
        printk("\017ID[0x%04X+%04X]", idx, (int )gctx->data_block);
      } else {
      }
    } else
    if (amdgpu_atom_debug != 0) {
      printk("\017ID[0x%04X]", idx);
    } else {
    }
  } else {
  }
  val = get_u32((ctx->ctx)->bios, (int )((u32 )gctx->data_block + idx));
  goto ldv_47878;
  case 3U:
  tmp___3 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___3;
  *ptr = *ptr + 1;
  if (gctx->fb_base + idx * 4U > (u32 )gctx->scratch_size_bytes) {
    drm_err("ATOM: fb read beyond scratch region: %d vs. %d\n", gctx->fb_base + idx * 4U,
            gctx->scratch_size_bytes);
    val = 0U;
  } else {
    val = *(gctx->scratch + (unsigned long )(gctx->fb_base / 4U + idx));
  }
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017FB[0x%02X]", idx);
    } else {
    }
  } else {
  }
  goto ldv_47878;
  case 5U: ;
  switch (align) {
  case 0U:
  val = get_u32((ctx->ctx)->bios, *ptr);
  *ptr = *ptr + 4;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017IMM 0x%08X\n", val);
    } else {
    }
  } else {
  }
  return (val);
  case 1U: ;
  case 2U: ;
  case 3U:
  tmp___4 = get_u16((ctx->ctx)->bios, *ptr);
  val = (u32 )tmp___4;
  *ptr = *ptr + 2;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017IMM 0x%04X\n", val);
    } else {
    }
  } else {
  }
  return (val);
  case 4U: ;
  case 5U: ;
  case 6U: ;
  case 7U:
  tmp___5 = get_u8((ctx->ctx)->bios, *ptr);
  val = (u32 )tmp___5;
  *ptr = *ptr + 1;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017IMM 0x%02X\n", val);
    } else {
    }
  } else {
  }
  return (val);
  }
  return (0U);
  case 6U:
  tmp___6 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___6;
  *ptr = *ptr + 1;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017PLL[0x%02X]", idx);
    } else {
    }
  } else {
  }
  val = (*((gctx->card)->pll_read))(gctx->card, idx);
  goto ldv_47878;
  case 7U:
  tmp___7 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___7;
  *ptr = *ptr + 1;
  if (print != 0) {
    if (amdgpu_atom_debug != 0) {
      printk("\017MC[0x%02X]", idx);
    } else {
    }
  } else {
  }
  val = (*((gctx->card)->mc_read))(gctx->card, idx);
  goto ldv_47878;
  }
  ldv_47878: ;
  if ((unsigned long )saved != (unsigned long )((u32 *)0U)) {
    *saved = val;
  } else {
  }
  val = atom_arg_mask[align] & val;
  val = val >> atom_arg_shift[align];
  if (print != 0) {
    switch (align) {
    case 0U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[31:0] -> 0x%08X\n", val);
    } else {
    }
    goto ldv_47906;
    case 1U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[15:0] -> 0x%04X\n", val);
    } else {
    }
    goto ldv_47906;
    case 2U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[23:8] -> 0x%04X\n", val);
    } else {
    }
    goto ldv_47906;
    case 3U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[31:16] -> 0x%04X\n", val);
    } else {
    }
    goto ldv_47906;
    case 4U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[7:0] -> 0x%02X\n", val);
    } else {
    }
    goto ldv_47906;
    case 5U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[15:8] -> 0x%02X\n", val);
    } else {
    }
    goto ldv_47906;
    case 6U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[23:16] -> 0x%02X\n", val);
    } else {
    }
    goto ldv_47906;
    case 7U: ;
    if (amdgpu_atom_debug != 0) {
      printk("\017.[31:24] -> 0x%02X\n", val);
    } else {
    }
    goto ldv_47906;
    }
    ldv_47906: ;
  } else {
  }
  return (val);
}
}
static void atom_skip_src_int(atom_exec_context *ctx , uint8_t attr , int *ptr )
{
  u32 align ;
  u32 arg ;
  {
  align = (u32 )((int )attr >> 3) & 7U;
  arg = (u32 )attr & 7U;
  switch (arg) {
  case 0U: ;
  case 4U:
  *ptr = *ptr + 2;
  goto ldv_47923;
  case 6U: ;
  case 7U: ;
  case 1U: ;
  case 2U: ;
  case 3U:
  *ptr = *ptr + 1;
  goto ldv_47923;
  case 5U: ;
  switch (align) {
  case 0U:
  *ptr = *ptr + 4;
  return;
  case 1U: ;
  case 2U: ;
  case 3U:
  *ptr = *ptr + 2;
  return;
  case 4U: ;
  case 5U: ;
  case 6U: ;
  case 7U:
  *ptr = *ptr + 1;
  return;
  }
  return;
  }
  ldv_47923: ;
  return;
}
}
static u32 atom_get_src(atom_exec_context *ctx , uint8_t attr , int *ptr )
{
  u32 tmp ;
  {
  tmp = atom_get_src_int(ctx, (int )attr, ptr, (u32 *)0U, 1);
  return (tmp);
}
}
static u32 atom_get_src_direct(atom_exec_context *ctx , uint8_t align , int *ptr )
{
  u32 val ;
  u16 tmp ;
  uint8_t tmp___0 ;
  {
  val = 3452816845U;
  switch ((int )align) {
  case 0:
  val = get_u32((ctx->ctx)->bios, *ptr);
  *ptr = *ptr + 4;
  goto ldv_47950;
  case 1: ;
  case 2: ;
  case 3:
  tmp = get_u16((ctx->ctx)->bios, *ptr);
  val = (u32 )tmp;
  *ptr = *ptr + 2;
  goto ldv_47950;
  case 4: ;
  case 5: ;
  case 6: ;
  case 7:
  tmp___0 = get_u8((ctx->ctx)->bios, *ptr);
  val = (u32 )tmp___0;
  *ptr = *ptr + 1;
  goto ldv_47950;
  }
  ldv_47950: ;
  return (val);
}
}
static u32 atom_get_dst(atom_exec_context *ctx , int arg , uint8_t attr , int *ptr ,
                        u32 *saved , int print )
{
  u32 tmp ;
  {
  tmp = atom_get_src_int(ctx, (int )((uint8_t )((int )((signed char )(atom_dst_to_src[((int )attr >> 3) & 7][((int )attr >> 6) & 3] << 3)) | (int )((signed char )arg))),
                         ptr, saved, print);
  return (tmp);
}
}
static void atom_skip_dst(atom_exec_context *ctx , int arg , uint8_t attr , int *ptr )
{
  {
  atom_skip_src_int(ctx, (int )((uint8_t )((int )((signed char )(atom_dst_to_src[((int )attr >> 3) & 7][((int )attr >> 6) & 3] << 3)) | (int )((signed char )arg))),
                    ptr);
  return;
}
}
static void atom_put_dst(atom_exec_context *ctx , int arg , uint8_t attr , int *ptr ,
                         u32 val , u32 saved )
{
  u32 align ;
  u32 old_val ;
  u32 idx ;
  struct atom_context *gctx ;
  u16 tmp ;
  uint8_t tmp___0 ;
  uint8_t tmp___1 ;
  uint8_t tmp___2 ;
  uint8_t tmp___3 ;
  uint8_t tmp___4 ;
  {
  align = (u32 )atom_dst_to_src[((int )attr >> 3) & 7][((int )attr >> 6) & 3];
  old_val = val;
  gctx = ctx->ctx;
  old_val = (atom_arg_mask[align] >> atom_arg_shift[align]) & old_val;
  val = val << atom_arg_shift[align];
  val = atom_arg_mask[align] & val;
  saved = ~ atom_arg_mask[align] & saved;
  val = val | saved;
  switch (arg) {
  case 0:
  tmp = get_u16((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp;
  *ptr = *ptr + 2;
  if (amdgpu_atom_debug != 0) {
    printk("\017REG[0x%04X]", idx);
  } else {
  }
  idx = (u32 )gctx->reg_block + idx;
  switch (gctx->io_mode) {
  case 0: ;
  if (idx == 0U) {
    (*((gctx->card)->reg_write))(gctx->card, idx, val << 2);
  } else {
    (*((gctx->card)->reg_write))(gctx->card, idx, val);
  }
  goto ldv_47986;
  case 1:
  printk("\016PCI registers are not implemented.\n");
  return;
  case 2:
  printk("\016SYSIO registers are not implemented.\n");
  return;
  default: ;
  if ((gctx->io_mode & 128) == 0) {
    printk("\016Bad IO mode.\n");
    return;
  } else {
  }
  if ((unsigned int )*(gctx->iio + ((unsigned long )gctx->io_mode & 255UL)) == 0U) {
    printk("\016Undefined indirect IO write method %d.\n", gctx->io_mode & 127);
    return;
  } else {
  }
  atom_iio_execute(gctx, (int )*(gctx->iio + ((unsigned long )gctx->io_mode & 255UL)),
                   idx, val);
  }
  ldv_47986: ;
  goto ldv_47990;
  case 1:
  tmp___0 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___0;
  *ptr = *ptr + 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017PS[0x%02X]", idx);
  } else {
  }
  *(ctx->ps + (unsigned long )idx) = val;
  goto ldv_47990;
  case 2:
  tmp___1 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___1;
  *ptr = *ptr + 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017WS[0x%02X]", idx);
  } else {
  }
  switch (idx) {
  case 64U:
  gctx->divmul[0] = val;
  goto ldv_47994;
  case 65U:
  gctx->divmul[1] = val;
  goto ldv_47994;
  case 66U:
  gctx->data_block = (u16 )val;
  goto ldv_47994;
  case 67U:
  gctx->shift = (uint8_t )val;
  goto ldv_47994;
  case 68U: ;
  case 69U: ;
  goto ldv_47994;
  case 70U:
  gctx->fb_base = val;
  goto ldv_47994;
  case 71U:
  gctx->io_attr = (u16 )val;
  goto ldv_47994;
  case 72U:
  gctx->reg_block = (u16 )val;
  goto ldv_47994;
  default:
  *(ctx->ws + (unsigned long )idx) = val;
  }
  ldv_47994: ;
  goto ldv_47990;
  case 3:
  tmp___2 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___2;
  *ptr = *ptr + 1;
  if (gctx->fb_base + idx * 4U > (u32 )gctx->scratch_size_bytes) {
    drm_err("ATOM: fb write beyond scratch region: %d vs. %d\n", gctx->fb_base + idx * 4U,
            gctx->scratch_size_bytes);
  } else {
    *(gctx->scratch + (unsigned long )(gctx->fb_base / 4U + idx)) = val;
  }
  if (amdgpu_atom_debug != 0) {
    printk("\017FB[0x%02X]", idx);
  } else {
  }
  goto ldv_47990;
  case 6:
  tmp___3 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___3;
  *ptr = *ptr + 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017PLL[0x%02X]", idx);
  } else {
  }
  (*((gctx->card)->pll_write))(gctx->card, idx, val);
  goto ldv_47990;
  case 7:
  tmp___4 = get_u8((ctx->ctx)->bios, *ptr);
  idx = (u32 )tmp___4;
  *ptr = *ptr + 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017MC[0x%02X]", idx);
  } else {
  }
  (*((gctx->card)->mc_write))(gctx->card, idx, val);
  return;
  }
  ldv_47990: ;
  switch (align) {
  case 0U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[31:0] <- 0x%08X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 1U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[15:0] <- 0x%04X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 2U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[23:8] <- 0x%04X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 3U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[31:16] <- 0x%04X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 4U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[7:0] <- 0x%02X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 5U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[15:8] <- 0x%02X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 6U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[23:16] <- 0x%02X\n", old_val);
  } else {
  }
  goto ldv_48008;
  case 7U: ;
  if (amdgpu_atom_debug != 0) {
    printk("\017.[31:24] <- 0x%02X\n", old_val);
  } else {
  }
  goto ldv_48008;
  }
  ldv_48008: ;
  return;
}
}
static void atom_op_add(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst + src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_and(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst & src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_beep(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  printk("ATOM BIOS beeped!\n");
  return;
}
}
static void atom_op_calltable(atom_exec_context *ctx , int *ptr , int arg )
{
  int idx ;
  int tmp ;
  uint8_t tmp___0 ;
  int r ;
  u16 tmp___1 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  idx = (int )tmp___0;
  r = 0;
  if (idx <= 73) {
    if (amdgpu_atom_debug != 0) {
      printk("\017");
      debug_print_spaces(debug_depth);
      printk("   table: %d (%s)\n", idx, atom_table_names[idx]);
    } else {
    }
  } else
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   table: %d\n", idx);
  } else {
  }
  tmp___1 = get_u16((ctx->ctx)->bios, (int )(((ctx->ctx)->cmd_table + (u32 )(idx * 2)) + 4U));
  if ((unsigned int )tmp___1 != 0U) {
    r = amdgpu_atom_execute_table_locked(ctx->ctx, idx, ctx->ps + (unsigned long )ctx->ps_shift);
  } else {
  }
  if (r != 0) {
    ctx->abort = 1;
  } else {
  }
  return;
}
}
static void atom_op_clear(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  attr = (unsigned int )attr & 56U;
  attr = (uint8_t )((int )((signed char )(atom_def_dst[(int )attr >> 3] << 6)) | (int )((signed char )attr));
  atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 0);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, 0U, saved);
  return;
}
}
static void atom_op_compare(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src1: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, (u32 *)0U, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src2: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  (ctx->ctx)->cs_equal = dst == src;
  (ctx->ctx)->cs_above = dst > src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   result: %s %s\n", (ctx->ctx)->cs_equal != 0 ? (char *)"EQ" : (char *)"NE",
           (ctx->ctx)->cs_above != 0 ? (char *)"GT" : (char *)"LE");
  } else {
  }
  return;
}
}
static void atom_op_delay(atom_exec_context *ctx , int *ptr , int arg )
{
  unsigned int count ;
  int tmp ;
  uint8_t tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  count = (unsigned int )tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   count: %d\n", count);
  } else {
  }
  if (arg == 0) {
    __udelay((unsigned long )count);
  } else {
    tmp___2 = drm_can_sleep();
    if (tmp___2) {
      tmp___3 = 0;
    } else {
      tmp___3 = 1;
    }
    if (tmp___3) {
      __ms = (unsigned long )count;
      goto ldv_48072;
      ldv_48071:
      __const_udelay(4295000UL);
      ldv_48072:
      tmp___1 = __ms;
      __ms = __ms - 1UL;
      if (tmp___1 != 0UL) {
        goto ldv_48071;
      } else {
      }
    } else {
      msleep(count);
    }
  }
  return;
}
}
static void atom_op_div(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src1: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, (u32 *)0U, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src2: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  if (src != 0U) {
    (ctx->ctx)->divmul[0] = dst / src;
    (ctx->ctx)->divmul[1] = dst % src;
  } else {
    (ctx->ctx)->divmul[0] = 0U;
    (ctx->ctx)->divmul[1] = 0U;
  }
  return;
}
}
static void atom_op_eot(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  return;
}
}
static void atom_op_jump(atom_exec_context *ctx , int *ptr , int arg )
{
  int execute ;
  int target ;
  u16 tmp ;
  unsigned long cjiffies ;
  unsigned int tmp___0 ;
  {
  execute = 0;
  tmp = get_u16((ctx->ctx)->bios, *ptr);
  target = (int )tmp;
  *ptr = *ptr + 2;
  switch (arg) {
  case 0:
  execute = (ctx->ctx)->cs_above;
  goto ldv_48096;
  case 1:
  execute = (ctx->ctx)->cs_above != 0 || (ctx->ctx)->cs_equal != 0;
  goto ldv_48096;
  case 2:
  execute = 1;
  goto ldv_48096;
  case 3:
  execute = (ctx->ctx)->cs_above == 0 && (ctx->ctx)->cs_equal == 0;
  goto ldv_48096;
  case 4:
  execute = (ctx->ctx)->cs_above == 0;
  goto ldv_48096;
  case 5:
  execute = (ctx->ctx)->cs_equal;
  goto ldv_48096;
  case 6:
  execute = (ctx->ctx)->cs_equal == 0;
  goto ldv_48096;
  }
  ldv_48096: ;
  if (arg != 2) {
    if (amdgpu_atom_debug != 0) {
      printk("\017");
      debug_print_spaces(debug_depth);
      printk("   taken: %s\n", execute != 0 ? (char *)"yes" : (char *)"no");
    } else {
    }
  } else {
  }
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   target: 0x%04X\n", target);
  } else {
  }
  if (execute != 0) {
    if (ctx->last_jump == (unsigned int )((int )ctx->start + target)) {
      cjiffies = jiffies;
      if ((long )(ctx->last_jump_jiffies - cjiffies) < 0L) {
        cjiffies = cjiffies - ctx->last_jump_jiffies;
        tmp___0 = jiffies_to_msecs(cjiffies);
        if (tmp___0 > 5000U) {
          drm_err("atombios stuck in loop for more than 5secs aborting\n");
          ctx->abort = 1;
        } else {
        }
      } else {
        ctx->last_jump_jiffies = jiffies;
      }
    } else {
      ctx->last_jump = (unsigned int )((int )ctx->start + target);
      ctx->last_jump_jiffies = jiffies;
    }
    *ptr = (int )ctx->start + target;
  } else {
  }
  return;
}
}
static void atom_op_mask(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 mask ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  mask = atom_get_src_direct(ctx, ((int )attr >> 3) & 7, ptr);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   mask: 0x%08x", mask);
  } else {
  }
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst & mask;
  dst = dst | src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_move(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if ((((int )attr >> 3) & 7) != 0) {
    atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 0);
  } else {
    atom_skip_dst(ctx, arg, (int )attr, ptr);
    saved = 3452816845U;
  }
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, src, saved);
  return;
}
}
static void atom_op_mul(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src1: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, (u32 *)0U, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src2: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  (ctx->ctx)->divmul[0] = dst * src;
  return;
}
}
static void atom_op_nop(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  return;
}
}
static void atom_op_or(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst | src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_postcard(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t val ;
  int tmp ;
  uint8_t tmp___0 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  val = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("POST card output: 0x%02X\n", (int )val);
  } else {
  }
  return;
}
}
static void atom_op_repeat(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  printk("\016unimplemented!\n");
  return;
}
}
static void atom_op_restorereg(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  printk("\016unimplemented!\n");
  return;
}
}
static void atom_op_savereg(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  printk("\016unimplemented!\n");
  return;
}
}
static void atom_op_setdatablock(atom_exec_context *ctx , int *ptr , int arg )
{
  int idx ;
  uint8_t tmp ;
  {
  tmp = get_u8((ctx->ctx)->bios, *ptr);
  idx = (int )tmp;
  *ptr = *ptr + 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   block: %d\n", idx);
  } else {
  }
  if (idx == 0) {
    (ctx->ctx)->data_block = 0U;
  } else
  if (idx == 255) {
    (ctx->ctx)->data_block = ctx->start;
  } else {
    (ctx->ctx)->data_block = get_u16((ctx->ctx)->bios, (int )(((ctx->ctx)->data_table + (u32 )(idx * 2)) + 4U));
  }
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   base: 0x%04X\n", (int )(ctx->ctx)->data_block);
  } else {
  }
  return;
}
}
static void atom_op_setfbbase(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   fb_base: ");
  } else {
  }
  (ctx->ctx)->fb_base = atom_get_src(ctx, (int )attr, ptr);
  return;
}
}
static void atom_op_setport(atom_exec_context *ctx , int *ptr , int arg )
{
  int port ;
  u16 tmp ;
  {
  switch (arg) {
  case 0:
  tmp = get_u16((ctx->ctx)->bios, *ptr);
  port = (int )tmp;
  if (port <= 4) {
    if (amdgpu_atom_debug != 0) {
      printk("\017");
      debug_print_spaces(debug_depth);
      printk("   port: %d (%s)\n", port, atom_io_names[port]);
    } else {
    }
  } else
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   port: %d\n", port);
  } else {
  }
  if (port == 0) {
    (ctx->ctx)->io_mode = 0;
  } else {
    (ctx->ctx)->io_mode = port | 128;
  }
  *ptr = *ptr + 2;
  goto ldv_48192;
  case 1:
  (ctx->ctx)->io_mode = 1;
  *ptr = *ptr + 1;
  goto ldv_48192;
  case 2:
  (ctx->ctx)->io_mode = 2;
  *ptr = *ptr + 1;
  goto ldv_48192;
  }
  ldv_48192: ;
  return;
}
}
static void atom_op_setregblock(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  (ctx->ctx)->reg_block = get_u16((ctx->ctx)->bios, *ptr);
  *ptr = *ptr + 2;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   base: 0x%04X\n", (int )(ctx->ctx)->reg_block);
  } else {
  }
  return;
}
}
static void atom_op_shift_left(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  uint8_t shift ;
  u32 saved ;
  u32 dst ;
  int dptr ;
  u32 tmp___1 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  attr = (unsigned int )attr & 56U;
  attr = (uint8_t )((int )((signed char )(atom_def_dst[(int )attr >> 3] << 6)) | (int )((signed char )attr));
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  tmp___1 = atom_get_src_direct(ctx, 4, ptr);
  shift = (uint8_t )tmp___1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   shift: %d\n", (int )shift);
  } else {
  }
  dst = dst << (int )shift;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_shift_right(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  uint8_t shift ;
  u32 saved ;
  u32 dst ;
  int dptr ;
  u32 tmp___1 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  attr = (unsigned int )attr & 56U;
  attr = (uint8_t )((int )((signed char )(atom_def_dst[(int )attr >> 3] << 6)) | (int )((signed char )attr));
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  tmp___1 = atom_get_src_direct(ctx, 4, ptr);
  shift = (uint8_t )tmp___1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   shift: %d\n", (int )shift);
  } else {
  }
  dst = dst >> (int )shift;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_shl(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  uint8_t shift ;
  u32 saved ;
  u32 dst ;
  int dptr ;
  u32 dst_align ;
  u32 tmp___1 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  dst_align = (u32 )atom_dst_to_src[((int )attr >> 3) & 7][((int )attr >> 6) & 3];
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  dst = saved;
  tmp___1 = atom_get_src(ctx, (int )attr, ptr);
  shift = (uint8_t )tmp___1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   shift: %d\n", (int )shift);
  } else {
  }
  dst = dst << (int )shift;
  dst = atom_arg_mask[dst_align] & dst;
  dst = dst >> atom_arg_shift[dst_align];
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_shr(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  uint8_t shift ;
  u32 saved ;
  u32 dst ;
  int dptr ;
  u32 dst_align ;
  u32 tmp___1 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  dst_align = (u32 )atom_dst_to_src[((int )attr >> 3) & 7][((int )attr >> 6) & 3];
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  dst = saved;
  tmp___1 = atom_get_src(ctx, (int )attr, ptr);
  shift = (uint8_t )tmp___1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   shift: %d\n", (int )shift);
  } else {
  }
  dst = dst >> (int )shift;
  dst = atom_arg_mask[dst_align] & dst;
  dst = dst >> atom_arg_shift[dst_align];
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_sub(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst - src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_switch(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 src ;
  u32 val ;
  u32 target ;
  u16 tmp___1 ;
  uint8_t tmp___2 ;
  u16 tmp___3 ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   switch: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  goto ldv_48262;
  ldv_48261:
  tmp___2 = get_u8((ctx->ctx)->bios, *ptr);
  if ((unsigned int )tmp___2 == 99U) {
    *ptr = *ptr + 1;
    if (amdgpu_atom_debug != 0) {
      printk("\017");
      debug_print_spaces(debug_depth);
      printk("   case: ");
    } else {
    }
    val = atom_get_src(ctx, (int )((uint8_t )(((int )((signed char )attr) & 56) | 5)),
                       ptr);
    tmp___1 = get_u16((ctx->ctx)->bios, *ptr);
    target = (u32 )tmp___1;
    if (val == src) {
      if (amdgpu_atom_debug != 0) {
        printk("\017");
        debug_print_spaces(debug_depth);
        printk("   target: %04X\n", target);
      } else {
      }
      *ptr = (int )((u32 )ctx->start + target);
      return;
    } else {
    }
    *ptr = *ptr + 2;
  } else {
    printk("\016Bad case.\n");
    return;
  }
  ldv_48262:
  tmp___3 = get_u16((ctx->ctx)->bios, *ptr);
  if ((unsigned int )tmp___3 != 23130U) {
    goto ldv_48261;
  } else {
  }
  *ptr = *ptr + 2;
  return;
}
}
static void atom_op_test(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src1: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, (u32 *)0U, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src2: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  (ctx->ctx)->cs_equal = (dst & src) == 0U;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   result: %s\n", (ctx->ctx)->cs_equal != 0 ? (char *)"EQ" : (char *)"NE");
  } else {
  }
  return;
}
}
static void atom_op_xor(atom_exec_context *ctx , int *ptr , int arg )
{
  uint8_t attr ;
  int tmp ;
  uint8_t tmp___0 ;
  u32 dst ;
  u32 src ;
  u32 saved ;
  int dptr ;
  {
  tmp = *ptr;
  *ptr = *ptr + 1;
  tmp___0 = get_u8((ctx->ctx)->bios, tmp);
  attr = tmp___0;
  dptr = *ptr;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  dst = atom_get_dst(ctx, arg, (int )attr, ptr, & saved, 1);
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   src: ");
  } else {
  }
  src = atom_get_src(ctx, (int )attr, ptr);
  dst = dst ^ src;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("   dst: ");
  } else {
  }
  atom_put_dst(ctx, arg, (int )attr, & dptr, dst, saved);
  return;
}
}
static void atom_op_debug(atom_exec_context *ctx , int *ptr , int arg )
{
  {
  printk("\016unimplemented!\n");
  return;
}
}
static struct __anonstruct_opcode_table_321 opcode_table[123U] =
  { {(void (*)(atom_exec_context * , int * , int ))0, 0},
        {& atom_op_move, 0},
        {& atom_op_move, 1},
        {& atom_op_move, 2},
        {& atom_op_move, 3},
        {& atom_op_move, 6},
        {& atom_op_move, 7},
        {& atom_op_and, 0},
        {& atom_op_and, 1},
        {& atom_op_and, 2},
        {& atom_op_and, 3},
        {& atom_op_and, 6},
        {& atom_op_and, 7},
        {& atom_op_or, 0},
        {& atom_op_or, 1},
        {& atom_op_or, 2},
        {& atom_op_or, 3},
        {& atom_op_or, 6},
        {& atom_op_or, 7},
        {& atom_op_shift_left, 0},
        {& atom_op_shift_left, 1},
        {& atom_op_shift_left, 2},
        {& atom_op_shift_left, 3},
        {& atom_op_shift_left, 6},
        {& atom_op_shift_left, 7},
        {& atom_op_shift_right, 0},
        {& atom_op_shift_right, 1},
        {& atom_op_shift_right, 2},
        {& atom_op_shift_right, 3},
        {& atom_op_shift_right, 6},
        {& atom_op_shift_right, 7},
        {& atom_op_mul, 0},
        {& atom_op_mul, 1},
        {& atom_op_mul, 2},
        {& atom_op_mul, 3},
        {& atom_op_mul, 6},
        {& atom_op_mul, 7},
        {& atom_op_div, 0},
        {& atom_op_div, 1},
        {& atom_op_div, 2},
        {& atom_op_div, 3},
        {& atom_op_div, 6},
        {& atom_op_div, 7},
        {& atom_op_add, 0},
        {& atom_op_add, 1},
        {& atom_op_add, 2},
        {& atom_op_add, 3},
        {& atom_op_add, 6},
        {& atom_op_add, 7},
        {& atom_op_sub, 0},
        {& atom_op_sub, 1},
        {& atom_op_sub, 2},
        {& atom_op_sub, 3},
        {& atom_op_sub, 6},
        {& atom_op_sub, 7},
        {& atom_op_setport, 0},
        {& atom_op_setport, 1},
        {& atom_op_setport, 2},
        {& atom_op_setregblock, 0},
        {& atom_op_setfbbase, 0},
        {& atom_op_compare, 0},
        {& atom_op_compare, 1},
        {& atom_op_compare, 2},
        {& atom_op_compare, 3},
        {& atom_op_compare, 6},
        {& atom_op_compare, 7},
        {& atom_op_switch, 0},
        {& atom_op_jump, 2},
        {& atom_op_jump, 5},
        {& atom_op_jump, 3},
        {& atom_op_jump, 0},
        {& atom_op_jump, 4},
        {& atom_op_jump, 1},
        {& atom_op_jump, 6},
        {& atom_op_test, 0},
        {& atom_op_test, 1},
        {& atom_op_test, 2},
        {& atom_op_test, 3},
        {& atom_op_test, 6},
        {& atom_op_test, 7},
        {& atom_op_delay, 1},
        {& atom_op_delay, 0},
        {& atom_op_calltable, 0},
        {& atom_op_repeat, 0},
        {& atom_op_clear, 0},
        {& atom_op_clear, 1},
        {& atom_op_clear, 2},
        {& atom_op_clear, 3},
        {& atom_op_clear, 6},
        {& atom_op_clear, 7},
        {& atom_op_nop, 0},
        {& atom_op_eot, 0},
        {& atom_op_mask, 0},
        {& atom_op_mask, 1},
        {& atom_op_mask, 2},
        {& atom_op_mask, 3},
        {& atom_op_mask, 6},
        {& atom_op_mask, 7},
        {& atom_op_postcard, 0},
        {& atom_op_beep, 0},
        {& atom_op_savereg, 0},
        {& atom_op_restorereg, 0},
        {& atom_op_setdatablock, 0},
        {& atom_op_xor, 0},
        {& atom_op_xor, 1},
        {& atom_op_xor, 2},
        {& atom_op_xor, 3},
        {& atom_op_xor, 6},
        {& atom_op_xor, 7},
        {& atom_op_shl, 0},
        {& atom_op_shl, 1},
        {& atom_op_shl, 2},
        {& atom_op_shl, 3},
        {& atom_op_shl, 6},
        {& atom_op_shl, 7},
        {& atom_op_shr, 0},
        {& atom_op_shr, 1},
        {& atom_op_shr, 2},
        {& atom_op_shr, 3},
        {& atom_op_shr, 6},
        {& atom_op_shr, 7},
        {& atom_op_debug, 0}};
static int amdgpu_atom_execute_table_locked(struct atom_context *ctx , int index ,
                                            u32 *params )
{
  int base ;
  u16 tmp ;
  int len ;
  int ws ;
  int ps ;
  int ptr ;
  unsigned char op ;
  atom_exec_context ectx ;
  int ret ;
  u16 tmp___0 ;
  uint8_t tmp___1 ;
  uint8_t tmp___2 ;
  void *tmp___3 ;
  int tmp___4 ;
  {
  tmp = get_u16(ctx->bios, (int )((ctx->cmd_table + (u32 )(index * 2)) + 4U));
  base = (int )tmp;
  ret = 0;
  if (base == 0) {
    return (-22);
  } else {
  }
  tmp___0 = get_u16(ctx->bios, base);
  len = (int )tmp___0;
  tmp___1 = get_u8(ctx->bios, base + 4);
  ws = (int )tmp___1;
  tmp___2 = get_u8(ctx->bios, base + 5);
  ps = (int )tmp___2 & 127;
  ptr = base + 6;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk(">> execute %04X (len %d, WS %d, PS %d)\n", base, len, ws, ps);
  } else {
  }
  ectx.ctx = ctx;
  ectx.ps_shift = ps / 4;
  ectx.start = (u16 )base;
  ectx.ps = params;
  ectx.abort = 0;
  ectx.last_jump = 0U;
  if (ws != 0) {
    tmp___3 = kzalloc((size_t )(ws * 4), 208U);
    ectx.ws = (u32 *)tmp___3;
  } else {
    ectx.ws = (u32 *)0U;
  }
  debug_depth = debug_depth + 1;
  ldv_48309:
  tmp___4 = ptr;
  ptr = ptr + 1;
  op = get_u8(ctx->bios, tmp___4);
  if ((unsigned int )op <= 122U) {
    if (amdgpu_atom_debug != 0) {
      printk("\017");
      debug_print_spaces(debug_depth);
      printk("%s @ 0x%04X\n", atom_op_names[(int )op], ptr + -1);
    } else {
    }
  } else
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("[%d] @ 0x%04X\n", (int )op, ptr + -1);
  } else {
  }
  if ((int )ectx.abort) {
    drm_err("atombios stuck executing %04X (len %d, WS %d, PS %d) @ 0x%04X\n", base,
            len, ws, ps, ptr + -1);
    ret = -22;
    goto free;
  } else {
  }
  if ((unsigned int )op <= 122U && (unsigned int )op != 0U) {
    (*(opcode_table[(int )op].func))(& ectx, & ptr, opcode_table[(int )op].arg);
  } else {
    goto ldv_48308;
  }
  if ((unsigned int )op == 91U) {
    goto ldv_48308;
  } else {
  }
  goto ldv_48309;
  ldv_48308:
  debug_depth = debug_depth - 1;
  if (amdgpu_atom_debug != 0) {
    printk("\017");
    debug_print_spaces(debug_depth);
    printk("<<\n");
  } else {
  }
  free: ;
  if (ws != 0) {
    kfree((void const *)ectx.ws);
  } else {
  }
  return (ret);
}
}
int amdgpu_atom_execute_table(struct atom_context *ctx , int index , u32 *params )
{
  int r ;
  {
  mutex_lock_nested(& ctx->mutex, 0U);
  ctx->data_block = 0U;
  ctx->reg_block = 0U;
  ctx->fb_base = 0U;
  ctx->io_mode = 0;
  ctx->divmul[0] = 0U;
  ctx->divmul[1] = 0U;
  r = amdgpu_atom_execute_table_locked(ctx, index, params);
  mutex_unlock(& ctx->mutex);
  return (r);
}
}
static int atom_iio_len[10U] =
  { 1, 2, 3, 3,
        3, 3, 4, 4,
        4, 3};
static void atom_index_iio(struct atom_context *ctx , int base )
{
  void *tmp ;
  uint8_t tmp___0 ;
  uint8_t tmp___1 ;
  uint8_t tmp___2 ;
  uint8_t tmp___3 ;
  {
  tmp = kzalloc(512UL, 208U);
  ctx->iio = (u16 *)tmp;
  if ((unsigned long )ctx->iio == (unsigned long )((u16 *)0U)) {
    return;
  } else {
  }
  goto ldv_48325;
  ldv_48324:
  tmp___0 = get_u8(ctx->bios, base + 1);
  *(ctx->iio + (unsigned long )tmp___0) = (unsigned int )((u16 )base) + 2U;
  base = base + 2;
  goto ldv_48322;
  ldv_48321:
  tmp___1 = get_u8(ctx->bios, base);
  base = atom_iio_len[(int )tmp___1] + base;
  ldv_48322:
  tmp___2 = get_u8(ctx->bios, base);
  if ((unsigned int )tmp___2 != 9U) {
    goto ldv_48321;
  } else {
  }
  base = base + 3;
  ldv_48325:
  tmp___3 = get_u8(ctx->bios, base);
  if ((unsigned int )tmp___3 == 1U) {
    goto ldv_48324;
  } else {
  }
  return;
}
}
struct atom_context *amdgpu_atom_parse(struct card_info *card , void *bios )
{
  int base ;
  struct atom_context *ctx ;
  void *tmp ;
  char *str ;
  char name[512U] ;
  int i ;
  u16 tmp___0 ;
  size_t tmp___1 ;
  int tmp___2 ;
  u16 tmp___3 ;
  size_t tmp___4 ;
  int tmp___5 ;
  u16 tmp___6 ;
  u16 tmp___7 ;
  u16 tmp___8 ;
  u16 tmp___9 ;
  {
  tmp = kzalloc(248UL, 208U);
  ctx = (struct atom_context *)tmp;
  if ((unsigned long )ctx == (unsigned long )((struct atom_context *)0)) {
    return ((struct atom_context *)0);
  } else {
  }
  ctx->card = card;
  ctx->bios = bios;
  tmp___0 = get_u16(ctx->bios, 0);
  if ((unsigned int )tmp___0 != 43605U) {
    printk("\016Invalid BIOS magic.\n");
    kfree((void const *)ctx);
    return ((struct atom_context *)0);
  } else {
  }
  tmp___1 = strlen(" 761295520");
  tmp___2 = strncmp((char const *)ctx->bios + 48U, " 761295520", tmp___1);
  if (tmp___2 != 0) {
    printk("\016Invalid ATI magic.\n");
    kfree((void const *)ctx);
    return ((struct atom_context *)0);
  } else {
  }
  tmp___3 = get_u16(ctx->bios, 72);
  base = (int )tmp___3;
  tmp___4 = strlen("ATOM");
  tmp___5 = strncmp((char const *)ctx->bios + ((unsigned long )base + 4UL), "ATOM",
                    tmp___4);
  if (tmp___5 != 0) {
    printk("\016Invalid ATOM magic.\n");
    kfree((void const *)ctx);
    return ((struct atom_context *)0);
  } else {
  }
  tmp___6 = get_u16(ctx->bios, base + 30);
  ctx->cmd_table = (u32 )tmp___6;
  tmp___7 = get_u16(ctx->bios, base + 32);
  ctx->data_table = (u32 )tmp___7;
  tmp___8 = get_u16(ctx->bios, (int )(ctx->data_table + 50U));
  atom_index_iio(ctx, (int )tmp___8 + 4);
  if ((unsigned long )ctx->iio == (unsigned long )((u16 *)0U)) {
    amdgpu_atom_destroy(ctx);
    return ((struct atom_context *)0);
  } else {
  }
  tmp___9 = get_u16(ctx->bios, base + 16);
  str = (char *)ctx->bios + (unsigned long )tmp___9;
  goto ldv_48337;
  ldv_48336:
  str = str + 1;
  ldv_48337: ;
  if ((int )((signed char )*str) != 0 && ((int )((signed char )*str) == 10 || (int )((signed char )*str) == 13)) {
    goto ldv_48336;
  } else {
  }
  i = 0;
  goto ldv_48341;
  ldv_48340:
  name[i] = *(str + (unsigned long )i);
  if ((int )((signed char )name[i]) <= 45 || (int )((signed char )name[i]) > 122) {
    name[i] = 0;
    goto ldv_48339;
  } else {
  }
  i = i + 1;
  ldv_48341: ;
  if (i <= 510) {
    goto ldv_48340;
  } else {
  }
  ldv_48339:
  printk("\016ATOM BIOS: %s\n", (char *)(& name));
  return (ctx);
}
}
int amdgpu_atom_asic_init(struct atom_context *ctx )
{
  int hwi ;
  u16 tmp ;
  u32 ps[16U] ;
  int ret ;
  u16 tmp___0 ;
  {
  tmp = get_u16(ctx->bios, (int )(ctx->data_table + 12U));
  hwi = (int )tmp;
  memset((void *)(& ps), 0, 64UL);
  ps[0] = get_u32(ctx->bios, hwi + 8);
  ps[1] = get_u32(ctx->bios, hwi + 12);
  if (ps[0] == 0U || ps[1] == 0U) {
    return (1);
  } else {
  }
  tmp___0 = get_u16(ctx->bios, (int )(ctx->cmd_table + 4U));
  if ((unsigned int )tmp___0 == 0U) {
    return (1);
  } else {
  }
  ret = amdgpu_atom_execute_table(ctx, 0, (u32 *)(& ps));
  if (ret != 0) {
    return (ret);
  } else {
  }
  memset((void *)(& ps), 0, 64UL);
  return (ret);
}
}
void amdgpu_atom_destroy(struct atom_context *ctx )
{
  {
  kfree((void const *)ctx->iio);
  kfree((void const *)ctx);
  return;
}
}
bool amdgpu_atom_parse_data_header(struct atom_context *ctx , int index , u16 *size ,
                                   uint8_t *frev , uint8_t *crev , u16 *data_start )
{
  int offset ;
  int idx ;
  u16 tmp ;
  u16 *mdt ;
  {
  offset = (index + 2) * 2;
  tmp = get_u16(ctx->bios, (int )(ctx->data_table + (u32 )offset));
  idx = (int )tmp;
  mdt = (u16 *)(ctx->bios + ((unsigned long )ctx->data_table + 4UL));
  if ((unsigned int )*(mdt + (unsigned long )index) == 0U) {
    return (0);
  } else {
  }
  if ((unsigned long )size != (unsigned long )((u16 *)0U)) {
    *size = get_u16(ctx->bios, idx);
  } else {
  }
  if ((unsigned long )frev != (unsigned long )((uint8_t *)0U)) {
    *frev = get_u8(ctx->bios, idx + 2);
  } else {
  }
  if ((unsigned long )crev != (unsigned long )((uint8_t *)0U)) {
    *crev = get_u8(ctx->bios, idx + 3);
  } else {
  }
  *data_start = (u16 )idx;
  return (1);
}
}
bool amdgpu_atom_parse_cmd_header(struct atom_context *ctx , int index , uint8_t *frev ,
                                  uint8_t *crev )
{
  int offset ;
  int idx ;
  u16 tmp ;
  u16 *mct ;
  {
  offset = (index + 2) * 2;
  tmp = get_u16(ctx->bios, (int )(ctx->cmd_table + (u32 )offset));
  idx = (int )tmp;
  mct = (u16 *)(ctx->bios + ((unsigned long )ctx->cmd_table + 4UL));
  if ((unsigned int )*(mct + (unsigned long )index) == 0U) {
    return (0);
  } else {
  }
  if ((unsigned long )frev != (unsigned long )((uint8_t *)0U)) {
    *frev = get_u8(ctx->bios, idx + 2);
  } else {
  }
  if ((unsigned long )crev != (unsigned long )((uint8_t *)0U)) {
    *crev = get_u8(ctx->bios, idx + 3);
  } else {
  }
  return (1);
}
}
int amdgpu_atom_allocate_fb_scratch(struct atom_context *ctx )
{
  int index ;
  u16 data_offset ;
  int usage_bytes ;
  struct _ATOM_VRAM_USAGE_BY_FIRMWARE *firmware_usage ;
  long tmp ;
  bool tmp___0 ;
  void *tmp___1 ;
  {
  index = 11;
  usage_bytes = 0;
  tmp___0 = amdgpu_atom_parse_data_header(ctx, index, (u16 *)0U, (uint8_t *)0U, (uint8_t *)0U,
                                          & data_offset);
  if ((int )tmp___0) {
    firmware_usage = (struct _ATOM_VRAM_USAGE_BY_FIRMWARE *)ctx->bios + (unsigned long )data_offset;
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_atom_allocate_fb_scratch", "atom firmware requested %08x %dkb\n",
                          firmware_usage->asFirmwareVramReserveInfo[0].ulStartAddrUsedByFirmware,
                          (int )firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb);
    } else {
    }
    usage_bytes = (int )firmware_usage->asFirmwareVramReserveInfo[0].usFirmwareUseInKb * 1024;
  } else {
  }
  ctx->scratch_size_bytes = 0;
  if (usage_bytes == 0) {
    usage_bytes = 20480;
  } else {
  }
  tmp___1 = kzalloc((size_t )usage_bytes, 208U);
  ctx->scratch = (u32 *)tmp___1;
  if ((unsigned long )ctx->scratch == (unsigned long )((u32 *)0U)) {
    return (-12);
  } else {
  }
  ctx->scratch_size_bytes = usage_bytes;
  return (0);
}
}
bool ldv_queue_work_on_89(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_90(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_91(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                          struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_92(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_93(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                  struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void __read_once_size(void const volatile *p , void *res , int size )
{
  {
  switch (size) {
  case 1:
  *((__u8 *)res) = *((__u8 volatile *)p);
  goto ldv_880;
  case 2:
  *((__u16 *)res) = *((__u16 volatile *)p);
  goto ldv_880;
  case 4:
  *((__u32 *)res) = *((__u32 volatile *)p);
  goto ldv_880;
  case 8:
  *((__u64 *)res) = *((__u64 volatile *)p);
  goto ldv_880;
  default:
  __asm__ volatile ("": : : "memory");
  memcpy(res, (void const *)p, (unsigned long )size);
  __asm__ volatile ("": : : "memory");
  }
  ldv_880: ;
  return;
}
}
__inline static int constant_test_bit(long nr , unsigned long const volatile *addr )
{
  {
  return ((int )((unsigned long )*(addr + (unsigned long )(nr >> 6)) >> ((int )nr & 63)) & 1);
}
}
__inline static int variable_test_bit(long nr , unsigned long const volatile *addr )
{
  int oldbit ;
  {
  __asm__ volatile ("bt %2,%1\n\tsbb %0,%0": "=r" (oldbit): "m" (*((unsigned long *)addr)),
                       "Ir" (nr));
  return (oldbit);
}
}
extern void __might_sleep(char const * , int , int ) ;
__inline static void INIT_LIST_HEAD(struct list_head *list )
{
  {
  list->next = list;
  list->prev = list;
  return;
}
}
extern void __list_add(struct list_head * , struct list_head * , struct list_head * ) ;
__inline static void list_add(struct list_head *new , struct list_head *head )
{
  {
  __list_add(new, head, head->next);
  return;
}
}
extern void list_del(struct list_head * ) ;
extern struct task_struct *current_task ;
__inline static struct task_struct *get_current(void)
{
  struct task_struct *pfo_ret__ ;
  {
  switch (8UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "p" (& current_task));
  goto ldv_3129;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_3129;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_3129;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_3129;
  default:
  __bad_percpu_size();
  }
  ldv_3129: ;
  return (pfo_ret__);
}
}
__inline static unsigned long arch_local_save_flags___0(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
extern void __xchg_wrong_size(void) ;
extern void __xadd_wrong_size(void) ;
__inline static int atomic_sub_and_test(int i , atomic_t *v )
{
  char c ;
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; subl %2, %0; sete %1": "+m" (v->counter),
                       "=qm" (c): "er" (i): "memory");
  return ((int )((signed char )c) != 0);
}
}
__inline static int atomic_add_return(int i , atomic_t *v )
{
  int __ret ;
  {
  __ret = i;
  switch (4UL) {
  case 1UL:
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; xaddb %b0, %1\n": "+q" (__ret),
                       "+m" (v->counter): : "memory", "cc");
  goto ldv_5659;
  case 2UL:
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; xaddw %w0, %1\n": "+r" (__ret),
                       "+m" (v->counter): : "memory", "cc");
  goto ldv_5659;
  case 4UL:
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; xaddl %0, %1\n": "+r" (__ret),
                       "+m" (v->counter): : "memory", "cc");
  goto ldv_5659;
  case 8UL:
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; xaddq %q0, %1\n": "+r" (__ret),
                       "+m" (v->counter): : "memory", "cc");
  goto ldv_5659;
  default:
  __xadd_wrong_size();
  }
  ldv_5659: ;
  return (__ret + i);
}
}
__inline static void atomic64_set(atomic64_t *v , long i )
{
  {
  v->counter = i;
  return;
}
}
__inline static long atomic64_xchg(atomic64_t *v , long new )
{
  long __ret ;
  {
  __ret = new;
  switch (8UL) {
  case 1UL:
  __asm__ volatile ("xchgb %b0, %1\n": "+q" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5794;
  case 2UL:
  __asm__ volatile ("xchgw %w0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5794;
  case 4UL:
  __asm__ volatile ("xchgl %0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5794;
  case 8UL:
  __asm__ volatile ("xchgq %q0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5794;
  default:
  __xchg_wrong_size();
  }
  ldv_5794: ;
  return (__ret);
}
}
__inline static int test_ti_thread_flag(struct thread_info *ti , int flag )
{
  int tmp ;
  {
  tmp = variable_test_bit((long )flag, (unsigned long const volatile *)(& ti->flags));
  return (tmp);
}
}
__inline static void __preempt_count_add(int val )
{
  int pao_ID__ ;
  {
  pao_ID__ = 0;
  switch (4UL) {
  case 1UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incb %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decb %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addb %1, %%gs:%0": "+m" (__preempt_count): "qi" (val));
  }
  goto ldv_6059;
  case 2UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incw %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decw %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addw %1, %%gs:%0": "+m" (__preempt_count): "ri" (val));
  }
  goto ldv_6059;
  case 4UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incl %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decl %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addl %1, %%gs:%0": "+m" (__preempt_count): "ri" (val));
  }
  goto ldv_6059;
  case 8UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incq %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decq %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addq %1, %%gs:%0": "+m" (__preempt_count): "re" (val));
  }
  goto ldv_6059;
  default:
  __bad_percpu_size();
  }
  ldv_6059: ;
  return;
}
}
__inline static void __preempt_count_sub(int val )
{
  int pao_ID__ ;
  {
  pao_ID__ = 0;
  switch (4UL) {
  case 1UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incb %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decb %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addb %1, %%gs:%0": "+m" (__preempt_count): "qi" (- val));
  }
  goto ldv_6071;
  case 2UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incw %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decw %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addw %1, %%gs:%0": "+m" (__preempt_count): "ri" (- val));
  }
  goto ldv_6071;
  case 4UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incl %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decl %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addl %1, %%gs:%0": "+m" (__preempt_count): "ri" (- val));
  }
  goto ldv_6071;
  case 8UL: ;
  if (pao_ID__ == 1) {
    __asm__ ("incq %%gs:%0": "+m" (__preempt_count));
  } else
  if (pao_ID__ == -1) {
    __asm__ ("decq %%gs:%0": "+m" (__preempt_count));
  } else {
    __asm__ ("addq %1, %%gs:%0": "+m" (__preempt_count): "re" (- val));
  }
  goto ldv_6071;
  default:
  __bad_percpu_size();
  }
  ldv_6071: ;
  return;
}
}
extern int debug_locks ;
extern void lockdep_init_map(struct lockdep_map * , char const * , struct lock_class_key * ,
                             int ) ;
extern int lock_is_held(struct lockdep_map * ) ;
extern void lockdep_rcu_suspicious(char const * , int const , char const * ) ;
__inline static int static_key_count(struct static_key *key )
{
  int tmp ;
  {
  tmp = atomic_read((atomic_t const *)(& key->enabled));
  return (tmp);
}
}
__inline static bool static_key_false(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
extern void __init_waitqueue_head(wait_queue_head_t * , char const * , struct lock_class_key * ) ;
__inline static void __add_wait_queue(wait_queue_head_t *head , wait_queue_t *new )
{
  {
  list_add(& new->task_list, & head->task_list);
  return;
}
}
__inline static void __remove_wait_queue(wait_queue_head_t *head , wait_queue_t *old )
{
  {
  list_del(& old->task_list);
  return;
}
}
extern void __wake_up(wait_queue_head_t * , unsigned int , int , void * ) ;
extern void __wake_up_locked(wait_queue_head_t * , unsigned int , int ) ;
extern long prepare_to_wait_event(wait_queue_head_t * , wait_queue_t * , int ) ;
extern void finish_wait(wait_queue_head_t * , wait_queue_t * ) ;
extern int down_read_trylock(struct rw_semaphore * ) ;
extern void up_read(struct rw_semaphore * ) ;
extern bool rcu_is_watching(void) ;
extern bool rcu_lockdep_current_cpu_online(void) ;
extern struct lockdep_map rcu_sched_lock_map ;
extern int debug_lockdep_rcu_enabled(void) ;
__inline static int rcu_read_lock_sched_held(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___0();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
__inline static void rcu_read_lock_sched_notrace(void)
{
  {
  __preempt_count_add(1);
  __asm__ volatile ("": : : "memory");
  return;
}
}
__inline static void rcu_read_unlock_sched_notrace(void)
{
  {
  __asm__ volatile ("": : : "memory");
  __preempt_count_sub(1);
  return;
}
}
extern void init_timer_key(struct timer_list * , unsigned int , char const * ,
                           struct lock_class_key * ) ;
extern void delayed_work_timer_fn(unsigned long ) ;
extern void __init_work(struct work_struct * , int ) ;
extern struct workqueue_struct *system_power_efficient_wq ;
bool ldv_queue_work_on_103(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_105(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_104(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_107(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_106(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_delayed_work(struct workqueue_struct *wq , struct delayed_work *dwork ,
                                        unsigned long delay )
{
  bool tmp ;
  {
  tmp = ldv_queue_delayed_work_on_104(8192, wq, dwork, delay);
  return (tmp);
}
}
__inline static void kref_get(struct kref *kref )
{
  bool __warned ;
  int __ret_warn_once ;
  int tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = atomic_add_return(1, & kref->refcount);
  __ret_warn_once = tmp <= 1;
  tmp___2 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___2 != 0L) {
    __ret_warn_on = ! __warned;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("include/linux/kref.h", 47);
    } else {
    }
    tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___1 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  ldv__builtin_expect(__ret_warn_once != 0, 0L);
  return;
}
}
__inline static int kref_sub(struct kref *kref , unsigned int count , void (*release)(struct kref * ) )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 71);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = atomic_sub_and_test((int )count, & kref->refcount);
  if (tmp___0 != 0) {
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int kref_put(struct kref *kref , void (*release)(struct kref * ) )
{
  int tmp ;
  {
  tmp = kref_sub(kref, 1U, release);
  return (tmp);
}
}
extern long schedule_timeout(long ) ;
extern int wake_up_process(struct task_struct * ) ;
__inline static int test_tsk_thread_flag(struct task_struct *tsk , int flag )
{
  int tmp ;
  {
  tmp = test_ti_thread_flag((struct thread_info *)tsk->stack, flag);
  return (tmp);
}
}
__inline static int signal_pending(struct task_struct *p )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = test_tsk_thread_flag(p, 2);
  tmp___0 = ldv__builtin_expect(tmp != 0, 0L);
  return ((int )tmp___0);
}
}
void invoke_work_1(void) ;
void call_and_disable_work_1(struct work_struct *work ) ;
void disable_work_1(struct work_struct *work ) ;
void activate_work_1(struct work_struct *work , int state ) ;
void call_and_disable_all_1(int state ) ;
extern int seq_printf(struct seq_file * , char const * , ...) ;
extern void fence_init(struct fence * , struct fence_ops const * , spinlock_t * ,
                       unsigned int , unsigned int ) ;
extern void fence_release(struct kref * ) ;
__inline static struct fence *fence_get(struct fence *fence )
{
  {
  if ((unsigned long )fence != (unsigned long )((struct fence *)0)) {
    kref_get(& fence->refcount);
  } else {
  }
  return (fence);
}
}
__inline static void fence_put(struct fence *fence )
{
  {
  if ((unsigned long )fence != (unsigned long )((struct fence *)0)) {
    kref_put(& fence->refcount, & fence_release);
  } else {
  }
  return;
}
}
extern int fence_signal(struct fence * ) ;
extern int fence_signal_locked(struct fence * ) ;
extern int fence_add_callback(struct fence * , struct fence_cb * , void (*)(struct fence * ,
                                                                            struct fence_cb * ) ) ;
extern bool fence_remove_callback(struct fence * , struct fence_cb * ) ;
int amdgpu_irq_update(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type ) ;
bool amdgpu_irq_get_delayed(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                            unsigned int type ) ;
void amdgpu_fence_driver_init_ring(struct amdgpu_ring *ring ) ;
int amdgpu_fence_driver_start_ring(struct amdgpu_ring *ring , struct amdgpu_irq_src *irq_src ,
                                   unsigned int irq_type ) ;
int amdgpu_fence_emit(struct amdgpu_ring *ring , void *owner , struct amdgpu_fence **fence ) ;
int amdgpu_fence_recreate(struct amdgpu_ring *ring , void *owner , uint64_t seq ,
                          struct amdgpu_fence **fence ) ;
void amdgpu_fence_process(struct amdgpu_ring *ring ) ;
int amdgpu_fence_wait_next(struct amdgpu_ring *ring ) ;
unsigned int amdgpu_fence_count_emitted(struct amdgpu_ring *ring ) ;
bool amdgpu_fence_signaled(struct amdgpu_fence *fence ) ;
int amdgpu_fence_wait(struct amdgpu_fence *fence , bool intr ) ;
int amdgpu_fence_wait_any(struct amdgpu_device *adev , struct amdgpu_fence **fences ,
                          bool intr ) ;
struct amdgpu_fence *amdgpu_fence_ref(struct amdgpu_fence *fence ) ;
void amdgpu_fence_unref(struct amdgpu_fence **fence ) ;
bool amdgpu_fence_need_sync(struct amdgpu_fence *fence , struct amdgpu_ring *dst_ring ) ;
void amdgpu_fence_note_sync(struct amdgpu_fence *fence , struct amdgpu_ring *dst_ring ) ;
int amdgpu_debugfs_fence_init(struct amdgpu_device *adev ) ;
struct fence_ops const amdgpu_fence_ops ;
__inline static struct amdgpu_fence *to_amdgpu_fence(struct fence *f )
{
  struct amdgpu_fence *__f ;
  struct fence const *__mptr ;
  {
  __mptr = (struct fence const *)f;
  __f = (struct amdgpu_fence *)__mptr;
  if ((unsigned long )__f->base.ops == (unsigned long )(& amdgpu_fence_ops)) {
    return (__f);
  } else {
  }
  return ((struct amdgpu_fence *)0);
}
}
struct tracepoint __tracepoint_amdgpu_fence_emit ;
__inline static void trace_amdgpu_fence_emit(struct drm_device *dev , int ring , u32 seqno )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_311 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_313 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false(& __tracepoint_amdgpu_fence_emit.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_emit.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               216, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44247:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct drm_device * , int , u32 ))it_func))(__data,
                                                                          dev, ring,
                                                                          seqno);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44247;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_emit.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             216, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
struct tracepoint __tracepoint_amdgpu_fence_wait_begin ;
__inline static void trace_amdgpu_fence_wait_begin(struct drm_device *dev , int ring ,
                                                   u32 seqno )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_315 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_317 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false(& __tracepoint_amdgpu_fence_wait_begin.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_wait_begin.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               223, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44308:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct drm_device * , int , u32 ))it_func))(__data,
                                                                          dev, ring,
                                                                          seqno);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44308;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_wait_begin.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             223, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
struct tracepoint __tracepoint_amdgpu_fence_wait_end ;
__inline static void trace_amdgpu_fence_wait_end(struct drm_device *dev , int ring ,
                                                 u32 seqno )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_319 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_321 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false(& __tracepoint_amdgpu_fence_wait_end.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_wait_end.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               230, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44369:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct drm_device * , int , u32 ))it_func))(__data,
                                                                          dev, ring,
                                                                          seqno);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44369;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_fence_wait_end.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             230, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
static void amdgpu_fence_write(struct amdgpu_ring *ring , u32 seq )
{
  struct amdgpu_fence_driver *drv ;
  {
  drv = & ring->fence_drv;
  if ((unsigned long )drv->cpu_addr != (unsigned long )((u32 volatile *)0U)) {
    *(drv->cpu_addr) = seq;
  } else {
  }
  return;
}
}
static u32 amdgpu_fence_read(struct amdgpu_ring *ring )
{
  struct amdgpu_fence_driver *drv ;
  u32 seq ;
  long tmp ;
  {
  drv = & ring->fence_drv;
  seq = 0U;
  if ((unsigned long )drv->cpu_addr != (unsigned long )((u32 volatile *)0U)) {
    seq = *(drv->cpu_addr);
  } else {
    tmp = atomic64_read((atomic64_t const *)(& drv->last_seq));
    seq = (unsigned int )tmp;
  }
  return (seq);
}
}
static void amdgpu_fence_schedule_check(struct amdgpu_ring *ring )
{
  {
  queue_delayed_work(system_power_efficient_wq, & ring->fence_drv.lockup_work, 125UL);
  return;
}
}
int amdgpu_fence_emit(struct amdgpu_ring *ring , void *owner , struct amdgpu_fence **fence )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  {
  adev = ring->adev;
  tmp = kmalloc(152UL, 208U);
  *fence = (struct amdgpu_fence *)tmp;
  if ((unsigned long )*fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return (-12);
  } else {
  }
  ring->fence_drv.sync_seq[ring->idx] = ring->fence_drv.sync_seq[ring->idx] + 1ULL;
  (*fence)->seq = ring->fence_drv.sync_seq[ring->idx];
  (*fence)->ring = ring;
  (*fence)->owner = owner;
  fence_init(& (*fence)->base, & amdgpu_fence_ops, & adev->fence_queue.lock, adev->fence_context + ring->idx,
             (unsigned int )(*fence)->seq);
  (*((ring->funcs)->emit_fence))(ring, ring->fence_drv.gpu_addr, (*fence)->seq, 2U);
  trace_amdgpu_fence_emit((ring->adev)->ddev, (int )ring->idx, (u32 )(*fence)->seq);
  return (0);
}
}
int amdgpu_fence_recreate(struct amdgpu_ring *ring , void *owner , uint64_t seq ,
                          struct amdgpu_fence **fence )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  {
  adev = ring->adev;
  if (ring->fence_drv.sync_seq[ring->idx] < seq) {
    return (-22);
  } else {
  }
  tmp = kmalloc(152UL, 208U);
  *fence = (struct amdgpu_fence *)tmp;
  if ((unsigned long )*fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return (-12);
  } else {
  }
  (*fence)->seq = seq;
  (*fence)->ring = ring;
  (*fence)->owner = owner;
  fence_init(& (*fence)->base, & amdgpu_fence_ops, & adev->fence_queue.lock, adev->fence_context + ring->idx,
             (unsigned int )(*fence)->seq);
  return (0);
}
}
static int amdgpu_fence_check_signaled(wait_queue_t *wait , unsigned int mode , int flags ,
                                       void *key )
{
  struct amdgpu_fence *fence ;
  struct amdgpu_device *adev ;
  u64 seq ;
  int ret ;
  wait_queue_t const *__mptr ;
  long tmp ;
  struct fence *__ff ;
  struct fence *__ff___0 ;
  struct fence *__ff___1 ;
  {
  __mptr = (wait_queue_t const *)wait;
  fence = (struct amdgpu_fence *)__mptr + 0xffffffffffffff90UL;
  adev = (fence->ring)->adev;
  tmp = atomic64_read((atomic64_t const *)(& (fence->ring)->fence_drv.last_seq));
  seq = (u64 )tmp;
  if (fence->seq <= seq) {
    ret = fence_signal_locked(& fence->base);
    if (ret == 0) {
      __ff = & fence->base;
      printk("\016f %u#%u: signaled from irq context\n", __ff->context, __ff->seqno);
    } else {
      __ff___0 = & fence->base;
      printk("\016f %u#%u: was already signaled\n", __ff___0->context, __ff___0->seqno);
    }
    amdgpu_irq_put(adev, (fence->ring)->fence_drv.irq_src, (fence->ring)->fence_drv.irq_type);
    __remove_wait_queue(& adev->fence_queue, & fence->fence_wake);
    fence_put(& fence->base);
  } else {
    __ff___1 = & fence->base;
    printk("\016f %u#%u: pending\n", __ff___1->context, __ff___1->seqno);
  }
  return (0);
}
}
static bool amdgpu_fence_activity(struct amdgpu_ring *ring )
{
  uint64_t seq ;
  uint64_t last_seq ;
  uint64_t last_emitted ;
  unsigned int count_loop ;
  bool wake ;
  long tmp ;
  u32 tmp___0 ;
  unsigned int tmp___1 ;
  long tmp___2 ;
  {
  count_loop = 0U;
  wake = 0;
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  last_seq = (uint64_t )tmp;
  ldv_44569:
  last_emitted = ring->fence_drv.sync_seq[ring->idx];
  tmp___0 = amdgpu_fence_read(ring);
  seq = (uint64_t )tmp___0;
  seq = (last_seq & 0xffffffff00000000ULL) | seq;
  if (seq < last_seq) {
    seq = seq & 4294967295ULL;
    seq = (last_emitted & 0xffffffff00000000ULL) | seq;
  } else {
  }
  if (seq <= last_seq || seq > last_emitted) {
    goto ldv_44568;
  } else {
  }
  wake = 1;
  last_seq = seq;
  tmp___1 = count_loop;
  count_loop = count_loop + 1U;
  if (tmp___1 > 10U) {
    goto ldv_44568;
  } else {
  }
  tmp___2 = atomic64_xchg(& ring->fence_drv.last_seq, (long )seq);
  if ((unsigned long long )tmp___2 > seq) {
    goto ldv_44569;
  } else {
  }
  ldv_44568: ;
  if (seq < last_emitted) {
    amdgpu_fence_schedule_check(ring);
  } else {
  }
  return (wake);
}
}
static void amdgpu_fence_check_lockup(struct work_struct *work )
{
  struct amdgpu_fence_driver *fence_drv ;
  struct amdgpu_ring *ring ;
  struct work_struct const *__mptr ;
  int tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  {
  __mptr = (struct work_struct const *)work;
  fence_drv = (struct amdgpu_fence_driver *)__mptr + 0xffffffffffffff48UL;
  ring = fence_drv->ring;
  tmp = down_read_trylock(& (ring->adev)->exclusive_lock);
  if (tmp == 0) {
    amdgpu_fence_schedule_check(ring);
    return;
  } else {
  }
  if ((int )fence_drv->delayed_irq && (int )((ring->adev)->ddev)->irq_enabled) {
    fence_drv->delayed_irq = 0;
    amdgpu_irq_update(ring->adev, fence_drv->irq_src, fence_drv->irq_type);
  } else {
  }
  tmp___2 = amdgpu_fence_activity(ring);
  if ((int )tmp___2) {
    __wake_up(& (ring->adev)->fence_queue, 3U, 0, (void *)0);
  } else {
    tmp___1 = (*((ring->funcs)->is_lockup))(ring);
    if ((int )tmp___1) {
      tmp___0 = atomic64_read((atomic64_t const *)(& fence_drv->last_seq));
      dev_warn((struct device const *)(ring->adev)->dev, "GPU lockup (current fence id 0x%016llx last fence id 0x%016llx on ring %d)\n",
               (unsigned long long )tmp___0, fence_drv->sync_seq[ring->idx], ring->idx);
      (ring->adev)->needs_reset = 1;
      __wake_up(& (ring->adev)->fence_queue, 3U, 0, (void *)0);
    } else {
    }
  }
  up_read(& (ring->adev)->exclusive_lock);
  return;
}
}
void amdgpu_fence_process(struct amdgpu_ring *ring )
{
  uint64_t seq ;
  uint64_t last_seq ;
  uint64_t last_emitted ;
  unsigned int count_loop ;
  bool wake ;
  long tmp ;
  u32 tmp___0 ;
  unsigned int tmp___1 ;
  long tmp___2 ;
  {
  count_loop = 0U;
  wake = 0;
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  last_seq = (uint64_t )tmp;
  ldv_44586:
  last_emitted = ring->fence_drv.sync_seq[ring->idx];
  tmp___0 = amdgpu_fence_read(ring);
  seq = (uint64_t )tmp___0;
  seq = (last_seq & 0xffffffff00000000ULL) | seq;
  if (seq < last_seq) {
    seq = seq & 4294967295ULL;
    seq = (last_emitted & 0xffffffff00000000ULL) | seq;
  } else {
  }
  if (seq <= last_seq || seq > last_emitted) {
    goto ldv_44585;
  } else {
  }
  wake = 1;
  last_seq = seq;
  tmp___1 = count_loop;
  count_loop = count_loop + 1U;
  if (tmp___1 > 10U) {
    goto ldv_44585;
  } else {
  }
  tmp___2 = atomic64_xchg(& ring->fence_drv.last_seq, (long )seq);
  if ((unsigned long long )tmp___2 > seq) {
    goto ldv_44586;
  } else {
  }
  ldv_44585: ;
  if ((int )wake) {
    __wake_up(& (ring->adev)->fence_queue, 3U, 0, (void *)0);
  } else {
  }
  return;
}
}
static bool amdgpu_fence_seq_signaled(struct amdgpu_ring *ring , u64 seq )
{
  long tmp ;
  long tmp___0 ;
  {
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  if ((unsigned long long )tmp >= seq) {
    return (1);
  } else {
  }
  amdgpu_fence_process(ring);
  tmp___0 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  if ((unsigned long long )tmp___0 >= seq) {
    return (1);
  } else {
  }
  return (0);
}
}
static bool amdgpu_fence_is_signaled(struct fence *f )
{
  struct amdgpu_fence *fence ;
  struct amdgpu_fence *tmp ;
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  long tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  {
  tmp = to_amdgpu_fence(f);
  fence = tmp;
  ring = fence->ring;
  adev = ring->adev;
  tmp___0 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  if ((unsigned long long )tmp___0 >= fence->seq) {
    return (1);
  } else {
  }
  tmp___2 = down_read_trylock(& adev->exclusive_lock);
  if (tmp___2 != 0) {
    amdgpu_fence_process(ring);
    up_read(& adev->exclusive_lock);
    tmp___1 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
    if ((unsigned long long )tmp___1 >= fence->seq) {
      return (1);
    } else {
    }
  } else {
  }
  return (0);
}
}
static bool amdgpu_fence_enable_signaling(struct fence *f )
{
  struct amdgpu_fence *fence ;
  struct amdgpu_fence *tmp ;
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  long tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  struct fence *__ff ;
  {
  tmp = to_amdgpu_fence(f);
  fence = tmp;
  ring = fence->ring;
  adev = ring->adev;
  tmp___0 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  if ((unsigned long long )tmp___0 >= fence->seq) {
    return (0);
  } else {
  }
  tmp___4 = down_read_trylock(& adev->exclusive_lock);
  if (tmp___4 != 0) {
    amdgpu_irq_get(adev, ring->fence_drv.irq_src, ring->fence_drv.irq_type);
    tmp___1 = amdgpu_fence_activity(ring);
    if ((int )tmp___1) {
      __wake_up_locked(& adev->fence_queue, 3U, 0);
    } else {
    }
    tmp___2 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
    if ((unsigned long long )tmp___2 >= fence->seq) {
      amdgpu_irq_put(adev, ring->fence_drv.irq_src, ring->fence_drv.irq_type);
      up_read(& adev->exclusive_lock);
      return (0);
    } else {
    }
    up_read(& adev->exclusive_lock);
  } else {
    tmp___3 = amdgpu_irq_get_delayed(adev, ring->fence_drv.irq_src, ring->fence_drv.irq_type);
    if ((int )tmp___3) {
      ring->fence_drv.delayed_irq = 1;
    } else {
    }
    amdgpu_fence_schedule_check(ring);
  }
  fence->fence_wake.flags = 0U;
  fence->fence_wake.private = (void *)0;
  fence->fence_wake.func = & amdgpu_fence_check_signaled;
  __add_wait_queue(& adev->fence_queue, & fence->fence_wake);
  fence_get(f);
  __ff = & fence->base;
  printk("\016f %u#%u: armed on ring %i!\n", __ff->context, __ff->seqno, ring->idx);
  return (1);
}
}
bool amdgpu_fence_signaled(struct amdgpu_fence *fence )
{
  struct fence *__ff ;
  int tmp ;
  bool tmp___0 ;
  {
  if ((unsigned long )fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return (1);
  } else {
  }
  tmp___0 = amdgpu_fence_seq_signaled(fence->ring, fence->seq);
  if ((int )tmp___0) {
    tmp = fence_signal(& fence->base);
    if (tmp == 0) {
      __ff = & fence->base;
      printk("\016f %u#%u: signaled from amdgpu_fence_signaled\n", __ff->context,
             __ff->seqno);
    } else {
    }
    return (1);
  } else {
  }
  return (0);
}
}
static bool amdgpu_fence_any_seq_signaled(struct amdgpu_device *adev , u64 *seq )
{
  unsigned int i ;
  bool tmp ;
  {
  i = 0U;
  goto ldv_44615;
  ldv_44614: ;
  if ((unsigned long )adev->rings[i] == (unsigned long )((struct amdgpu_ring *)0) || *(seq + (unsigned long )i) == 0ULL) {
    goto ldv_44613;
  } else {
  }
  tmp = amdgpu_fence_seq_signaled(adev->rings[i], *(seq + (unsigned long )i));
  if ((int )tmp) {
    return (1);
  } else {
  }
  ldv_44613:
  i = i + 1U;
  ldv_44615: ;
  if (i <= 15U) {
    goto ldv_44614;
  } else {
  }
  return (0);
}
}
static long amdgpu_fence_wait_seq_timeout(struct amdgpu_device *adev , u64 *target_seq ,
                                          bool intr , long timeout )
{
  uint64_t last_seq[16U] ;
  bool signaled ;
  int i ;
  long r ;
  bool tmp ;
  struct amdgpu_ring *ring ;
  long tmp___0 ;
  long __ret ;
  wait_queue_t __wait ;
  long __ret___0 ;
  long __int ;
  long tmp___1 ;
  bool __cond ;
  bool __cond___0 ;
  long __ret___1 ;
  wait_queue_t __wait___0 ;
  long __ret___2 ;
  long __int___0 ;
  long tmp___2 ;
  bool __cond___1 ;
  bool __cond___2 ;
  struct amdgpu_ring *ring___0 ;
  long tmp___3 ;
  struct amdgpu_ring *ring___1 ;
  long tmp___4 ;
  bool tmp___5 ;
  long tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  {
  if (timeout == 0L) {
    tmp = amdgpu_fence_any_seq_signaled(adev, target_seq);
    return ((long )tmp);
  } else {
  }
  goto ldv_44663;
  ldv_44673:
  i = 0;
  goto ldv_44630;
  ldv_44629:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0) || *(target_seq + (unsigned long )i) == 0ULL) {
    goto ldv_44628;
  } else {
  }
  tmp___0 = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  last_seq[i] = (uint64_t )tmp___0;
  trace_amdgpu_fence_wait_begin(adev->ddev, i, (u32 )*(target_seq + (unsigned long )i));
  amdgpu_irq_get(adev, ring->fence_drv.irq_src, ring->fence_drv.irq_type);
  ldv_44628:
  i = i + 1;
  ldv_44630: ;
  if (i <= 15) {
    goto ldv_44629;
  } else {
  }
  if ((int )intr) {
    __ret = 125L;
    __might_sleep("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c",
                  583, 0);
    signaled = amdgpu_fence_any_seq_signaled(adev, target_seq);
    __cond___0 = (bool )((int )signaled || (int )adev->needs_reset);
    if ((int )__cond___0 && __ret == 0L) {
      __ret = 1L;
    } else {
    }
    if (((int )__cond___0 || __ret == 0L) == 0) {
      __ret___0 = 125L;
      INIT_LIST_HEAD(& __wait.task_list);
      __wait.flags = 0U;
      ldv_44642:
      tmp___1 = prepare_to_wait_event(& adev->fence_queue, & __wait, 1);
      __int = tmp___1;
      signaled = amdgpu_fence_any_seq_signaled(adev, target_seq);
      __cond = (bool )((int )signaled || (int )adev->needs_reset);
      if ((int )__cond && __ret___0 == 0L) {
        __ret___0 = 1L;
      } else {
      }
      if (((int )__cond || __ret___0 == 0L) != 0) {
        goto ldv_44641;
      } else {
      }
      if (__int != 0L) {
        __ret___0 = __int;
        goto ldv_44641;
      } else {
      }
      __ret___0 = schedule_timeout(__ret___0);
      goto ldv_44642;
      ldv_44641:
      finish_wait(& adev->fence_queue, & __wait);
      __ret = __ret___0;
    } else {
    }
    r = __ret;
  } else {
    __ret___1 = 125L;
    __might_sleep("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_fence.c",
                  587, 0);
    signaled = amdgpu_fence_any_seq_signaled(adev, target_seq);
    __cond___2 = (bool )((int )signaled || (int )adev->needs_reset);
    if ((int )__cond___2 && __ret___1 == 0L) {
      __ret___1 = 1L;
    } else {
    }
    if (((int )__cond___2 || __ret___1 == 0L) == 0) {
      __ret___2 = 125L;
      INIT_LIST_HEAD(& __wait___0.task_list);
      __wait___0.flags = 0U;
      ldv_44655:
      tmp___2 = prepare_to_wait_event(& adev->fence_queue, & __wait___0, 2);
      __int___0 = tmp___2;
      signaled = amdgpu_fence_any_seq_signaled(adev, target_seq);
      __cond___1 = (bool )((int )signaled || (int )adev->needs_reset);
      if ((int )__cond___1 && __ret___2 == 0L) {
        __ret___2 = 1L;
      } else {
      }
      if (((int )__cond___1 || __ret___2 == 0L) != 0) {
        goto ldv_44654;
      } else {
      }
      __ret___2 = schedule_timeout(__ret___2);
      goto ldv_44655;
      ldv_44654:
      finish_wait(& adev->fence_queue, & __wait___0);
      __ret___1 = __ret___2;
    } else {
    }
    r = __ret___1;
  }
  i = 0;
  goto ldv_44661;
  ldv_44660:
  ring___0 = adev->rings[i];
  if ((unsigned long )ring___0 == (unsigned long )((struct amdgpu_ring *)0) || *(target_seq + (unsigned long )i) == 0ULL) {
    goto ldv_44659;
  } else {
  }
  amdgpu_irq_put(adev, ring___0->fence_drv.irq_src, ring___0->fence_drv.irq_type);
  trace_amdgpu_fence_wait_end(adev->ddev, i, (u32 )*(target_seq + (unsigned long )i));
  ldv_44659:
  i = i + 1;
  ldv_44661: ;
  if (i <= 15) {
    goto ldv_44660;
  } else {
  }
  tmp___3 = ldv__builtin_expect(r < 0L, 0L);
  if (tmp___3 != 0L) {
    return (r);
  } else {
  }
  tmp___6 = ldv__builtin_expect((long )(! signaled), 0L);
  if (tmp___6 != 0L) {
    if ((int )adev->needs_reset) {
      return (-35L);
    } else {
    }
    if (r != 0L) {
      goto ldv_44663;
    } else {
    }
    i = 0;
    goto ldv_44668;
    ldv_44667:
    ring___1 = adev->rings[i];
    if ((unsigned long )ring___1 == (unsigned long )((struct amdgpu_ring *)0) || *(target_seq + (unsigned long )i) == 0ULL) {
      goto ldv_44665;
    } else {
    }
    tmp___4 = atomic64_read((atomic64_t const *)(& ring___1->fence_drv.last_seq));
    if (last_seq[i] != (unsigned long long )tmp___4) {
      goto ldv_44666;
    } else {
    }
    ldv_44665:
    i = i + 1;
    ldv_44668: ;
    if (i <= 15) {
      goto ldv_44667;
    } else {
    }
    ldv_44666: ;
    if (i != 16) {
      goto ldv_44663;
    } else {
    }
    i = 0;
    goto ldv_44672;
    ldv_44671: ;
    if ((unsigned long )adev->rings[i] == (unsigned long )((struct amdgpu_ring *)0) || *(target_seq + (unsigned long )i) == 0ULL) {
      goto ldv_44669;
    } else {
    }
    tmp___5 = (*(((adev->rings[i])->funcs)->is_lockup))(adev->rings[i]);
    if ((int )tmp___5) {
      goto ldv_44670;
    } else {
    }
    ldv_44669:
    i = i + 1;
    ldv_44672: ;
    if (i <= 15) {
      goto ldv_44671;
    } else {
    }
    ldv_44670: ;
    if (i <= 15) {
      dev_warn((struct device const *)adev->dev, "GPU lockup (waiting for 0x%016llx last fence id 0x%016llx on ring %d)\n",
               *(target_seq + (unsigned long )i), last_seq[i], i);
      adev->needs_reset = 1;
      __wake_up(& adev->fence_queue, 3U, 0, (void *)0);
      return (-35L);
    } else {
    }
    if (timeout != 9223372036854775807L) {
      timeout = timeout + -125L;
      if (timeout <= 0L) {
        return (0L);
      } else {
      }
    } else {
    }
  } else {
  }
  ldv_44663:
  tmp___7 = amdgpu_fence_any_seq_signaled(adev, target_seq);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
    goto ldv_44673;
  } else {
  }
  return (timeout);
}
}
int amdgpu_fence_wait(struct amdgpu_fence *fence , bool intr )
{
  uint64_t seq[16U] ;
  long r ;
  int tmp ;
  struct fence *__ff ;
  {
  seq[0] = 0ULL;
  seq[1] = 0ULL;
  seq[2] = 0ULL;
  seq[3] = 0ULL;
  seq[4] = 0ULL;
  seq[5] = 0ULL;
  seq[6] = 0ULL;
  seq[7] = 0ULL;
  seq[8] = 0ULL;
  seq[9] = 0ULL;
  seq[10] = 0ULL;
  seq[11] = 0ULL;
  seq[12] = 0ULL;
  seq[13] = 0ULL;
  seq[14] = 0ULL;
  seq[15] = 0ULL;
  seq[(fence->ring)->idx] = fence->seq;
  r = amdgpu_fence_wait_seq_timeout((fence->ring)->adev, (u64 *)(& seq), (int )intr,
                                    9223372036854775807L);
  if (r < 0L) {
    return ((int )r);
  } else {
  }
  tmp = fence_signal(& fence->base);
  r = (long )tmp;
  if (r == 0L) {
    __ff = & fence->base;
    printk("\016f %u#%u: signaled from fence_wait\n", __ff->context, __ff->seqno);
  } else {
  }
  return (0);
}
}
int amdgpu_fence_wait_any(struct amdgpu_device *adev , struct amdgpu_fence **fences ,
                          bool intr )
{
  uint64_t seq[16U] ;
  unsigned int i ;
  unsigned int num_rings ;
  long r ;
  {
  num_rings = 0U;
  i = 0U;
  goto ldv_44693;
  ldv_44692:
  seq[i] = 0ULL;
  if ((unsigned long )*(fences + (unsigned long )i) == (unsigned long )((struct amdgpu_fence *)0)) {
    goto ldv_44691;
  } else {
  }
  seq[i] = (*(fences + (unsigned long )i))->seq;
  num_rings = num_rings + 1U;
  ldv_44691:
  i = i + 1U;
  ldv_44693: ;
  if (i <= 15U) {
    goto ldv_44692;
  } else {
  }
  if (num_rings == 0U) {
    return (-2);
  } else {
  }
  r = amdgpu_fence_wait_seq_timeout(adev, (u64 *)(& seq), (int )intr, 9223372036854775807L);
  if (r < 0L) {
    return ((int )r);
  } else {
  }
  return (0);
}
}
int amdgpu_fence_wait_next(struct amdgpu_ring *ring )
{
  uint64_t seq[16U] ;
  long r ;
  long tmp ;
  {
  seq[0] = 0ULL;
  seq[1] = 0ULL;
  seq[2] = 0ULL;
  seq[3] = 0ULL;
  seq[4] = 0ULL;
  seq[5] = 0ULL;
  seq[6] = 0ULL;
  seq[7] = 0ULL;
  seq[8] = 0ULL;
  seq[9] = 0ULL;
  seq[10] = 0ULL;
  seq[11] = 0ULL;
  seq[12] = 0ULL;
  seq[13] = 0ULL;
  seq[14] = 0ULL;
  seq[15] = 0ULL;
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  seq[ring->idx] = (unsigned long long )tmp + 1ULL;
  if (seq[ring->idx] >= ring->fence_drv.sync_seq[ring->idx]) {
    return (-2);
  } else {
  }
  r = amdgpu_fence_wait_seq_timeout(ring->adev, (u64 *)(& seq), 0, 9223372036854775807L);
  if (r < 0L) {
    return ((int )r);
  } else {
  }
  return (0);
}
}
int amdgpu_fence_wait_empty(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  uint64_t seq[16U] ;
  long r ;
  {
  adev = ring->adev;
  seq[0] = 0ULL;
  seq[1] = 0ULL;
  seq[2] = 0ULL;
  seq[3] = 0ULL;
  seq[4] = 0ULL;
  seq[5] = 0ULL;
  seq[6] = 0ULL;
  seq[7] = 0ULL;
  seq[8] = 0ULL;
  seq[9] = 0ULL;
  seq[10] = 0ULL;
  seq[11] = 0ULL;
  seq[12] = 0ULL;
  seq[13] = 0ULL;
  seq[14] = 0ULL;
  seq[15] = 0ULL;
  seq[ring->idx] = ring->fence_drv.sync_seq[ring->idx];
  if (seq[ring->idx] == 0ULL) {
    return (0);
  } else {
  }
  r = amdgpu_fence_wait_seq_timeout(adev, (u64 *)(& seq), 0, 9223372036854775807L);
  if (r < 0L) {
    if (r == -35L) {
      return (-35);
    } else {
    }
    dev_err((struct device const *)adev->dev, "error waiting for ring[%d] to become idle (%ld)\n",
            ring->idx, r);
  } else {
  }
  return (0);
}
}
struct amdgpu_fence *amdgpu_fence_ref(struct amdgpu_fence *fence )
{
  {
  fence_get(& fence->base);
  return (fence);
}
}
void amdgpu_fence_unref(struct amdgpu_fence **fence )
{
  struct amdgpu_fence *tmp ;
  {
  tmp = *fence;
  *fence = (struct amdgpu_fence *)0;
  if ((unsigned long )tmp != (unsigned long )((struct amdgpu_fence *)0)) {
    fence_put(& tmp->base);
  } else {
  }
  return;
}
}
unsigned int amdgpu_fence_count_emitted(struct amdgpu_ring *ring )
{
  uint64_t emitted ;
  long tmp ;
  {
  amdgpu_fence_process(ring);
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  emitted = ring->fence_drv.sync_seq[ring->idx] - (unsigned long long )tmp;
  if (emitted > 268435456ULL) {
    emitted = 268435456ULL;
  } else {
  }
  return ((unsigned int )emitted);
}
}
bool amdgpu_fence_need_sync(struct amdgpu_fence *fence , struct amdgpu_ring *dst_ring )
{
  struct amdgpu_fence_driver *fdrv ;
  {
  if ((unsigned long )fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return (0);
  } else {
  }
  if ((unsigned long )fence->ring == (unsigned long )dst_ring) {
    return (0);
  } else {
  }
  fdrv = & dst_ring->fence_drv;
  if (fence->seq <= fdrv->sync_seq[(fence->ring)->idx]) {
    return (0);
  } else {
  }
  return (1);
}
}
void amdgpu_fence_note_sync(struct amdgpu_fence *fence , struct amdgpu_ring *dst_ring )
{
  struct amdgpu_fence_driver *dst ;
  struct amdgpu_fence_driver *src ;
  unsigned int i ;
  uint64_t _max1 ;
  uint64_t _max2 ;
  {
  if ((unsigned long )fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return;
  } else {
  }
  if ((unsigned long )fence->ring == (unsigned long )dst_ring) {
    return;
  } else {
  }
  src = & (fence->ring)->fence_drv;
  dst = & dst_ring->fence_drv;
  i = 0U;
  goto ldv_44734;
  ldv_44733: ;
  if (dst_ring->idx == i) {
    goto ldv_44729;
  } else {
  }
  _max1 = dst->sync_seq[i];
  _max2 = src->sync_seq[i];
  dst->sync_seq[i] = _max1 > _max2 ? _max1 : _max2;
  ldv_44729:
  i = i + 1U;
  ldv_44734: ;
  if (i <= 15U) {
    goto ldv_44733;
  } else {
  }
  return;
}
}
int amdgpu_fence_driver_start_ring(struct amdgpu_ring *ring , struct amdgpu_irq_src *irq_src ,
                                   unsigned int irq_type )
{
  struct amdgpu_device *adev ;
  uint64_t index ;
  long tmp ;
  {
  adev = ring->adev;
  if ((unsigned long )(& adev->uvd.ring) != (unsigned long )ring) {
    ring->fence_drv.cpu_addr = adev->wb.wb + (unsigned long )ring->fence_offs;
    ring->fence_drv.gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->fence_offs * 4U);
  } else {
    index = (uint64_t )((unsigned long )(adev->uvd.fw)->size + 7UL) & 0xfffffffffffffff8ULL;
    ring->fence_drv.cpu_addr = (u32 volatile *)(adev->uvd.cpu_addr + index);
    ring->fence_drv.gpu_addr = adev->uvd.gpu_addr + index;
  }
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  amdgpu_fence_write(ring, (u32 )tmp);
  ring->fence_drv.initialized = 1;
  ring->fence_drv.irq_src = irq_src;
  ring->fence_drv.irq_type = irq_type;
  _dev_info((struct device const *)adev->dev, "fence driver on ring %d use gpu addr 0x%016llx, cpu addr 0x%p\n",
            ring->idx, ring->fence_drv.gpu_addr, ring->fence_drv.cpu_addr);
  return (0);
}
}
void amdgpu_fence_driver_init_ring(struct amdgpu_ring *ring )
{
  int i ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  {
  ring->fence_drv.cpu_addr = (u32 volatile *)0U;
  ring->fence_drv.gpu_addr = 0ULL;
  i = 0;
  goto ldv_44748;
  ldv_44747:
  ring->fence_drv.sync_seq[i] = 0ULL;
  i = i + 1;
  ldv_44748: ;
  if (i <= 15) {
    goto ldv_44747;
  } else {
  }
  atomic64_set(& ring->fence_drv.last_seq, 0L);
  ring->fence_drv.initialized = 0;
  __init_work(& ring->fence_drv.lockup_work.work, 0);
  __constr_expr_0.counter = 137438953408L;
  ring->fence_drv.lockup_work.work.data = __constr_expr_0;
  lockdep_init_map(& ring->fence_drv.lockup_work.work.lockdep_map, "(&(&ring->fence_drv.lockup_work)->work)",
                   & __key, 0);
  INIT_LIST_HEAD(& ring->fence_drv.lockup_work.work.entry);
  ring->fence_drv.lockup_work.work.func = & amdgpu_fence_check_lockup;
  init_timer_key(& ring->fence_drv.lockup_work.timer, 2097152U, "(&(&ring->fence_drv.lockup_work)->timer)",
                 & __key___0);
  ring->fence_drv.lockup_work.timer.function = & delayed_work_timer_fn;
  ring->fence_drv.lockup_work.timer.data = (unsigned long )(& ring->fence_drv.lockup_work);
  ring->fence_drv.ring = ring;
  return;
}
}
int amdgpu_fence_driver_init(struct amdgpu_device *adev )
{
  struct lock_class_key __key ;
  int tmp ;
  {
  __init_waitqueue_head(& adev->fence_queue, "&adev->fence_queue", & __key);
  tmp = amdgpu_debugfs_fence_init(adev);
  if (tmp != 0) {
    dev_err((struct device const *)adev->dev, "fence debugfs file creation failed\n");
  } else {
  }
  return (0);
}
}
void amdgpu_fence_driver_fini(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  struct amdgpu_ring *ring ;
  {
  mutex_lock_nested(& adev->ring_lock, 0U);
  i = 0;
  goto ldv_44765;
  ldv_44764:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0) || ! ring->fence_drv.initialized) {
    goto ldv_44763;
  } else {
  }
  r = amdgpu_fence_wait_empty(ring);
  if (r != 0) {
    amdgpu_fence_driver_force_completion(adev);
  } else {
  }
  __wake_up(& adev->fence_queue, 3U, 0, (void *)0);
  ring->fence_drv.initialized = 0;
  ldv_44763:
  i = i + 1;
  ldv_44765: ;
  if (i <= 15) {
    goto ldv_44764;
  } else {
  }
  mutex_unlock(& adev->ring_lock);
  return;
}
}
void amdgpu_fence_driver_force_completion(struct amdgpu_device *adev )
{
  int i ;
  struct amdgpu_ring *ring ;
  {
  i = 0;
  goto ldv_44774;
  ldv_44773:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0) || ! ring->fence_drv.initialized) {
    goto ldv_44772;
  } else {
  }
  amdgpu_fence_write(ring, (u32 )ring->fence_drv.sync_seq[i]);
  ldv_44772:
  i = i + 1;
  ldv_44774: ;
  if (i <= 15) {
    goto ldv_44773;
  } else {
  }
  return;
}
}
static int amdgpu_debugfs_fence_info(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int i ;
  int j ;
  struct amdgpu_ring *ring ;
  long tmp ;
  struct amdgpu_ring *other ;
  {
  node = (struct drm_info_node *)m->private;
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  i = 0;
  goto ldv_44792;
  ldv_44791:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0) || ! ring->fence_drv.initialized) {
    goto ldv_44786;
  } else {
  }
  amdgpu_fence_process(ring);
  seq_printf(m, "--- ring %d (%s) ---\n", i, (char *)(& ring->name));
  tmp = atomic64_read((atomic64_t const *)(& ring->fence_drv.last_seq));
  seq_printf(m, "Last signaled fence 0x%016llx\n", (unsigned long long )tmp);
  seq_printf(m, "Last emitted        0x%016llx\n", ring->fence_drv.sync_seq[i]);
  j = 0;
  goto ldv_44789;
  ldv_44788:
  other = adev->rings[j];
  if (((i != j && (unsigned long )other != (unsigned long )((struct amdgpu_ring *)0)) && (int )other->fence_drv.initialized) && ring->fence_drv.sync_seq[j] != 0ULL) {
    seq_printf(m, "Last sync to ring %d 0x%016llx\n", j, ring->fence_drv.sync_seq[j]);
  } else {
  }
  j = j + 1;
  ldv_44789: ;
  if (j <= 15) {
    goto ldv_44788;
  } else {
  }
  ldv_44786:
  i = i + 1;
  ldv_44792: ;
  if (i <= 15) {
    goto ldv_44791;
  } else {
  }
  return (0);
}
}
static struct drm_info_list amdgpu_debugfs_fence_list[1U] = { {"amdgpu_fence_info", & amdgpu_debugfs_fence_info, 0U, (void *)0}};
int amdgpu_debugfs_fence_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = amdgpu_debugfs_add_files(adev, (struct drm_info_list *)(& amdgpu_debugfs_fence_list),
                                 1U);
  return (tmp);
}
}
static char const *amdgpu_fence_get_driver_name(struct fence *fence )
{
  {
  return ("amdgpu");
}
}
static char const *amdgpu_fence_get_timeline_name(struct fence *f )
{
  struct amdgpu_fence *fence ;
  struct amdgpu_fence *tmp ;
  {
  tmp = to_amdgpu_fence(f);
  fence = tmp;
  return ((char const *)(& (fence->ring)->name));
}
}
__inline static bool amdgpu_test_signaled(struct amdgpu_fence *fence )
{
  int tmp ;
  {
  tmp = constant_test_bit(0L, (unsigned long const volatile *)(& fence->base.flags));
  return (tmp != 0);
}
}
static void amdgpu_fence_wait_cb(struct fence *fence , struct fence_cb *cb )
{
  struct amdgpu_wait_cb *wait ;
  struct fence_cb const *__mptr ;
  {
  __mptr = (struct fence_cb const *)cb;
  wait = (struct amdgpu_wait_cb *)__mptr;
  wake_up_process(wait->task);
  return;
}
}
static long amdgpu_fence_default_wait(struct fence *f , bool intr , long t )
{
  struct amdgpu_fence *fence ;
  struct amdgpu_fence *tmp ;
  struct amdgpu_device *adev ;
  struct amdgpu_wait_cb cb ;
  int tmp___0 ;
  struct task_struct *tmp___1 ;
  long volatile __ret ;
  struct task_struct *tmp___2 ;
  struct task_struct *tmp___3 ;
  struct task_struct *tmp___4 ;
  struct task_struct *tmp___5 ;
  struct task_struct *tmp___6 ;
  long volatile __ret___0 ;
  struct task_struct *tmp___7 ;
  struct task_struct *tmp___8 ;
  struct task_struct *tmp___9 ;
  struct task_struct *tmp___10 ;
  bool tmp___11 ;
  struct task_struct *tmp___12 ;
  int tmp___13 ;
  struct task_struct *tmp___14 ;
  struct task_struct *tmp___15 ;
  {
  tmp = to_amdgpu_fence(f);
  fence = tmp;
  adev = (fence->ring)->adev;
  cb.task = get_current();
  tmp___0 = fence_add_callback(f, & cb.base, & amdgpu_fence_wait_cb);
  if (tmp___0 != 0) {
    return (t);
  } else {
  }
  goto ldv_44844;
  ldv_44843: ;
  if ((int )intr) {
    tmp___1 = get_current();
    tmp___1->task_state_change = 0UL;
    __ret = 1L;
    switch (8UL) {
    case 1UL:
    tmp___2 = get_current();
    __asm__ volatile ("xchgb %b0, %1\n": "+q" (__ret), "+m" (tmp___2->state): : "memory",
                         "cc");
    goto ldv_44828;
    case 2UL:
    tmp___3 = get_current();
    __asm__ volatile ("xchgw %w0, %1\n": "+r" (__ret), "+m" (tmp___3->state): : "memory",
                         "cc");
    goto ldv_44828;
    case 4UL:
    tmp___4 = get_current();
    __asm__ volatile ("xchgl %0, %1\n": "+r" (__ret), "+m" (tmp___4->state): : "memory",
                         "cc");
    goto ldv_44828;
    case 8UL:
    tmp___5 = get_current();
    __asm__ volatile ("xchgq %q0, %1\n": "+r" (__ret), "+m" (tmp___5->state): : "memory",
                         "cc");
    goto ldv_44828;
    default:
    __xchg_wrong_size();
    }
    ldv_44828: ;
  } else {
    tmp___6 = get_current();
    tmp___6->task_state_change = 0UL;
    __ret___0 = 2L;
    switch (8UL) {
    case 1UL:
    tmp___7 = get_current();
    __asm__ volatile ("xchgb %b0, %1\n": "+q" (__ret___0), "+m" (tmp___7->state): : "memory",
                         "cc");
    goto ldv_44836;
    case 2UL:
    tmp___8 = get_current();
    __asm__ volatile ("xchgw %w0, %1\n": "+r" (__ret___0), "+m" (tmp___8->state): : "memory",
                         "cc");
    goto ldv_44836;
    case 4UL:
    tmp___9 = get_current();
    __asm__ volatile ("xchgl %0, %1\n": "+r" (__ret___0), "+m" (tmp___9->state): : "memory",
                         "cc");
    goto ldv_44836;
    case 8UL:
    tmp___10 = get_current();
    __asm__ volatile ("xchgq %q0, %1\n": "+r" (__ret___0), "+m" (tmp___10->state): : "memory",
                         "cc");
    goto ldv_44836;
    default:
    __xchg_wrong_size();
    }
    ldv_44836: ;
  }
  tmp___11 = amdgpu_test_signaled(fence);
  if ((int )tmp___11) {
    goto ldv_44842;
  } else {
  }
  if ((int )adev->needs_reset) {
    t = -35L;
    goto ldv_44842;
  } else {
  }
  t = schedule_timeout(t);
  if (t > 0L && (int )intr) {
    tmp___12 = get_current();
    tmp___13 = signal_pending(tmp___12);
    if (tmp___13 != 0) {
      t = -512L;
    } else {
    }
  } else {
  }
  ldv_44844: ;
  if (t > 0L) {
    goto ldv_44843;
  } else {
  }
  ldv_44842:
  tmp___14 = get_current();
  tmp___14->task_state_change = 0UL;
  tmp___15 = get_current();
  tmp___15->state = 0L;
  fence_remove_callback(f, & cb.base);
  return (t);
}
}
struct fence_ops const amdgpu_fence_ops =
     {& amdgpu_fence_get_driver_name, & amdgpu_fence_get_timeline_name, & amdgpu_fence_enable_signaling,
    & amdgpu_fence_is_signaled, & amdgpu_fence_default_wait, (void (*)(struct fence * ))0,
    0, 0, 0};
void work_init_1(void)
{
  {
  ldv_work_1_0 = 0;
  ldv_work_1_1 = 0;
  ldv_work_1_2 = 0;
  ldv_work_1_3 = 0;
  return;
}
}
void invoke_work_1(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_1_0 == 2 || ldv_work_1_0 == 3) {
    ldv_work_1_0 = 4;
    amdgpu_fence_check_lockup(ldv_work_struct_1_0);
    ldv_work_1_0 = 1;
  } else {
  }
  goto ldv_44855;
  case 1: ;
  if (ldv_work_1_1 == 2 || ldv_work_1_1 == 3) {
    ldv_work_1_1 = 4;
    amdgpu_fence_check_lockup(ldv_work_struct_1_0);
    ldv_work_1_1 = 1;
  } else {
  }
  goto ldv_44855;
  case 2: ;
  if (ldv_work_1_2 == 2 || ldv_work_1_2 == 3) {
    ldv_work_1_2 = 4;
    amdgpu_fence_check_lockup(ldv_work_struct_1_0);
    ldv_work_1_2 = 1;
  } else {
  }
  goto ldv_44855;
  case 3: ;
  if (ldv_work_1_3 == 2 || ldv_work_1_3 == 3) {
    ldv_work_1_3 = 4;
    amdgpu_fence_check_lockup(ldv_work_struct_1_0);
    ldv_work_1_3 = 1;
  } else {
  }
  goto ldv_44855;
  default:
  ldv_stop();
  }
  ldv_44855: ;
  return;
}
}
void call_and_disable_work_1(struct work_struct *work )
{
  {
  if ((ldv_work_1_0 == 2 || ldv_work_1_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_1_0) {
    amdgpu_fence_check_lockup(work);
    ldv_work_1_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_1_1 == 2 || ldv_work_1_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_1_1) {
    amdgpu_fence_check_lockup(work);
    ldv_work_1_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_1_2 == 2 || ldv_work_1_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_1_2) {
    amdgpu_fence_check_lockup(work);
    ldv_work_1_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_1_3 == 2 || ldv_work_1_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_1_3) {
    amdgpu_fence_check_lockup(work);
    ldv_work_1_3 = 1;
    return;
  } else {
  }
  return;
}
}
void disable_work_1(struct work_struct *work )
{
  {
  if ((ldv_work_1_0 == 3 || ldv_work_1_0 == 2) && (unsigned long )ldv_work_struct_1_0 == (unsigned long )work) {
    ldv_work_1_0 = 1;
  } else {
  }
  if ((ldv_work_1_1 == 3 || ldv_work_1_1 == 2) && (unsigned long )ldv_work_struct_1_1 == (unsigned long )work) {
    ldv_work_1_1 = 1;
  } else {
  }
  if ((ldv_work_1_2 == 3 || ldv_work_1_2 == 2) && (unsigned long )ldv_work_struct_1_2 == (unsigned long )work) {
    ldv_work_1_2 = 1;
  } else {
  }
  if ((ldv_work_1_3 == 3 || ldv_work_1_3 == 2) && (unsigned long )ldv_work_struct_1_3 == (unsigned long )work) {
    ldv_work_1_3 = 1;
  } else {
  }
  return;
}
}
void ldv_initialize_fence_ops_164(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(88UL);
  amdgpu_fence_ops_group0 = (struct fence *)tmp;
  return;
}
}
void activate_work_1(struct work_struct *work , int state )
{
  {
  if (ldv_work_1_0 == 0) {
    ldv_work_struct_1_0 = work;
    ldv_work_1_0 = state;
    return;
  } else {
  }
  if (ldv_work_1_1 == 0) {
    ldv_work_struct_1_1 = work;
    ldv_work_1_1 = state;
    return;
  } else {
  }
  if (ldv_work_1_2 == 0) {
    ldv_work_struct_1_2 = work;
    ldv_work_1_2 = state;
    return;
  } else {
  }
  if (ldv_work_1_3 == 0) {
    ldv_work_struct_1_3 = work;
    ldv_work_1_3 = state;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_1(int state )
{
  {
  if (ldv_work_1_0 == state) {
    call_and_disable_work_1(ldv_work_struct_1_0);
  } else {
  }
  if (ldv_work_1_1 == state) {
    call_and_disable_work_1(ldv_work_struct_1_1);
  } else {
  }
  if (ldv_work_1_2 == state) {
    call_and_disable_work_1(ldv_work_struct_1_2);
  } else {
  }
  if (ldv_work_1_3 == state) {
    call_and_disable_work_1(ldv_work_struct_1_3);
  } else {
  }
  return;
}
}
void ldv_main_exported_164(void)
{
  bool ldvarg1105 ;
  long ldvarg1104 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg1105), 0, 1UL);
  ldv_memset((void *)(& ldvarg1104), 0, 8UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_164 == 1) {
    amdgpu_fence_default_wait(amdgpu_fence_ops_group0, (int )ldvarg1105, ldvarg1104);
    ldv_state_variable_164 = 1;
  } else {
  }
  goto ldv_44884;
  case 1: ;
  if (ldv_state_variable_164 == 1) {
    amdgpu_fence_is_signaled(amdgpu_fence_ops_group0);
    ldv_state_variable_164 = 1;
  } else {
  }
  goto ldv_44884;
  case 2: ;
  if (ldv_state_variable_164 == 1) {
    amdgpu_fence_get_timeline_name(amdgpu_fence_ops_group0);
    ldv_state_variable_164 = 1;
  } else {
  }
  goto ldv_44884;
  case 3: ;
  if (ldv_state_variable_164 == 1) {
    amdgpu_fence_get_driver_name(amdgpu_fence_ops_group0);
    ldv_state_variable_164 = 1;
  } else {
  }
  goto ldv_44884;
  case 4: ;
  if (ldv_state_variable_164 == 1) {
    amdgpu_fence_enable_signaling(amdgpu_fence_ops_group0);
    ldv_state_variable_164 = 1;
  } else {
  }
  goto ldv_44884;
  default:
  ldv_stop();
  }
  ldv_44884: ;
  return;
}
}
bool ldv_queue_work_on_103(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_104(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_105(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_106(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_107(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern void warn_slowpath_fmt(char const * , int const , char const * , ...) ;
__inline static long PTR_ERR(void const *ptr ) ;
__inline static bool IS_ERR(void const *ptr ) ;
__inline static void atomic64_add(long i , atomic64_t *v )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; addq %1,%0": "=m" (v->counter): "er" (i),
                       "m" (v->counter));
  return;
}
}
bool ldv_queue_work_on_117(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_119(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_118(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_121(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_120(struct workqueue_struct *ldv_func_arg1 ) ;
extern int drm_mm_dump_table(struct seq_file * , struct drm_mm * ) ;
extern void put_page(struct page * ) ;
extern long get_user_pages(struct task_struct * , struct mm_struct * , unsigned long ,
                           unsigned long , int , int , struct page ** , struct vm_area_struct ** ) ;
extern int set_page_dirty(struct page * ) ;
extern struct vm_area_struct *find_vma(struct mm_struct * , unsigned long ) ;
extern bool drm_vma_node_is_allowed(struct drm_vma_offset_node * , struct file * ) ;
__inline static int drm_vma_node_verify_access(struct drm_vma_offset_node *node ,
                                               struct file *filp )
{
  bool tmp ;
  {
  tmp = drm_vma_node_is_allowed(node, filp);
  return ((int )tmp ? 0 : -13);
}
}
extern int ttm_bo_init_mm(struct ttm_bo_device * , unsigned int , unsigned long ) ;
extern int ttm_bo_clean_mm(struct ttm_bo_device * , unsigned int ) ;
extern int ttm_bo_mmap(struct file * , struct vm_area_struct * , struct ttm_bo_device * ) ;
extern int ttm_mem_global_init(struct ttm_mem_global * ) ;
extern void ttm_mem_global_release(struct ttm_mem_global * ) ;
extern int drm_global_item_ref(struct drm_global_reference * ) ;
extern void drm_global_item_unref(struct drm_global_reference * ) ;
extern int ttm_dma_tt_init(struct ttm_dma_tt * , struct ttm_bo_device * , unsigned long ,
                           u32 , struct page * ) ;
extern void ttm_dma_tt_fini(struct ttm_dma_tt * ) ;
extern int ttm_tt_bind(struct ttm_tt * , struct ttm_mem_reg * ) ;
extern int ttm_tt_set_placement_caching(struct ttm_tt * , u32 ) ;
extern int ttm_bo_mem_space(struct ttm_buffer_object * , struct ttm_placement * ,
                            struct ttm_mem_reg * , bool , bool ) ;
extern void ttm_bo_mem_put(struct ttm_buffer_object * , struct ttm_mem_reg * ) ;
extern void ttm_bo_global_release(struct drm_global_reference * ) ;
extern int ttm_bo_global_init(struct drm_global_reference * ) ;
extern int ttm_bo_device_release(struct ttm_bo_device * ) ;
extern int ttm_bo_device_init(struct ttm_bo_device * , struct ttm_bo_global * , struct ttm_bo_driver * ,
                              struct address_space * , uint64_t , bool ) ;
extern int ttm_bo_move_ttm(struct ttm_buffer_object * , bool , bool , struct ttm_mem_reg * ) ;
extern int ttm_bo_move_memcpy(struct ttm_buffer_object * , bool , bool , struct ttm_mem_reg * ) ;
extern int ttm_bo_move_accel_cleanup(struct ttm_buffer_object * , struct fence * ,
                                     bool , bool , struct ttm_mem_reg * ) ;
extern struct ttm_mem_type_manager_func const ttm_bo_manager_func ;
extern int ttm_pool_populate(struct ttm_tt * ) ;
extern void ttm_pool_unpopulate(struct ttm_tt * ) ;
extern int ttm_page_alloc_debugfs(struct seq_file * , void * ) ;
extern int ttm_dma_page_alloc_debugfs(struct seq_file * , void * ) ;
extern int ttm_dma_populate(struct ttm_dma_tt * , struct device * ) ;
extern void ttm_dma_unpopulate(struct ttm_dma_tt * , struct device * ) ;
__inline static struct page *sg_page(struct scatterlist *sg )
{
  long tmp ;
  long tmp___0 ;
  {
  tmp = ldv__builtin_expect(sg->sg_magic != 2271560481UL, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/scatterlist.h"),
                         "i" (123), "i" (12UL));
    ldv_29852: ;
    goto ldv_29852;
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )((int )sg->page_link) & 1L, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/scatterlist.h"),
                         "i" (124), "i" (12UL));
    ldv_29853: ;
    goto ldv_29853;
  } else {
  }
  return ((struct page *)(sg->page_link & 0xfffffffffffffffcUL));
}
}
__inline static void *sg_virt(struct scatterlist *sg )
{
  struct page *tmp ;
  void *tmp___0 ;
  {
  tmp = sg_page(sg);
  tmp___0 = lowmem_page_address((struct page const *)tmp);
  return (tmp___0 + (unsigned long )sg->offset);
}
}
extern struct scatterlist *sg_next(struct scatterlist * ) ;
extern void sg_free_table(struct sg_table * ) ;
extern int sg_alloc_table_from_pages(struct sg_table * , struct page ** , unsigned int ,
                                     unsigned long , unsigned long , gfp_t ) ;
extern bool __sg_page_iter_next(struct sg_page_iter * ) ;
extern void __sg_page_iter_start(struct sg_page_iter * , struct scatterlist * , unsigned int ,
                                 unsigned long ) ;
__inline static struct page *sg_page_iter_page(struct sg_page_iter *piter )
{
  struct page *tmp ;
  {
  tmp = sg_page(piter->sg);
  return ((struct page *)-24189255811072L + ((unsigned long )(((long )tmp + 24189255811072L) / 64L) + (unsigned long )piter->sg_pgoffset));
}
}
extern void debug_dma_map_sg(struct device * , struct scatterlist * , int , int ,
                             int ) ;
extern void debug_dma_unmap_sg(struct device * , struct scatterlist * , int , int ) ;
extern unsigned long swiotlb_nr_tbl(void) ;
__inline static int dma_map_sg_attrs(struct device *dev , struct scatterlist *sg ,
                                     int nents , enum dma_data_direction dir , struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int i ;
  int ents ;
  struct scatterlist *s ;
  void *tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  i = 0;
  s = sg;
  goto ldv_30432;
  ldv_30431:
  tmp___0 = sg_virt(s);
  kmemcheck_mark_initialized(tmp___0, s->length);
  i = i + 1;
  s = sg_next(s);
  ldv_30432: ;
  if (i < nents) {
    goto ldv_30431;
  } else {
  }
  tmp___1 = valid_dma_direction((int )dir);
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (56), "i" (12UL));
    ldv_30434: ;
    goto ldv_30434;
  } else {
  }
  ents = (*(ops->map_sg))(dev, sg, nents, dir, attrs);
  tmp___3 = ldv__builtin_expect(ents < 0, 0L);
  if (tmp___3 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (58), "i" (12UL));
    ldv_30435: ;
    goto ldv_30435;
  } else {
  }
  debug_dma_map_sg(dev, sg, nents, ents, (int )dir);
  return (ents);
}
}
__inline static void dma_unmap_sg_attrs(struct device *dev , struct scatterlist *sg ,
                                        int nents , enum dma_data_direction dir ,
                                        struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (70), "i" (12UL));
    ldv_30444: ;
    goto ldv_30444;
  } else {
  }
  debug_dma_unmap_sg(dev, sg, nents, (int )dir);
  if ((unsigned long )ops->unmap_sg != (unsigned long )((void (*)(struct device * ,
                                                                  struct scatterlist * ,
                                                                  int , enum dma_data_direction ,
                                                                  struct dma_attrs * ))0)) {
    (*(ops->unmap_sg))(dev, sg, nents, dir, attrs);
  } else {
  }
  return;
}
}
__inline static dma_addr_t dma_map_page___0(struct device *dev , struct page *page ,
                                            size_t offset , size_t size , enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  void *tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = lowmem_page_address((struct page const *)page);
  kmemcheck_mark_initialized(tmp___0 + offset, (unsigned int )size);
  tmp___1 = valid_dma_direction((int )dir);
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (84), "i" (12UL));
    ldv_30454: ;
    goto ldv_30454;
  } else {
  }
  addr = (*(ops->map_page))(dev, page, offset, size, dir, (struct dma_attrs *)0);
  debug_dma_map_page(dev, page, offset, size, (int )dir, addr, 0);
  return (addr);
}
}
__inline static void dma_unmap_page___0(struct device *dev , dma_addr_t addr , size_t size ,
                                        enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (96), "i" (12UL));
    ldv_30462: ;
    goto ldv_30462;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, (struct dma_attrs *)0);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 0);
  return;
}
}
extern unsigned long clear_user(void * , unsigned long ) ;
__inline static void *kmap(struct page *page )
{
  void *tmp ;
  {
  __might_sleep("include/linux/highmem.h", 58, 0);
  tmp = lowmem_page_address((struct page const *)page);
  return (tmp);
}
}
__inline static void kunmap(struct page *page )
{
  {
  return;
}
}
__inline static dma_addr_t pci_map_page___0(struct pci_dev *hwdev , struct page *page ,
                                            unsigned long offset , size_t size , int direction )
{
  dma_addr_t tmp ;
  {
  tmp = dma_map_page___0((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                         page, offset, size, (enum dma_data_direction )direction);
  return (tmp);
}
}
__inline static void pci_unmap_page___0(struct pci_dev *hwdev , dma_addr_t dma_address ,
                                        size_t size , int direction )
{
  {
  dma_unmap_page___0((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                     dma_address, size, (enum dma_data_direction )direction);
  return;
}
}
extern void release_pages(struct page ** , int , bool ) ;
extern int drm_prime_sg_to_page_addr_arrays(struct sg_table * , struct page ** , dma_addr_t * ,
                                            int ) ;
extern void mark_page_accessed(struct page * ) ;
int amdgpu_copy_buffer(struct amdgpu_ring *ring , uint64_t src_offset , uint64_t dst_offset ,
                       u32 byte_count , struct reservation_object *resv , struct amdgpu_fence **fence ) ;
void amdgpu_sync_create(struct amdgpu_sync *sync ) ;
int amdgpu_sync_resv(struct amdgpu_device *adev , struct amdgpu_sync *sync , struct reservation_object *resv ,
                     void *owner ) ;
int amdgpu_sync_rings(struct amdgpu_sync *sync , struct amdgpu_ring *ring ) ;
void amdgpu_sync_free(struct amdgpu_device *adev , struct amdgpu_sync *sync , struct amdgpu_fence *fence ) ;
void amdgpu_gart_fini(struct amdgpu_device *adev ) ;
void amdgpu_gart_unbind(struct amdgpu_device *adev , unsigned int offset , int pages ) ;
int amdgpu_gart_bind(struct amdgpu_device *adev , unsigned int offset , int pages ,
                     struct page **pagelist , dma_addr_t *dma_addr , u32 flags ) ;
int amdgpu_ring_lock(struct amdgpu_ring *ring , unsigned int ndw ) ;
void amdgpu_ring_unlock_commit(struct amdgpu_ring *ring ) ;
void amdgpu_ring_unlock_undo(struct amdgpu_ring *ring ) ;
void amdgpu_ttm_placement_from_domain(struct amdgpu_bo *rbo , u32 domain ) ;
bool amdgpu_ttm_bo_is_amdgpu_bo(struct ttm_buffer_object *bo ) ;
int amdgpu_ttm_tt_set_userptr(struct ttm_tt *ttm , uint64_t addr , u32 flags ) ;
bool amdgpu_ttm_tt_has_userptr(struct ttm_tt *ttm ) ;
bool amdgpu_ttm_tt_is_readonly(struct ttm_tt *ttm ) ;
u32 amdgpu_ttm_tt_pte_flags(struct amdgpu_device *adev , struct ttm_tt *ttm , struct ttm_mem_reg *mem ) ;
void amdgpu_ttm_set_active_vram_size(struct amdgpu_device *adev , u64 size ) ;
void amdgpu_bo_move_notify(struct ttm_buffer_object *bo , struct ttm_mem_reg *new_mem ) ;
int amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo ) ;
static int amdgpu_ttm_debugfs_init(struct amdgpu_device *adev ) ;
static void amdgpu_ttm_debugfs_fini(struct amdgpu_device *adev ) ;
static struct amdgpu_device *amdgpu_get_adev(struct ttm_bo_device *bdev )
{
  struct amdgpu_mman *mman ;
  struct amdgpu_device *adev ;
  struct ttm_bo_device const *__mptr ;
  struct amdgpu_mman const *__mptr___0 ;
  {
  __mptr = (struct ttm_bo_device const *)bdev;
  mman = (struct amdgpu_mman *)__mptr + 0xffffffffffffffa8UL;
  __mptr___0 = (struct amdgpu_mman const *)mman;
  adev = (struct amdgpu_device *)__mptr___0 + 0xfffffffffffff3c0UL;
  return (adev);
}
}
static int amdgpu_ttm_mem_global_init(struct drm_global_reference *ref )
{
  int tmp ;
  {
  tmp = ttm_mem_global_init((struct ttm_mem_global *)ref->object);
  return (tmp);
}
}
static void amdgpu_ttm_mem_global_release(struct drm_global_reference *ref )
{
  {
  ttm_mem_global_release((struct ttm_mem_global *)ref->object);
  return;
}
}
static int amdgpu_ttm_global_init(struct amdgpu_device *adev )
{
  struct drm_global_reference *global_ref ;
  int r ;
  {
  adev->mman.mem_global_referenced = 0;
  global_ref = & adev->mman.mem_global_ref;
  global_ref->global_type = 0;
  global_ref->size = 504UL;
  global_ref->init = & amdgpu_ttm_mem_global_init;
  global_ref->release = & amdgpu_ttm_mem_global_release;
  r = drm_global_item_ref(global_ref);
  if (r != 0) {
    drm_err("Failed setting up TTM memory accounting subsystem.\n");
    return (r);
  } else {
  }
  adev->mman.bo_global_ref.mem_glob = (struct ttm_mem_global *)adev->mman.mem_global_ref.object;
  global_ref = & adev->mman.bo_global_ref.ref;
  global_ref->global_type = 1;
  global_ref->size = 592UL;
  global_ref->init = & ttm_bo_global_init;
  global_ref->release = & ttm_bo_global_release;
  r = drm_global_item_ref(global_ref);
  if (r != 0) {
    drm_err("Failed setting up TTM BO subsystem.\n");
    drm_global_item_unref(& adev->mman.mem_global_ref);
    return (r);
  } else {
  }
  adev->mman.mem_global_referenced = 1;
  return (0);
}
}
static void amdgpu_ttm_global_fini(struct amdgpu_device *adev )
{
  {
  if ((int )adev->mman.mem_global_referenced) {
    drm_global_item_unref(& adev->mman.bo_global_ref.ref);
    drm_global_item_unref(& adev->mman.mem_global_ref);
    adev->mman.mem_global_referenced = 0;
  } else {
  }
  return;
}
}
static int amdgpu_invalidate_caches(struct ttm_bo_device *bdev , u32 flags )
{
  {
  return (0);
}
}
static int amdgpu_init_mem_type(struct ttm_bo_device *bdev , u32 type , struct ttm_mem_type_manager *man )
{
  struct amdgpu_device *adev ;
  {
  adev = amdgpu_get_adev(bdev);
  switch (type) {
  case 0U:
  man->flags = 2U;
  man->available_caching = 458752U;
  man->default_caching = 65536U;
  goto ldv_44688;
  case 1U:
  man->func = & ttm_bo_manager_func;
  man->gpu_offset = adev->mc.gtt_start;
  man->available_caching = 458752U;
  man->default_caching = 65536U;
  man->flags = 10U;
  goto ldv_44688;
  case 2U:
  man->func = & ttm_bo_manager_func;
  man->gpu_offset = adev->mc.vram_start;
  man->flags = 3U;
  man->available_caching = 393216U;
  man->default_caching = 262144U;
  goto ldv_44688;
  case 3U: ;
  case 4U: ;
  case 5U:
  man->func = & ttm_bo_manager_func;
  man->gpu_offset = 0ULL;
  man->flags = 9U;
  man->available_caching = 131072U;
  man->default_caching = 131072U;
  goto ldv_44688;
  default:
  drm_err("Unsupported memory type %u\n", type);
  return (-22);
  }
  ldv_44688: ;
  return (0);
}
}
static void amdgpu_evict_flags(struct ttm_buffer_object *bo , struct ttm_placement *placement )
{
  struct amdgpu_bo *rbo ;
  struct ttm_place placements ;
  bool tmp ;
  int tmp___0 ;
  struct ttm_buffer_object const *__mptr ;
  {
  placements.fpfn = 0U;
  placements.lpfn = 0U;
  placements.flags = 458753U;
  tmp = amdgpu_ttm_bo_is_amdgpu_bo(bo);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    placement->placement = (struct ttm_place const *)(& placements);
    placement->busy_placement = (struct ttm_place const *)(& placements);
    placement->num_placement = 1U;
    placement->num_busy_placement = 1U;
    return;
  } else {
  }
  __mptr = (struct ttm_buffer_object const *)bo;
  rbo = (struct amdgpu_bo *)__mptr + 0xffffffffffffff98UL;
  switch (bo->mem.mem_type) {
  case 2U: ;
  if (! ((rbo->adev)->mman.buffer_funcs_ring)->ready) {
    amdgpu_ttm_placement_from_domain(rbo, 1U);
  } else {
    amdgpu_ttm_placement_from_domain(rbo, 2U);
  }
  goto ldv_44704;
  case 1U: ;
  default:
  amdgpu_ttm_placement_from_domain(rbo, 1U);
  }
  ldv_44704:
  *placement = rbo->placement;
  return;
}
}
static int amdgpu_verify_access(struct ttm_buffer_object *bo , struct file *filp )
{
  struct amdgpu_bo *rbo ;
  struct ttm_buffer_object const *__mptr ;
  int tmp ;
  {
  __mptr = (struct ttm_buffer_object const *)bo;
  rbo = (struct amdgpu_bo *)__mptr + 0xffffffffffffff98UL;
  tmp = drm_vma_node_verify_access(& rbo->gem_base.vma_node, filp);
  return (tmp);
}
}
static void amdgpu_move_null(struct ttm_buffer_object *bo , struct ttm_mem_reg *new_mem )
{
  struct ttm_mem_reg *old_mem ;
  long tmp ;
  {
  old_mem = & bo->mem;
  tmp = ldv__builtin_expect((unsigned long )old_mem->mm_node != (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c"),
                         "i" (218), "i" (12UL));
    ldv_44719: ;
    goto ldv_44719;
  } else {
  }
  *old_mem = *new_mem;
  new_mem->mm_node = (void *)0;
  return;
}
}
static int amdgpu_move_blit(struct ttm_buffer_object *bo , bool evict , bool no_wait_gpu ,
                            struct ttm_mem_reg *new_mem , struct ttm_mem_reg *old_mem )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  uint64_t old_start ;
  uint64_t new_start ;
  struct amdgpu_fence *fence ;
  int r ;
  {
  adev = amdgpu_get_adev(bo->bdev);
  ring = adev->mman.buffer_funcs_ring;
  old_start = (uint64_t )(old_mem->start << 12);
  new_start = (uint64_t )(new_mem->start << 12);
  switch (old_mem->mem_type) {
  case 2U:
  old_start = adev->mc.vram_start + old_start;
  goto ldv_44734;
  case 1U:
  old_start = adev->mc.gtt_start + old_start;
  goto ldv_44734;
  default:
  drm_err("Unknown placement %d\n", old_mem->mem_type);
  return (-22);
  }
  ldv_44734: ;
  switch (new_mem->mem_type) {
  case 2U:
  new_start = adev->mc.vram_start + new_start;
  goto ldv_44738;
  case 1U:
  new_start = adev->mc.gtt_start + new_start;
  goto ldv_44738;
  default:
  drm_err("Unknown placement %d\n", old_mem->mem_type);
  return (-22);
  }
  ldv_44738: ;
  if (! ring->ready) {
    drm_err("Trying to move memory with ring turned off.\n");
    return (-22);
  } else {
  }
  r = amdgpu_copy_buffer(ring, old_start, new_start, (u32 )new_mem->num_pages * 4096U,
                         bo->resv, & fence);
  r = ttm_bo_move_accel_cleanup(bo, & fence->base, (int )evict, (int )no_wait_gpu,
                                new_mem);
  amdgpu_fence_unref(& fence);
  return (r);
}
}
static int amdgpu_move_vram_ram(struct ttm_buffer_object *bo , bool evict , bool interruptible ,
                                bool no_wait_gpu , struct ttm_mem_reg *new_mem )
{
  struct amdgpu_device *adev ;
  struct ttm_mem_reg *old_mem ;
  struct ttm_mem_reg tmp_mem ;
  struct ttm_place placements ;
  struct ttm_placement placement ;
  int r ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  old_mem = & bo->mem;
  adev = amdgpu_get_adev(bo->bdev);
  tmp_mem = *new_mem;
  tmp_mem.mm_node = (void *)0;
  placement.num_placement = 1U;
  placement.placement = (struct ttm_place const *)(& placements);
  placement.num_busy_placement = 1U;
  placement.busy_placement = (struct ttm_place const *)(& placements);
  placements.fpfn = 0U;
  placements.lpfn = 0U;
  placements.flags = 458754U;
  r = ttm_bo_mem_space(bo, & placement, & tmp_mem, (int )interruptible, (int )no_wait_gpu);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    return (r);
  } else {
  }
  r = ttm_tt_set_placement_caching(bo->ttm, tmp_mem.placement);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    goto out_cleanup;
  } else {
  }
  r = ttm_tt_bind(bo->ttm, & tmp_mem);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_move_blit(bo, 1, (int )no_wait_gpu, & tmp_mem, old_mem);
  tmp___2 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___2 != 0L) {
    goto out_cleanup;
  } else {
  }
  r = ttm_bo_move_ttm(bo, 1, (int )no_wait_gpu, new_mem);
  out_cleanup:
  ttm_bo_mem_put(bo, & tmp_mem);
  return (r);
}
}
static int amdgpu_move_ram_vram(struct ttm_buffer_object *bo , bool evict , bool interruptible ,
                                bool no_wait_gpu , struct ttm_mem_reg *new_mem )
{
  struct amdgpu_device *adev ;
  struct ttm_mem_reg *old_mem ;
  struct ttm_mem_reg tmp_mem ;
  struct ttm_placement placement ;
  struct ttm_place placements ;
  int r ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  old_mem = & bo->mem;
  adev = amdgpu_get_adev(bo->bdev);
  tmp_mem = *new_mem;
  tmp_mem.mm_node = (void *)0;
  placement.num_placement = 1U;
  placement.placement = (struct ttm_place const *)(& placements);
  placement.num_busy_placement = 1U;
  placement.busy_placement = (struct ttm_place const *)(& placements);
  placements.fpfn = 0U;
  placements.lpfn = 0U;
  placements.flags = 458754U;
  r = ttm_bo_mem_space(bo, & placement, & tmp_mem, (int )interruptible, (int )no_wait_gpu);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    return (r);
  } else {
  }
  r = ttm_bo_move_ttm(bo, 1, (int )no_wait_gpu, & tmp_mem);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_move_blit(bo, 1, (int )no_wait_gpu, new_mem, old_mem);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
  } else {
  }
  out_cleanup:
  ttm_bo_mem_put(bo, & tmp_mem);
  return (r);
}
}
static int amdgpu_bo_move(struct ttm_buffer_object *bo , bool evict , bool interruptible ,
                          bool no_wait_gpu , struct ttm_mem_reg *new_mem )
{
  struct amdgpu_device *adev ;
  struct ttm_mem_reg *old_mem ;
  int r ;
  {
  old_mem = & bo->mem;
  adev = amdgpu_get_adev(bo->bdev);
  if (old_mem->mem_type == 0U && (unsigned long )bo->ttm == (unsigned long )((struct ttm_tt *)0)) {
    amdgpu_move_null(bo, new_mem);
    return (0);
  } else {
  }
  if ((old_mem->mem_type == 1U && new_mem->mem_type == 0U) || (old_mem->mem_type == 0U && new_mem->mem_type == 1U)) {
    amdgpu_move_null(bo, new_mem);
    return (0);
  } else {
  }
  if (((unsigned long )adev->mman.buffer_funcs == (unsigned long )((struct amdgpu_buffer_funcs const *)0) || (unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )((struct amdgpu_ring *)0)) || ! (adev->mman.buffer_funcs_ring)->ready) {
    goto memcpy;
  } else {
  }
  if (old_mem->mem_type == 2U && new_mem->mem_type == 0U) {
    r = amdgpu_move_vram_ram(bo, (int )evict, (int )interruptible, (int )no_wait_gpu,
                             new_mem);
  } else
  if (old_mem->mem_type == 0U && new_mem->mem_type == 2U) {
    r = amdgpu_move_ram_vram(bo, (int )evict, (int )interruptible, (int )no_wait_gpu,
                             new_mem);
  } else {
    r = amdgpu_move_blit(bo, (int )evict, (int )no_wait_gpu, new_mem, old_mem);
  }
  if (r != 0) {
    memcpy:
    r = ttm_bo_move_memcpy(bo, (int )evict, (int )no_wait_gpu, new_mem);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  atomic64_add((long )((unsigned long long )bo->num_pages << 12), & adev->num_bytes_moved);
  return (0);
}
}
static int amdgpu_ttm_io_mem_reserve(struct ttm_bo_device *bdev , struct ttm_mem_reg *mem )
{
  struct ttm_mem_type_manager *man ;
  struct amdgpu_device *adev ;
  struct amdgpu_device *tmp ;
  {
  man = (struct ttm_mem_type_manager *)(& bdev->man) + (unsigned long )mem->mem_type;
  tmp = amdgpu_get_adev(bdev);
  adev = tmp;
  mem->bus.addr = (void *)0;
  mem->bus.offset = 0UL;
  mem->bus.size = mem->num_pages << 12;
  mem->bus.base = 0UL;
  mem->bus.is_iomem = 0;
  if ((man->flags & 2U) == 0U) {
    return (-22);
  } else {
  }
  switch (mem->mem_type) {
  case 0U: ;
  return (0);
  case 1U: ;
  goto ldv_44788;
  case 2U:
  mem->bus.offset = mem->start << 12;
  if ((unsigned long long )(mem->bus.offset + mem->bus.size) > adev->mc.visible_vram_size) {
    return (-22);
  } else {
  }
  mem->bus.base = (unsigned long )adev->mc.aper_base;
  mem->bus.is_iomem = 1;
  goto ldv_44788;
  default: ;
  return (-22);
  }
  ldv_44788: ;
  return (0);
}
}
static void amdgpu_ttm_io_mem_free(struct ttm_bo_device *bdev , struct ttm_mem_reg *mem )
{
  {
  return;
}
}
static int amdgpu_ttm_tt_pin_userptr(struct ttm_tt *ttm )
{
  struct amdgpu_device *adev ;
  struct amdgpu_device *tmp ;
  struct amdgpu_ttm_tt *gtt ;
  unsigned int pinned ;
  unsigned int nents ;
  int r ;
  int write ;
  enum dma_data_direction direction ;
  struct task_struct *tmp___0 ;
  unsigned long end ;
  struct vm_area_struct *vma ;
  unsigned int num_pages ;
  uint64_t userptr ;
  struct page **pages ;
  struct task_struct *tmp___1 ;
  struct task_struct *tmp___2 ;
  long tmp___3 ;
  int tmp___4 ;
  {
  tmp = amdgpu_get_adev(ttm->bdev);
  adev = tmp;
  gtt = (struct amdgpu_ttm_tt *)ttm;
  pinned = 0U;
  write = (gtt->userflags & 1U) == 0U;
  direction = write == 0;
  tmp___0 = get_current();
  if ((unsigned long )tmp___0->mm != (unsigned long )gtt->usermm) {
    return (-1);
  } else {
  }
  if ((gtt->userflags & 2U) != 0U) {
    end = (unsigned long )(gtt->userptr + (unsigned long long )(ttm->num_pages * 4096UL));
    vma = find_vma(gtt->usermm, (unsigned long )gtt->userptr);
    if (((unsigned long )vma == (unsigned long )((struct vm_area_struct *)0) || (unsigned long )vma->vm_file != (unsigned long )((struct file *)0)) || vma->vm_end < end) {
      return (-1);
    } else {
    }
  } else {
  }
  ldv_44818:
  num_pages = (unsigned int )ttm->num_pages - pinned;
  userptr = gtt->userptr + (unsigned long long )((unsigned long )pinned * 4096UL);
  pages = ttm->pages + (unsigned long )pinned;
  tmp___1 = get_current();
  tmp___2 = get_current();
  tmp___3 = get_user_pages(tmp___2, tmp___1->mm, (unsigned long )userptr, (unsigned long )num_pages,
                           write, 0, pages, (struct vm_area_struct **)0);
  r = (int )tmp___3;
  if (r < 0) {
    goto release_pages;
  } else {
  }
  pinned = pinned + (unsigned int )r;
  if ((unsigned long )pinned < ttm->num_pages) {
    goto ldv_44818;
  } else {
  }
  r = sg_alloc_table_from_pages(ttm->sg, ttm->pages, (unsigned int )ttm->num_pages,
                                0UL, ttm->num_pages << 12, 208U);
  if (r != 0) {
    goto release_sg;
  } else {
  }
  r = -12;
  tmp___4 = dma_map_sg_attrs(adev->dev, (ttm->sg)->sgl, (int )(ttm->sg)->nents, direction,
                             (struct dma_attrs *)0);
  nents = (unsigned int )tmp___4;
  if ((ttm->sg)->nents != nents) {
    goto release_sg;
  } else {
  }
  drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages, gtt->ttm.dma_address, (int )ttm->num_pages);
  return (0);
  release_sg:
  kfree((void const *)ttm->sg);
  release_pages:
  release_pages(ttm->pages, (int )pinned, 0);
  return (r);
}
}
static void amdgpu_ttm_tt_unpin_userptr(struct ttm_tt *ttm )
{
  struct amdgpu_device *adev ;
  struct amdgpu_device *tmp ;
  struct amdgpu_ttm_tt *gtt ;
  struct sg_page_iter sg_iter ;
  int write ;
  enum dma_data_direction direction ;
  struct page *page ;
  struct page *tmp___0 ;
  bool tmp___1 ;
  {
  tmp = amdgpu_get_adev(ttm->bdev);
  adev = tmp;
  gtt = (struct amdgpu_ttm_tt *)ttm;
  write = (gtt->userflags & 1U) == 0U;
  direction = write == 0;
  if ((unsigned long )(ttm->sg)->sgl == (unsigned long )((struct scatterlist *)0)) {
    return;
  } else {
  }
  dma_unmap_sg_attrs(adev->dev, (ttm->sg)->sgl, (int )(ttm->sg)->nents, direction,
                     (struct dma_attrs *)0);
  __sg_page_iter_start(& sg_iter, (ttm->sg)->sgl, (ttm->sg)->nents, 0UL);
  goto ldv_44831;
  ldv_44830:
  tmp___0 = sg_page_iter_page(& sg_iter);
  page = tmp___0;
  if ((gtt->userflags & 1U) == 0U) {
    set_page_dirty(page);
  } else {
  }
  mark_page_accessed(page);
  put_page(page);
  ldv_44831:
  tmp___1 = __sg_page_iter_next(& sg_iter);
  if ((int )tmp___1) {
    goto ldv_44830;
  } else {
  }
  sg_free_table(ttm->sg);
  return;
}
}
static int amdgpu_ttm_backend_bind(struct ttm_tt *ttm , struct ttm_mem_reg *bo_mem )
{
  struct amdgpu_ttm_tt *gtt ;
  u32 flags ;
  u32 tmp ;
  int r ;
  int __ret_warn_on ;
  long tmp___0 ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  tmp = amdgpu_ttm_tt_pte_flags(gtt->adev, ttm, bo_mem);
  flags = tmp;
  if (gtt->userptr != 0ULL) {
    amdgpu_ttm_tt_pin_userptr(ttm);
  } else {
  }
  gtt->offset = (u64 )(bo_mem->start << 12);
  if (ttm->num_pages == 0UL) {
    __ret_warn_on = 1;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c",
                        596, "nothing to bind %lu pages for mreg %p back %p!\n", ttm->num_pages,
                        bo_mem, ttm);
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
  } else {
  }
  if ((bo_mem->mem_type == 3U || bo_mem->mem_type == 4U) || bo_mem->mem_type == 5U) {
    return (-22);
  } else {
  }
  r = amdgpu_gart_bind(gtt->adev, (unsigned int )gtt->offset, (int )ttm->num_pages,
                       ttm->pages, gtt->ttm.dma_address, flags);
  if (r != 0) {
    drm_err("failed to bind %lu pages at 0x%08X\n", ttm->num_pages, (unsigned int )gtt->offset);
    return (r);
  } else {
  }
  return (0);
}
}
static int amdgpu_ttm_backend_unbind(struct ttm_tt *ttm )
{
  struct amdgpu_ttm_tt *gtt ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  if ((int )(gtt->adev)->gart.ready) {
    amdgpu_gart_unbind(gtt->adev, (unsigned int )gtt->offset, (int )ttm->num_pages);
  } else {
  }
  if (gtt->userptr != 0ULL) {
    amdgpu_ttm_tt_unpin_userptr(ttm);
  } else {
  }
  return (0);
}
}
static void amdgpu_ttm_backend_destroy(struct ttm_tt *ttm )
{
  struct amdgpu_ttm_tt *gtt ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  ttm_dma_tt_fini(& gtt->ttm);
  kfree((void const *)gtt);
  return;
}
}
static struct ttm_backend_func amdgpu_backend_func = {& amdgpu_ttm_backend_bind, & amdgpu_ttm_backend_unbind, & amdgpu_ttm_backend_destroy};
static struct ttm_tt *amdgpu_ttm_tt_create(struct ttm_bo_device *bdev , unsigned long size ,
                                           u32 page_flags , struct page *dummy_read_page )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ttm_tt *gtt ;
  void *tmp ;
  int tmp___0 ;
  {
  adev = amdgpu_get_adev(bdev);
  tmp = kzalloc(152UL, 208U);
  gtt = (struct amdgpu_ttm_tt *)tmp;
  if ((unsigned long )gtt == (unsigned long )((struct amdgpu_ttm_tt *)0)) {
    return ((struct ttm_tt *)0);
  } else {
  }
  gtt->ttm.ttm.func = & amdgpu_backend_func;
  gtt->adev = adev;
  tmp___0 = ttm_dma_tt_init(& gtt->ttm, bdev, size, page_flags, dummy_read_page);
  if (tmp___0 != 0) {
    kfree((void const *)gtt);
    return ((struct ttm_tt *)0);
  } else {
  }
  return (& gtt->ttm.ttm);
}
}
static int amdgpu_ttm_tt_populate(struct ttm_tt *ttm )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ttm_tt *gtt ;
  unsigned int i ;
  int r ;
  bool slave ;
  void *tmp ;
  int tmp___0 ;
  unsigned long tmp___1 ;
  int tmp___2 ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  slave = (ttm->page_flags & 256U) != 0U;
  if ((unsigned int )ttm->state != 2U) {
    return (0);
  } else {
  }
  if ((unsigned long )gtt != (unsigned long )((struct amdgpu_ttm_tt *)0) && gtt->userptr != 0ULL) {
    tmp = kzalloc(16UL, 208U);
    ttm->sg = (struct sg_table *)tmp;
    if ((unsigned long )ttm->sg == (unsigned long )((struct sg_table *)0)) {
      return (-12);
    } else {
    }
    ttm->page_flags = ttm->page_flags | 256U;
    ttm->state = 1;
    return (0);
  } else {
  }
  if ((int )slave && (unsigned long )ttm->sg != (unsigned long )((struct sg_table *)0)) {
    drm_prime_sg_to_page_addr_arrays(ttm->sg, ttm->pages, gtt->ttm.dma_address, (int )ttm->num_pages);
    ttm->state = 1;
    return (0);
  } else {
  }
  adev = amdgpu_get_adev(ttm->bdev);
  tmp___1 = swiotlb_nr_tbl();
  if (tmp___1 != 0UL) {
    tmp___0 = ttm_dma_populate(& gtt->ttm, adev->dev);
    return (tmp___0);
  } else {
  }
  r = ttm_pool_populate(ttm);
  if (r != 0) {
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_44871;
  ldv_44870:
  *(gtt->ttm.dma_address + (unsigned long )i) = pci_map_page___0(adev->pdev, *(ttm->pages + (unsigned long )i),
                                                                 0UL, 4096UL, 0);
  tmp___2 = pci_dma_mapping_error(adev->pdev, *(gtt->ttm.dma_address + (unsigned long )i));
  if (tmp___2 != 0) {
    goto ldv_44868;
    ldv_44867:
    pci_unmap_page___0(adev->pdev, *(gtt->ttm.dma_address + (unsigned long )i), 4096UL,
                       0);
    *(gtt->ttm.dma_address + (unsigned long )i) = 0ULL;
    ldv_44868:
    i = i - 1U;
    if (i != 0U) {
      goto ldv_44867;
    } else {
    }
    ttm_pool_unpopulate(ttm);
    return (-14);
  } else {
  }
  i = i + 1U;
  ldv_44871: ;
  if ((unsigned long )i < ttm->num_pages) {
    goto ldv_44870;
  } else {
  }
  return (0);
}
}
static void amdgpu_ttm_tt_unpopulate(struct ttm_tt *ttm )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ttm_tt *gtt ;
  unsigned int i ;
  bool slave ;
  unsigned long tmp ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  slave = (ttm->page_flags & 256U) != 0U;
  if ((unsigned long )gtt != (unsigned long )((struct amdgpu_ttm_tt *)0) && gtt->userptr != 0ULL) {
    kfree((void const *)ttm->sg);
    ttm->page_flags = ttm->page_flags & 4294967039U;
    return;
  } else {
  }
  if ((int )slave) {
    return;
  } else {
  }
  adev = amdgpu_get_adev(ttm->bdev);
  tmp = swiotlb_nr_tbl();
  if (tmp != 0UL) {
    ttm_dma_unpopulate(& gtt->ttm, adev->dev);
    return;
  } else {
  }
  i = 0U;
  goto ldv_44881;
  ldv_44880: ;
  if (*(gtt->ttm.dma_address + (unsigned long )i) != 0ULL) {
    pci_unmap_page___0(adev->pdev, *(gtt->ttm.dma_address + (unsigned long )i), 4096UL,
                       0);
  } else {
  }
  i = i + 1U;
  ldv_44881: ;
  if ((unsigned long )i < ttm->num_pages) {
    goto ldv_44880;
  } else {
  }
  ttm_pool_unpopulate(ttm);
  return;
}
}
int amdgpu_ttm_tt_set_userptr(struct ttm_tt *ttm , uint64_t addr , u32 flags )
{
  struct amdgpu_ttm_tt *gtt ;
  struct task_struct *tmp ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  if ((unsigned long )gtt == (unsigned long )((struct amdgpu_ttm_tt *)0)) {
    return (-22);
  } else {
  }
  gtt->userptr = addr;
  tmp = get_current();
  gtt->usermm = tmp->mm;
  gtt->userflags = flags;
  return (0);
}
}
bool amdgpu_ttm_tt_has_userptr(struct ttm_tt *ttm )
{
  struct amdgpu_ttm_tt *gtt ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  if ((unsigned long )gtt == (unsigned long )((struct amdgpu_ttm_tt *)0)) {
    return (0);
  } else {
  }
  return (gtt->userptr != 0ULL);
}
}
bool amdgpu_ttm_tt_is_readonly(struct ttm_tt *ttm )
{
  struct amdgpu_ttm_tt *gtt ;
  {
  gtt = (struct amdgpu_ttm_tt *)ttm;
  if ((unsigned long )gtt == (unsigned long )((struct amdgpu_ttm_tt *)0)) {
    return (0);
  } else {
  }
  return (((int )gtt->userflags & 1) != 0);
}
}
u32 amdgpu_ttm_tt_pte_flags(struct amdgpu_device *adev , struct ttm_tt *ttm , struct ttm_mem_reg *mem )
{
  u32 flags ;
  bool tmp ;
  int tmp___0 ;
  {
  flags = 0U;
  if ((unsigned long )mem != (unsigned long )((struct ttm_mem_reg *)0) && mem->mem_type != 0U) {
    flags = flags | 1U;
  } else {
  }
  if ((unsigned long )mem != (unsigned long )((struct ttm_mem_reg *)0) && mem->mem_type == 1U) {
    flags = flags | 2U;
  } else {
  }
  if ((unsigned long )ttm == (unsigned long )((struct ttm_tt *)0) || (unsigned int )ttm->caching_state == 2U) {
    flags = flags | 4U;
  } else {
  }
  if ((unsigned int )adev->asic_type > 4U) {
    flags = flags | 16U;
  } else {
  }
  flags = flags | 32U;
  tmp = amdgpu_ttm_tt_is_readonly(ttm);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    flags = flags | 64U;
  } else {
  }
  return (flags);
}
}
static struct ttm_bo_driver amdgpu_bo_driver =
     {& amdgpu_ttm_tt_create, & amdgpu_ttm_tt_populate, & amdgpu_ttm_tt_unpopulate,
    & amdgpu_invalidate_caches, & amdgpu_init_mem_type, & amdgpu_evict_flags, & amdgpu_bo_move,
    & amdgpu_verify_access, & amdgpu_bo_move_notify, & amdgpu_bo_fault_reserve_notify,
    0, & amdgpu_ttm_io_mem_reserve, & amdgpu_ttm_io_mem_free};
int amdgpu_ttm_init(struct amdgpu_device *adev )
{
  int r ;
  {
  r = amdgpu_ttm_global_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = ttm_bo_device_init(& adev->mman.bdev, (struct ttm_bo_global *)adev->mman.bo_global_ref.ref.object,
                         & amdgpu_bo_driver, ((adev->ddev)->anon_inode)->i_mapping,
                         1048576ULL, (int )adev->need_dma32);
  if (r != 0) {
    drm_err("failed initializing buffer object driver(%d).\n", r);
    return (r);
  } else {
  }
  adev->mman.initialized = 1;
  r = ttm_bo_init_mm(& adev->mman.bdev, 2U, (unsigned long )(adev->mc.real_vram_size >> 12));
  if (r != 0) {
    drm_err("Failed initializing VRAM heap.\n");
    return (r);
  } else {
  }
  amdgpu_ttm_set_active_vram_size(adev, adev->mc.visible_vram_size);
  r = amdgpu_bo_create(adev, 262144UL, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & adev->stollen_vga_memory);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_reserve(adev->stollen_vga_memory, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->stollen_vga_memory, 4U, (u64 *)0ULL);
  amdgpu_bo_unreserve(adev->stollen_vga_memory);
  if (r != 0) {
    amdgpu_bo_unref(& adev->stollen_vga_memory);
    return (r);
  } else {
  }
  printk("\016[drm] amdgpu: %uM of VRAM memory ready\n", (unsigned int )(adev->mc.real_vram_size / 1048576ULL));
  r = ttm_bo_init_mm(& adev->mman.bdev, 1U, (unsigned long )(adev->mc.gtt_size >> 12));
  if (r != 0) {
    drm_err("Failed initializing GTT heap.\n");
    return (r);
  } else {
  }
  printk("\016[drm] amdgpu: %uM of GTT memory ready.\n", (unsigned int )(adev->mc.gtt_size / 1048576ULL));
  adev->gds.mem.total_size = adev->gds.mem.total_size << 2;
  adev->gds.mem.gfx_partition_size = adev->gds.mem.gfx_partition_size << 2;
  adev->gds.mem.cs_partition_size = adev->gds.mem.cs_partition_size << 2;
  adev->gds.gws.total_size = adev->gds.gws.total_size << 12;
  adev->gds.gws.gfx_partition_size = adev->gds.gws.gfx_partition_size << 12;
  adev->gds.gws.cs_partition_size = adev->gds.gws.cs_partition_size << 12;
  adev->gds.oa.total_size = adev->gds.oa.total_size << 12;
  adev->gds.oa.gfx_partition_size = adev->gds.oa.gfx_partition_size << 12;
  adev->gds.oa.cs_partition_size = adev->gds.oa.cs_partition_size << 12;
  r = ttm_bo_init_mm(& adev->mman.bdev, 3U, (unsigned long )(adev->gds.mem.total_size >> 12));
  if (r != 0) {
    drm_err("Failed initializing GDS heap.\n");
    return (r);
  } else {
  }
  r = ttm_bo_init_mm(& adev->mman.bdev, 4U, (unsigned long )(adev->gds.gws.total_size >> 12));
  if (r != 0) {
    drm_err("Failed initializing gws heap.\n");
    return (r);
  } else {
  }
  r = ttm_bo_init_mm(& adev->mman.bdev, 5U, (unsigned long )(adev->gds.oa.total_size >> 12));
  if (r != 0) {
    drm_err("Failed initializing oa heap.\n");
    return (r);
  } else {
  }
  r = amdgpu_ttm_debugfs_init(adev);
  if (r != 0) {
    drm_err("Failed to init debugfs\n");
    return (r);
  } else {
  }
  return (0);
}
}
void amdgpu_ttm_fini(struct amdgpu_device *adev )
{
  int r ;
  {
  if (! adev->mman.initialized) {
    return;
  } else {
  }
  amdgpu_ttm_debugfs_fini(adev);
  if ((unsigned long )adev->stollen_vga_memory != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->stollen_vga_memory, 0);
    if (r == 0) {
      amdgpu_bo_unpin(adev->stollen_vga_memory);
      amdgpu_bo_unreserve(adev->stollen_vga_memory);
    } else {
    }
    amdgpu_bo_unref(& adev->stollen_vga_memory);
  } else {
  }
  ttm_bo_clean_mm(& adev->mman.bdev, 2U);
  ttm_bo_clean_mm(& adev->mman.bdev, 1U);
  ttm_bo_clean_mm(& adev->mman.bdev, 3U);
  ttm_bo_clean_mm(& adev->mman.bdev, 4U);
  ttm_bo_clean_mm(& adev->mman.bdev, 5U);
  ttm_bo_device_release(& adev->mman.bdev);
  amdgpu_gart_fini(adev);
  amdgpu_ttm_global_fini(adev);
  adev->mman.initialized = 0;
  printk("\016[drm] amdgpu: ttm finalized\n");
  return;
}
}
void amdgpu_ttm_set_active_vram_size(struct amdgpu_device *adev , u64 size )
{
  struct ttm_mem_type_manager *man ;
  {
  if (! adev->mman.initialized) {
    return;
  } else {
  }
  man = (struct ttm_mem_type_manager *)(& adev->mman.bdev.man) + 2UL;
  man->size = size >> 12;
  return;
}
}
int amdgpu_mmap(struct file *filp , struct vm_area_struct *vma )
{
  struct drm_file *file_priv ;
  struct amdgpu_device *adev ;
  long tmp ;
  int tmp___0 ;
  {
  tmp = ldv__builtin_expect((unsigned long long )vma->vm_pgoff <= 1048575ULL, 0L);
  if (tmp != 0L) {
    return (-22);
  } else {
  }
  file_priv = (struct drm_file *)filp->private_data;
  adev = (struct amdgpu_device *)((file_priv->minor)->dev)->dev_private;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0)) {
    return (-22);
  } else {
  }
  tmp___0 = ttm_bo_mmap(filp, vma, & adev->mman.bdev);
  return (tmp___0);
}
}
int amdgpu_copy_buffer(struct amdgpu_ring *ring , uint64_t src_offset , uint64_t dst_offset ,
                       u32 byte_count , struct reservation_object *resv , struct amdgpu_fence **fence )
{
  struct amdgpu_device *adev ;
  struct amdgpu_sync sync ;
  u32 max_bytes ;
  unsigned int num_loops ;
  unsigned int num_dw ;
  unsigned int i ;
  int r ;
  u32 cur_size_in_bytes ;
  u32 _min1 ;
  u32 _min2 ;
  {
  adev = ring->adev;
  amdgpu_sync_create(& sync);
  if ((unsigned long )resv != (unsigned long )((struct reservation_object *)0)) {
    r = amdgpu_sync_resv(adev, & sync, resv, (void *)0);
    if (r != 0) {
      drm_err("sync failed (%d).\n", r);
      amdgpu_sync_free(adev, & sync, (struct amdgpu_fence *)0);
      return (r);
    } else {
    }
  } else {
  }
  max_bytes = (adev->mman.buffer_funcs)->copy_max_bytes;
  num_loops = ((byte_count + max_bytes) - 1U) / max_bytes;
  num_dw = (unsigned int )(adev->mman.buffer_funcs)->copy_num_dw * num_loops;
  num_dw = num_dw + 96U;
  r = amdgpu_ring_lock(ring, num_dw);
  if (r != 0) {
    drm_err("ring lock failed (%d).\n", r);
    amdgpu_sync_free(adev, & sync, (struct amdgpu_fence *)0);
    return (r);
  } else {
  }
  amdgpu_sync_rings(& sync, ring);
  i = 0U;
  goto ldv_44943;
  ldv_44942:
  _min1 = byte_count;
  _min2 = max_bytes;
  cur_size_in_bytes = _min1 < _min2 ? _min1 : _min2;
  (*((adev->mman.buffer_funcs)->emit_copy_buffer))(ring, src_offset, dst_offset, cur_size_in_bytes);
  src_offset = (uint64_t )cur_size_in_bytes + src_offset;
  dst_offset = (uint64_t )cur_size_in_bytes + dst_offset;
  byte_count = byte_count - cur_size_in_bytes;
  i = i + 1U;
  ldv_44943: ;
  if (i < num_loops) {
    goto ldv_44942;
  } else {
  }
  r = amdgpu_fence_emit(ring, (void *)2, fence);
  if (r != 0) {
    amdgpu_ring_unlock_undo(ring);
    amdgpu_sync_free(adev, & sync, (struct amdgpu_fence *)0);
    return (r);
  } else {
  }
  amdgpu_ring_unlock_commit(ring);
  amdgpu_sync_free(adev, & sync, *fence);
  return (0);
}
}
static int amdgpu_mm_dump_table(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  unsigned int ttm_pl ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct drm_mm *mm ;
  int ret ;
  struct ttm_bo_global *glob ;
  {
  node = (struct drm_info_node *)m->private;
  ttm_pl = (unsigned int )*((int *)(node->info_ent)->data);
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  mm = (struct drm_mm *)adev->mman.bdev.man[ttm_pl].priv;
  glob = adev->mman.bdev.glob;
  spin_lock(& glob->lru_lock);
  ret = drm_mm_dump_table(m, mm);
  spin_unlock(& glob->lru_lock);
  return (ret);
}
}
static int ttm_pl_vram = 2;
static int ttm_pl_tt = 1;
static struct drm_info_list amdgpu_ttm_debugfs_list[4U] = { {"amdgpu_vram_mm", & amdgpu_mm_dump_table, 0U, (void *)(& ttm_pl_vram)},
        {"amdgpu_gtt_mm", & amdgpu_mm_dump_table, 0U, (void *)(& ttm_pl_tt)},
        {"ttm_page_pool", & ttm_page_alloc_debugfs, 0U, (void *)0},
        {"ttm_dma_page_pool", & ttm_dma_page_alloc_debugfs, 0U, (void *)0}};
static ssize_t amdgpu_ttm_vram_read(struct file *f , char *buf , size_t size , loff_t *pos )
{
  struct amdgpu_device *adev ;
  ssize_t result ;
  int r ;
  unsigned long flags ;
  u32 value ;
  raw_spinlock_t *tmp ;
  int __ret_pu ;
  u32 __pu_val ;
  {
  adev = (struct amdgpu_device *)(f->f_inode)->i_private;
  result = 0L;
  if ((size & 3UL) != 0UL || (*pos & 3LL) != 0LL) {
    return (-22L);
  } else {
  }
  goto ldv_44983;
  ldv_44982: ;
  if ((unsigned long long )*pos >= adev->mc.mc_vram_size) {
    return (result);
  } else {
  }
  tmp = spinlock_check(& adev->mmio_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 0U, (unsigned int )*pos | 2147483648U, 0);
  amdgpu_mm_wreg(adev, 6U, (u32 )(*pos >> 31), 0);
  value = amdgpu_mm_rreg(adev, 1U, 0);
  spin_unlock_irqrestore(& adev->mmio_idx_lock, flags);
  __might_fault("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c",
                1103);
  __pu_val = value;
  switch (4UL) {
  case 1UL:
  __asm__ volatile ("call __put_user_1": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_44976;
  case 2UL:
  __asm__ volatile ("call __put_user_2": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_44976;
  case 4UL:
  __asm__ volatile ("call __put_user_4": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_44976;
  case 8UL:
  __asm__ volatile ("call __put_user_8": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_44976;
  default:
  __asm__ volatile ("call __put_user_X": "=a" (__ret_pu): "0" (__pu_val), "c" ((u32 *)buf): "ebx");
  goto ldv_44976;
  }
  ldv_44976:
  r = __ret_pu;
  if (r != 0) {
    return ((ssize_t )r);
  } else {
  }
  result = result + 4L;
  buf = buf + 4UL;
  *pos = *pos + 4LL;
  size = size - 4UL;
  ldv_44983: ;
  if (size != 0UL) {
    goto ldv_44982;
  } else {
  }
  return (result);
}
}
static struct file_operations const amdgpu_ttm_vram_fops =
     {& __this_module, & default_llseek, & amdgpu_ttm_vram_read, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
static ssize_t amdgpu_ttm_gtt_read(struct file *f , char *buf , size_t size , loff_t *pos )
{
  struct amdgpu_device *adev ;
  ssize_t result ;
  int r ;
  loff_t p ;
  unsigned int off ;
  size_t cur_size ;
  size_t __min1 ;
  size_t __min2 ;
  struct page *page ;
  void *ptr ;
  unsigned long tmp ;
  unsigned long tmp___0 ;
  {
  adev = (struct amdgpu_device *)(f->f_inode)->i_private;
  result = 0L;
  goto ldv_45004;
  ldv_45003:
  p = (loff_t )((unsigned long long )*pos / 4096ULL);
  off = (unsigned int )*pos & 4095U;
  __min1 = size;
  __min2 = 4096UL - (unsigned long )off;
  cur_size = __min1 < __min2 ? __min1 : __min2;
  if ((loff_t )adev->gart.num_cpu_pages <= p) {
    return (result);
  } else {
  }
  page = *(adev->gart.pages + (unsigned long )p);
  if ((unsigned long )page != (unsigned long )((struct page *)0)) {
    ptr = kmap(page);
    ptr = ptr + (unsigned long )off;
    tmp = copy_to_user((void *)buf, (void const *)ptr, cur_size);
    r = (int )tmp;
    kunmap(*(adev->gart.pages + (unsigned long )p));
  } else {
    tmp___0 = clear_user((void *)buf, cur_size);
    r = (int )tmp___0;
  }
  if (r != 0) {
    return (-14L);
  } else {
  }
  result = (ssize_t )((unsigned long )result + cur_size);
  buf = buf + cur_size;
  *pos = (loff_t )((unsigned long long )*pos + (unsigned long long )cur_size);
  size = size - cur_size;
  ldv_45004: ;
  if (size != 0UL) {
    goto ldv_45003;
  } else {
  }
  return (result);
}
}
static struct file_operations const amdgpu_ttm_gtt_fops =
     {& __this_module, & default_llseek, & amdgpu_ttm_gtt_read, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
static int amdgpu_ttm_debugfs_init(struct amdgpu_device *adev )
{
  unsigned int count ;
  struct drm_minor *minor ;
  struct dentry *ent ;
  struct dentry *root ;
  long tmp ;
  bool tmp___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  unsigned long tmp___3 ;
  int tmp___4 ;
  {
  minor = (adev->ddev)->primary;
  root = minor->debugfs_root;
  ent = debugfs_create_file("amdgpu_vram", 33060, root, (void *)adev, & amdgpu_ttm_vram_fops);
  tmp___0 = IS_ERR((void const *)ent);
  if ((int )tmp___0) {
    tmp = PTR_ERR((void const *)ent);
    return ((int )tmp);
  } else {
  }
  i_size_write(ent->d_inode, (loff_t )adev->mc.mc_vram_size);
  adev->mman.vram = ent;
  ent = debugfs_create_file("amdgpu_gtt", 33060, root, (void *)adev, & amdgpu_ttm_gtt_fops);
  tmp___2 = IS_ERR((void const *)ent);
  if ((int )tmp___2) {
    tmp___1 = PTR_ERR((void const *)ent);
    return ((int )tmp___1);
  } else {
  }
  i_size_write(ent->d_inode, (loff_t )adev->mc.gtt_size);
  adev->mman.gtt = ent;
  count = 4U;
  tmp___3 = swiotlb_nr_tbl();
  if (tmp___3 == 0UL) {
    count = count - 1U;
  } else {
  }
  tmp___4 = amdgpu_debugfs_add_files(adev, (struct drm_info_list *)(& amdgpu_ttm_debugfs_list),
                                     count);
  return (tmp___4);
}
}
static void amdgpu_ttm_debugfs_fini(struct amdgpu_device *adev )
{
  {
  debugfs_remove(adev->mman.vram);
  adev->mman.vram = (struct dentry *)0;
  debugfs_remove(adev->mman.gtt);
  adev->mman.gtt = (struct dentry *)0;
  return;
}
}
extern int ldv_open_161(void) ;
extern int ldv_release_161(void) ;
extern int ldv_open_160(void) ;
int ldv_retval_21 ;
int ldv_retval_4 ;
extern int ldv_release_160(void) ;
void ldv_initialize_ttm_bo_driver_162(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  {
  tmp = ldv_init_zalloc(2592UL);
  amdgpu_bo_driver_group2 = (struct ttm_bo_device *)tmp;
  tmp___0 = ldv_init_zalloc(80UL);
  amdgpu_bo_driver_group0 = (struct ttm_tt *)tmp___0;
  tmp___1 = ldv_init_zalloc(96UL);
  amdgpu_bo_driver_group1 = (struct ttm_mem_reg *)tmp___1;
  tmp___2 = ldv_init_zalloc(872UL);
  amdgpu_bo_driver_group3 = (struct ttm_buffer_object *)tmp___2;
  return;
}
}
void ldv_initialize_ttm_backend_func_163(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(80UL);
  amdgpu_backend_func_group0 = (struct ttm_tt *)tmp;
  return;
}
}
void ldv_file_operations_160(void)
{
  void *tmp ;
  {
  amdgpu_ttm_gtt_fops_group1 = ldv_init_zalloc(1000UL);
  tmp = ldv_init_zalloc(504UL);
  amdgpu_ttm_gtt_fops_group2 = (struct file *)tmp;
  return;
}
}
void ldv_file_operations_161(void)
{
  void *tmp ;
  {
  amdgpu_ttm_vram_fops_group1 = ldv_init_zalloc(1000UL);
  tmp = ldv_init_zalloc(504UL);
  amdgpu_ttm_vram_fops_group2 = (struct file *)tmp;
  return;
}
}
void ldv_main_exported_163(void)
{
  struct ttm_mem_reg *ldvarg102 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  ldvarg102 = (struct ttm_mem_reg *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_163 == 1) {
    amdgpu_ttm_backend_unbind(amdgpu_backend_func_group0);
    ldv_state_variable_163 = 1;
  } else {
  }
  goto ldv_45046;
  case 1: ;
  if (ldv_state_variable_163 == 1) {
    amdgpu_ttm_backend_destroy(amdgpu_backend_func_group0);
    ldv_state_variable_163 = 1;
  } else {
  }
  goto ldv_45046;
  case 2: ;
  if (ldv_state_variable_163 == 1) {
    amdgpu_ttm_backend_bind(amdgpu_backend_func_group0, ldvarg102);
    ldv_state_variable_163 = 1;
  } else {
  }
  goto ldv_45046;
  default:
  ldv_stop();
  }
  ldv_45046: ;
  return;
}
}
void ldv_main_exported_161(void)
{
  loff_t *ldvarg72 ;
  void *tmp ;
  loff_t ldvarg71 ;
  char *ldvarg74 ;
  void *tmp___0 ;
  int ldvarg70 ;
  size_t ldvarg73 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(8UL);
  ldvarg72 = (loff_t *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg74 = (char *)tmp___0;
  ldv_memset((void *)(& ldvarg71), 0, 8UL);
  ldv_memset((void *)(& ldvarg70), 0, 4UL);
  ldv_memset((void *)(& ldvarg73), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_161 == 2) {
    amdgpu_ttm_vram_read(amdgpu_ttm_vram_fops_group2, ldvarg74, ldvarg73, ldvarg72);
    ldv_state_variable_161 = 2;
  } else {
  }
  goto ldv_45059;
  case 1: ;
  if (ldv_state_variable_161 == 2) {
    default_llseek(amdgpu_ttm_vram_fops_group2, ldvarg71, ldvarg70);
    ldv_state_variable_161 = 2;
  } else {
  }
  goto ldv_45059;
  case 2: ;
  if (ldv_state_variable_161 == 1) {
    ldv_retval_4 = ldv_open_161();
    if (ldv_retval_4 == 0) {
      ldv_state_variable_161 = 2;
      ref_cnt = ref_cnt + 1;
    } else {
    }
  } else {
  }
  goto ldv_45059;
  case 3: ;
  if (ldv_state_variable_161 == 2) {
    ldv_release_161();
    ldv_state_variable_161 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_45059;
  default:
  ldv_stop();
  }
  ldv_45059: ;
  return;
}
}
void ldv_main_exported_162(void)
{
  bool ldvarg567 ;
  struct file *ldvarg575 ;
  void *tmp ;
  struct ttm_placement *ldvarg576 ;
  void *tmp___0 ;
  u32 ldvarg571 ;
  u32 ldvarg569 ;
  bool ldvarg566 ;
  unsigned long ldvarg573 ;
  bool ldvarg568 ;
  struct page *ldvarg574 ;
  void *tmp___1 ;
  u32 ldvarg572 ;
  struct ttm_mem_type_manager *ldvarg570 ;
  void *tmp___2 ;
  int tmp___3 ;
  {
  tmp = __VERIFIER_nondet_pointer();
  ldvarg575 = (struct file *)tmp;
  tmp___0 = ldv_init_zalloc(32UL);
  ldvarg576 = (struct ttm_placement *)tmp___0;
  tmp___1 = ldv_init_zalloc(64UL);
  ldvarg574 = (struct page *)tmp___1;
  tmp___2 = ldv_init_zalloc(256UL);
  ldvarg570 = (struct ttm_mem_type_manager *)tmp___2;
  ldv_memset((void *)(& ldvarg567), 0, 1UL);
  ldv_memset((void *)(& ldvarg571), 0, 4UL);
  ldv_memset((void *)(& ldvarg569), 0, 4UL);
  ldv_memset((void *)(& ldvarg566), 0, 1UL);
  ldv_memset((void *)(& ldvarg573), 0, 8UL);
  ldv_memset((void *)(& ldvarg568), 0, 1UL);
  ldv_memset((void *)(& ldvarg572), 0, 4UL);
  tmp___3 = __VERIFIER_nondet_int();
  switch (tmp___3) {
  case 0: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_ttm_tt_populate(amdgpu_bo_driver_group0);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 1: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_ttm_tt_unpopulate(amdgpu_bo_driver_group0);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 2: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_evict_flags(amdgpu_bo_driver_group3, ldvarg576);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 3: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_ttm_io_mem_reserve(amdgpu_bo_driver_group2, amdgpu_bo_driver_group1);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 4: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_verify_access(amdgpu_bo_driver_group3, ldvarg575);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 5: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_bo_move_notify(amdgpu_bo_driver_group3, amdgpu_bo_driver_group1);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 6: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_ttm_tt_create(amdgpu_bo_driver_group2, ldvarg573, ldvarg572, ldvarg574);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 7: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_ttm_io_mem_free(amdgpu_bo_driver_group2, amdgpu_bo_driver_group1);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 8: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_init_mem_type(amdgpu_bo_driver_group2, ldvarg571, ldvarg570);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 9: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_bo_fault_reserve_notify(amdgpu_bo_driver_group3);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 10: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_invalidate_caches(amdgpu_bo_driver_group2, ldvarg569);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  case 11: ;
  if (ldv_state_variable_162 == 1) {
    amdgpu_bo_move(amdgpu_bo_driver_group3, (int )ldvarg567, (int )ldvarg566, (int )ldvarg568,
                   amdgpu_bo_driver_group1);
    ldv_state_variable_162 = 1;
  } else {
  }
  goto ldv_45079;
  default:
  ldv_stop();
  }
  ldv_45079: ;
  return;
}
}
void ldv_main_exported_160(void)
{
  char *ldvarg451 ;
  void *tmp ;
  size_t ldvarg450 ;
  loff_t ldvarg449 ;
  loff_t *ldvarg452 ;
  void *tmp___0 ;
  int ldvarg448 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg451 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(8UL);
  ldvarg452 = (loff_t *)tmp___0;
  ldv_memset((void *)(& ldvarg450), 0, 8UL);
  ldv_memset((void *)(& ldvarg449), 0, 8UL);
  ldv_memset((void *)(& ldvarg448), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_160 == 2) {
    amdgpu_ttm_gtt_read(amdgpu_ttm_gtt_fops_group2, ldvarg451, ldvarg450, ldvarg452);
    ldv_state_variable_160 = 2;
  } else {
  }
  goto ldv_45101;
  case 1: ;
  if (ldv_state_variable_160 == 2) {
    default_llseek(amdgpu_ttm_gtt_fops_group2, ldvarg449, ldvarg448);
    ldv_state_variable_160 = 2;
  } else {
  }
  goto ldv_45101;
  case 2: ;
  if (ldv_state_variable_160 == 1) {
    ldv_retval_21 = ldv_open_160();
    if (ldv_retval_21 == 0) {
      ldv_state_variable_160 = 2;
      ref_cnt = ref_cnt + 1;
    } else {
    }
  } else {
  }
  goto ldv_45101;
  case 3: ;
  if (ldv_state_variable_160 == 2) {
    ldv_release_160();
    ldv_state_variable_160 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_45101;
  default:
  ldv_stop();
  }
  ldv_45101: ;
  return;
}
}
bool ldv_queue_work_on_117(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_118(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_119(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_120(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_121(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern void __list_del_entry(struct list_head * ) ;
__inline static void list_del_init(struct list_head *entry )
{
  {
  __list_del_entry(entry);
  INIT_LIST_HEAD(entry);
  return;
}
}
__inline static unsigned long arch_local_save_flags___1(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static void atomic64_sub(long i , atomic64_t *v )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; subq %1,%0": "=m" (v->counter): "er" (i),
                       "m" (v->counter));
  return;
}
}
__inline static bool static_key_false___0(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int rcu_read_lock_sched_held___0(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___1();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
bool ldv_queue_work_on_131(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_133(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_132(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_135(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_134(struct workqueue_struct *ldv_func_arg1 ) ;
extern int arch_phys_wc_add(unsigned long , unsigned long ) ;
extern void arch_phys_wc_del(int ) ;
__inline static void kref_get___0(struct kref *kref )
{
  bool __warned ;
  int __ret_warn_once ;
  int tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = atomic_add_return(1, & kref->refcount);
  __ret_warn_once = tmp <= 1;
  tmp___2 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___2 != 0L) {
    __ret_warn_on = ! __warned;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("include/linux/kref.h", 47);
    } else {
    }
    tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___1 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  ldv__builtin_expect(__ret_warn_once != 0, 0L);
  return;
}
}
__inline static int kref_sub___0(struct kref *kref , unsigned int count , void (*release)(struct kref * ) )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 71);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = atomic_sub_and_test((int )count, & kref->refcount);
  if (tmp___0 != 0) {
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int kref_put___0(struct kref *kref , void (*release)(struct kref * ) )
{
  int tmp ;
  {
  tmp = kref_sub___0(kref, 1U, release);
  return (tmp);
}
}
extern void reservation_object_add_shared_fence(struct reservation_object * , struct fence * ) ;
extern void reservation_object_add_excl_fence(struct reservation_object * , struct fence * ) ;
__inline static struct ttm_buffer_object *ttm_bo_reference(struct ttm_buffer_object *bo )
{
  {
  kref_get___0(& bo->kref);
  return (bo);
}
}
extern int ttm_bo_validate(struct ttm_buffer_object * , struct ttm_placement * , bool ,
                           bool ) ;
extern void ttm_bo_unref(struct ttm_buffer_object ** ) ;
extern size_t ttm_bo_dma_acc_size(struct ttm_bo_device * , unsigned long , unsigned int ) ;
extern int ttm_bo_init(struct ttm_bo_device * , struct ttm_buffer_object * , unsigned long ,
                       enum ttm_bo_type , struct ttm_placement * , u32 , bool ,
                       struct file * , size_t , struct sg_table * , struct reservation_object * ,
                       void (*)(struct ttm_buffer_object * ) ) ;
extern int ttm_bo_evict_mm(struct ttm_bo_device * , unsigned int ) ;
__inline static void *ttm_kmap_obj_virtual(struct ttm_bo_kmap_obj *map , bool *is_iomem )
{
  {
  *is_iomem = ((unsigned int )map->bo_kmap_type & 128U) != 0U;
  return (map->virtual);
}
}
extern int ttm_bo_kmap(struct ttm_buffer_object * , unsigned long , unsigned long ,
                       struct ttm_bo_kmap_obj * ) ;
extern void ttm_bo_kunmap(struct ttm_bo_kmap_obj * ) ;
extern int ttm_fbdev_mmap(struct vm_area_struct * , struct ttm_buffer_object * ) ;
extern void drm_gem_object_release(struct drm_gem_object * ) ;
extern void drm_gem_object_free(struct kref * ) ;
extern int drm_gem_object_init(struct drm_device * , struct drm_gem_object * , size_t ) ;
__inline static void drm_gem_object_unreference(struct drm_gem_object *obj )
{
  {
  if ((unsigned long )obj != (unsigned long )((struct drm_gem_object *)0)) {
    kref_put___0(& obj->refcount, & drm_gem_object_free);
  } else {
  }
  return;
}
}
void amdgpu_vm_bo_invalidate(struct amdgpu_device *adev , struct amdgpu_bo *bo ) ;
__inline static u64 amdgpu_bo_gpu_offset(struct amdgpu_bo *bo )
{
  {
  return (bo->tbo.offset);
}
}
__inline static unsigned long amdgpu_bo_size(struct amdgpu_bo *bo )
{
  {
  return (bo->tbo.num_pages << 12);
}
}
int amdgpu_bo_create_restricted(struct amdgpu_device *adev , unsigned long size ,
                                int byte_align , bool kernel , u32 domain , u64 flags ,
                                struct sg_table *sg , struct ttm_placement *placement ,
                                struct amdgpu_bo **bo_ptr ) ;
struct amdgpu_bo *amdgpu_bo_ref(struct amdgpu_bo *bo ) ;
int amdgpu_bo_pin_restricted(struct amdgpu_bo *bo , u32 domain , u64 min_offset ,
                             u64 max_offset , u64 *gpu_addr ) ;
void amdgpu_bo_force_delete(struct amdgpu_device *adev ) ;
int amdgpu_bo_init(struct amdgpu_device *adev ) ;
void amdgpu_bo_fini(struct amdgpu_device *adev ) ;
int amdgpu_bo_fbdev_mmap(struct amdgpu_bo *bo , struct vm_area_struct *vma ) ;
int amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo , u64 tiling_flags ) ;
void amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo , u64 *tiling_flags ) ;
int amdgpu_bo_set_metadata(struct amdgpu_bo *bo , void *metadata , u32 metadata_size ,
                           uint64_t flags ) ;
int amdgpu_bo_get_metadata(struct amdgpu_bo *bo , void *buffer , size_t buffer_size ,
                           u32 *metadata_size , uint64_t *flags ) ;
void amdgpu_bo_fence(struct amdgpu_bo *bo , struct amdgpu_fence *fence , bool shared ) ;
struct tracepoint __tracepoint_amdgpu_bo_create ;
__inline static void trace_amdgpu_bo_create(struct amdgpu_bo *bo )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_275 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_277 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___0(& __tracepoint_amdgpu_bo_create.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_bo_create.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___0();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               27, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43701:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_bo * ))it_func))(__data, bo);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43701;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_bo_create.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___0();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             27, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
static u64 amdgpu_get_vis_part_size(struct amdgpu_device *adev , struct ttm_mem_reg *mem )
{
  u64 ret ;
  {
  ret = 0ULL;
  if ((unsigned long long )(mem->start << 12) < adev->mc.visible_vram_size) {
    ret = (unsigned long long )((mem->start << 12) + mem->size) > adev->mc.visible_vram_size ? adev->mc.visible_vram_size - (unsigned long long )(mem->start << 12) : (unsigned long long )mem->size;
  } else {
  }
  return (ret);
}
}
static void amdgpu_update_memory_usage(struct amdgpu_device *adev , struct ttm_mem_reg *old_mem ,
                                       struct ttm_mem_reg *new_mem )
{
  u64 vis_size ;
  {
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0)) {
    return;
  } else {
  }
  if ((unsigned long )new_mem != (unsigned long )((struct ttm_mem_reg *)0)) {
    switch (new_mem->mem_type) {
    case 1U:
    atomic64_add((long )new_mem->size, & adev->gtt_usage);
    goto ldv_44507;
    case 2U:
    atomic64_add((long )new_mem->size, & adev->vram_usage);
    vis_size = amdgpu_get_vis_part_size(adev, new_mem);
    atomic64_add((long )vis_size, & adev->vram_vis_usage);
    goto ldv_44507;
    }
    ldv_44507: ;
  } else {
  }
  if ((unsigned long )old_mem != (unsigned long )((struct ttm_mem_reg *)0)) {
    switch (old_mem->mem_type) {
    case 1U:
    atomic64_sub((long )old_mem->size, & adev->gtt_usage);
    goto ldv_44510;
    case 2U:
    atomic64_sub((long )old_mem->size, & adev->vram_usage);
    vis_size = amdgpu_get_vis_part_size(adev, old_mem);
    atomic64_sub((long )vis_size, & adev->vram_vis_usage);
    goto ldv_44510;
    }
    ldv_44510: ;
  } else {
  }
  return;
}
}
static void amdgpu_ttm_bo_destroy(struct ttm_buffer_object *tbo )
{
  struct amdgpu_bo *bo ;
  struct ttm_buffer_object const *__mptr ;
  {
  __mptr = (struct ttm_buffer_object const *)tbo;
  bo = (struct amdgpu_bo *)__mptr + 0xffffffffffffff98UL;
  amdgpu_update_memory_usage(bo->adev, & bo->tbo.mem, (struct ttm_mem_reg *)0);
  mutex_lock_nested(& (bo->adev)->gem.mutex, 0U);
  list_del_init(& bo->list);
  mutex_unlock(& (bo->adev)->gem.mutex);
  drm_gem_object_release(& bo->gem_base);
  kfree((void const *)bo->metadata);
  kfree((void const *)bo);
  return;
}
}
bool amdgpu_ttm_bo_is_amdgpu_bo(struct ttm_buffer_object *bo )
{
  {
  if ((unsigned long )bo->destroy == (unsigned long )(& amdgpu_ttm_bo_destroy)) {
    return (1);
  } else {
  }
  return (0);
}
}
static void amdgpu_ttm_placement_init(struct amdgpu_device *adev , struct ttm_placement *placement ,
                                      struct ttm_place *placements , u32 domain ,
                                      u64 flags )
{
  u32 c ;
  u32 i ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  {
  c = 0U;
  placement->placement = (struct ttm_place const *)placements;
  placement->busy_placement = (struct ttm_place const *)placements;
  if ((domain & 4U) != 0U) {
    if ((flags & 2ULL) != 0ULL && adev->mc.visible_vram_size < adev->mc.real_vram_size) {
      (placements + (unsigned long )c)->fpfn = (unsigned int )(adev->mc.visible_vram_size >> 12);
      tmp = c;
      c = c + 1U;
      (placements + (unsigned long )tmp)->flags = 393220U;
    } else {
    }
    (placements + (unsigned long )c)->fpfn = 0U;
    tmp___0 = c;
    c = c + 1U;
    (placements + (unsigned long )tmp___0)->flags = 393220U;
  } else {
  }
  if ((domain & 2U) != 0U) {
    if ((flags & 4ULL) != 0ULL) {
      (placements + (unsigned long )c)->fpfn = 0U;
      tmp___1 = c;
      c = c + 1U;
      (placements + (unsigned long )tmp___1)->flags = 393218U;
    } else {
      (placements + (unsigned long )c)->fpfn = 0U;
      tmp___2 = c;
      c = c + 1U;
      (placements + (unsigned long )tmp___2)->flags = 65538U;
    }
  } else {
  }
  if ((int )domain & 1) {
    if ((flags & 4ULL) != 0ULL) {
      (placements + (unsigned long )c)->fpfn = 0U;
      tmp___3 = c;
      c = c + 1U;
      (placements + (unsigned long )tmp___3)->flags = 393217U;
    } else {
      (placements + (unsigned long )c)->fpfn = 0U;
      tmp___4 = c;
      c = c + 1U;
      (placements + (unsigned long )tmp___4)->flags = 65537U;
    }
  } else {
  }
  if ((domain & 8U) != 0U) {
    (placements + (unsigned long )c)->fpfn = 0U;
    tmp___5 = c;
    c = c + 1U;
    (placements + (unsigned long )tmp___5)->flags = 131080U;
  } else {
  }
  if ((domain & 16U) != 0U) {
    (placements + (unsigned long )c)->fpfn = 0U;
    tmp___6 = c;
    c = c + 1U;
    (placements + (unsigned long )tmp___6)->flags = 131088U;
  } else {
  }
  if ((domain & 32U) != 0U) {
    (placements + (unsigned long )c)->fpfn = 0U;
    tmp___7 = c;
    c = c + 1U;
    (placements + (unsigned long )tmp___7)->flags = 131104U;
  } else {
  }
  if (c == 0U) {
    (placements + (unsigned long )c)->fpfn = 0U;
    tmp___8 = c;
    c = c + 1U;
    (placements + (unsigned long )tmp___8)->flags = 458753U;
  } else {
  }
  placement->num_placement = c;
  placement->num_busy_placement = c;
  i = 0U;
  goto ldv_44531;
  ldv_44530: ;
  if (((int )flags & 1 && ((placements + (unsigned long )i)->flags & 4U) != 0U) && (placements + (unsigned long )i)->fpfn == 0U) {
    (placements + (unsigned long )i)->lpfn = (unsigned int )(adev->mc.visible_vram_size >> 12);
  } else {
    (placements + (unsigned long )i)->lpfn = 0U;
  }
  i = i + 1U;
  ldv_44531: ;
  if (i < c) {
    goto ldv_44530;
  } else {
  }
  return;
}
}
void amdgpu_ttm_placement_from_domain(struct amdgpu_bo *rbo , u32 domain )
{
  {
  amdgpu_ttm_placement_init(rbo->adev, & rbo->placement, (struct ttm_place *)(& rbo->placements),
                            domain, rbo->flags);
  return;
}
}
static void amdgpu_fill_placement_to_bo(struct amdgpu_bo *bo , struct ttm_placement *placement )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect(placement->num_placement > 4U, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c"),
                         "i" (203), "i" (12UL));
    ldv_44541: ;
    goto ldv_44541;
  } else {
  }
  memcpy((void *)(& bo->placements), (void const *)placement->placement, (unsigned long )placement->num_placement * 12UL);
  bo->placement.num_placement = placement->num_placement;
  bo->placement.num_busy_placement = placement->num_busy_placement;
  bo->placement.placement = (struct ttm_place const *)(& bo->placements);
  bo->placement.busy_placement = (struct ttm_place const *)(& bo->placements);
  return;
}
}
int amdgpu_bo_create_restricted(struct amdgpu_device *adev , unsigned long size ,
                                int byte_align , bool kernel , u32 domain , u64 flags ,
                                struct sg_table *sg , struct ttm_placement *placement ,
                                struct amdgpu_bo **bo_ptr )
{
  struct amdgpu_bo *bo ;
  enum ttm_bo_type type ;
  unsigned long page_align ;
  size_t acc_size ;
  int r ;
  unsigned long __y ;
  void *tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  if ((domain & 56U) == 0U) {
    if ((unsigned int )adev->asic_type > 4U) {
      if ((byte_align & 32767) != 0) {
        byte_align = (byte_align + 32767) & -32768;
      } else {
      }
      if ((size & 32767UL) != 0UL) {
        size = (size + 32767UL) & 0xffffffffffff8000UL;
      } else {
      }
    } else {
    }
  } else {
  }
  __y = 4096UL;
  page_align = ((((unsigned long )byte_align + __y) - 1UL) / __y) * __y >> 12;
  size = (size + 4095UL) & 0xfffffffffffff000UL;
  if ((int )kernel) {
    type = 1;
  } else
  if ((unsigned long )sg != (unsigned long )((struct sg_table *)0)) {
    type = 2;
  } else {
    type = 0;
  }
  *bo_ptr = (struct amdgpu_bo *)0;
  acc_size = ttm_bo_dma_acc_size(& adev->mman.bdev, size, 1400U);
  tmp = kzalloc(1400UL, 208U);
  bo = (struct amdgpu_bo *)tmp;
  if ((unsigned long )bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (-12);
  } else {
  }
  r = drm_gem_object_init(adev->ddev, & bo->gem_base, size);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    kfree((void const *)bo);
    return (r);
  } else {
  }
  bo->adev = adev;
  INIT_LIST_HEAD(& bo->list);
  INIT_LIST_HEAD(& bo->va);
  bo->initial_domain = domain & 63U;
  bo->flags = flags;
  amdgpu_fill_placement_to_bo(bo, placement);
  r = ttm_bo_init(& adev->mman.bdev, & bo->tbo, size, type, & bo->placement, (u32 )page_align,
                  (int )((bool )(! ((int )kernel != 0))), (struct file *)0, acc_size,
                  sg, (struct reservation_object *)0, & amdgpu_ttm_bo_destroy);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    return (r);
  } else {
  }
  *bo_ptr = bo;
  trace_amdgpu_bo_create(bo);
  return (0);
}
}
int amdgpu_bo_create(struct amdgpu_device *adev , unsigned long size , int byte_align ,
                     bool kernel , u32 domain , u64 flags , struct sg_table *sg ,
                     struct amdgpu_bo **bo_ptr )
{
  struct ttm_placement placement ;
  struct ttm_place placements[4U] ;
  int tmp ;
  {
  placement.num_placement = 0U;
  placement.placement = 0;
  placement.num_busy_placement = 0U;
  placement.busy_placement = 0;
  memset((void *)(& placements), 0, 48UL);
  amdgpu_ttm_placement_init(adev, & placement, (struct ttm_place *)(& placements),
                            domain, flags);
  tmp = amdgpu_bo_create_restricted(adev, size, byte_align, (int )kernel, domain,
                                    flags, sg, & placement, bo_ptr);
  return (tmp);
}
}
int amdgpu_bo_kmap(struct amdgpu_bo *bo , void **ptr )
{
  bool is_iomem ;
  int r ;
  {
  if ((bo->flags & 2ULL) != 0ULL) {
    return (-1);
  } else {
  }
  if ((unsigned long )bo->kptr != (unsigned long )((void *)0)) {
    if ((unsigned long )ptr != (unsigned long )((void **)0)) {
      *ptr = bo->kptr;
    } else {
    }
    return (0);
  } else {
  }
  r = ttm_bo_kmap(& bo->tbo, 0UL, bo->tbo.num_pages, & bo->kmap);
  if (r != 0) {
    return (r);
  } else {
  }
  bo->kptr = ttm_kmap_obj_virtual(& bo->kmap, & is_iomem);
  if ((unsigned long )ptr != (unsigned long )((void **)0)) {
    *ptr = bo->kptr;
  } else {
  }
  return (0);
}
}
void amdgpu_bo_kunmap(struct amdgpu_bo *bo )
{
  {
  if ((unsigned long )bo->kptr == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  bo->kptr = (void *)0;
  ttm_bo_kunmap(& bo->kmap);
  return;
}
}
struct amdgpu_bo *amdgpu_bo_ref(struct amdgpu_bo *bo )
{
  {
  if ((unsigned long )bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return ((struct amdgpu_bo *)0);
  } else {
  }
  ttm_bo_reference(& bo->tbo);
  return (bo);
}
}
void amdgpu_bo_unref(struct amdgpu_bo **bo )
{
  struct ttm_buffer_object *tbo ;
  {
  if ((unsigned long )*bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return;
  } else {
  }
  tbo = & (*bo)->tbo;
  ttm_bo_unref(& tbo);
  if ((unsigned long )tbo == (unsigned long )((struct ttm_buffer_object *)0)) {
    *bo = (struct amdgpu_bo *)0;
  } else {
  }
  return;
}
}
int amdgpu_bo_pin_restricted(struct amdgpu_bo *bo , u32 domain , u64 min_offset ,
                             u64 max_offset , u64 *gpu_addr )
{
  int r ;
  int i ;
  unsigned int fpfn ;
  unsigned int lpfn ;
  bool tmp ;
  bool __warned ;
  int __ret_warn_once ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  u64 domain_start ;
  bool __warned___0 ;
  int __ret_warn_once___0 ;
  u64 tmp___4 ;
  int __ret_warn_on___0 ;
  long tmp___5 ;
  long tmp___6 ;
  long tmp___7 ;
  bool __warned___1 ;
  int __ret_warn_once___1 ;
  int __ret_warn_on___1 ;
  long tmp___8 ;
  long tmp___9 ;
  long tmp___10 ;
  long tmp___11 ;
  unsigned long tmp___12 ;
  unsigned long tmp___13 ;
  long tmp___14 ;
  {
  tmp = amdgpu_ttm_tt_has_userptr(bo->tbo.ttm);
  if ((int )tmp) {
    return (-1);
  } else {
  }
  __ret_warn_once = min_offset > max_offset;
  tmp___2 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___2 != 0L) {
    __ret_warn_on = ! __warned;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c",
                         373);
    } else {
    }
    tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___1 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  tmp___3 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___3 != 0L) {
    return (-22);
  } else {
  }
  if (bo->pin_count != 0U) {
    bo->pin_count = bo->pin_count + 1U;
    if ((unsigned long )gpu_addr != (unsigned long )((u64 *)0ULL)) {
      *gpu_addr = amdgpu_bo_gpu_offset(bo);
    } else {
    }
    if (max_offset != 0ULL) {
      if (domain == 4U) {
        domain_start = (bo->adev)->mc.vram_start;
      } else {
        domain_start = (bo->adev)->mc.gtt_start;
      }
      tmp___4 = amdgpu_bo_gpu_offset(bo);
      __ret_warn_once___0 = tmp___4 - domain_start > max_offset;
      tmp___7 = ldv__builtin_expect(__ret_warn_once___0 != 0, 0L);
      if (tmp___7 != 0L) {
        __ret_warn_on___0 = ! __warned___0;
        tmp___5 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
        if (tmp___5 != 0L) {
          warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c",
                             388);
        } else {
        }
        tmp___6 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
        if (tmp___6 != 0L) {
          __warned___0 = 1;
        } else {
        }
      } else {
      }
      ldv__builtin_expect(__ret_warn_once___0 != 0, 0L);
    } else {
    }
    return (0);
  } else {
  }
  amdgpu_ttm_placement_from_domain(bo, domain);
  i = 0;
  goto ldv_44616;
  ldv_44615: ;
  if (((bo->placements[i].flags & 4U) != 0U && (bo->flags & 2ULL) == 0ULL) && (max_offset == 0ULL || (bo->adev)->mc.visible_vram_size < max_offset)) {
    __ret_warn_once___1 = (bo->adev)->mc.visible_vram_size < min_offset;
    tmp___10 = ldv__builtin_expect(__ret_warn_once___1 != 0, 0L);
    if (tmp___10 != 0L) {
      __ret_warn_on___1 = ! __warned___1;
      tmp___8 = ldv__builtin_expect(__ret_warn_on___1 != 0, 0L);
      if (tmp___8 != 0L) {
        warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c",
                           400);
      } else {
      }
      tmp___9 = ldv__builtin_expect(__ret_warn_on___1 != 0, 0L);
      if (tmp___9 != 0L) {
        __warned___1 = 1;
      } else {
      }
    } else {
    }
    tmp___11 = ldv__builtin_expect(__ret_warn_once___1 != 0, 0L);
    if (tmp___11 != 0L) {
      return (-22);
    } else {
    }
    fpfn = (unsigned int )(min_offset >> 12);
    lpfn = (unsigned int )((bo->adev)->mc.visible_vram_size >> 12);
  } else {
    fpfn = (unsigned int )(min_offset >> 12);
    lpfn = (unsigned int )(max_offset >> 12);
  }
  if (bo->placements[i].fpfn < fpfn) {
    bo->placements[i].fpfn = fpfn;
  } else {
  }
  if (lpfn != 0U && bo->placements[i].lpfn > lpfn) {
    bo->placements[i].lpfn = lpfn;
  } else {
  }
  bo->placements[i].flags = bo->placements[i].flags | 2097152U;
  i = i + 1;
  ldv_44616: ;
  if ((unsigned int )i < bo->placement.num_placement) {
    goto ldv_44615;
  } else {
  }
  r = ttm_bo_validate(& bo->tbo, & bo->placement, 0, 0);
  tmp___14 = ldv__builtin_expect(r == 0, 1L);
  if (tmp___14 != 0L) {
    bo->pin_count = 1U;
    if ((unsigned long )gpu_addr != (unsigned long )((u64 *)0ULL)) {
      *gpu_addr = amdgpu_bo_gpu_offset(bo);
    } else {
    }
    if (domain == 4U) {
      tmp___12 = amdgpu_bo_size(bo);
      (bo->adev)->vram_pin_size = (bo->adev)->vram_pin_size + (unsigned long long )tmp___12;
    } else {
      tmp___13 = amdgpu_bo_size(bo);
      (bo->adev)->gart_pin_size = (bo->adev)->gart_pin_size + (unsigned long long )tmp___13;
    }
  } else {
    dev_err((struct device const *)(bo->adev)->dev, "%p pin failed\n", bo);
  }
  return (r);
}
}
int amdgpu_bo_pin(struct amdgpu_bo *bo , u32 domain , u64 *gpu_addr )
{
  int tmp ;
  {
  tmp = amdgpu_bo_pin_restricted(bo, domain, 0ULL, 0ULL, gpu_addr);
  return (tmp);
}
}
int amdgpu_bo_unpin(struct amdgpu_bo *bo )
{
  int r ;
  int i ;
  unsigned long tmp ;
  unsigned long tmp___0 ;
  long tmp___1 ;
  {
  if (bo->pin_count == 0U) {
    dev_warn((struct device const *)(bo->adev)->dev, "%p unpin not necessary\n",
             bo);
    return (0);
  } else {
  }
  bo->pin_count = bo->pin_count - 1U;
  if (bo->pin_count != 0U) {
    return (0);
  } else {
  }
  i = 0;
  goto ldv_44629;
  ldv_44628:
  bo->placements[i].lpfn = 0U;
  bo->placements[i].flags = bo->placements[i].flags & 4292870143U;
  i = i + 1;
  ldv_44629: ;
  if ((unsigned int )i < bo->placement.num_placement) {
    goto ldv_44628;
  } else {
  }
  r = ttm_bo_validate(& bo->tbo, & bo->placement, 0, 0);
  tmp___1 = ldv__builtin_expect(r == 0, 1L);
  if (tmp___1 != 0L) {
    if (bo->tbo.mem.mem_type == 2U) {
      tmp = amdgpu_bo_size(bo);
      (bo->adev)->vram_pin_size = (bo->adev)->vram_pin_size - (unsigned long long )tmp;
    } else {
      tmp___0 = amdgpu_bo_size(bo);
      (bo->adev)->gart_pin_size = (bo->adev)->gart_pin_size - (unsigned long long )tmp___0;
    }
  } else {
    dev_err((struct device const *)(bo->adev)->dev, "%p validate failed for unpin\n",
            bo);
  }
  return (r);
}
}
int amdgpu_bo_evict_vram(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = ttm_bo_evict_mm(& adev->mman.bdev, 2U);
  return (tmp);
}
}
void amdgpu_bo_force_delete(struct amdgpu_device *adev )
{
  struct amdgpu_bo *bo ;
  struct amdgpu_bo *n ;
  int tmp ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  tmp = list_empty((struct list_head const *)(& adev->gem.objects));
  if (tmp != 0) {
    return;
  } else {
  }
  dev_err((struct device const *)adev->dev, "Userspace still has active objects !\n");
  __mptr = (struct list_head const *)adev->gem.objects.next;
  bo = (struct amdgpu_bo *)__mptr;
  __mptr___0 = (struct list_head const *)bo->list.next;
  n = (struct amdgpu_bo *)__mptr___0;
  goto ldv_44646;
  ldv_44645:
  mutex_lock_nested(& (adev->ddev)->struct_mutex, 0U);
  dev_err((struct device const *)adev->dev, "%p %p %lu %lu force free\n", & bo->gem_base,
          bo, bo->gem_base.size, *((unsigned long *)(& bo->gem_base.refcount)));
  mutex_lock_nested(& (bo->adev)->gem.mutex, 0U);
  list_del_init(& bo->list);
  mutex_unlock(& (bo->adev)->gem.mutex);
  drm_gem_object_unreference(& bo->gem_base);
  mutex_unlock(& (adev->ddev)->struct_mutex);
  bo = n;
  __mptr___1 = (struct list_head const *)n->list.next;
  n = (struct amdgpu_bo *)__mptr___1;
  ldv_44646: ;
  if ((unsigned long )(& bo->list) != (unsigned long )(& adev->gem.objects)) {
    goto ldv_44645;
  } else {
  }
  return;
}
}
int amdgpu_bo_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  adev->mc.vram_mtrr = arch_phys_wc_add((unsigned long )adev->mc.aper_base, (unsigned long )adev->mc.aper_size);
  printk("\016[drm] Detected VRAM RAM=%lluM, BAR=%lluM\n", adev->mc.mc_vram_size >> 20,
         adev->mc.aper_size >> 20);
  printk("\016[drm] RAM width %dbits DDR\n", adev->mc.vram_width);
  tmp = amdgpu_ttm_init(adev);
  return (tmp);
}
}
void amdgpu_bo_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_ttm_fini(adev);
  arch_phys_wc_del(adev->mc.vram_mtrr);
  return;
}
}
int amdgpu_bo_fbdev_mmap(struct amdgpu_bo *bo , struct vm_area_struct *vma )
{
  int tmp ;
  {
  tmp = ttm_fbdev_mmap(vma, & bo->tbo);
  return (tmp);
}
}
int amdgpu_bo_set_tiling_flags(struct amdgpu_bo *bo , u64 tiling_flags )
{
  {
  if (((tiling_flags >> 9) & 7ULL) > 6ULL) {
    return (-22);
  } else {
  }
  bo->tiling_flags = tiling_flags;
  return (0);
}
}
void amdgpu_bo_get_tiling_flags(struct amdgpu_bo *bo , u64 *tiling_flags )
{
  int __ret_warn_on ;
  int tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  if (debug_locks != 0) {
    tmp = lock_is_held(& (bo->tbo.resv)->lock.base.dep_map);
    if (tmp == 0) {
      tmp___0 = 1;
    } else {
      tmp___0 = 0;
    }
  } else {
    tmp___0 = 0;
  }
  __ret_warn_on = tmp___0;
  tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_object.c",
                       530);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if ((unsigned long )tiling_flags != (unsigned long )((u64 *)0ULL)) {
    *tiling_flags = bo->tiling_flags;
  } else {
  }
  return;
}
}
int amdgpu_bo_set_metadata(struct amdgpu_bo *bo , void *metadata , u32 metadata_size ,
                           uint64_t flags )
{
  void *buffer ;
  {
  if (metadata_size == 0U) {
    if (bo->metadata_size != 0U) {
      kfree((void const *)bo->metadata);
      bo->metadata_size = 0U;
    } else {
    }
    return (0);
  } else {
  }
  if ((unsigned long )metadata == (unsigned long )((void *)0)) {
    return (-22);
  } else {
  }
  buffer = kzalloc((size_t )metadata_size, 208U);
  if ((unsigned long )buffer == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  memcpy(buffer, (void const *)metadata, (size_t )metadata_size);
  kfree((void const *)bo->metadata);
  bo->metadata_flags = flags;
  bo->metadata = buffer;
  bo->metadata_size = metadata_size;
  return (0);
}
}
int amdgpu_bo_get_metadata(struct amdgpu_bo *bo , void *buffer , size_t buffer_size ,
                           u32 *metadata_size , uint64_t *flags )
{
  {
  if ((unsigned long )buffer == (unsigned long )((void *)0) && (unsigned long )metadata_size == (unsigned long )((u32 *)0U)) {
    return (-22);
  } else {
  }
  if ((unsigned long )buffer != (unsigned long )((void *)0)) {
    if ((size_t )bo->metadata_size > buffer_size) {
      return (-22);
    } else {
    }
    if (bo->metadata_size != 0U) {
      memcpy(buffer, (void const *)bo->metadata, (size_t )bo->metadata_size);
    } else {
    }
  } else {
  }
  if ((unsigned long )metadata_size != (unsigned long )((u32 *)0U)) {
    *metadata_size = bo->metadata_size;
  } else {
  }
  if ((unsigned long )flags != (unsigned long )((uint64_t *)0ULL)) {
    *flags = bo->metadata_flags;
  } else {
  }
  return (0);
}
}
void amdgpu_bo_move_notify(struct ttm_buffer_object *bo , struct ttm_mem_reg *new_mem )
{
  struct amdgpu_bo *rbo ;
  bool tmp ;
  int tmp___0 ;
  struct ttm_buffer_object const *__mptr ;
  {
  tmp = amdgpu_ttm_bo_is_amdgpu_bo(bo);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  __mptr = (struct ttm_buffer_object const *)bo;
  rbo = (struct amdgpu_bo *)__mptr + 0xffffffffffffff98UL;
  amdgpu_vm_bo_invalidate(rbo->adev, rbo);
  if ((unsigned long )new_mem == (unsigned long )((struct ttm_mem_reg *)0)) {
    return;
  } else {
  }
  amdgpu_update_memory_usage(rbo->adev, & bo->mem, new_mem);
  return;
}
}
int amdgpu_bo_fault_reserve_notify(struct ttm_buffer_object *bo )
{
  struct amdgpu_device *adev ;
  struct amdgpu_bo *abo ;
  unsigned long offset ;
  unsigned long size ;
  unsigned long lpfn ;
  int i ;
  int r ;
  bool tmp ;
  int tmp___0 ;
  struct ttm_buffer_object const *__mptr ;
  int tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  {
  tmp = amdgpu_ttm_bo_is_amdgpu_bo(bo);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (0);
  } else {
  }
  __mptr = (struct ttm_buffer_object const *)bo;
  abo = (struct amdgpu_bo *)__mptr + 0xffffffffffffff98UL;
  adev = abo->adev;
  if (bo->mem.mem_type != 2U) {
    return (0);
  } else {
  }
  size = bo->mem.num_pages << 12;
  offset = bo->mem.start << 12;
  if ((unsigned long long )(offset + size) <= adev->mc.visible_vram_size) {
    return (0);
  } else {
  }
  amdgpu_ttm_placement_from_domain(abo, 4U);
  lpfn = (unsigned long )(adev->mc.visible_vram_size >> 12);
  i = 0;
  goto ldv_44702;
  ldv_44701: ;
  if ((abo->placements[i].flags & 4U) != 0U && (abo->placements[i].lpfn == 0U || (unsigned long )abo->placements[i].lpfn > lpfn)) {
    abo->placements[i].lpfn = (unsigned int )lpfn;
  } else {
  }
  i = i + 1;
  ldv_44702: ;
  if ((unsigned int )i < abo->placement.num_placement) {
    goto ldv_44701;
  } else {
  }
  r = ttm_bo_validate(bo, & abo->placement, 0, 0);
  tmp___3 = ldv__builtin_expect(r == -12, 0L);
  if (tmp___3 != 0L) {
    amdgpu_ttm_placement_from_domain(abo, 2U);
    tmp___1 = ttm_bo_validate(bo, & abo->placement, 0, 0);
    return (tmp___1);
  } else {
    tmp___2 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___2 != 0L) {
      return (r);
    } else {
    }
  }
  offset = bo->mem.start << 12;
  if ((unsigned long long )(offset + size) > adev->mc.visible_vram_size) {
    return (-22);
  } else {
  }
  return (0);
}
}
void amdgpu_bo_fence(struct amdgpu_bo *bo , struct amdgpu_fence *fence , bool shared )
{
  struct reservation_object *resv ;
  {
  resv = bo->tbo.resv;
  if ((int )shared) {
    reservation_object_add_shared_fence(resv, & fence->base);
  } else {
    reservation_object_add_excl_fence(resv, & fence->base);
  }
  return;
}
}
bool ldv_queue_work_on_131(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_132(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_133(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_134(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_135(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_145(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_147(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_146(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_149(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_148(struct workqueue_struct *ldv_func_arg1 ) ;
extern void *dma_alloc_attrs(struct device * , size_t , dma_addr_t * , gfp_t , struct dma_attrs * ) ;
extern void dma_free_attrs(struct device * , size_t , void * , dma_addr_t , struct dma_attrs * ) ;
__inline static void *pci_alloc_consistent(struct pci_dev *hwdev , size_t size , dma_addr_t *dma_handle )
{
  void *tmp ;
  {
  tmp = dma_alloc_attrs((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                        size, dma_handle, 32U, (struct dma_attrs *)0);
  return (tmp);
}
}
__inline static void pci_free_consistent(struct pci_dev *hwdev , size_t size , void *vaddr ,
                                         dma_addr_t dma_handle )
{
  {
  dma_free_attrs((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                 size, vaddr, dma_handle, (struct dma_attrs *)0);
  return;
}
}
extern void *vzalloc(unsigned long ) ;
extern void vfree(void const * ) ;
int amdgpu_gart_table_ram_alloc(struct amdgpu_device *adev ) ;
void amdgpu_gart_table_ram_free(struct amdgpu_device *adev ) ;
int amdgpu_gart_table_vram_alloc(struct amdgpu_device *adev ) ;
void amdgpu_gart_table_vram_free(struct amdgpu_device *adev ) ;
int amdgpu_gart_table_vram_pin(struct amdgpu_device *adev ) ;
void amdgpu_gart_table_vram_unpin(struct amdgpu_device *adev ) ;
int amdgpu_gart_init(struct amdgpu_device *adev ) ;
int amdgpu_gart_table_ram_alloc(struct amdgpu_device *adev )
{
  void *ptr ;
  {
  ptr = pci_alloc_consistent(adev->pdev, (size_t )adev->gart.table_size, & adev->gart.table_addr);
  if ((unsigned long )ptr == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  adev->gart.ptr = ptr;
  memset(adev->gart.ptr, 0, (size_t )adev->gart.table_size);
  return (0);
}
}
void amdgpu_gart_table_ram_free(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->gart.ptr == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  pci_free_consistent(adev->pdev, (size_t )adev->gart.table_size, adev->gart.ptr,
                      adev->gart.table_addr);
  adev->gart.ptr = (void *)0;
  adev->gart.table_addr = 0ULL;
  return;
}
}
int amdgpu_gart_table_vram_alloc(struct amdgpu_device *adev )
{
  int r ;
  {
  if ((unsigned long )adev->gart.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, (unsigned long )adev->gart.table_size, 4096, 1, 4U,
                         0ULL, (struct sg_table *)0, & adev->gart.robj);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  return (0);
}
}
int amdgpu_gart_table_vram_pin(struct amdgpu_device *adev )
{
  uint64_t gpu_addr ;
  int r ;
  long tmp ;
  {
  r = amdgpu_bo_reserve(adev->gart.robj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->gart.robj, 4U, & gpu_addr);
  if (r != 0) {
    amdgpu_bo_unreserve(adev->gart.robj);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->gart.robj, & adev->gart.ptr);
  if (r != 0) {
    amdgpu_bo_unpin(adev->gart.robj);
  } else {
  }
  amdgpu_bo_unreserve(adev->gart.robj);
  adev->gart.table_addr = gpu_addr;
  return (r);
}
}
void amdgpu_gart_table_vram_unpin(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->gart.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    return;
  } else {
  }
  r = amdgpu_bo_reserve(adev->gart.robj, 0);
  tmp = ldv__builtin_expect(r == 0, 1L);
  if (tmp != 0L) {
    amdgpu_bo_kunmap(adev->gart.robj);
    amdgpu_bo_unpin(adev->gart.robj);
    amdgpu_bo_unreserve(adev->gart.robj);
    adev->gart.ptr = (void *)0;
  } else {
  }
  return;
}
}
void amdgpu_gart_table_vram_free(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->gart.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    return;
  } else {
  }
  amdgpu_bo_unref(& adev->gart.robj);
  return;
}
}
void amdgpu_gart_unbind(struct amdgpu_device *adev , unsigned int offset , int pages )
{
  unsigned int t ;
  unsigned int p ;
  int i ;
  int j ;
  u64 page_base ;
  u32 flags ;
  int __ret_warn_on ;
  long tmp ;
  {
  flags = 2U;
  if (! adev->gart.ready) {
    __ret_warn_on = 1;
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c",
                        233, "trying to unbind memory from uninitialized GART !\n");
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
    return;
  } else {
  }
  t = offset / 4096U;
  p = t;
  i = 0;
  goto ldv_43681;
  ldv_43680: ;
  if ((unsigned long )*(adev->gart.pages + (unsigned long )p) != (unsigned long )((struct page *)0)) {
    *(adev->gart.pages + (unsigned long )p) = (struct page *)0;
    *(adev->gart.pages_addr + (unsigned long )p) = adev->dummy_page.addr;
    page_base = *(adev->gart.pages_addr + (unsigned long )p);
    if ((unsigned long )adev->gart.ptr == (unsigned long )((void *)0)) {
      goto ldv_43676;
    } else {
    }
    j = 0;
    goto ldv_43678;
    ldv_43677:
    (*((adev->gart.gart_funcs)->set_pte_pde))(adev, adev->gart.ptr, t, page_base,
                                              flags);
    page_base = page_base + 4096ULL;
    j = j + 1;
    t = t + 1U;
    ldv_43678: ;
    if (j == 0) {
      goto ldv_43677;
    } else {
    }
  } else {
  }
  ldv_43676:
  i = i + 1;
  p = p + 1U;
  ldv_43681: ;
  if (i < pages) {
    goto ldv_43680;
  } else {
  }
  __asm__ volatile ("mfence": : : "memory");
  (*((adev->gart.gart_funcs)->flush_gpu_tlb))(adev, 0U);
  return;
}
}
int amdgpu_gart_bind(struct amdgpu_device *adev , unsigned int offset , int pages ,
                     struct page **pagelist , dma_addr_t *dma_addr , u32 flags )
{
  unsigned int t ;
  unsigned int p ;
  uint64_t page_base ;
  int i ;
  int j ;
  int __ret_warn_on ;
  long tmp ;
  {
  if (! adev->gart.ready) {
    __ret_warn_on = 1;
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_gart.c",
                        281, "trying to bind memory to uninitialized GART !\n");
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
    return (-22);
  } else {
  }
  t = offset / 4096U;
  p = t;
  i = 0;
  goto ldv_43702;
  ldv_43701:
  *(adev->gart.pages_addr + (unsigned long )p) = *(dma_addr + (unsigned long )i);
  *(adev->gart.pages + (unsigned long )p) = *(pagelist + (unsigned long )i);
  if ((unsigned long )adev->gart.ptr != (unsigned long )((void *)0)) {
    page_base = *(adev->gart.pages_addr + (unsigned long )p);
    j = 0;
    goto ldv_43699;
    ldv_43698:
    (*((adev->gart.gart_funcs)->set_pte_pde))(adev, adev->gart.ptr, t, page_base,
                                              flags);
    page_base = page_base + 4096ULL;
    j = j + 1;
    t = t + 1U;
    ldv_43699: ;
    if (j == 0) {
      goto ldv_43698;
    } else {
    }
  } else {
  }
  i = i + 1;
  p = p + 1U;
  ldv_43702: ;
  if (i < pages) {
    goto ldv_43701;
  } else {
  }
  __asm__ volatile ("mfence": : : "memory");
  (*((adev->gart.gart_funcs)->flush_gpu_tlb))(adev, 0U);
  return (0);
}
}
int amdgpu_gart_init(struct amdgpu_device *adev )
{
  int r ;
  int i ;
  void *tmp ;
  void *tmp___0 ;
  {
  if ((unsigned long )adev->gart.pages != (unsigned long )((struct page **)0)) {
    return (0);
  } else {
  }
  r = amdgpu_dummy_page_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gart.num_cpu_pages = (unsigned int )(adev->mc.gtt_size / 4096ULL);
  adev->gart.num_gpu_pages = (unsigned int )(adev->mc.gtt_size / 4096ULL);
  printk("\016[drm] GART: num cpu pages %u, num gpu pages %u\n", adev->gart.num_cpu_pages,
         adev->gart.num_gpu_pages);
  tmp = vzalloc((unsigned long )adev->gart.num_cpu_pages * 8UL);
  adev->gart.pages = (struct page **)tmp;
  if ((unsigned long )adev->gart.pages == (unsigned long )((struct page **)0)) {
    amdgpu_gart_fini(adev);
    return (-12);
  } else {
  }
  tmp___0 = vzalloc((unsigned long )adev->gart.num_cpu_pages * 8UL);
  adev->gart.pages_addr = (dma_addr_t *)tmp___0;
  if ((unsigned long )adev->gart.pages_addr == (unsigned long )((dma_addr_t *)0ULL)) {
    amdgpu_gart_fini(adev);
    return (-12);
  } else {
  }
  i = 0;
  goto ldv_43710;
  ldv_43709:
  *(adev->gart.pages_addr + (unsigned long )i) = adev->dummy_page.addr;
  i = i + 1;
  ldv_43710: ;
  if ((unsigned int )i < adev->gart.num_cpu_pages) {
    goto ldv_43709;
  } else {
  }
  return (0);
}
}
void amdgpu_gart_fini(struct amdgpu_device *adev )
{
  {
  if (((unsigned long )adev->gart.pages != (unsigned long )((struct page **)0) && (unsigned long )adev->gart.pages_addr != (unsigned long )((dma_addr_t *)0ULL)) && (int )adev->gart.ready) {
    amdgpu_gart_unbind(adev, 0U, (int )adev->gart.num_cpu_pages);
  } else {
  }
  adev->gart.ready = 0;
  vfree((void const *)adev->gart.pages);
  vfree((void const *)adev->gart.pages_addr);
  adev->gart.pages = (struct page **)0;
  adev->gart.pages_addr = (dma_addr_t *)0ULL;
  amdgpu_dummy_page_fini(adev);
  return;
}
}
bool ldv_queue_work_on_145(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_146(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_147(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_148(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_149(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_159(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_161(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_160(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_163(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_162(struct workqueue_struct *ldv_func_arg1 ) ;
extern int drm_mode_connector_attach_encoder(struct drm_connector * , struct drm_encoder * ) ;
struct drm_connector *amdgpu_get_connector_for_encoder_init(struct drm_encoder *encoder ) ;
struct drm_encoder *amdgpu_get_external_encoder(struct drm_encoder *encoder ) ;
void amdgpu_encoder_set_active_device(struct drm_encoder *encoder ) ;
void amdgpu_panel_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode *adjusted_mode ) ;
void amdgpu_atombios_encoder_init_backlight(struct amdgpu_encoder *amdgpu_encoder ,
                                            struct drm_connector *drm_connector ) ;
void amdgpu_link_encoder_connector(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct list_head const *__mptr ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct drm_encoder const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  struct list_head const *__mptr___4 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_47848;
  ldv_47847:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  __mptr___1 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  goto ldv_47845;
  ldv_47844:
  __mptr___2 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___2;
  if ((amdgpu_encoder->devices & amdgpu_connector->devices) != 0U) {
    drm_mode_connector_attach_encoder(connector, encoder);
    if (((long )amdgpu_encoder->devices & 34L) != 0L) {
      amdgpu_atombios_encoder_init_backlight(amdgpu_encoder, connector);
      adev->mode_info.bl_encoder = amdgpu_encoder;
    } else {
    }
  } else {
  }
  __mptr___3 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___3 + 0xfffffffffffffff8UL;
  ldv_47845: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_47844;
  } else {
  }
  __mptr___4 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___4 + 0xffffffffffffffe8UL;
  ldv_47848: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_47847;
  } else {
  }
  return;
}
}
void amdgpu_encoder_set_active_device(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct list_head const *__mptr___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  long tmp ;
  struct list_head const *__mptr___2 ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_47867;
  ldv_47866: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    amdgpu_encoder->active_device = amdgpu_encoder->devices & amdgpu_connector->devices;
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_encoder_set_active_device", "setting active device to %08x from %08x %08x for encoder %d\n",
                          amdgpu_encoder->active_device, amdgpu_encoder->devices,
                          amdgpu_connector->devices, encoder->encoder_type);
    } else {
    }
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_47867: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_47866;
  } else {
  }
  return;
}
}
struct drm_connector *amdgpu_get_connector_for_encoder(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_47885;
  ldv_47884:
  __mptr___1 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
  if ((amdgpu_encoder->active_device & amdgpu_connector->devices) != 0U) {
    return (connector);
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_47885: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_47884;
  } else {
  }
  return ((struct drm_connector *)0);
}
}
struct drm_connector *amdgpu_get_connector_for_encoder_init(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_47903;
  ldv_47902:
  __mptr___1 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
  if ((amdgpu_encoder->devices & amdgpu_connector->devices) != 0U) {
    return (connector);
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_47903: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_47902;
  } else {
  }
  return ((struct drm_connector *)0);
}
}
struct drm_encoder *amdgpu_get_external_encoder(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_encoder *other_encoder ;
  struct amdgpu_encoder *other_amdgpu_encoder ;
  struct list_head const *__mptr___0 ;
  struct drm_encoder const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  dev = encoder->dev;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if ((int )amdgpu_encoder->is_ext_encoder) {
    return ((struct drm_encoder *)0);
  } else {
  }
  __mptr___0 = (struct list_head const *)dev->mode_config.encoder_list.next;
  other_encoder = (struct drm_encoder *)__mptr___0 + 0xfffffffffffffff8UL;
  goto ldv_47922;
  ldv_47921: ;
  if ((unsigned long )other_encoder == (unsigned long )encoder) {
    goto ldv_47918;
  } else {
  }
  __mptr___1 = (struct drm_encoder const *)other_encoder;
  other_amdgpu_encoder = (struct amdgpu_encoder *)__mptr___1;
  if ((int )other_amdgpu_encoder->is_ext_encoder && (amdgpu_encoder->devices & other_amdgpu_encoder->devices) != 0U) {
    return (other_encoder);
  } else {
  }
  ldv_47918:
  __mptr___2 = (struct list_head const *)other_encoder->head.next;
  other_encoder = (struct drm_encoder *)__mptr___2 + 0xfffffffffffffff8UL;
  ldv_47922: ;
  if ((unsigned long )(& other_encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_47921;
  } else {
  }
  return ((struct drm_encoder *)0);
}
}
u16 amdgpu_encoder_get_dp_bridge_encoder_id(struct drm_encoder *encoder )
{
  struct drm_encoder *other_encoder ;
  struct drm_encoder *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  {
  tmp = amdgpu_get_external_encoder(encoder);
  other_encoder = tmp;
  if ((unsigned long )other_encoder != (unsigned long )((struct drm_encoder *)0)) {
    __mptr = (struct drm_encoder const *)other_encoder;
    amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
    switch (amdgpu_encoder->encoder_id) {
    case 35U: ;
    case 34U: ;
    return ((u16 )amdgpu_encoder->encoder_id);
    default: ;
    return (0U);
    }
  } else {
  }
  return (0U);
}
}
void amdgpu_panel_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_display_mode *native_mode ;
  unsigned int hblank ;
  unsigned int vblank ;
  unsigned int hover ;
  unsigned int vover ;
  unsigned int hsync_width ;
  unsigned int vsync_width ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  native_mode = & amdgpu_encoder->native_mode;
  hblank = (unsigned int )(native_mode->htotal - native_mode->hdisplay);
  vblank = (unsigned int )(native_mode->vtotal - native_mode->vdisplay);
  hover = (unsigned int )(native_mode->hsync_start - native_mode->hdisplay);
  vover = (unsigned int )(native_mode->vsync_start - native_mode->vdisplay);
  hsync_width = (unsigned int )(native_mode->hsync_end - native_mode->hsync_start);
  vsync_width = (unsigned int )(native_mode->vsync_end - native_mode->vsync_start);
  adjusted_mode->clock = native_mode->clock;
  adjusted_mode->flags = native_mode->flags;
  adjusted_mode->hdisplay = native_mode->hdisplay;
  adjusted_mode->vdisplay = native_mode->vdisplay;
  adjusted_mode->htotal = (int )((unsigned int )native_mode->hdisplay + hblank);
  adjusted_mode->hsync_start = (int )((unsigned int )native_mode->hdisplay + hover);
  adjusted_mode->hsync_end = (int )((unsigned int )adjusted_mode->hsync_start + hsync_width);
  adjusted_mode->vtotal = (int )((unsigned int )native_mode->vdisplay + vblank);
  adjusted_mode->vsync_start = (int )((unsigned int )native_mode->vdisplay + vover);
  adjusted_mode->vsync_end = (int )((unsigned int )adjusted_mode->vsync_start + vsync_width);
  drm_mode_set_crtcinfo(adjusted_mode, 1);
  adjusted_mode->crtc_hdisplay = native_mode->hdisplay;
  adjusted_mode->crtc_vdisplay = native_mode->vdisplay;
  adjusted_mode->crtc_htotal = (int )((unsigned int )adjusted_mode->crtc_hdisplay + hblank);
  adjusted_mode->crtc_hsync_start = (int )((unsigned int )adjusted_mode->crtc_hdisplay + hover);
  adjusted_mode->crtc_hsync_end = (int )((unsigned int )adjusted_mode->crtc_hsync_start + hsync_width);
  adjusted_mode->crtc_vtotal = (int )((unsigned int )adjusted_mode->crtc_vdisplay + vblank);
  adjusted_mode->crtc_vsync_start = (int )((unsigned int )adjusted_mode->crtc_vdisplay + vover);
  adjusted_mode->crtc_vsync_end = (int )((unsigned int )adjusted_mode->crtc_vsync_start + vsync_width);
  return;
}
}
bool amdgpu_dig_monitor_is_duallink(struct drm_encoder *encoder , u32 pixel_clock )
{
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  struct drm_connector const *__mptr ;
  struct edid *tmp ;
  bool tmp___0 ;
  struct edid *tmp___1 ;
  bool tmp___2 ;
  {
  connector = amdgpu_get_connector_for_encoder(encoder);
  if ((unsigned long )connector == (unsigned long )((struct drm_connector *)0)) {
    connector = amdgpu_get_connector_for_encoder_init(encoder);
  } else {
  }
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  switch (connector->connector_type) {
  case 2: ;
  case 12: ;
  if ((int )amdgpu_connector->use_digital) {
    tmp = amdgpu_connector_edid(connector);
    tmp___0 = drm_detect_hdmi_monitor(tmp);
    if ((int )tmp___0) {
      if (pixel_clock > 340000U) {
        return (1);
      } else {
        return (0);
      }
    } else
    if (pixel_clock > 165000U) {
      return (1);
    } else {
      return (0);
    }
  } else {
    return (0);
  }
  case 3: ;
  case 11: ;
  case 10:
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dp_sink_type == 19U || (unsigned int )dig_connector->dp_sink_type == 20U) {
    return (0);
  } else {
    tmp___1 = amdgpu_connector_edid(connector);
    tmp___2 = drm_detect_hdmi_monitor(tmp___1);
    if ((int )tmp___2) {
      if (pixel_clock > 340000U) {
        return (1);
      } else {
        return (0);
      }
    } else
    if (pixel_clock > 165000U) {
      return (1);
    } else {
      return (0);
    }
  }
  default: ;
  return (0);
  }
}
}
bool ldv_queue_work_on_159(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_160(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_161(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_162(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_163(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
void *ldv_err_ptr(long error ) ;
__inline static void *ERR_PTR(long error ) ;
extern void __cmpxchg_wrong_size(void) ;
__inline static int atomic_dec_and_test(atomic_t *v )
{
  char c ;
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; decl %0; sete %1": "+m" (v->counter),
                       "=qm" (c): : "memory");
  return ((int )((signed char )c) != 0);
}
}
__inline static int atomic_cmpxchg(atomic_t *v , int old , int new )
{
  int __ret ;
  int __old ;
  int __new ;
  u8 volatile *__ptr ;
  u16 volatile *__ptr___0 ;
  u32 volatile *__ptr___1 ;
  u64 volatile *__ptr___2 ;
  {
  __old = old;
  __new = new;
  switch (4UL) {
  case 1UL:
  __ptr = (u8 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgb %2,%1": "=a" (__ret),
                       "+m" (*__ptr): "q" (__new), "0" (__old): "memory");
  goto ldv_5679;
  case 2UL:
  __ptr___0 = (u16 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgw %2,%1": "=a" (__ret),
                       "+m" (*__ptr___0): "r" (__new), "0" (__old): "memory");
  goto ldv_5679;
  case 4UL:
  __ptr___1 = (u32 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgl %2,%1": "=a" (__ret),
                       "+m" (*__ptr___1): "r" (__new), "0" (__old): "memory");
  goto ldv_5679;
  case 8UL:
  __ptr___2 = (u64 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgq %2,%1": "=a" (__ret),
                       "+m" (*__ptr___2): "r" (__new), "0" (__old): "memory");
  goto ldv_5679;
  default:
  __cmpxchg_wrong_size();
  }
  ldv_5679: ;
  return (__ret);
}
}
__inline static int __atomic_add_unless(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless(v, a, u);
  return (tmp != u);
}
}
extern void lock_acquire(struct lockdep_map * , unsigned int , int , int , int ,
                         struct lockdep_map * , unsigned long ) ;
extern void lock_release(struct lockdep_map * , int , unsigned long ) ;
extern void down_read(struct rw_semaphore * ) ;
extern ktime_t ktime_get(void) ;
bool ldv_queue_work_on_173(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_175(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_174(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_177(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_176(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_173(8192, wq, work);
  return (tmp);
}
}
__inline static void kref_get___1(struct kref *kref )
{
  bool __warned ;
  int __ret_warn_once ;
  int tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = atomic_add_return(1, & kref->refcount);
  __ret_warn_once = tmp <= 1;
  tmp___2 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___2 != 0L) {
    __ret_warn_on = ! __warned;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("include/linux/kref.h", 47);
    } else {
    }
    tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___1 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  ldv__builtin_expect(__ret_warn_once != 0, 0L);
  return;
}
}
__inline static int kref_sub___1(struct kref *kref , unsigned int count , void (*release)(struct kref * ) )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 71);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = atomic_sub_and_test((int )count, & kref->refcount);
  if (tmp___0 != 0) {
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int kref_put___1(struct kref *kref , void (*release)(struct kref * ) )
{
  int tmp ;
  {
  tmp = kref_sub___1(kref, 1U, release);
  return (tmp);
}
}
__inline static int kref_put_mutex(struct kref *kref , void (*release)(struct kref * ) ,
                                   struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
void call_and_disable_work_3(struct work_struct *work ) ;
void disable_work_3(struct work_struct *work ) ;
void activate_work_3(struct work_struct *work , int state ) ;
void disable_work_2(struct work_struct *work ) ;
void invoke_work_3(void) ;
void call_and_disable_work_2(struct work_struct *work ) ;
void call_and_disable_all_3(int state ) ;
void invoke_work_2(void) ;
extern int i2c_transfer(struct i2c_adapter * , struct i2c_msg * , int ) ;
extern int drm_framebuffer_init(struct drm_device * , struct drm_framebuffer * , struct drm_framebuffer_funcs const * ) ;
extern void drm_framebuffer_cleanup(struct drm_framebuffer * ) ;
extern struct drm_property *drm_property_create_enum(struct drm_device * , int ,
                                                     char const * , struct drm_prop_enum_list const * ,
                                                     int ) ;
extern struct drm_property *drm_property_create_range(struct drm_device * , int ,
                                                      char const * , uint64_t ,
                                                      uint64_t ) ;
extern int drm_mode_create_scaling_mode_property(struct drm_device * ) ;
extern int drm_edid_header_is_valid(u8 const * ) ;
extern int drm_vblank_get(struct drm_device * , int ) ;
extern void drm_vblank_put(struct drm_device * , int ) ;
__inline static struct fence *fence_get___0(struct fence *fence )
{
  {
  if ((unsigned long )fence != (unsigned long )((struct fence *)0)) {
    kref_get___1(& fence->refcount);
  } else {
  }
  return (fence);
}
}
__inline static void fence_put___0(struct fence *fence )
{
  {
  if ((unsigned long )fence != (unsigned long )((struct fence *)0)) {
    kref_put___1(& fence->refcount, & fence_release);
  } else {
  }
  return;
}
}
extern long fence_wait_timeout(struct fence * , bool , long ) ;
__inline static long fence_wait(struct fence *fence , bool intr )
{
  long ret ;
  {
  ret = fence_wait_timeout(fence, (int )intr, 9223372036854775807L);
  return (0L < ret ? 0L : ret);
}
}
__inline static struct fence *reservation_object_get_excl(struct reservation_object *obj )
{
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = debug_lockdep_rcu_enabled();
  if (tmp != 0 && ! __warned) {
    tmp___0 = lock_is_held(& obj->lock.base.dep_map);
    if (tmp___0 == 0) {
      __warned = 1;
      lockdep_rcu_suspicious("include/linux/reservation.h", 120, "suspicious rcu_dereference_protected() usage");
    } else {
    }
  } else {
  }
  return (obj->fence_excl);
}
}
__inline static void drm_gem_object_reference(struct drm_gem_object *obj )
{
  {
  kref_get___1(& obj->refcount);
  return;
}
}
__inline static void drm_gem_object_unreference_unlocked(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
extern int drm_gem_handle_create(struct drm_file * , struct drm_gem_object * , u32 * ) ;
extern struct drm_gem_object *drm_gem_object_lookup(struct drm_device * , struct drm_file * ,
                                                    u32 ) ;
__inline static u32 dfixed_div(fixed20_12 A , fixed20_12 B )
{
  u64 tmp ;
  u32 __base ;
  u32 __rem ;
  {
  tmp = (unsigned long long )A.full << 13;
  __base = B.full;
  __rem = (u32 )(tmp % (u64 )__base);
  tmp = tmp / (u64 )__base;
  tmp = tmp + 1ULL;
  tmp = tmp / 2ULL;
  return ((u32 )tmp);
}
}
extern int drm_crtc_helper_set_config(struct drm_mode_set * ) ;
extern void drm_helper_mode_fill_fb_struct(struct drm_framebuffer * , struct drm_mode_fb_cmd2 * ) ;
int amdgpu_framebuffer_init(struct drm_device *dev , struct amdgpu_framebuffer *rfb ,
                            struct drm_mode_fb_cmd2 *mode_cmd , struct drm_gem_object *obj ) ;
bool amdgpu_crtc_scaling_mode_fixup(struct drm_crtc *crtc , struct drm_display_mode const *mode ,
                                    struct drm_display_mode *adjusted_mode ) ;
void amdgpu_fb_output_poll_changed(struct amdgpu_device *adev ) ;
void amdgpu_print_display_setup(struct drm_device *dev ) ;
int amdgpu_modeset_create_props(struct amdgpu_device *adev ) ;
int amdgpu_crtc_set_config(struct drm_mode_set *set ) ;
int amdgpu_crtc_page_flip(struct drm_crtc *crtc , struct drm_framebuffer *fb , struct drm_pending_vblank_event *event ,
                          u32 page_flip_flags ) ;
struct drm_mode_config_funcs const amdgpu_mode_funcs ;
void amdgpu_update_display_priority(struct amdgpu_device *adev ) ;
static void amdgpu_flip_work_func(struct work_struct *__work )
{
  struct amdgpu_flip_work *work ;
  struct work_struct const *__mptr ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpuCrtc ;
  struct drm_crtc *crtc ;
  struct amdgpu_fence *fence ;
  unsigned long flags ;
  int r ;
  long tmp ;
  raw_spinlock_t *tmp___0 ;
  {
  __mptr = (struct work_struct const *)__work;
  work = (struct amdgpu_flip_work *)__mptr;
  adev = work->adev;
  amdgpuCrtc = adev->mode_info.crtcs[work->crtc_id];
  crtc = & amdgpuCrtc->base;
  down_read(& adev->exclusive_lock);
  if ((unsigned long )work->fence != (unsigned long )((struct fence *)0)) {
    fence = to_amdgpu_fence(work->fence);
    if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0)) {
      r = amdgpu_fence_wait(fence, 0);
      if (r == -35) {
        up_read(& adev->exclusive_lock);
        r = amdgpu_gpu_reset(adev);
        down_read(& adev->exclusive_lock);
      } else {
      }
    } else {
      tmp = fence_wait(work->fence, 0);
      r = (int )tmp;
    }
    if (r != 0) {
      drm_err("failed to wait on page flip fence (%d)!\n", r);
    } else {
    }
    fence_put___0(work->fence);
    work->fence = (struct fence *)0;
  } else {
  }
  tmp___0 = spinlock_check(& (crtc->dev)->event_lock);
  flags = _raw_spin_lock_irqsave(tmp___0);
  amdgpu_irq_get(adev, & adev->pageflip_irq, (unsigned int )work->crtc_id);
  (*((adev->mode_info.funcs)->page_flip))(adev, work->crtc_id, work->base);
  amdgpuCrtc->pflip_status = 2;
  spin_unlock_irqrestore(& (crtc->dev)->event_lock, flags);
  up_read(& adev->exclusive_lock);
  return;
}
}
static void amdgpu_unpin_work_func(struct work_struct *__work )
{
  struct amdgpu_flip_work *work ;
  struct work_struct const *__mptr ;
  int r ;
  long tmp ;
  long tmp___0 ;
  {
  __mptr = (struct work_struct const *)__work;
  work = (struct amdgpu_flip_work *)__mptr + 0xffffffffffffffb0UL;
  r = amdgpu_bo_reserve(work->old_rbo, 0);
  tmp___0 = ldv__builtin_expect(r == 0, 1L);
  if (tmp___0 != 0L) {
    r = amdgpu_bo_unpin(work->old_rbo);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      drm_err("failed to unpin buffer after flip\n");
    } else {
    }
    amdgpu_bo_unreserve(work->old_rbo);
  } else {
    drm_err("failed to reserve buffer after flip\n");
  }
  drm_gem_object_unreference_unlocked(& (work->old_rbo)->gem_base);
  kfree((void const *)work);
  return;
}
}
int amdgpu_crtc_page_flip(struct drm_crtc *crtc , struct drm_framebuffer *fb , struct drm_pending_vblank_event *event ,
                          u32 page_flip_flags )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_framebuffer *old_amdgpu_fb ;
  struct amdgpu_framebuffer *new_amdgpu_fb ;
  struct drm_gem_object *obj ;
  struct amdgpu_flip_work *work ;
  struct amdgpu_bo *new_rbo ;
  unsigned long flags ;
  u64 tiling_flags ;
  u64 base ;
  int r ;
  void *tmp ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_1 ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_gem_object const *__mptr___1 ;
  struct drm_framebuffer const *__mptr___2 ;
  struct drm_gem_object const *__mptr___3 ;
  long tmp___0 ;
  long tmp___1 ;
  struct fence *tmp___2 ;
  raw_spinlock_t *tmp___3 ;
  long tmp___4 ;
  int tmp___5 ;
  long tmp___6 ;
  int tmp___7 ;
  long tmp___8 ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  tmp = kzalloc(208UL, 208U);
  work = (struct amdgpu_flip_work *)tmp;
  if ((unsigned long )work == (unsigned long )((struct amdgpu_flip_work *)0)) {
    return (-12);
  } else {
  }
  __init_work(& work->flip_work, 0);
  __constr_expr_0.counter = 137438953408L;
  work->flip_work.data = __constr_expr_0;
  lockdep_init_map(& work->flip_work.lockdep_map, "(&work->flip_work)", & __key, 0);
  INIT_LIST_HEAD(& work->flip_work.entry);
  work->flip_work.func = & amdgpu_flip_work_func;
  __init_work(& work->unpin_work, 0);
  __constr_expr_1.counter = 137438953408L;
  work->unpin_work.data = __constr_expr_1;
  lockdep_init_map(& work->unpin_work.lockdep_map, "(&work->unpin_work)", & __key___0,
                   0);
  INIT_LIST_HEAD(& work->unpin_work.entry);
  work->unpin_work.func = & amdgpu_unpin_work_func;
  work->event = event;
  work->adev = adev;
  work->crtc_id = amdgpu_crtc->crtc_id;
  __mptr___0 = (struct drm_framebuffer const *)(crtc->primary)->fb;
  old_amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
  obj = old_amdgpu_fb->obj;
  drm_gem_object_reference(obj);
  __mptr___1 = (struct drm_gem_object const *)obj;
  work->old_rbo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
  __mptr___2 = (struct drm_framebuffer const *)fb;
  new_amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___2;
  obj = new_amdgpu_fb->obj;
  __mptr___3 = (struct drm_gem_object const *)obj;
  new_rbo = (struct amdgpu_bo *)__mptr___3 + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(new_rbo, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    drm_err("failed to reserve new rbo buffer before flip\n");
    goto cleanup;
  } else {
  }
  r = amdgpu_bo_pin_restricted(new_rbo, 4U, 0ULL, 0ULL, & base);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    amdgpu_bo_unreserve(new_rbo);
    r = -22;
    drm_err("failed to pin new rbo buffer before flip\n");
    goto cleanup;
  } else {
  }
  tmp___2 = reservation_object_get_excl(new_rbo->tbo.resv);
  work->fence = fence_get___0(tmp___2);
  amdgpu_bo_get_tiling_flags(new_rbo, & tiling_flags);
  amdgpu_bo_unreserve(new_rbo);
  work->base = base;
  r = drm_vblank_get(crtc->dev, amdgpu_crtc->crtc_id);
  if (r != 0) {
    drm_err("failed to get vblank before flip\n");
    goto pflip_cleanup;
  } else {
  }
  tmp___3 = spinlock_check(& (crtc->dev)->event_lock);
  flags = _raw_spin_lock_irqsave(tmp___3);
  if ((unsigned int )amdgpu_crtc->pflip_status != 0U) {
    tmp___4 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___4 != 0L) {
      drm_ut_debug_printk("amdgpu_crtc_page_flip", "flip queue: crtc already busy\n");
    } else {
    }
    spin_unlock_irqrestore(& (crtc->dev)->event_lock, flags);
    r = -16;
    goto vblank_cleanup;
  } else {
  }
  amdgpu_crtc->pflip_status = 1;
  amdgpu_crtc->pflip_works = work;
  (crtc->primary)->fb = fb;
  spin_unlock_irqrestore(& (crtc->dev)->event_lock, flags);
  queue_work(amdgpu_crtc->pflip_queue, & work->flip_work);
  return (0);
  vblank_cleanup:
  drm_vblank_put(crtc->dev, amdgpu_crtc->crtc_id);
  pflip_cleanup:
  tmp___5 = amdgpu_bo_reserve(new_rbo, 0);
  tmp___6 = ldv__builtin_expect(tmp___5 != 0, 0L);
  if (tmp___6 != 0L) {
    drm_err("failed to reserve new rbo in error path\n");
    goto cleanup;
  } else {
  }
  tmp___7 = amdgpu_bo_unpin(new_rbo);
  tmp___8 = ldv__builtin_expect(tmp___7 != 0, 0L);
  if (tmp___8 != 0L) {
    drm_err("failed to unpin new rbo in error path\n");
  } else {
  }
  amdgpu_bo_unreserve(new_rbo);
  cleanup:
  drm_gem_object_unreference_unlocked(& (work->old_rbo)->gem_base);
  fence_put___0(work->fence);
  kfree((void const *)work);
  return (r);
}
}
int amdgpu_crtc_set_config(struct drm_mode_set *set )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct drm_crtc *crtc ;
  bool active ;
  int ret ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  active = 0;
  if ((unsigned long )set == (unsigned long )((struct drm_mode_set *)0) || (unsigned long )set->crtc == (unsigned long )((struct drm_crtc *)0)) {
    return (-22);
  } else {
  }
  dev = (set->crtc)->dev;
  ret = pm_runtime_get_sync(dev->dev);
  if (ret < 0) {
    return (ret);
  } else {
  }
  ret = drm_crtc_helper_set_config(set);
  __mptr = (struct list_head const *)dev->mode_config.crtc_list.next;
  crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
  goto ldv_48021;
  ldv_48020: ;
  if ((int )crtc->enabled) {
    active = 1;
  } else {
  }
  __mptr___0 = (struct list_head const *)crtc->head.next;
  crtc = (struct drm_crtc *)__mptr___0 + 0xfffffffffffffff0UL;
  ldv_48021: ;
  if ((unsigned long )(& crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
    goto ldv_48020;
  } else {
  }
  pm_runtime_mark_last_busy(dev->dev);
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((int )active && ! adev->have_disp_power_ref) {
    adev->have_disp_power_ref = 1;
    return (ret);
  } else {
  }
  if (! active && (int )adev->have_disp_power_ref) {
    pm_runtime_put_autosuspend(dev->dev);
    adev->have_disp_power_ref = 0;
  } else {
  }
  pm_runtime_put_autosuspend(dev->dev);
  return (ret);
}
}
static char const *encoder_names[38U] =
  { "NONE", "INTERNAL_LVDS", "INTERNAL_TMDS1", "INTERNAL_TMDS2",
        "INTERNAL_DAC1", "INTERNAL_DAC2", "INTERNAL_SDVOA", "INTERNAL_SDVOB",
        "SI170B", "CH7303", "CH7301", "INTERNAL_DVO1",
        "EXTERNAL_SDVOA", "EXTERNAL_SDVOB", "TITFP513", "INTERNAL_LVTM1",
        "VT1623", "HDMI_SI1930", "HDMI_INTERNAL", "INTERNAL_KLDSCP_TMDS1",
        "INTERNAL_KLDSCP_DVO1", "INTERNAL_KLDSCP_DAC1", "INTERNAL_KLDSCP_DAC2", "SI178",
        "MVPU_FPGA", "INTERNAL_DDI", "VT1625", "HDMI_SI1932",
        "DP_AN9801", "DP_DP501", "INTERNAL_UNIPHY", "INTERNAL_KLDSCP_LVTMA",
        "INTERNAL_UNIPHY1", "INTERNAL_UNIPHY2", "NUTMEG", "TRAVIS",
        "INTERNAL_VCE", "INTERNAL_UNIPHY3"};
static char const *hpd_names[6U] = { "HPD1", "HPD2", "HPD3", "HPD4",
        "HPD5", "HPD6"};
void amdgpu_print_display_setup(struct drm_device *dev )
{
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  u32 devices ;
  int i ;
  struct list_head const *__mptr ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct drm_encoder const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  struct list_head const *__mptr___4 ;
  {
  i = 0;
  printk("\016[drm] AMDGPU Display Connectors\n");
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_48050;
  ldv_48049:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  printk("\016[drm] Connector %d:\n", i);
  printk("\016[drm]   %s\n", connector->name);
  if ((unsigned int )amdgpu_connector->hpd.hpd != 255U) {
    printk("\016[drm]   %s\n", hpd_names[(unsigned int )amdgpu_connector->hpd.hpd]);
  } else {
  }
  if ((unsigned long )amdgpu_connector->ddc_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    printk("\016[drm]   DDC: 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x 0x%x\n", (amdgpu_connector->ddc_bus)->rec.mask_clk_reg,
           (amdgpu_connector->ddc_bus)->rec.mask_data_reg, (amdgpu_connector->ddc_bus)->rec.a_clk_reg,
           (amdgpu_connector->ddc_bus)->rec.a_data_reg, (amdgpu_connector->ddc_bus)->rec.en_clk_reg,
           (amdgpu_connector->ddc_bus)->rec.en_data_reg, (amdgpu_connector->ddc_bus)->rec.y_clk_reg,
           (amdgpu_connector->ddc_bus)->rec.y_data_reg);
    if ((int )amdgpu_connector->router.ddc_valid) {
      printk("\016[drm]   DDC Router 0x%x/0x%x\n", (int )amdgpu_connector->router.ddc_mux_control_pin,
             (int )amdgpu_connector->router.ddc_mux_state);
    } else {
    }
    if ((int )amdgpu_connector->router.cd_valid) {
      printk("\016[drm]   Clock/Data Router 0x%x/0x%x\n", (int )amdgpu_connector->router.cd_mux_control_pin,
             (int )amdgpu_connector->router.cd_mux_state);
    } else {
    }
  } else
  if (((((connector->connector_type == 1 || connector->connector_type == 2) || connector->connector_type == 3) || connector->connector_type == 4) || connector->connector_type == 11) || connector->connector_type == 12) {
    printk("\016[drm]   DDC: no ddc bus - possible BIOS bug - please report to xorg-driver-ati@lists.x.org\n");
  } else {
  }
  printk("\016[drm]   Encoders:\n");
  __mptr___1 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  goto ldv_48047;
  ldv_48046:
  __mptr___2 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___2;
  devices = amdgpu_encoder->devices & amdgpu_connector->devices;
  if (devices != 0U) {
    if ((int )devices & 1) {
      printk("\016[drm]     CRT1: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 16L) != 0L) {
      printk("\016[drm]     CRT2: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 2L) != 0L) {
      printk("\016[drm]     LCD1: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 8L) != 0L) {
      printk("\016[drm]     DFP1: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 128L) != 0L) {
      printk("\016[drm]     DFP2: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 512L) != 0L) {
      printk("\016[drm]     DFP3: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 1024L) != 0L) {
      printk("\016[drm]     DFP4: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 2048L) != 0L) {
      printk("\016[drm]     DFP5: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 64L) != 0L) {
      printk("\016[drm]     DFP6: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 4L) != 0L) {
      printk("\016[drm]     TV1: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
    if (((long )devices & 256L) != 0L) {
      printk("\016[drm]     CV: %s\n", encoder_names[amdgpu_encoder->encoder_id]);
    } else {
    }
  } else {
  }
  __mptr___3 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___3 + 0xfffffffffffffff8UL;
  ldv_48047: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_48046;
  } else {
  }
  i = i + 1;
  __mptr___4 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___4 + 0xffffffffffffffe8UL;
  ldv_48050: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_48049;
  } else {
  }
  return;
}
}
bool amdgpu_ddc_probe(struct amdgpu_connector *amdgpu_connector , bool use_aux )
{
  u8 out ;
  u8 buf[8U] ;
  int ret ;
  struct i2c_msg msgs[2U] ;
  int tmp ;
  {
  out = 0U;
  msgs[0].addr = 80U;
  msgs[0].flags = 0U;
  msgs[0].len = 1U;
  msgs[0].buf = & out;
  msgs[1].addr = 80U;
  msgs[1].flags = 1U;
  msgs[1].len = 8U;
  msgs[1].buf = (__u8 *)(& buf);
  if ((int )amdgpu_connector->router.ddc_valid) {
    amdgpu_i2c_router_select_ddc_port(amdgpu_connector);
  } else {
  }
  if ((int )use_aux) {
    ret = i2c_transfer(& (amdgpu_connector->ddc_bus)->aux.ddc, (struct i2c_msg *)(& msgs),
                       2);
  } else {
    ret = i2c_transfer(& (amdgpu_connector->ddc_bus)->adapter, (struct i2c_msg *)(& msgs),
                       2);
  }
  if (ret != 2) {
    return (0);
  } else {
  }
  tmp = drm_edid_header_is_valid((u8 const *)(& buf));
  if (tmp <= 5) {
    return (0);
  } else {
  }
  return (1);
}
}
static void amdgpu_user_framebuffer_destroy(struct drm_framebuffer *fb )
{
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct drm_framebuffer const *__mptr ;
  {
  __mptr = (struct drm_framebuffer const *)fb;
  amdgpu_fb = (struct amdgpu_framebuffer *)__mptr;
  if ((unsigned long )amdgpu_fb->obj != (unsigned long )((struct drm_gem_object *)0)) {
    drm_gem_object_unreference_unlocked(amdgpu_fb->obj);
  } else {
  }
  drm_framebuffer_cleanup(fb);
  kfree((void const *)amdgpu_fb);
  return;
}
}
static int amdgpu_user_framebuffer_create_handle(struct drm_framebuffer *fb , struct drm_file *file_priv ,
                                                 unsigned int *handle )
{
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct drm_framebuffer const *__mptr ;
  int tmp ;
  {
  __mptr = (struct drm_framebuffer const *)fb;
  amdgpu_fb = (struct amdgpu_framebuffer *)__mptr;
  tmp = drm_gem_handle_create(file_priv, amdgpu_fb->obj, handle);
  return (tmp);
}
}
static struct drm_framebuffer_funcs const amdgpu_fb_funcs = {& amdgpu_user_framebuffer_destroy, & amdgpu_user_framebuffer_create_handle, 0};
int amdgpu_framebuffer_init(struct drm_device *dev , struct amdgpu_framebuffer *rfb ,
                            struct drm_mode_fb_cmd2 *mode_cmd , struct drm_gem_object *obj )
{
  int ret ;
  {
  rfb->obj = obj;
  drm_helper_mode_fill_fb_struct(& rfb->base, mode_cmd);
  ret = drm_framebuffer_init(dev, & rfb->base, & amdgpu_fb_funcs);
  if (ret != 0) {
    rfb->obj = (struct drm_gem_object *)0;
    return (ret);
  } else {
  }
  return (0);
}
}
static struct drm_framebuffer *amdgpu_user_framebuffer_create(struct drm_device *dev ,
                                                              struct drm_file *file_priv ,
                                                              struct drm_mode_fb_cmd2 *mode_cmd )
{
  struct drm_gem_object *obj ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  int ret ;
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  {
  obj = drm_gem_object_lookup(dev, file_priv, mode_cmd->handles[0]);
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    dev_err((struct device const *)(& (dev->pdev)->dev), "No GEM object associated to handle 0x%08X, can\'t create framebuffer\n",
            mode_cmd->handles[0]);
    tmp = ERR_PTR(-2L);
    return ((struct drm_framebuffer *)tmp);
  } else {
  }
  tmp___0 = kzalloc(176UL, 208U);
  amdgpu_fb = (struct amdgpu_framebuffer *)tmp___0;
  if ((unsigned long )amdgpu_fb == (unsigned long )((struct amdgpu_framebuffer *)0)) {
    drm_gem_object_unreference_unlocked(obj);
    tmp___1 = ERR_PTR(-12L);
    return ((struct drm_framebuffer *)tmp___1);
  } else {
  }
  ret = amdgpu_framebuffer_init(dev, amdgpu_fb, mode_cmd, obj);
  if (ret != 0) {
    kfree((void const *)amdgpu_fb);
    drm_gem_object_unreference_unlocked(obj);
    tmp___2 = ERR_PTR((long )ret);
    return ((struct drm_framebuffer *)tmp___2);
  } else {
  }
  return (& amdgpu_fb->base);
}
}
static void amdgpu_output_poll_changed(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_fb_output_poll_changed(adev);
  return;
}
}
struct drm_mode_config_funcs const amdgpu_mode_funcs = {& amdgpu_user_framebuffer_create, & amdgpu_output_poll_changed, 0, 0, 0, 0, 0};
static struct drm_prop_enum_list amdgpu_underscan_enum_list[3U] = { {0, (char *)"off"},
        {1, (char *)"on"},
        {2, (char *)"auto"}};
static struct drm_prop_enum_list amdgpu_audio_enum_list[3U] = { {0, (char *)"off"},
        {1, (char *)"on"},
        {2, (char *)"auto"}};
static struct drm_prop_enum_list amdgpu_dither_enum_list[2U] = { {0, (char *)"off"},
        {1, (char *)"on"}};
int amdgpu_modeset_create_props(struct amdgpu_device *adev )
{
  int sz ;
  {
  if ((int )adev->is_atom_bios) {
    adev->mode_info.coherent_mode_property = drm_property_create_range(adev->ddev,
                                                                       0, "coherent",
                                                                       0ULL, 1ULL);
    if ((unsigned long )adev->mode_info.coherent_mode_property == (unsigned long )((struct drm_property *)0)) {
      return (-12);
    } else {
    }
  } else {
  }
  adev->mode_info.load_detect_property = drm_property_create_range(adev->ddev, 0,
                                                                   "load detection",
                                                                   0ULL, 1ULL);
  if ((unsigned long )adev->mode_info.load_detect_property == (unsigned long )((struct drm_property *)0)) {
    return (-12);
  } else {
  }
  drm_mode_create_scaling_mode_property(adev->ddev);
  sz = 3;
  adev->mode_info.underscan_property = drm_property_create_enum(adev->ddev, 0, "underscan",
                                                                (struct drm_prop_enum_list const *)(& amdgpu_underscan_enum_list),
                                                                sz);
  adev->mode_info.underscan_hborder_property = drm_property_create_range(adev->ddev,
                                                                         0, "underscan hborder",
                                                                         0ULL, 128ULL);
  if ((unsigned long )adev->mode_info.underscan_hborder_property == (unsigned long )((struct drm_property *)0)) {
    return (-12);
  } else {
  }
  adev->mode_info.underscan_vborder_property = drm_property_create_range(adev->ddev,
                                                                         0, "underscan vborder",
                                                                         0ULL, 128ULL);
  if ((unsigned long )adev->mode_info.underscan_vborder_property == (unsigned long )((struct drm_property *)0)) {
    return (-12);
  } else {
  }
  sz = 3;
  adev->mode_info.audio_property = drm_property_create_enum(adev->ddev, 0, "audio",
                                                            (struct drm_prop_enum_list const *)(& amdgpu_audio_enum_list),
                                                            sz);
  sz = 2;
  adev->mode_info.dither_property = drm_property_create_enum(adev->ddev, 0, "dither",
                                                             (struct drm_prop_enum_list const *)(& amdgpu_dither_enum_list),
                                                             sz);
  return (0);
}
}
void amdgpu_update_display_priority(struct amdgpu_device *adev )
{
  {
  if (amdgpu_disp_priority == 0 || amdgpu_disp_priority > 2) {
    adev->mode_info.disp_priority = 0;
  } else {
    adev->mode_info.disp_priority = amdgpu_disp_priority;
  }
  return;
}
}
static bool is_hdtv_mode(struct drm_display_mode const *mode )
{
  {
  if (((((int )mode->vdisplay == 480 && (int )mode->hdisplay == 720) || (int )mode->vdisplay == 576) || (int )mode->vdisplay == 720) || (int )mode->vdisplay == 1080) {
    return (1);
  } else {
    return (0);
  }
}
}
bool amdgpu_crtc_scaling_mode_fixup(struct drm_crtc *crtc , struct drm_display_mode const *mode ,
                                    struct drm_display_mode *adjusted_mode )
{
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 src_v ;
  u32 dst_v ;
  u32 src_h ;
  u32 dst_h ;
  struct list_head const *__mptr___0 ;
  struct drm_encoder const *__mptr___1 ;
  struct drm_connector const *__mptr___2 ;
  struct edid *tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  struct list_head const *__mptr___3 ;
  fixed20_12 a ;
  fixed20_12 b ;
  {
  dev = crtc->dev;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  src_v = 1U;
  dst_v = 1U;
  src_h = 1U;
  dst_h = 1U;
  amdgpu_crtc->h_border = 0U;
  amdgpu_crtc->v_border = 0U;
  __mptr___0 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___0 + 0xfffffffffffffff8UL;
  goto ldv_48141;
  ldv_48140: ;
  if ((unsigned long )encoder->crtc != (unsigned long )crtc) {
    goto ldv_48135;
  } else {
  }
  __mptr___1 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___1;
  connector = amdgpu_get_connector_for_encoder(encoder);
  __mptr___2 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___2;
  if ((unsigned int )amdgpu_encoder->rmx_type == 0U) {
    amdgpu_crtc->rmx_type = 0;
  } else
  if ((int )mode->hdisplay < amdgpu_encoder->native_mode.hdisplay || (int )mode->vdisplay < amdgpu_encoder->native_mode.vdisplay) {
    amdgpu_crtc->rmx_type = amdgpu_encoder->rmx_type;
  } else {
    amdgpu_crtc->rmx_type = 0;
  }
  memcpy((void *)(& amdgpu_crtc->native_mode), (void const *)(& amdgpu_encoder->native_mode),
           208UL);
  src_v = (u32 )crtc->mode.vdisplay;
  dst_v = (u32 )amdgpu_crtc->native_mode.vdisplay;
  src_h = (u32 )crtc->mode.hdisplay;
  dst_h = (u32 )amdgpu_crtc->native_mode.hdisplay;
  if (((unsigned int )mode->flags & 16U) == 0U) {
    if ((unsigned int )amdgpu_encoder->underscan_type == 1U) {
      goto _L;
    } else
    if ((unsigned int )amdgpu_encoder->underscan_type == 2U) {
      tmp = amdgpu_connector_edid(connector);
      tmp___0 = drm_detect_hdmi_monitor(tmp);
      if ((int )tmp___0) {
        tmp___1 = is_hdtv_mode(mode);
        if ((int )tmp___1) {
          _L:
          if (amdgpu_encoder->underscan_hborder != 0U) {
            amdgpu_crtc->h_border = (u8 )amdgpu_encoder->underscan_hborder;
          } else {
            amdgpu_crtc->h_border = (unsigned int )((u8 )(mode->hdisplay >> 5)) + 16U;
          }
          if (amdgpu_encoder->underscan_vborder != 0U) {
            amdgpu_crtc->v_border = (u8 )amdgpu_encoder->underscan_vborder;
          } else {
            amdgpu_crtc->v_border = (unsigned int )((u8 )(mode->vdisplay >> 5)) + 16U;
          }
          amdgpu_crtc->rmx_type = 1;
          src_v = (u32 )crtc->mode.vdisplay;
          dst_v = (u32 )(crtc->mode.vdisplay + (int )amdgpu_crtc->v_border * -2);
          src_h = (u32 )crtc->mode.hdisplay;
          dst_h = (u32 )(crtc->mode.hdisplay + (int )amdgpu_crtc->h_border * -2);
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  ldv_48135:
  __mptr___3 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___3 + 0xfffffffffffffff8UL;
  ldv_48141: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_48140;
  } else {
  }
  if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
    a.full = src_v << 12;
    b.full = dst_v << 12;
    amdgpu_crtc->vsc.full = dfixed_div(a, b);
    a.full = src_h << 12;
    b.full = dst_h << 12;
    amdgpu_crtc->hsc.full = dfixed_div(a, b);
  } else {
    amdgpu_crtc->vsc.full = 4096U;
    amdgpu_crtc->hsc.full = 4096U;
  }
  return (1);
}
}
int amdgpu_get_crtc_scanoutpos(struct drm_device *dev , int crtc , unsigned int flags ,
                               int *vpos , int *hpos , ktime_t *stime , ktime_t *etime )
{
  u32 vbl ;
  u32 position ;
  int vbl_start ;
  int vbl_end ;
  int vtotal ;
  int ret ;
  bool in_vbl ;
  struct amdgpu_device *adev ;
  int tmp ;
  {
  vbl = 0U;
  position = 0U;
  ret = 0;
  in_vbl = 1;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((unsigned long )stime != (unsigned long )((ktime_t *)0)) {
    *stime = ktime_get();
  } else {
  }
  tmp = (*((adev->mode_info.funcs)->page_flip_get_scanoutpos))(adev, crtc, & vbl,
                                                               & position);
  if (tmp == 0) {
    ret = ret | 1;
  } else {
  }
  if ((unsigned long )etime != (unsigned long )((ktime_t *)0)) {
    *etime = ktime_get();
  } else {
  }
  *vpos = (int )position & 8191;
  *hpos = (int )(position >> 16) & 8191;
  if (vbl != 0U) {
    ret = ret | 4;
    vbl_start = (int )vbl & 8191;
    vbl_end = (int )(vbl >> 16) & 8191;
  } else {
    vbl_start = (adev->mode_info.crtcs[crtc])->base.hwmode.crtc_vdisplay;
    vbl_end = 0;
  }
  if (*vpos < vbl_start && *vpos >= vbl_end) {
    in_vbl = 0;
  } else {
  }
  if ((int )in_vbl && *vpos >= vbl_start) {
    vtotal = (adev->mode_info.crtcs[crtc])->base.hwmode.crtc_vtotal;
    *vpos = *vpos - vtotal;
  } else {
  }
  *vpos = *vpos - vbl_end;
  if ((int )in_vbl) {
    ret = ret | 2;
  } else {
  }
  if ((int )flags & 1 && ! in_vbl) {
    vbl_start = (adev->mode_info.crtcs[crtc])->base.hwmode.crtc_vdisplay;
    vtotal = (adev->mode_info.crtcs[crtc])->base.hwmode.crtc_vtotal;
    if (vbl_start - *vpos < vtotal / 100) {
      *vpos = *vpos - vtotal;
      ret = ret | 8;
    } else {
    }
  } else {
  }
  return (ret);
}
}
int amdgpu_crtc_idx_to_irq_type(struct amdgpu_device *adev , int crtc )
{
  {
  if (crtc < 0 || adev->mode_info.num_crtc <= crtc) {
    return (255);
  } else {
  }
  switch (crtc) {
  case 0: ;
  return (0);
  case 1: ;
  return (1);
  case 2: ;
  return (2);
  case 3: ;
  return (3);
  case 4: ;
  return (4);
  case 5: ;
  return (5);
  default: ;
  return (255);
  }
}
}
extern int ldv_probe_159(void) ;
void call_and_disable_work_3(struct work_struct *work )
{
  {
  if ((ldv_work_3_0 == 2 || ldv_work_3_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_3_0) {
    amdgpu_unpin_work_func(work);
    ldv_work_3_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_3_1 == 2 || ldv_work_3_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_3_1) {
    amdgpu_unpin_work_func(work);
    ldv_work_3_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_3_2 == 2 || ldv_work_3_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_3_2) {
    amdgpu_unpin_work_func(work);
    ldv_work_3_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_3_3 == 2 || ldv_work_3_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_3_3) {
    amdgpu_unpin_work_func(work);
    ldv_work_3_3 = 1;
    return;
  } else {
  }
  return;
}
}
void ldv_initialize_drm_mode_config_funcs_158(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(3320UL);
  amdgpu_mode_funcs_group0 = (struct drm_device *)tmp;
  return;
}
}
void disable_work_3(struct work_struct *work )
{
  {
  if ((ldv_work_3_0 == 3 || ldv_work_3_0 == 2) && (unsigned long )ldv_work_struct_3_0 == (unsigned long )work) {
    ldv_work_3_0 = 1;
  } else {
  }
  if ((ldv_work_3_1 == 3 || ldv_work_3_1 == 2) && (unsigned long )ldv_work_struct_3_1 == (unsigned long )work) {
    ldv_work_3_1 = 1;
  } else {
  }
  if ((ldv_work_3_2 == 3 || ldv_work_3_2 == 2) && (unsigned long )ldv_work_struct_3_2 == (unsigned long )work) {
    ldv_work_3_2 = 1;
  } else {
  }
  if ((ldv_work_3_3 == 3 || ldv_work_3_3 == 2) && (unsigned long )ldv_work_struct_3_3 == (unsigned long )work) {
    ldv_work_3_3 = 1;
  } else {
  }
  return;
}
}
void activate_work_3(struct work_struct *work , int state )
{
  {
  if (ldv_work_3_0 == 0) {
    ldv_work_struct_3_0 = work;
    ldv_work_3_0 = state;
    return;
  } else {
  }
  if (ldv_work_3_1 == 0) {
    ldv_work_struct_3_1 = work;
    ldv_work_3_1 = state;
    return;
  } else {
  }
  if (ldv_work_3_2 == 0) {
    ldv_work_struct_3_2 = work;
    ldv_work_3_2 = state;
    return;
  } else {
  }
  if (ldv_work_3_3 == 0) {
    ldv_work_struct_3_3 = work;
    ldv_work_3_3 = state;
    return;
  } else {
  }
  return;
}
}
void disable_work_2(struct work_struct *work )
{
  {
  if ((ldv_work_2_0 == 3 || ldv_work_2_0 == 2) && (unsigned long )ldv_work_struct_2_0 == (unsigned long )work) {
    ldv_work_2_0 = 1;
  } else {
  }
  if ((ldv_work_2_1 == 3 || ldv_work_2_1 == 2) && (unsigned long )ldv_work_struct_2_1 == (unsigned long )work) {
    ldv_work_2_1 = 1;
  } else {
  }
  if ((ldv_work_2_2 == 3 || ldv_work_2_2 == 2) && (unsigned long )ldv_work_struct_2_2 == (unsigned long )work) {
    ldv_work_2_2 = 1;
  } else {
  }
  if ((ldv_work_2_3 == 3 || ldv_work_2_3 == 2) && (unsigned long )ldv_work_struct_2_3 == (unsigned long )work) {
    ldv_work_2_3 = 1;
  } else {
  }
  return;
}
}
void invoke_work_3(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_3_0 == 2 || ldv_work_3_0 == 3) {
    ldv_work_3_0 = 4;
    amdgpu_unpin_work_func(ldv_work_struct_3_0);
    ldv_work_3_0 = 1;
  } else {
  }
  goto ldv_48199;
  case 1: ;
  if (ldv_work_3_1 == 2 || ldv_work_3_1 == 3) {
    ldv_work_3_1 = 4;
    amdgpu_unpin_work_func(ldv_work_struct_3_0);
    ldv_work_3_1 = 1;
  } else {
  }
  goto ldv_48199;
  case 2: ;
  if (ldv_work_3_2 == 2 || ldv_work_3_2 == 3) {
    ldv_work_3_2 = 4;
    amdgpu_unpin_work_func(ldv_work_struct_3_0);
    ldv_work_3_2 = 1;
  } else {
  }
  goto ldv_48199;
  case 3: ;
  if (ldv_work_3_3 == 2 || ldv_work_3_3 == 3) {
    ldv_work_3_3 = 4;
    amdgpu_unpin_work_func(ldv_work_struct_3_0);
    ldv_work_3_3 = 1;
  } else {
  }
  goto ldv_48199;
  default:
  ldv_stop();
  }
  ldv_48199: ;
  return;
}
}
void call_and_disable_work_2(struct work_struct *work )
{
  {
  if ((ldv_work_2_0 == 2 || ldv_work_2_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_2_0) {
    amdgpu_flip_work_func(work);
    ldv_work_2_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_2_1 == 2 || ldv_work_2_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_2_1) {
    amdgpu_flip_work_func(work);
    ldv_work_2_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_2_2 == 2 || ldv_work_2_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_2_2) {
    amdgpu_flip_work_func(work);
    ldv_work_2_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_2_3 == 2 || ldv_work_2_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_2_3) {
    amdgpu_flip_work_func(work);
    ldv_work_2_3 = 1;
    return;
  } else {
  }
  return;
}
}
void ldv_initialize_drm_framebuffer_funcs_159(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(168UL);
  amdgpu_fb_funcs_group0 = (struct drm_framebuffer *)tmp;
  return;
}
}
void activate_work_2(struct work_struct *work , int state )
{
  {
  if (ldv_work_2_0 == 0) {
    ldv_work_struct_2_0 = work;
    ldv_work_2_0 = state;
    return;
  } else {
  }
  if (ldv_work_2_1 == 0) {
    ldv_work_struct_2_1 = work;
    ldv_work_2_1 = state;
    return;
  } else {
  }
  if (ldv_work_2_2 == 0) {
    ldv_work_struct_2_2 = work;
    ldv_work_2_2 = state;
    return;
  } else {
  }
  if (ldv_work_2_3 == 0) {
    ldv_work_struct_2_3 = work;
    ldv_work_2_3 = state;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_3(int state )
{
  {
  if (ldv_work_3_0 == state) {
    call_and_disable_work_3(ldv_work_struct_3_0);
  } else {
  }
  if (ldv_work_3_1 == state) {
    call_and_disable_work_3(ldv_work_struct_3_1);
  } else {
  }
  if (ldv_work_3_2 == state) {
    call_and_disable_work_3(ldv_work_struct_3_2);
  } else {
  }
  if (ldv_work_3_3 == state) {
    call_and_disable_work_3(ldv_work_struct_3_3);
  } else {
  }
  return;
}
}
void work_init_3(void)
{
  {
  ldv_work_3_0 = 0;
  ldv_work_3_1 = 0;
  ldv_work_3_2 = 0;
  ldv_work_3_3 = 0;
  return;
}
}
void call_and_disable_all_2(int state )
{
  {
  if (ldv_work_2_0 == state) {
    call_and_disable_work_2(ldv_work_struct_2_0);
  } else {
  }
  if (ldv_work_2_1 == state) {
    call_and_disable_work_2(ldv_work_struct_2_1);
  } else {
  }
  if (ldv_work_2_2 == state) {
    call_and_disable_work_2(ldv_work_struct_2_2);
  } else {
  }
  if (ldv_work_2_3 == state) {
    call_and_disable_work_2(ldv_work_struct_2_3);
  } else {
  }
  return;
}
}
void invoke_work_2(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_2_0 == 2 || ldv_work_2_0 == 3) {
    ldv_work_2_0 = 4;
    amdgpu_flip_work_func(ldv_work_struct_2_0);
    ldv_work_2_0 = 1;
  } else {
  }
  goto ldv_48231;
  case 1: ;
  if (ldv_work_2_1 == 2 || ldv_work_2_1 == 3) {
    ldv_work_2_1 = 4;
    amdgpu_flip_work_func(ldv_work_struct_2_0);
    ldv_work_2_1 = 1;
  } else {
  }
  goto ldv_48231;
  case 2: ;
  if (ldv_work_2_2 == 2 || ldv_work_2_2 == 3) {
    ldv_work_2_2 = 4;
    amdgpu_flip_work_func(ldv_work_struct_2_0);
    ldv_work_2_2 = 1;
  } else {
  }
  goto ldv_48231;
  case 3: ;
  if (ldv_work_2_3 == 2 || ldv_work_2_3 == 3) {
    ldv_work_2_3 = 4;
    amdgpu_flip_work_func(ldv_work_struct_2_0);
    ldv_work_2_3 = 1;
  } else {
  }
  goto ldv_48231;
  default:
  ldv_stop();
  }
  ldv_48231: ;
  return;
}
}
void work_init_2(void)
{
  {
  ldv_work_2_0 = 0;
  ldv_work_2_1 = 0;
  ldv_work_2_2 = 0;
  ldv_work_2_3 = 0;
  return;
}
}
void ldv_main_exported_159(void)
{
  unsigned int *ldvarg446 ;
  void *tmp ;
  struct drm_file *ldvarg447 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(4UL);
  ldvarg446 = (unsigned int *)tmp;
  tmp___0 = ldv_init_zalloc(744UL);
  ldvarg447 = (struct drm_file *)tmp___0;
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_159 == 2) {
    amdgpu_user_framebuffer_destroy(amdgpu_fb_funcs_group0);
    ldv_state_variable_159 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48245;
  case 1: ;
  if (ldv_state_variable_159 == 1) {
    amdgpu_user_framebuffer_create_handle(amdgpu_fb_funcs_group0, ldvarg447, ldvarg446);
    ldv_state_variable_159 = 1;
  } else {
  }
  if (ldv_state_variable_159 == 2) {
    amdgpu_user_framebuffer_create_handle(amdgpu_fb_funcs_group0, ldvarg447, ldvarg446);
    ldv_state_variable_159 = 2;
  } else {
  }
  goto ldv_48245;
  case 2: ;
  if (ldv_state_variable_159 == 1) {
    ldv_probe_159();
    ldv_state_variable_159 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48245;
  default:
  ldv_stop();
  }
  ldv_48245: ;
  return;
}
}
void ldv_main_exported_158(void)
{
  struct drm_file *ldvarg1015 ;
  void *tmp ;
  struct drm_mode_fb_cmd2 *ldvarg1014 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(744UL);
  ldvarg1015 = (struct drm_file *)tmp;
  tmp___0 = ldv_init_zalloc(104UL);
  ldvarg1014 = (struct drm_mode_fb_cmd2 *)tmp___0;
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_158 == 1) {
    amdgpu_user_framebuffer_create(amdgpu_mode_funcs_group0, ldvarg1015, ldvarg1014);
    ldv_state_variable_158 = 1;
  } else {
  }
  goto ldv_48255;
  case 1: ;
  if (ldv_state_variable_158 == 1) {
    amdgpu_output_poll_changed(amdgpu_mode_funcs_group0);
    ldv_state_variable_158 = 1;
  } else {
  }
  goto ldv_48255;
  default:
  ldv_stop();
  }
  ldv_48255: ;
  return;
}
}
__inline static void *ERR_PTR(long error )
{
  void *tmp ;
  {
  tmp = ldv_err_ptr(error);
  return (tmp);
}
}
bool ldv_queue_work_on_173(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_174(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_175(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_176(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_177(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern int snprintf(char * , size_t , char const * , ...) ;
extern unsigned long __usecs_to_jiffies(unsigned int const ) ;
__inline static unsigned long usecs_to_jiffies(unsigned int const u )
{
  unsigned long tmp___1 ;
  {
  tmp___1 = __usecs_to_jiffies(u);
  return (tmp___1);
}
}
bool ldv_queue_work_on_187(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_189(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_188(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_191(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_190(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static void dev_set_drvdata(struct device *dev , void *data )
{
  {
  dev->driver_data = data;
  return;
}
}
__inline static void *i2c_get_adapdata(struct i2c_adapter const *dev )
{
  void *tmp ;
  {
  tmp = dev_get_drvdata(& dev->dev);
  return (tmp);
}
}
__inline static void i2c_set_adapdata(struct i2c_adapter *dev , void *data )
{
  {
  dev_set_drvdata(& dev->dev, data);
  return;
}
}
extern int i2c_add_adapter(struct i2c_adapter * ) ;
extern void i2c_del_adapter(struct i2c_adapter * ) ;
extern int i2c_bit_add_bus(struct i2c_adapter * ) ;
void amdgpu_i2c_destroy(struct amdgpu_i2c_chan *i2c ) ;
void amdgpu_i2c_init(struct amdgpu_device *adev ) ;
void amdgpu_i2c_add(struct amdgpu_device *adev , struct amdgpu_i2c_bus_rec *rec ,
                    char const *name ) ;
void amdgpu_i2c_router_select_cd_port(struct amdgpu_connector *amdgpu_connector ) ;
int amdgpu_atombios_i2c_xfer(struct i2c_adapter *i2c_adap , struct i2c_msg *msgs ,
                             int num ) ;
u32 amdgpu_atombios_i2c_func(struct i2c_adapter *adap ) ;
static int amdgpu_i2c_pre_xfer(struct i2c_adapter *i2c_adap )
{
  struct amdgpu_i2c_chan *i2c ;
  void *tmp ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 temp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  {
  tmp = i2c_get_adapdata((struct i2c_adapter const *)i2c_adap);
  i2c = (struct amdgpu_i2c_chan *)tmp;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  mutex_lock_nested(& i2c->mutex, 0U);
  if ((int )rec->hw_capable) {
    temp = amdgpu_mm_rreg(adev, rec->mask_clk_reg, 0);
    temp = temp & 4294901759U;
    amdgpu_mm_wreg(adev, rec->mask_clk_reg, temp, 0);
  } else {
  }
  tmp___0 = amdgpu_mm_rreg(adev, rec->a_clk_reg, 0);
  temp = tmp___0 & ~ rec->a_clk_mask;
  amdgpu_mm_wreg(adev, rec->a_clk_reg, temp, 0);
  tmp___1 = amdgpu_mm_rreg(adev, rec->a_data_reg, 0);
  temp = tmp___1 & ~ rec->a_data_mask;
  amdgpu_mm_wreg(adev, rec->a_data_reg, temp, 0);
  tmp___2 = amdgpu_mm_rreg(adev, rec->en_clk_reg, 0);
  temp = tmp___2 & ~ rec->en_clk_mask;
  amdgpu_mm_wreg(adev, rec->en_clk_reg, temp, 0);
  tmp___3 = amdgpu_mm_rreg(adev, rec->en_data_reg, 0);
  temp = tmp___3 & ~ rec->en_data_mask;
  amdgpu_mm_wreg(adev, rec->en_data_reg, temp, 0);
  tmp___4 = amdgpu_mm_rreg(adev, rec->mask_clk_reg, 0);
  temp = tmp___4 | rec->mask_clk_mask;
  amdgpu_mm_wreg(adev, rec->mask_clk_reg, temp, 0);
  temp = amdgpu_mm_rreg(adev, rec->mask_clk_reg, 0);
  tmp___5 = amdgpu_mm_rreg(adev, rec->mask_data_reg, 0);
  temp = tmp___5 | rec->mask_data_mask;
  amdgpu_mm_wreg(adev, rec->mask_data_reg, temp, 0);
  temp = amdgpu_mm_rreg(adev, rec->mask_data_reg, 0);
  return (0);
}
}
static void amdgpu_i2c_post_xfer(struct i2c_adapter *i2c_adap )
{
  struct amdgpu_i2c_chan *i2c ;
  void *tmp ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 temp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  {
  tmp = i2c_get_adapdata((struct i2c_adapter const *)i2c_adap);
  i2c = (struct amdgpu_i2c_chan *)tmp;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  tmp___0 = amdgpu_mm_rreg(adev, rec->mask_clk_reg, 0);
  temp = tmp___0 & ~ rec->mask_clk_mask;
  amdgpu_mm_wreg(adev, rec->mask_clk_reg, temp, 0);
  temp = amdgpu_mm_rreg(adev, rec->mask_clk_reg, 0);
  tmp___1 = amdgpu_mm_rreg(adev, rec->mask_data_reg, 0);
  temp = tmp___1 & ~ rec->mask_data_mask;
  amdgpu_mm_wreg(adev, rec->mask_data_reg, temp, 0);
  temp = amdgpu_mm_rreg(adev, rec->mask_data_reg, 0);
  mutex_unlock(& i2c->mutex);
  return;
}
}
static int amdgpu_i2c_get_clock(void *i2c_priv )
{
  struct amdgpu_i2c_chan *i2c ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 val ;
  {
  i2c = (struct amdgpu_i2c_chan *)i2c_priv;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  val = amdgpu_mm_rreg(adev, rec->y_clk_reg, 0);
  val = rec->y_clk_mask & val;
  return (val != 0U);
}
}
static int amdgpu_i2c_get_data(void *i2c_priv )
{
  struct amdgpu_i2c_chan *i2c ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 val ;
  {
  i2c = (struct amdgpu_i2c_chan *)i2c_priv;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  val = amdgpu_mm_rreg(adev, rec->y_data_reg, 0);
  val = rec->y_data_mask & val;
  return (val != 0U);
}
}
static void amdgpu_i2c_set_clock(void *i2c_priv , int clock )
{
  struct amdgpu_i2c_chan *i2c ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 val ;
  u32 tmp ;
  {
  i2c = (struct amdgpu_i2c_chan *)i2c_priv;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  tmp = amdgpu_mm_rreg(adev, rec->en_clk_reg, 0);
  val = tmp & ~ rec->en_clk_mask;
  val = (clock == 0 ? rec->en_clk_mask : 0U) | val;
  amdgpu_mm_wreg(adev, rec->en_clk_reg, val, 0);
  return;
}
}
static void amdgpu_i2c_set_data(void *i2c_priv , int data )
{
  struct amdgpu_i2c_chan *i2c ;
  struct amdgpu_device *adev ;
  struct amdgpu_i2c_bus_rec *rec ;
  u32 val ;
  u32 tmp ;
  {
  i2c = (struct amdgpu_i2c_chan *)i2c_priv;
  adev = (struct amdgpu_device *)(i2c->dev)->dev_private;
  rec = & i2c->rec;
  tmp = amdgpu_mm_rreg(adev, rec->en_data_reg, 0);
  val = tmp & ~ rec->en_data_mask;
  val = (data == 0 ? rec->en_data_mask : 0U) | val;
  amdgpu_mm_wreg(adev, rec->en_data_reg, val, 0);
  return;
}
}
static struct i2c_algorithm const amdgpu_atombios_i2c_algo = {& amdgpu_atombios_i2c_xfer, 0, & amdgpu_atombios_i2c_func, 0, 0};
struct amdgpu_i2c_chan *amdgpu_i2c_create(struct drm_device *dev , struct amdgpu_i2c_bus_rec *rec ,
                                          char const *name )
{
  struct amdgpu_i2c_chan *i2c ;
  int ret ;
  void *tmp ;
  struct lock_class_key __key ;
  unsigned long tmp___0 ;
  {
  if ((int )rec->mm_i2c && amdgpu_hw_i2c == 0) {
    return ((struct amdgpu_i2c_chan *)0);
  } else {
  }
  tmp = kzalloc(4384UL, 208U);
  i2c = (struct amdgpu_i2c_chan *)tmp;
  if ((unsigned long )i2c == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    return ((struct amdgpu_i2c_chan *)0);
  } else {
  }
  i2c->rec = *rec;
  i2c->adapter.owner = & __this_module;
  i2c->adapter.class = 8U;
  i2c->adapter.dev.parent = & (dev->pdev)->dev;
  i2c->dev = dev;
  i2c_set_adapdata(& i2c->adapter, (void *)i2c);
  __mutex_init(& i2c->mutex, "&i2c->mutex", & __key);
  if ((int )rec->hw_capable && amdgpu_hw_i2c != 0) {
    snprintf((char *)(& i2c->adapter.name), 48UL, "AMDGPU i2c hw bus %s", name);
    i2c->adapter.algo = & amdgpu_atombios_i2c_algo;
    ret = i2c_add_adapter(& i2c->adapter);
    if (ret != 0) {
      drm_err("Failed to register hw i2c %s\n", name);
      goto out_free;
    } else {
    }
  } else {
    snprintf((char *)(& i2c->adapter.name), 48UL, "AMDGPU i2c bit bus %s", name);
    i2c->adapter.algo_data = (void *)(& i2c->bit);
    i2c->bit.pre_xfer = & amdgpu_i2c_pre_xfer;
    i2c->bit.post_xfer = & amdgpu_i2c_post_xfer;
    i2c->bit.setsda = & amdgpu_i2c_set_data;
    i2c->bit.setscl = & amdgpu_i2c_set_clock;
    i2c->bit.getsda = & amdgpu_i2c_get_data;
    i2c->bit.getscl = & amdgpu_i2c_get_clock;
    i2c->bit.udelay = 10;
    tmp___0 = usecs_to_jiffies(2200U);
    i2c->bit.timeout = (int )tmp___0;
    i2c->bit.data = (void *)i2c;
    ret = i2c_bit_add_bus(& i2c->adapter);
    if (ret != 0) {
      drm_err("Failed to register bit i2c %s\n", name);
      goto out_free;
    } else {
    }
  }
  return (i2c);
  out_free:
  kfree((void const *)i2c);
  return ((struct amdgpu_i2c_chan *)0);
}
}
void amdgpu_i2c_destroy(struct amdgpu_i2c_chan *i2c )
{
  {
  if ((unsigned long )i2c == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    return;
  } else {
  }
  i2c_del_adapter(& i2c->adapter);
  kfree((void const *)i2c);
  return;
}
}
void amdgpu_i2c_init(struct amdgpu_device *adev )
{
  {
  if (amdgpu_hw_i2c != 0) {
    printk("\016[drm] hw_i2c forced on, you may experience display detection problems!\n");
  } else {
  }
  if ((int )adev->is_atom_bios) {
    amdgpu_atombios_i2c_init(adev);
  } else {
  }
  return;
}
}
void amdgpu_i2c_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_48012;
  ldv_48011: ;
  if ((unsigned long )adev->i2c_bus[i] != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    amdgpu_i2c_destroy(adev->i2c_bus[i]);
    adev->i2c_bus[i] = (struct amdgpu_i2c_chan *)0;
  } else {
  }
  i = i + 1;
  ldv_48012: ;
  if (i <= 15) {
    goto ldv_48011;
  } else {
  }
  return;
}
}
void amdgpu_i2c_add(struct amdgpu_device *adev , struct amdgpu_i2c_bus_rec *rec ,
                    char const *name )
{
  struct drm_device *dev ;
  int i ;
  {
  dev = adev->ddev;
  i = 0;
  goto ldv_48022;
  ldv_48021: ;
  if ((unsigned long )adev->i2c_bus[i] == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    adev->i2c_bus[i] = amdgpu_i2c_create(dev, rec, name);
    return;
  } else {
  }
  i = i + 1;
  ldv_48022: ;
  if (i <= 15) {
    goto ldv_48021;
  } else {
  }
  return;
}
}
struct amdgpu_i2c_chan *amdgpu_i2c_lookup(struct amdgpu_device *adev , struct amdgpu_i2c_bus_rec *i2c_bus )
{
  int i ;
  {
  i = 0;
  goto ldv_48030;
  ldv_48029: ;
  if ((unsigned long )adev->i2c_bus[i] != (unsigned long )((struct amdgpu_i2c_chan *)0) && (int )(adev->i2c_bus[i])->rec.i2c_id == (int )i2c_bus->i2c_id) {
    return (adev->i2c_bus[i]);
  } else {
  }
  i = i + 1;
  ldv_48030: ;
  if (i <= 15) {
    goto ldv_48029;
  } else {
  }
  return ((struct amdgpu_i2c_chan *)0);
}
}
static void amdgpu_i2c_get_byte(struct amdgpu_i2c_chan *i2c_bus , u8 slave_addr ,
                                u8 addr , u8 *val )
{
  u8 out_buf[2U] ;
  u8 in_buf[2U] ;
  struct i2c_msg msgs[2U] ;
  long tmp ;
  long tmp___0 ;
  int tmp___1 ;
  {
  msgs[0].addr = (unsigned short )slave_addr;
  msgs[0].flags = 0U;
  msgs[0].len = 1U;
  msgs[0].buf = (__u8 *)(& out_buf);
  msgs[1].addr = (unsigned short )slave_addr;
  msgs[1].flags = 1U;
  msgs[1].len = 1U;
  msgs[1].buf = (__u8 *)(& in_buf);
  out_buf[0] = addr;
  out_buf[1] = 0U;
  tmp___1 = i2c_transfer(& i2c_bus->adapter, (struct i2c_msg *)(& msgs), 2);
  if (tmp___1 == 2) {
    *val = in_buf[0];
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_i2c_get_byte", "val = 0x%02x\n", (int )*val);
    } else {
    }
  } else {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_i2c_get_byte", "i2c 0x%02x 0x%02x read failed\n",
                          (int )addr, (int )*val);
    } else {
    }
  }
  return;
}
}
static void amdgpu_i2c_put_byte(struct amdgpu_i2c_chan *i2c_bus , u8 slave_addr ,
                                u8 addr , u8 val )
{
  uint8_t out_buf[2U] ;
  struct i2c_msg msg ;
  long tmp ;
  int tmp___0 ;
  {
  msg.addr = (unsigned short )slave_addr;
  msg.flags = 0U;
  msg.len = 2U;
  msg.buf = (__u8 *)(& out_buf);
  out_buf[0] = addr;
  out_buf[1] = val;
  tmp___0 = i2c_transfer(& i2c_bus->adapter, & msg, 1);
  if (tmp___0 != 1) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_i2c_put_byte", "i2c 0x%02x 0x%02x write failed\n",
                          (int )addr, (int )val);
    } else {
    }
  } else {
  }
  return;
}
}
void amdgpu_i2c_router_select_ddc_port(struct amdgpu_connector *amdgpu_connector )
{
  u8 val ;
  {
  if (! amdgpu_connector->router.ddc_valid) {
    return;
  } else {
  }
  if ((unsigned long )amdgpu_connector->router_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    return;
  } else {
  }
  amdgpu_i2c_get_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      3, & val);
  val = (u8 )(~ ((int )((signed char )amdgpu_connector->router.ddc_mux_control_pin)) & (int )((signed char )val));
  amdgpu_i2c_put_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      3, (int )val);
  amdgpu_i2c_get_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      1, & val);
  val = (u8 )(~ ((int )((signed char )amdgpu_connector->router.ddc_mux_control_pin)) & (int )((signed char )val));
  val = (u8 )((int )amdgpu_connector->router.ddc_mux_state | (int )val);
  amdgpu_i2c_put_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      1, (int )val);
  return;
}
}
void amdgpu_i2c_router_select_cd_port(struct amdgpu_connector *amdgpu_connector )
{
  u8 val ;
  {
  if (! amdgpu_connector->router.cd_valid) {
    return;
  } else {
  }
  if ((unsigned long )amdgpu_connector->router_bus == (unsigned long )((struct amdgpu_i2c_chan *)0)) {
    return;
  } else {
  }
  amdgpu_i2c_get_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      3, & val);
  val = (u8 )(~ ((int )((signed char )amdgpu_connector->router.cd_mux_control_pin)) & (int )((signed char )val));
  amdgpu_i2c_put_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      3, (int )val);
  amdgpu_i2c_get_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      1, & val);
  val = (u8 )(~ ((int )((signed char )amdgpu_connector->router.cd_mux_control_pin)) & (int )((signed char )val));
  val = (u8 )((int )amdgpu_connector->router.cd_mux_state | (int )val);
  amdgpu_i2c_put_byte(amdgpu_connector->router_bus, (int )amdgpu_connector->router.i2c_addr,
                      1, (int )val);
  return;
}
}
void ldv_initialize_i2c_algorithm_157(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1936UL);
  amdgpu_atombios_i2c_algo_group0 = (struct i2c_adapter *)tmp;
  return;
}
}
void ldv_main_exported_157(void)
{
  struct i2c_msg *ldvarg158 ;
  void *tmp ;
  int ldvarg157 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(16UL);
  ldvarg158 = (struct i2c_msg *)tmp;
  ldv_memset((void *)(& ldvarg157), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_157 == 1) {
    amdgpu_atombios_i2c_xfer(amdgpu_atombios_i2c_algo_group0, ldvarg158, ldvarg157);
    ldv_state_variable_157 = 1;
  } else {
  }
  goto ldv_48068;
  case 1: ;
  if (ldv_state_variable_157 == 1) {
    amdgpu_atombios_i2c_func(amdgpu_atombios_i2c_algo_group0);
    ldv_state_variable_157 = 1;
  } else {
  }
  goto ldv_48068;
  default:
  ldv_stop();
  }
  ldv_48068: ;
  return;
}
}
bool ldv_queue_work_on_187(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_188(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_189(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_190(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_191(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern char *strcpy(char * , char const * ) ;
__inline static int __atomic_add_unless___0(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___0(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___0(v, a, u);
  return (tmp != u);
}
}
bool ldv_queue_work_on_201(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_203(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_202(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_205(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_204(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static void memset_io(void volatile *addr , unsigned char val , size_t count )
{
  {
  memset((void *)addr, (int )val, count);
  return;
}
}
__inline static int kref_sub___2(struct kref *kref , unsigned int count , void (*release)(struct kref * ) )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 71);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = atomic_sub_and_test((int )count, & kref->refcount);
  if (tmp___0 != 0) {
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int kref_put___2(struct kref *kref , void (*release)(struct kref * ) )
{
  int tmp ;
  {
  tmp = kref_sub___2(kref, 1U, release);
  return (tmp);
}
}
__inline static int kref_put_mutex___0(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___0(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
extern void cfb_fillrect(struct fb_info * , struct fb_fillrect const * ) ;
extern void cfb_copyarea(struct fb_info * , struct fb_copyarea const * ) ;
extern void cfb_imageblit(struct fb_info * , struct fb_image const * ) ;
extern int unregister_framebuffer(struct fb_info * ) ;
extern void fb_set_suspend(struct fb_info * , int ) ;
extern struct fb_info *framebuffer_alloc(size_t , struct device * ) ;
extern void framebuffer_release(struct fb_info * ) ;
extern int fb_alloc_cmap(struct fb_cmap * , int , int ) ;
extern void fb_dealloc_cmap(struct fb_cmap * ) ;
extern void drm_framebuffer_unregister_private(struct drm_framebuffer * ) ;
extern u32 drm_mode_legacy_fb_format(u32 , u32 ) ;
extern void drm_fb_get_bpp_depth(u32 , unsigned int * , int * ) ;
extern void drm_helper_disable_unused_functions(struct drm_device * ) ;
__inline static void drm_gem_object_unreference___0(struct drm_gem_object *obj )
{
  {
  if ((unsigned long )obj != (unsigned long )((struct drm_gem_object *)0)) {
    kref_put___2(& obj->refcount, & drm_gem_object_free);
  } else {
  }
  return;
}
}
__inline static void drm_gem_object_unreference_unlocked___0(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___0(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
int amdgpu_fbdev_total_size(struct amdgpu_device *adev ) ;
int amdgpu_align_pitch(struct amdgpu_device *adev , int width , int bpp , bool tiled ) ;
int amdgpu_gem_object_create(struct amdgpu_device *adev , unsigned long size , int alignment ,
                             u32 initial_domain , u64 flags , bool kernel , struct drm_gem_object **obj ) ;
extern void drm_fb_helper_prepare(struct drm_device * , struct drm_fb_helper * , struct drm_fb_helper_funcs const * ) ;
extern int drm_fb_helper_init(struct drm_device * , struct drm_fb_helper * , int ,
                              int ) ;
extern void drm_fb_helper_fini(struct drm_fb_helper * ) ;
extern int drm_fb_helper_blank(int , struct fb_info * ) ;
extern int drm_fb_helper_pan_display(struct fb_var_screeninfo * , struct fb_info * ) ;
extern int drm_fb_helper_set_par(struct fb_info * ) ;
extern int drm_fb_helper_check_var(struct fb_var_screeninfo * , struct fb_info * ) ;
extern void drm_fb_helper_fill_var(struct fb_info * , struct drm_fb_helper * , u32 ,
                                   u32 ) ;
extern void drm_fb_helper_fill_fix(struct fb_info * , u32 , u32 ) ;
extern int drm_fb_helper_setcmap(struct fb_cmap * , struct fb_info * ) ;
extern int drm_fb_helper_hotplug_event(struct drm_fb_helper * ) ;
extern int drm_fb_helper_initial_config(struct drm_fb_helper * , int ) ;
extern int drm_fb_helper_single_add_all_connectors(struct drm_fb_helper * ) ;
extern int drm_fb_helper_debug_enter(struct fb_info * ) ;
extern int drm_fb_helper_debug_leave(struct fb_info * ) ;
extern void vga_switcheroo_client_fb_set(struct pci_dev * , struct fb_info * ) ;
static struct fb_ops amdgpufb_ops =
     {& __this_module, 0, 0, 0, 0, & drm_fb_helper_check_var, & drm_fb_helper_set_par,
    0, & drm_fb_helper_setcmap, & drm_fb_helper_blank, & drm_fb_helper_pan_display,
    & cfb_fillrect, & cfb_copyarea, & cfb_imageblit, 0, 0, 0, 0, 0, 0, 0, 0, & drm_fb_helper_debug_enter,
    & drm_fb_helper_debug_leave};
int amdgpu_align_pitch(struct amdgpu_device *adev , int width , int bpp , bool tiled )
{
  int aligned ;
  int pitch_mask ;
  {
  aligned = width;
  pitch_mask = 0;
  switch (bpp / 8) {
  case 1:
  pitch_mask = 255;
  goto ldv_43832;
  case 2:
  pitch_mask = 127;
  goto ldv_43832;
  case 3: ;
  case 4:
  pitch_mask = 63;
  goto ldv_43832;
  }
  ldv_43832:
  aligned = aligned + pitch_mask;
  aligned = ~ pitch_mask & aligned;
  return (aligned);
}
}
static void amdgpufb_destroy_pinned_object(struct drm_gem_object *gobj )
{
  struct amdgpu_bo *rbo ;
  struct drm_gem_object const *__mptr ;
  int ret ;
  long tmp ;
  {
  __mptr = (struct drm_gem_object const *)gobj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  ret = amdgpu_bo_reserve(rbo, 0);
  tmp = ldv__builtin_expect(ret == 0, 1L);
  if (tmp != 0L) {
    amdgpu_bo_kunmap(rbo);
    amdgpu_bo_unpin(rbo);
    amdgpu_bo_unreserve(rbo);
  } else {
  }
  drm_gem_object_unreference_unlocked___0(gobj);
  return;
}
}
static int amdgpufb_create_pinned_object(struct amdgpu_fbdev *rfbdev , struct drm_mode_fb_cmd2 *mode_cmd ,
                                         struct drm_gem_object **gobj_p )
{
  struct amdgpu_device *adev ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *rbo ;
  bool fb_tiled ;
  u32 tiling_flags ;
  int ret ;
  int aligned_size ;
  int size ;
  int height ;
  u32 bpp ;
  u32 depth ;
  int tmp ;
  struct drm_gem_object const *__mptr ;
  long tmp___0 ;
  {
  adev = rfbdev->adev;
  gobj = (struct drm_gem_object *)0;
  rbo = (struct amdgpu_bo *)0;
  fb_tiled = 0;
  tiling_flags = 0U;
  height = (int )mode_cmd->height;
  drm_fb_get_bpp_depth(mode_cmd->pixel_format, & depth, (int *)(& bpp));
  tmp = amdgpu_align_pitch(adev, (int )mode_cmd->width, (int )bpp, (int )fb_tiled);
  mode_cmd->pitches[0] = (u32 )tmp * ((bpp + 1U) / 8U);
  height = (int )(mode_cmd->height + 7U) & -8;
  size = (int )(mode_cmd->pitches[0] * (__u32 )height);
  aligned_size = (size + 4095) & -4096;
  ret = amdgpu_gem_object_create(adev, (unsigned long )aligned_size, 0, 4U, 0ULL,
                                 1, & gobj);
  if (ret != 0) {
    printk("\vfailed to allocate framebuffer (%d)\n", aligned_size);
    return (-12);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  if ((int )fb_tiled) {
    tiling_flags = 4U;
  } else {
  }
  ret = amdgpu_bo_reserve(rbo, 0);
  tmp___0 = ldv__builtin_expect(ret != 0, 0L);
  if (tmp___0 != 0L) {
    goto out_unref;
  } else {
  }
  if (tiling_flags != 0U) {
    ret = amdgpu_bo_set_tiling_flags(rbo, (u64 )tiling_flags);
    if (ret != 0) {
      dev_err((struct device const *)adev->dev, "FB failed to set tiling flags\n");
    } else {
    }
  } else {
  }
  ret = amdgpu_bo_pin_restricted(rbo, 4U, 0ULL, 0ULL, (u64 *)0ULL);
  if (ret != 0) {
    amdgpu_bo_unreserve(rbo);
    goto out_unref;
  } else {
  }
  ret = amdgpu_bo_kmap(rbo, (void **)0);
  amdgpu_bo_unreserve(rbo);
  if (ret != 0) {
    goto out_unref;
  } else {
  }
  *gobj_p = gobj;
  return (0);
  out_unref:
  amdgpufb_destroy_pinned_object(gobj);
  *gobj_p = (struct drm_gem_object *)0;
  return (ret);
}
}
static int amdgpufb_create(struct drm_fb_helper *helper , struct drm_fb_helper_surface_size *sizes )
{
  struct amdgpu_fbdev *rfbdev ;
  struct amdgpu_device *adev ;
  struct fb_info *info ;
  struct drm_framebuffer *fb ;
  struct drm_mode_fb_cmd2 mode_cmd ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *rbo ;
  struct device *device ;
  int ret ;
  unsigned long tmp ;
  struct drm_gem_object const *__mptr ;
  unsigned long tmp___0 ;
  u64 tmp___1 ;
  unsigned long tmp___2 ;
  unsigned long tmp___3 ;
  {
  rfbdev = (struct amdgpu_fbdev *)helper;
  adev = rfbdev->adev;
  fb = (struct drm_framebuffer *)0;
  gobj = (struct drm_gem_object *)0;
  rbo = (struct amdgpu_bo *)0;
  device = & (adev->pdev)->dev;
  mode_cmd.width = sizes->surface_width;
  mode_cmd.height = sizes->surface_height;
  if (sizes->surface_bpp == 24U) {
    sizes->surface_bpp = 32U;
  } else {
  }
  mode_cmd.pixel_format = drm_mode_legacy_fb_format(sizes->surface_bpp, sizes->surface_depth);
  ret = amdgpufb_create_pinned_object(rfbdev, & mode_cmd, & gobj);
  if (ret != 0) {
    drm_err("failed to create fbcon object %d\n", ret);
    return (ret);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  info = framebuffer_alloc(0UL, device);
  if ((unsigned long )info == (unsigned long )((struct fb_info *)0)) {
    ret = -12;
    goto out_unref;
  } else {
  }
  info->par = (void *)rfbdev;
  ret = amdgpu_framebuffer_init(adev->ddev, & rfbdev->rfb, & mode_cmd, gobj);
  if (ret != 0) {
    drm_err("failed to initialize framebuffer %d\n", ret);
    goto out_unref;
  } else {
  }
  fb = & rfbdev->rfb.base;
  rfbdev->helper.fb = fb;
  rfbdev->helper.fbdev = info;
  tmp___0 = amdgpu_bo_size(rbo);
  memset_io((void volatile *)rbo->kptr, 0, tmp___0);
  strcpy((char *)(& info->fix.id), "amdgpudrmfb");
  drm_fb_helper_fill_fix(info, fb->pitches[0], fb->depth);
  info->flags = 2097153;
  info->fbops = & amdgpufb_ops;
  tmp___1 = amdgpu_bo_gpu_offset(rbo);
  tmp = (unsigned long )(tmp___1 - adev->mc.vram_start);
  info->fix.smem_start = (unsigned long )(adev->mc.aper_base + (unsigned long long )tmp);
  tmp___2 = amdgpu_bo_size(rbo);
  info->fix.smem_len = (__u32 )tmp___2;
  info->screen_base = (char *)rbo->kptr;
  info->screen_size = amdgpu_bo_size(rbo);
  drm_fb_helper_fill_var(info, & rfbdev->helper, sizes->fb_width, sizes->fb_height);
  info->apertures = alloc_apertures(1U);
  if ((unsigned long )info->apertures == (unsigned long )((struct apertures_struct *)0)) {
    ret = -12;
    goto out_unref;
  } else {
  }
  (info->apertures)->ranges[0].base = (adev->ddev)->mode_config.fb_base;
  (info->apertures)->ranges[0].size = adev->mc.aper_size;
  if ((unsigned long )info->screen_base == (unsigned long )((char *)0)) {
    ret = -28;
    goto out_unref;
  } else {
  }
  ret = fb_alloc_cmap(& info->cmap, 256, 0);
  if (ret != 0) {
    ret = -12;
    goto out_unref;
  } else {
  }
  printk("\016[drm] fb mappable at 0x%lX\n", info->fix.smem_start);
  printk("\016[drm] vram apper at 0x%lX\n", (unsigned long )adev->mc.aper_base);
  tmp___3 = amdgpu_bo_size(rbo);
  printk("\016[drm] size %lu\n", tmp___3);
  printk("\016[drm] fb depth is %d\n", fb->depth);
  printk("\016[drm]    pitch is %d\n", fb->pitches[0]);
  vga_switcheroo_client_fb_set((adev->ddev)->pdev, info);
  return (0);
  out_unref: ;
  if ((unsigned long )fb != (unsigned long )((struct drm_framebuffer *)0) && ret != 0) {
    drm_gem_object_unreference___0(gobj);
    drm_framebuffer_unregister_private(fb);
    drm_framebuffer_cleanup(fb);
    kfree((void const *)fb);
  } else {
  }
  return (ret);
}
}
void amdgpu_fb_output_poll_changed(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.rfbdev != (unsigned long )((struct amdgpu_fbdev *)0)) {
    drm_fb_helper_hotplug_event(& (adev->mode_info.rfbdev)->helper);
  } else {
  }
  return;
}
}
static int amdgpu_fbdev_destroy(struct drm_device *dev , struct amdgpu_fbdev *rfbdev )
{
  struct fb_info *info ;
  struct amdgpu_framebuffer *rfb ;
  {
  rfb = & rfbdev->rfb;
  if ((unsigned long )rfbdev->helper.fbdev != (unsigned long )((struct fb_info *)0)) {
    info = rfbdev->helper.fbdev;
    unregister_framebuffer(info);
    if (info->cmap.len != 0U) {
      fb_dealloc_cmap(& info->cmap);
    } else {
    }
    framebuffer_release(info);
  } else {
  }
  if ((unsigned long )rfb->obj != (unsigned long )((struct drm_gem_object *)0)) {
    amdgpufb_destroy_pinned_object(rfb->obj);
    rfb->obj = (struct drm_gem_object *)0;
  } else {
  }
  drm_fb_helper_fini(& rfbdev->helper);
  drm_framebuffer_unregister_private(& rfb->base);
  drm_framebuffer_cleanup(& rfb->base);
  return (0);
}
}
static void amdgpu_crtc_fb_gamma_set(struct drm_crtc *crtc , u16 red , u16 green ,
                                     u16 blue , int regno )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  amdgpu_crtc->lut_r[regno] = (u16 )((int )red >> 6);
  amdgpu_crtc->lut_g[regno] = (u16 )((int )green >> 6);
  amdgpu_crtc->lut_b[regno] = (u16 )((int )blue >> 6);
  return;
}
}
static void amdgpu_crtc_fb_gamma_get(struct drm_crtc *crtc , u16 *red , u16 *green ,
                                     u16 *blue , int regno )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  *red = (int )amdgpu_crtc->lut_r[regno] << 6U;
  *green = (int )amdgpu_crtc->lut_g[regno] << 6U;
  *blue = (int )amdgpu_crtc->lut_b[regno] << 6U;
  return;
}
}
static struct drm_fb_helper_funcs const amdgpu_fb_helper_funcs = {& amdgpu_crtc_fb_gamma_set, & amdgpu_crtc_fb_gamma_get, & amdgpufb_create, 0};
int amdgpu_fbdev_init(struct amdgpu_device *adev )
{
  struct amdgpu_fbdev *rfbdev ;
  int bpp_sel ;
  int ret ;
  void *tmp ;
  {
  bpp_sel = 32;
  if (! adev->mode_info.mode_config_initialized) {
    return (0);
  } else {
  }
  if (adev->mc.real_vram_size <= 33554432ULL) {
    bpp_sel = 8;
  } else {
  }
  tmp = kzalloc(360UL, 208U);
  rfbdev = (struct amdgpu_fbdev *)tmp;
  if ((unsigned long )rfbdev == (unsigned long )((struct amdgpu_fbdev *)0)) {
    return (-12);
  } else {
  }
  rfbdev->adev = adev;
  adev->mode_info.rfbdev = rfbdev;
  drm_fb_helper_prepare(adev->ddev, & rfbdev->helper, & amdgpu_fb_helper_funcs);
  ret = drm_fb_helper_init(adev->ddev, & rfbdev->helper, adev->mode_info.num_crtc,
                           4);
  if (ret != 0) {
    kfree((void const *)rfbdev);
    return (ret);
  } else {
  }
  drm_fb_helper_single_add_all_connectors(& rfbdev->helper);
  drm_helper_disable_unused_functions(adev->ddev);
  drm_fb_helper_initial_config(& rfbdev->helper, bpp_sel);
  return (0);
}
}
void amdgpu_fbdev_fini(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.rfbdev == (unsigned long )((struct amdgpu_fbdev *)0)) {
    return;
  } else {
  }
  amdgpu_fbdev_destroy(adev->ddev, adev->mode_info.rfbdev);
  kfree((void const *)adev->mode_info.rfbdev);
  adev->mode_info.rfbdev = (struct amdgpu_fbdev *)0;
  return;
}
}
void amdgpu_fbdev_set_suspend(struct amdgpu_device *adev , int state )
{
  {
  if ((unsigned long )adev->mode_info.rfbdev != (unsigned long )((struct amdgpu_fbdev *)0)) {
    fb_set_suspend((adev->mode_info.rfbdev)->helper.fbdev, state);
  } else {
  }
  return;
}
}
int amdgpu_fbdev_total_size(struct amdgpu_device *adev )
{
  struct amdgpu_bo *robj ;
  int size ;
  struct drm_gem_object const *__mptr ;
  unsigned long tmp ;
  {
  size = 0;
  if ((unsigned long )adev->mode_info.rfbdev == (unsigned long )((struct amdgpu_fbdev *)0)) {
    return (0);
  } else {
  }
  __mptr = (struct drm_gem_object const *)(adev->mode_info.rfbdev)->rfb.obj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  tmp = amdgpu_bo_size(robj);
  size = (int )((unsigned int )tmp + (unsigned int )size);
  return (size);
}
}
bool amdgpu_fbdev_robj_is_fb(struct amdgpu_device *adev , struct amdgpu_bo *robj )
{
  struct drm_gem_object const *__mptr ;
  {
  if ((unsigned long )adev->mode_info.rfbdev == (unsigned long )((struct amdgpu_fbdev *)0)) {
    return (0);
  } else {
  }
  __mptr = (struct drm_gem_object const *)(adev->mode_info.rfbdev)->rfb.obj;
  if ((unsigned long )((struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL) == (unsigned long )robj) {
    return (1);
  } else {
  }
  return (0);
}
}
void ldv_initialize_fb_ops_156(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(160UL);
  amdgpufb_ops_group0 = (struct fb_var_screeninfo *)tmp;
  tmp___0 = ldv_init_zalloc(1608UL);
  amdgpufb_ops_group1 = (struct fb_info *)tmp___0;
  return;
}
}
void ldv_initialize_drm_fb_helper_funcs_155(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1160UL);
  amdgpu_fb_helper_funcs_group0 = (struct drm_crtc *)tmp;
  return;
}
}
void ldv_main_exported_155(void)
{
  u16 ldvarg1003 ;
  u16 *ldvarg999 ;
  void *tmp ;
  u16 *ldvarg998 ;
  void *tmp___0 ;
  struct drm_fb_helper_surface_size *ldvarg996 ;
  void *tmp___1 ;
  int ldvarg1001 ;
  u16 ldvarg1004 ;
  u16 *ldvarg1000 ;
  void *tmp___2 ;
  int ldvarg1005 ;
  struct drm_fb_helper *ldvarg997 ;
  void *tmp___3 ;
  u16 ldvarg1002 ;
  int tmp___4 ;
  {
  tmp = ldv_init_zalloc(2UL);
  ldvarg999 = (u16 *)tmp;
  tmp___0 = ldv_init_zalloc(2UL);
  ldvarg998 = (u16 *)tmp___0;
  tmp___1 = ldv_init_zalloc(24UL);
  ldvarg996 = (struct drm_fb_helper_surface_size *)tmp___1;
  tmp___2 = ldv_init_zalloc(2UL);
  ldvarg1000 = (u16 *)tmp___2;
  tmp___3 = ldv_init_zalloc(160UL);
  ldvarg997 = (struct drm_fb_helper *)tmp___3;
  ldv_memset((void *)(& ldvarg1003), 0, 2UL);
  ldv_memset((void *)(& ldvarg1001), 0, 4UL);
  ldv_memset((void *)(& ldvarg1004), 0, 2UL);
  ldv_memset((void *)(& ldvarg1005), 0, 4UL);
  ldv_memset((void *)(& ldvarg1002), 0, 2UL);
  tmp___4 = __VERIFIER_nondet_int();
  switch (tmp___4) {
  case 0: ;
  if (ldv_state_variable_155 == 1) {
    amdgpu_crtc_fb_gamma_set(amdgpu_fb_helper_funcs_group0, (int )ldvarg1003, (int )ldvarg1002,
                             (int )ldvarg1004, ldvarg1005);
    ldv_state_variable_155 = 1;
  } else {
  }
  goto ldv_43955;
  case 1: ;
  if (ldv_state_variable_155 == 1) {
    amdgpu_crtc_fb_gamma_get(amdgpu_fb_helper_funcs_group0, ldvarg999, ldvarg998,
                             ldvarg1000, ldvarg1001);
    ldv_state_variable_155 = 1;
  } else {
  }
  goto ldv_43955;
  case 2: ;
  if (ldv_state_variable_155 == 1) {
    amdgpufb_create(ldvarg997, ldvarg996);
    ldv_state_variable_155 = 1;
  } else {
  }
  goto ldv_43955;
  default:
  ldv_stop();
  }
  ldv_43955: ;
  return;
}
}
void ldv_main_exported_156(void)
{
  struct fb_fillrect *ldvarg934 ;
  void *tmp ;
  struct fb_cmap *ldvarg932 ;
  void *tmp___0 ;
  int ldvarg931 ;
  struct fb_copyarea *ldvarg933 ;
  void *tmp___1 ;
  struct fb_image *ldvarg930 ;
  void *tmp___2 ;
  int tmp___3 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg934 = (struct fb_fillrect *)tmp;
  tmp___0 = ldv_init_zalloc(40UL);
  ldvarg932 = (struct fb_cmap *)tmp___0;
  tmp___1 = ldv_init_zalloc(24UL);
  ldvarg933 = (struct fb_copyarea *)tmp___1;
  tmp___2 = ldv_init_zalloc(80UL);
  ldvarg930 = (struct fb_image *)tmp___2;
  ldv_memset((void *)(& ldvarg931), 0, 4UL);
  tmp___3 = __VERIFIER_nondet_int();
  switch (tmp___3) {
  case 0: ;
  if (ldv_state_variable_156 == 1) {
    cfb_fillrect(amdgpufb_ops_group1, (struct fb_fillrect const *)ldvarg934);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 1: ;
  if (ldv_state_variable_156 == 1) {
    cfb_copyarea(amdgpufb_ops_group1, (struct fb_copyarea const *)ldvarg933);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 2: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_debug_enter(amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 3: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_set_par(amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 4: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_pan_display(amdgpufb_ops_group0, amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 5: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_setcmap(ldvarg932, amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 6: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_blank(ldvarg931, amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 7: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_debug_leave(amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 8: ;
  if (ldv_state_variable_156 == 1) {
    drm_fb_helper_check_var(amdgpufb_ops_group0, amdgpufb_ops_group1);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  case 9: ;
  if (ldv_state_variable_156 == 1) {
    cfb_imageblit(amdgpufb_ops_group1, (struct fb_image const *)ldvarg930);
    ldv_state_variable_156 = 1;
  } else {
  }
  goto ldv_43968;
  default:
  ldv_stop();
  }
  ldv_43968: ;
  return;
}
}
bool ldv_queue_work_on_201(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_202(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_203(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_204(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_205(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void list_add_tail(struct list_head *new , struct list_head *head )
{
  {
  __list_add(new, head->prev, head);
  return;
}
}
__inline static int __atomic_add_unless___1(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___1(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___1(v, a, u);
  return (tmp != u);
}
}
extern unsigned long nsecs_to_jiffies(u64 ) ;
bool ldv_queue_work_on_215(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_217(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_216(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_219(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_218(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static int kref_put_mutex___1(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___1(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static pid_t task_pid_nr(struct task_struct *tsk )
{
  {
  return (tsk->pid);
}
}
extern void kvfree(void const * ) ;
__inline static void drm_free_large(void *ptr )
{
  {
  kvfree((void const *)ptr);
  return;
}
}
__inline static __u64 drm_vma_node_offset_addr(struct drm_vma_offset_node *node )
{
  {
  return (node->vm_node.start << 12);
}
}
extern void drm_prime_gem_destroy(struct drm_gem_object * , struct sg_table * ) ;
extern long reservation_object_wait_timeout_rcu(struct reservation_object * , bool ,
                                                bool , unsigned long ) ;
extern bool reservation_object_test_signaled_rcu(struct reservation_object * , bool ) ;
extern void ttm_eu_backoff_reservation(struct ww_acquire_ctx * , struct list_head * ) ;
extern int ttm_eu_reserve_buffers(struct ww_acquire_ctx * , struct list_head * , bool ,
                                  struct list_head * ) ;
__inline static void drm_gem_object_unreference_unlocked___1(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___1(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
unsigned long amdgpu_gem_timeout(uint64_t timeout_ns ) ;
int amdgpu_gem_init(struct amdgpu_device *adev ) ;
void amdgpu_gem_fini(struct amdgpu_device *adev ) ;
int amdgpu_mn_register(struct amdgpu_bo *bo , unsigned long addr ) ;
void amdgpu_mn_unregister(struct amdgpu_bo *bo ) ;
struct amdgpu_bo_list_entry *amdgpu_vm_get_bos(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                               struct list_head *head ) ;
int amdgpu_vm_clear_freed(struct amdgpu_device *adev , struct amdgpu_vm *vm ) ;
int amdgpu_vm_bo_update(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va ,
                        struct ttm_mem_reg *mem ) ;
struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm , struct amdgpu_bo *bo ) ;
struct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                      struct amdgpu_bo *bo ) ;
int amdgpu_vm_bo_map(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va , uint64_t saddr ,
                     uint64_t offset , uint64_t size , u32 flags ) ;
int amdgpu_vm_bo_unmap(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va , uint64_t saddr ) ;
void amdgpu_vm_bo_rmv(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va ) ;
__inline static unsigned int amdgpu_mem_type_to_domain(u32 mem_type )
{
  {
  switch (mem_type) {
  case 2U: ;
  return (4U);
  case 1U: ;
  return (2U);
  case 0U: ;
  return (1U);
  case 3U: ;
  return (8U);
  case 4U: ;
  return (16U);
  case 5U: ;
  return (32U);
  default: ;
  goto ldv_43505;
  }
  ldv_43505: ;
  return (0U);
}
}
__inline static u64 amdgpu_bo_mmap_offset(struct amdgpu_bo *bo )
{
  __u64 tmp ;
  {
  tmp = drm_vma_node_offset_addr(& bo->tbo.vma_node);
  return (tmp);
}
}
void amdgpu_gem_object_free(struct drm_gem_object *gobj )
{
  struct amdgpu_bo *robj ;
  struct drm_gem_object const *__mptr ;
  {
  __mptr = (struct drm_gem_object const *)gobj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  if ((unsigned long )robj != (unsigned long )((struct amdgpu_bo *)0)) {
    if ((unsigned long )robj->gem_base.import_attach != (unsigned long )((struct dma_buf_attachment *)0)) {
      drm_prime_gem_destroy(& robj->gem_base, robj->tbo.sg);
    } else {
    }
    amdgpu_mn_unregister(robj);
    amdgpu_bo_unref(& robj);
  } else {
  }
  return;
}
}
int amdgpu_gem_object_create(struct amdgpu_device *adev , unsigned long size , int alignment ,
                             u32 initial_domain , u64 flags , bool kernel , struct drm_gem_object **obj )
{
  struct amdgpu_bo *robj ;
  unsigned long max_size ;
  int r ;
  long tmp ;
  struct task_struct *tmp___0 ;
  {
  *obj = (struct drm_gem_object *)0;
  if ((unsigned int )alignment <= 4095U) {
    alignment = 4096;
  } else {
  }
  if ((initial_domain & 56U) == 0U) {
    max_size = (unsigned long )(adev->mc.gtt_size - adev->gart_pin_size);
    if (size > max_size) {
      tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
      if (tmp != 0L) {
        drm_ut_debug_printk("amdgpu_gem_object_create", "Allocation size %ldMb bigger than %ldMb limit\n",
                            size >> 20, max_size >> 20);
      } else {
      }
      return (-12);
    } else {
    }
  } else {
  }
  retry:
  r = amdgpu_bo_create(adev, size, alignment, (int )kernel, initial_domain, flags,
                       (struct sg_table *)0, & robj);
  if (r != 0) {
    if (r != -512) {
      if (initial_domain == 4U) {
        initial_domain = initial_domain | 2U;
        goto retry;
      } else {
      }
      drm_err("Failed to allocate GEM object (%ld, %d, %u, %d)\n", size, initial_domain,
              alignment, r);
    } else {
    }
    return (r);
  } else {
  }
  *obj = & robj->gem_base;
  tmp___0 = get_current();
  robj->pid = task_pid_nr(tmp___0);
  mutex_lock_nested(& adev->gem.mutex, 0U);
  list_add_tail(& robj->list, & adev->gem.objects);
  mutex_unlock(& adev->gem.mutex);
  return (0);
}
}
int amdgpu_gem_init(struct amdgpu_device *adev )
{
  {
  INIT_LIST_HEAD(& adev->gem.objects);
  return (0);
}
}
void amdgpu_gem_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_bo_force_delete(adev);
  return;
}
}
int amdgpu_gem_object_open(struct drm_gem_object *obj , struct drm_file *file_priv )
{
  struct amdgpu_bo *rbo ;
  struct drm_gem_object const *__mptr ;
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_vm *vm ;
  struct amdgpu_bo_va *bo_va ;
  int r ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  adev = rbo->adev;
  fpriv = (struct amdgpu_fpriv *)file_priv->driver_priv;
  vm = & fpriv->vm;
  r = amdgpu_bo_reserve(rbo, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  bo_va = amdgpu_vm_bo_find(vm, rbo);
  if ((unsigned long )bo_va == (unsigned long )((struct amdgpu_bo_va *)0)) {
    bo_va = amdgpu_vm_bo_add(adev, vm, rbo);
  } else {
    bo_va->ref_count = bo_va->ref_count + 1U;
  }
  amdgpu_bo_unreserve(rbo);
  return (0);
}
}
void amdgpu_gem_object_close(struct drm_gem_object *obj , struct drm_file *file_priv )
{
  struct amdgpu_bo *rbo ;
  struct drm_gem_object const *__mptr ;
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_vm *vm ;
  struct amdgpu_bo_va *bo_va ;
  int r ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  adev = rbo->adev;
  fpriv = (struct amdgpu_fpriv *)file_priv->driver_priv;
  vm = & fpriv->vm;
  r = amdgpu_bo_reserve(rbo, 1);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "leaking bo va because we fail to reserve bo (%d)\n",
            r);
    return;
  } else {
  }
  bo_va = amdgpu_vm_bo_find(vm, rbo);
  if ((unsigned long )bo_va != (unsigned long )((struct amdgpu_bo_va *)0)) {
    bo_va->ref_count = bo_va->ref_count - 1U;
    if (bo_va->ref_count == 0U) {
      amdgpu_vm_bo_rmv(adev, bo_va);
    } else {
    }
  } else {
  }
  amdgpu_bo_unreserve(rbo);
  return;
}
}
static int amdgpu_gem_handle_lockup(struct amdgpu_device *adev , int r )
{
  {
  if (r == -35) {
    r = amdgpu_gpu_reset(adev);
    if (r == 0) {
      r = -11;
    } else {
    }
  } else {
  }
  return (r);
}
}
int amdgpu_gem_create_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct amdgpu_device *adev ;
  union drm_amdgpu_gem_create *args ;
  uint64_t size ;
  struct drm_gem_object *gobj ;
  u32 handle ;
  bool kernel ;
  int r ;
  unsigned long __y ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  args = (union drm_amdgpu_gem_create *)data;
  size = args->in.bo_size;
  kernel = 0;
  down_read(& adev->exclusive_lock);
  if ((args->in.domains & 56ULL) != 0ULL) {
    kernel = 1;
    if (args->in.domains == 8ULL) {
      size = size << 2;
    } else
    if (args->in.domains == 16ULL) {
      size = size << 12;
    } else
    if (args->in.domains == 32ULL) {
      size = size << 12;
    } else {
      r = -22;
      goto error_unlock;
    }
  } else {
  }
  __y = 4096UL;
  size = ((((unsigned long long )__y + size) - 1ULL) / (unsigned long long )__y) * (unsigned long long )__y;
  r = amdgpu_gem_object_create(adev, (unsigned long )size, (int )args->in.alignment,
                               (unsigned int )args->in.domains, args->in.domain_flags,
                               (int )kernel, & gobj);
  if (r != 0) {
    goto error_unlock;
  } else {
  }
  r = drm_gem_handle_create(filp, gobj, & handle);
  drm_gem_object_unreference_unlocked___1(gobj);
  if (r != 0) {
    goto error_unlock;
  } else {
  }
  memset((void *)args, 0, 32UL);
  args->out.handle = handle;
  up_read(& adev->exclusive_lock);
  return (0);
  error_unlock:
  up_read(& adev->exclusive_lock);
  r = amdgpu_gem_handle_lockup(adev, r);
  return (r);
}
}
int amdgpu_gem_userptr_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct amdgpu_device *adev ;
  struct drm_amdgpu_gem_userptr *args ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *bo ;
  u32 handle ;
  int r ;
  struct drm_gem_object const *__mptr ;
  struct task_struct *tmp ;
  struct task_struct *tmp___0 ;
  struct task_struct *tmp___1 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  args = (struct drm_amdgpu_gem_userptr *)data;
  if (((unsigned long )(args->addr | args->size) & 4095UL) != 0UL) {
    return (-22);
  } else {
  }
  if ((args->flags & 4294967280U) != 0U) {
    return (-22);
  } else {
  }
  if ((args->flags & 2U) == 0U || (args->flags & 8U) == 0U) {
    return (-13);
  } else {
  }
  down_read(& adev->exclusive_lock);
  r = amdgpu_gem_object_create(adev, (unsigned long )args->size, 0, 1U, 0ULL, 0, & gobj);
  if (r != 0) {
    goto handle_lockup;
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  r = amdgpu_ttm_tt_set_userptr(bo->tbo.ttm, args->addr, args->flags);
  if (r != 0) {
    goto release_object;
  } else {
  }
  if ((args->flags & 8U) != 0U) {
    r = amdgpu_mn_register(bo, (unsigned long )args->addr);
    if (r != 0) {
      goto release_object;
    } else {
    }
  } else {
  }
  if ((args->flags & 4U) != 0U) {
    tmp = get_current();
    down_read(& (tmp->mm)->mmap_sem);
    r = amdgpu_bo_reserve(bo, 1);
    if (r != 0) {
      tmp___0 = get_current();
      up_read(& (tmp___0->mm)->mmap_sem);
      goto release_object;
    } else {
    }
    amdgpu_ttm_placement_from_domain(bo, 2U);
    r = ttm_bo_validate(& bo->tbo, & bo->placement, 1, 0);
    amdgpu_bo_unreserve(bo);
    tmp___1 = get_current();
    up_read(& (tmp___1->mm)->mmap_sem);
    if (r != 0) {
      goto release_object;
    } else {
    }
  } else {
  }
  r = drm_gem_handle_create(filp, gobj, & handle);
  drm_gem_object_unreference_unlocked___1(gobj);
  if (r != 0) {
    goto handle_lockup;
  } else {
  }
  args->handle = handle;
  up_read(& adev->exclusive_lock);
  return (0);
  release_object:
  drm_gem_object_unreference_unlocked___1(gobj);
  handle_lockup:
  up_read(& adev->exclusive_lock);
  r = amdgpu_gem_handle_lockup(adev, r);
  return (r);
}
}
int amdgpu_mode_dumb_mmap(struct drm_file *filp , struct drm_device *dev , u32 handle ,
                          uint64_t *offset_p )
{
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *robj ;
  struct drm_gem_object const *__mptr ;
  bool tmp ;
  {
  gobj = drm_gem_object_lookup(dev, filp, handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    return (-2);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  tmp = amdgpu_ttm_tt_has_userptr(robj->tbo.ttm);
  if ((int )tmp || (robj->flags & 2ULL) != 0ULL) {
    drm_gem_object_unreference_unlocked___1(gobj);
    return (-1);
  } else {
  }
  *offset_p = amdgpu_bo_mmap_offset(robj);
  drm_gem_object_unreference_unlocked___1(gobj);
  return (0);
}
}
int amdgpu_gem_mmap_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  union drm_amdgpu_gem_mmap *args ;
  u32 handle ;
  int tmp ;
  {
  args = (union drm_amdgpu_gem_mmap *)data;
  handle = args->in.handle;
  memset((void *)args, 0, 8UL);
  tmp = amdgpu_mode_dumb_mmap(filp, dev, handle, & args->out.addr_ptr);
  return (tmp);
}
}
unsigned long amdgpu_gem_timeout(uint64_t timeout_ns )
{
  unsigned long timeout_jiffies ;
  ktime_t timeout ;
  ktime_t __constr_expr_0 ;
  ktime_t tmp ;
  {
  if ((long long )timeout_ns < 0LL) {
    return (9223372036854775807UL);
  } else {
  }
  tmp = ktime_get();
  __constr_expr_0.tv64 = (long long )((unsigned long long )tmp.tv64 - timeout_ns);
  timeout = __constr_expr_0;
  if (timeout.tv64 < 0LL) {
    return (0UL);
  } else {
  }
  timeout_jiffies = nsecs_to_jiffies((u64 )timeout.tv64);
  if ((long )timeout_jiffies < 0L) {
    return (9223372036854775806UL);
  } else {
  }
  return (timeout_jiffies);
}
}
int amdgpu_gem_wait_idle_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct amdgpu_device *adev ;
  union drm_amdgpu_gem_wait_idle *args ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *robj ;
  u32 handle ;
  unsigned long timeout ;
  unsigned long tmp ;
  int r ;
  long ret ;
  struct drm_gem_object const *__mptr ;
  bool tmp___0 ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  args = (union drm_amdgpu_gem_wait_idle *)data;
  handle = args->in.handle;
  tmp = amdgpu_gem_timeout(args->in.timeout);
  timeout = tmp;
  r = 0;
  gobj = drm_gem_object_lookup(dev, filp, handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    return (-2);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  if (timeout == 0UL) {
    tmp___0 = reservation_object_test_signaled_rcu(robj->tbo.resv, 1);
    ret = (long )tmp___0;
  } else {
    ret = reservation_object_wait_timeout_rcu(robj->tbo.resv, 1, 1, timeout);
  }
  if (ret >= 0L) {
    memset((void *)args, 0, 16UL);
    args->out.status = ret == 0L;
  } else {
    r = (int )ret;
  }
  drm_gem_object_unreference_unlocked___1(gobj);
  r = amdgpu_gem_handle_lockup(adev, r);
  return (r);
}
}
int amdgpu_gem_metadata_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct drm_amdgpu_gem_metadata *args ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *robj ;
  int r ;
  long tmp ;
  struct drm_gem_object const *__mptr ;
  long tmp___0 ;
  {
  args = (struct drm_amdgpu_gem_metadata *)data;
  r = -1;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_gem_metadata_ioctl", "%d \n", args->handle);
  } else {
  }
  gobj = drm_gem_object_lookup(dev, filp, args->handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    return (-2);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(robj, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    goto out;
  } else {
  }
  if (args->op == 2U) {
    amdgpu_bo_get_tiling_flags(robj, & args->data.tiling_info);
    r = amdgpu_bo_get_metadata(robj, (void *)(& args->data.data), 256UL, & args->data.data_size_bytes,
                               & args->data.flags);
  } else
  if (args->op == 1U) {
    r = amdgpu_bo_set_tiling_flags(robj, args->data.tiling_info);
    if (r == 0) {
      r = amdgpu_bo_set_metadata(robj, (void *)(& args->data.data), args->data.data_size_bytes,
                                 args->data.flags);
    } else {
    }
  } else {
  }
  amdgpu_bo_unreserve(robj);
  out:
  drm_gem_object_unreference_unlocked___1(gobj);
  return (r);
}
}
static void amdgpu_gem_va_update_vm(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va )
{
  struct ttm_validate_buffer tv ;
  struct ttm_validate_buffer *entry ;
  struct amdgpu_bo_list_entry *vm_bos ;
  struct ww_acquire_ctx ticket ;
  struct list_head list ;
  unsigned int domain ;
  int r ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  INIT_LIST_HEAD(& list);
  tv.bo = & (bo_va->bo)->tbo;
  tv.shared = 1;
  list_add(& tv.head, & list);
  vm_bos = amdgpu_vm_get_bos(adev, bo_va->vm, & list);
  if ((unsigned long )vm_bos == (unsigned long )((struct amdgpu_bo_list_entry *)0)) {
    return;
  } else {
  }
  r = ttm_eu_reserve_buffers(& ticket, & list, 1, (struct list_head *)0);
  if (r != 0) {
    goto error_free;
  } else {
  }
  __mptr = (struct list_head const *)list.next;
  entry = (struct ttm_validate_buffer *)__mptr;
  goto ldv_43793;
  ldv_43792:
  domain = amdgpu_mem_type_to_domain((entry->bo)->mem.mem_type);
  if (domain == 1U) {
    goto error_unreserve;
  } else {
  }
  __mptr___0 = (struct list_head const *)entry->head.next;
  entry = (struct ttm_validate_buffer *)__mptr___0;
  ldv_43793: ;
  if ((unsigned long )(& entry->head) != (unsigned long )(& list)) {
    goto ldv_43792;
  } else {
  }
  mutex_lock_nested(& (bo_va->vm)->mutex, 0U);
  r = amdgpu_vm_clear_freed(adev, bo_va->vm);
  if (r != 0) {
    goto error_unlock;
  } else {
  }
  r = amdgpu_vm_bo_update(adev, bo_va, & (bo_va->bo)->tbo.mem);
  error_unlock:
  mutex_unlock(& (bo_va->vm)->mutex);
  error_unreserve:
  ttm_eu_backoff_reservation(& ticket, & list);
  error_free:
  drm_free_large((void *)vm_bos);
  if (r != 0 && r != -512) {
    drm_err("Couldn\'t update BO_VA (%d)\n", r);
  } else {
  }
  return;
}
}
int amdgpu_gem_va_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct drm_amdgpu_gem_va *args ;
  struct drm_gem_object *gobj ;
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_bo *rbo ;
  struct amdgpu_bo_va *bo_va ;
  u32 invalid_flags ;
  u32 va_flags ;
  int r ;
  struct drm_gem_object const *__mptr ;
  {
  args = (struct drm_amdgpu_gem_va *)data;
  adev = (struct amdgpu_device *)dev->dev_private;
  fpriv = (struct amdgpu_fpriv *)filp->driver_priv;
  va_flags = 0U;
  r = 0;
  if (! adev->vm_manager.enabled) {
    return (-25);
  } else {
  }
  if (args->va_address <= 8388607ULL) {
    dev_err((struct device const *)(& (dev->pdev)->dev), "va_address 0x%lX is in reserved area 0x%X\n",
            (unsigned long )args->va_address, 8388608);
    return (-22);
  } else {
  }
  invalid_flags = 4294967280U;
  if ((args->flags & invalid_flags) != 0U) {
    dev_err((struct device const *)(& (dev->pdev)->dev), "invalid flags 0x%08X vs 0x%08X\n",
            args->flags, invalid_flags);
    return (-22);
  } else {
  }
  switch (args->operation) {
  case 1U: ;
  case 2U: ;
  goto ldv_43812;
  default:
  dev_err((struct device const *)(& (dev->pdev)->dev), "unsupported operation %d\n",
          args->operation);
  return (-22);
  }
  ldv_43812:
  gobj = drm_gem_object_lookup(dev, filp, args->handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    return (-2);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  rbo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(rbo, 0);
  if (r != 0) {
    drm_gem_object_unreference_unlocked___1(gobj);
    return (r);
  } else {
  }
  bo_va = amdgpu_vm_bo_find(& fpriv->vm, rbo);
  if ((unsigned long )bo_va == (unsigned long )((struct amdgpu_bo_va *)0)) {
    amdgpu_bo_unreserve(rbo);
    return (-2);
  } else {
  }
  switch (args->operation) {
  case 1U: ;
  if ((args->flags & 2U) != 0U) {
    va_flags = va_flags | 32U;
  } else {
  }
  if ((args->flags & 4U) != 0U) {
    va_flags = va_flags | 64U;
  } else {
  }
  if ((args->flags & 8U) != 0U) {
    va_flags = va_flags | 16U;
  } else {
  }
  r = amdgpu_vm_bo_map(adev, bo_va, args->va_address, args->offset_in_bo, args->map_size,
                       va_flags);
  goto ldv_43817;
  case 2U:
  r = amdgpu_vm_bo_unmap(adev, bo_va, args->va_address);
  goto ldv_43817;
  default: ;
  goto ldv_43817;
  }
  ldv_43817: ;
  if (r == 0 && (args->flags & 1U) == 0U) {
    amdgpu_gem_va_update_vm(adev, bo_va);
  } else {
  }
  drm_gem_object_unreference_unlocked___1(gobj);
  return (r);
}
}
int amdgpu_gem_op_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct drm_amdgpu_gem_op *args ;
  struct drm_gem_object *gobj ;
  struct amdgpu_bo *robj ;
  int r ;
  struct drm_gem_object const *__mptr ;
  long tmp ;
  struct drm_amdgpu_gem_create_in info ;
  void *out ;
  unsigned long tmp___0 ;
  bool tmp___1 ;
  {
  args = (struct drm_amdgpu_gem_op *)data;
  gobj = drm_gem_object_lookup(dev, filp, args->handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    return (-2);
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  robj = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(robj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    goto out;
  } else {
  }
  switch (args->op) {
  case 0U:
  out = (void *)args->value;
  info.bo_size = (uint64_t )robj->gem_base.size;
  info.alignment = (uint64_t )(robj->tbo.mem.page_alignment << 12);
  info.domains = (uint64_t )robj->initial_domain;
  info.domain_flags = robj->flags;
  tmp___0 = copy_to_user(out, (void const *)(& info), 32UL);
  if (tmp___0 != 0UL) {
    r = -14;
  } else {
  }
  goto ldv_43835;
  case 1U:
  tmp___1 = amdgpu_ttm_tt_has_userptr(robj->tbo.ttm);
  if ((int )tmp___1) {
    r = -1;
    goto ldv_43835;
  } else {
  }
  robj->initial_domain = (u32 )args->value & 7U;
  goto ldv_43835;
  default:
  r = -22;
  }
  ldv_43835:
  amdgpu_bo_unreserve(robj);
  out:
  drm_gem_object_unreference_unlocked___1(gobj);
  return (r);
}
}
int amdgpu_mode_dumb_create(struct drm_file *file_priv , struct drm_device *dev ,
                            struct drm_mode_create_dumb *args )
{
  struct amdgpu_device *adev ;
  struct drm_gem_object *gobj ;
  u32 handle ;
  int r ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_align_pitch(adev, (int )args->width, (int )args->bpp, 0);
  args->pitch = (u32 )tmp * ((args->bpp + 1U) / 8U);
  args->size = (uint64_t )(args->pitch * args->height);
  args->size = (args->size + 4095ULL) & 0xfffffffffffff000ULL;
  r = amdgpu_gem_object_create(adev, (unsigned long )args->size, 0, 4U, 0ULL, 0, & gobj);
  if (r != 0) {
    return (-12);
  } else {
  }
  r = drm_gem_handle_create(file_priv, gobj, & handle);
  drm_gem_object_unreference_unlocked___1(gobj);
  if (r != 0) {
    return (r);
  } else {
  }
  args->handle = handle;
  return (0);
}
}
static int amdgpu_debugfs_gem_info(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_bo *rbo ;
  unsigned int i ;
  struct list_head const *__mptr ;
  unsigned int domain ;
  char const *placement ;
  unsigned long tmp ;
  unsigned long tmp___0 ;
  struct list_head const *__mptr___0 ;
  {
  node = (struct drm_info_node *)m->private;
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  i = 0U;
  mutex_lock_nested(& adev->gem.mutex, 0U);
  __mptr = (struct list_head const *)adev->gem.objects.next;
  rbo = (struct amdgpu_bo *)__mptr;
  goto ldv_43868;
  ldv_43867:
  domain = amdgpu_mem_type_to_domain(rbo->tbo.mem.mem_type);
  switch (domain) {
  case 4U:
  placement = "VRAM";
  goto ldv_43863;
  case 2U:
  placement = " GTT";
  goto ldv_43863;
  case 1U: ;
  default:
  placement = " CPU";
  goto ldv_43863;
  }
  ldv_43863:
  tmp = amdgpu_bo_size(rbo);
  tmp___0 = amdgpu_bo_size(rbo);
  seq_printf(m, "bo[0x%08x] %8ldkB %8ldMB %s pid %8ld\n", i, tmp___0 >> 10, tmp >> 20,
             placement, (unsigned long )rbo->pid);
  i = i + 1U;
  __mptr___0 = (struct list_head const *)rbo->list.next;
  rbo = (struct amdgpu_bo *)__mptr___0;
  ldv_43868: ;
  if ((unsigned long )(& rbo->list) != (unsigned long )(& adev->gem.objects)) {
    goto ldv_43867;
  } else {
  }
  mutex_unlock(& adev->gem.mutex);
  return (0);
}
}
static struct drm_info_list amdgpu_debugfs_gem_list[1U] = { {"amdgpu_gem_info", & amdgpu_debugfs_gem_info, 0U, (void *)0}};
int amdgpu_gem_debugfs_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = amdgpu_debugfs_add_files(adev, (struct drm_info_list *)(& amdgpu_debugfs_gem_list),
                                 1U);
  return (tmp);
  return (0);
}
}
bool ldv_queue_work_on_215(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_216(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_217(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_218(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_219(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static int fls64(__u64 x )
{
  int bitpos ;
  {
  bitpos = -1;
  __asm__ ("bsrq %1,%q0": "+r" (bitpos): "rm" (x));
  return (bitpos + 1);
}
}
__inline static unsigned int fls_long(unsigned long l )
{
  int tmp___0 ;
  {
  tmp___0 = fls64((__u64 )l);
  return ((unsigned int )tmp___0);
}
}
__inline static int __ilog2_u64(u64 n )
{
  int tmp ;
  {
  tmp = fls64(n);
  return (tmp + -1);
}
}
__inline static unsigned long __roundup_pow_of_two(unsigned long n )
{
  unsigned int tmp ;
  {
  tmp = fls_long(n - 1UL);
  return (1UL << (int )tmp);
}
}
extern u64 jiffies_64 ;
bool ldv_queue_work_on_229(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_231(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_230(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_233(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_232(struct workqueue_struct *ldv_func_arg1 ) ;
extern int seq_puts(struct seq_file * , char const * ) ;
void amdgpu_ring_free_size(struct amdgpu_ring *ring ) ;
int amdgpu_ring_alloc(struct amdgpu_ring *ring , unsigned int ndw ) ;
void amdgpu_ring_commit(struct amdgpu_ring *ring ) ;
void amdgpu_ring_undo(struct amdgpu_ring *ring ) ;
void amdgpu_ring_lockup_update(struct amdgpu_ring *ring ) ;
bool amdgpu_ring_test_lockup(struct amdgpu_ring *ring ) ;
int amdgpu_ring_init(struct amdgpu_device *adev , struct amdgpu_ring *ring , unsigned int ring_size ,
                     u32 nop , u32 align_mask , struct amdgpu_irq_src *irq_src , unsigned int irq_type ,
                     enum amdgpu_ring_type ring_type ) ;
void amdgpu_ring_fini(struct amdgpu_ring *ring ) ;
__inline static void amdgpu_ring_write(struct amdgpu_ring *ring , u32 v )
{
  unsigned int tmp ;
  {
  if (ring->count_dw <= 0) {
    drm_err("amdgpu: writing more dwords to the ring than expected!\n");
  } else {
  }
  tmp = ring->wptr;
  ring->wptr = ring->wptr + 1U;
  *(ring->ring + (unsigned long )tmp) = v;
  ring->wptr = ring->wptr & ring->ptr_mask;
  ring->count_dw = ring->count_dw - 1;
  ring->ring_free_dw = ring->ring_free_dw - 1U;
  return;
}
}
static int amdgpu_debugfs_ring_init(struct amdgpu_device *adev , struct amdgpu_ring *ring ) ;
void amdgpu_ring_free_size(struct amdgpu_ring *ring )
{
  u32 rptr ;
  u32 tmp ;
  {
  tmp = (*((ring->funcs)->get_rptr))(ring);
  rptr = tmp;
  ring->ring_free_dw = ring->ring_size / 4U + rptr;
  ring->ring_free_dw = ring->ring_free_dw - ring->wptr;
  ring->ring_free_dw = ring->ring_free_dw & ring->ptr_mask;
  if (ring->ring_free_dw == 0U) {
    ring->ring_free_dw = ring->ring_size / 4U;
    amdgpu_ring_lockup_update(ring);
  } else {
  }
  return;
}
}
int amdgpu_ring_alloc(struct amdgpu_ring *ring , unsigned int ndw )
{
  int r ;
  {
  if (ring->ring_size / 4U < ndw) {
    return (-12);
  } else {
  }
  amdgpu_ring_free_size(ring);
  ndw = (ring->align_mask + ndw) & ~ ring->align_mask;
  goto ldv_47766;
  ldv_47765:
  amdgpu_ring_free_size(ring);
  if (ring->ring_free_dw > ndw) {
    goto ldv_47764;
  } else {
  }
  r = amdgpu_fence_wait_next(ring);
  if (r != 0) {
    return (r);
  } else {
  }
  ldv_47766: ;
  if (ring->ring_free_dw - 1U < ndw) {
    goto ldv_47765;
  } else {
  }
  ldv_47764:
  ring->count_dw = (int )ndw;
  ring->wptr_old = ring->wptr;
  return (0);
}
}
int amdgpu_ring_lock(struct amdgpu_ring *ring , unsigned int ndw )
{
  int r ;
  {
  mutex_lock_nested(ring->ring_lock, 0U);
  r = amdgpu_ring_alloc(ring, ndw);
  if (r != 0) {
    mutex_unlock(ring->ring_lock);
    return (r);
  } else {
  }
  return (0);
}
}
void amdgpu_ring_commit(struct amdgpu_ring *ring )
{
  {
  goto ldv_47776;
  ldv_47775:
  amdgpu_ring_write(ring, ring->nop);
  ldv_47776: ;
  if ((ring->wptr & ring->align_mask) != 0U) {
    goto ldv_47775;
  } else {
  }
  __asm__ volatile ("mfence": : : "memory");
  (*((ring->funcs)->set_wptr))(ring);
  return;
}
}
void amdgpu_ring_unlock_commit(struct amdgpu_ring *ring )
{
  {
  amdgpu_ring_commit(ring);
  mutex_unlock(ring->ring_lock);
  return;
}
}
void amdgpu_ring_undo(struct amdgpu_ring *ring )
{
  {
  ring->wptr = ring->wptr_old;
  return;
}
}
void amdgpu_ring_unlock_undo(struct amdgpu_ring *ring )
{
  {
  amdgpu_ring_undo(ring);
  mutex_unlock(ring->ring_lock);
  return;
}
}
void amdgpu_ring_lockup_update(struct amdgpu_ring *ring )
{
  u32 tmp ;
  {
  tmp = (*((ring->funcs)->get_rptr))(ring);
  atomic_set(& ring->last_rptr, (int )tmp);
  atomic64_set(& ring->last_activity, (long )jiffies_64);
  return;
}
}
bool amdgpu_ring_test_lockup(struct amdgpu_ring *ring )
{
  u32 rptr ;
  u32 tmp ;
  uint64_t last ;
  long tmp___0 ;
  uint64_t elapsed ;
  int tmp___1 ;
  unsigned int tmp___2 ;
  {
  tmp = (*((ring->funcs)->get_rptr))(ring);
  rptr = tmp;
  tmp___0 = atomic64_read((atomic64_t const *)(& ring->last_activity));
  last = (uint64_t )tmp___0;
  tmp___1 = atomic_read((atomic_t const *)(& ring->last_rptr));
  if ((u32 )tmp___1 != rptr) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___2 = jiffies_to_msecs((unsigned long const )(jiffies_64 - last));
  elapsed = (uint64_t )tmp___2;
  if (amdgpu_lockup_timeout != 0 && (uint64_t )amdgpu_lockup_timeout <= elapsed) {
    dev_err((struct device const *)(ring->adev)->dev, "ring %d stalled for more than %llumsec\n",
            ring->idx, elapsed);
    return (1);
  } else {
  }
  return (0);
}
}
unsigned int amdgpu_ring_backup(struct amdgpu_ring *ring , u32 **data )
{
  unsigned int size ;
  unsigned int ptr ;
  unsigned int i ;
  unsigned int tmp ;
  void *tmp___0 ;
  unsigned int tmp___1 ;
  {
  mutex_lock_nested(ring->ring_lock, 0U);
  *data = (u32 *)0U;
  if ((unsigned long )ring->ring_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    mutex_unlock(ring->ring_lock);
    return (0U);
  } else {
  }
  tmp = amdgpu_fence_count_emitted(ring);
  if (tmp == 0U) {
    mutex_unlock(ring->ring_lock);
    return (0U);
  } else {
  }
  ptr = *(ring->next_rptr_cpu_addr);
  size = ring->wptr + ring->ring_size / 4U;
  size = size - ptr;
  size = ring->ptr_mask & size;
  if (size == 0U) {
    mutex_unlock(ring->ring_lock);
    return (0U);
  } else {
  }
  tmp___0 = kmalloc_array((size_t )size, 4UL, 208U);
  *data = (u32 *)tmp___0;
  if ((unsigned long )*data == (unsigned long )((u32 *)0U)) {
    mutex_unlock(ring->ring_lock);
    return (0U);
  } else {
  }
  i = 0U;
  goto ldv_47804;
  ldv_47803:
  tmp___1 = ptr;
  ptr = ptr + 1U;
  *(*data + (unsigned long )i) = *(ring->ring + (unsigned long )tmp___1);
  ptr = ring->ptr_mask & ptr;
  i = i + 1U;
  ldv_47804: ;
  if (i < size) {
    goto ldv_47803;
  } else {
  }
  mutex_unlock(ring->ring_lock);
  return (size);
}
}
int amdgpu_ring_restore(struct amdgpu_ring *ring , unsigned int size , u32 *data )
{
  int i ;
  int r ;
  {
  if (size == 0U || (unsigned long )data == (unsigned long )((u32 *)0U)) {
    return (0);
  } else {
  }
  r = amdgpu_ring_lock(ring, size);
  if (r != 0) {
    return (r);
  } else {
  }
  i = 0;
  goto ldv_47814;
  ldv_47813:
  amdgpu_ring_write(ring, *(data + (unsigned long )i));
  i = i + 1;
  ldv_47814: ;
  if ((unsigned int )i < size) {
    goto ldv_47813;
  } else {
  }
  amdgpu_ring_unlock_commit(ring);
  kfree((void const *)data);
  return (0);
}
}
int amdgpu_ring_init(struct amdgpu_device *adev , struct amdgpu_ring *ring , unsigned int ring_size ,
                     u32 nop , u32 align_mask , struct amdgpu_irq_src *irq_src , unsigned int irq_type ,
                     enum amdgpu_ring_type ring_type )
{
  u32 rb_bufsz ;
  int r ;
  unsigned int tmp ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  int tmp___3 ;
  {
  if ((unsigned long )ring->adev == (unsigned long )((struct amdgpu_device *)0)) {
    if (adev->num_rings > 15U) {
      return (-22);
    } else {
    }
    ring->adev = adev;
    tmp = adev->num_rings;
    adev->num_rings = adev->num_rings + 1U;
    ring->idx = tmp;
    adev->rings[ring->idx] = ring;
    amdgpu_fence_driver_init_ring(ring);
  } else {
  }
  r = amdgpu_wb_get(adev, & ring->rptr_offs);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) ring rptr_offs wb alloc failed\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_wb_get(adev, & ring->wptr_offs);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) ring wptr_offs wb alloc failed\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_wb_get(adev, & ring->fence_offs);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) ring fence_offs wb alloc failed\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_wb_get(adev, & ring->next_rptr_offs);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) ring next_rptr wb alloc failed\n",
            r);
    return (r);
  } else {
  }
  ring->next_rptr_gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->next_rptr_offs * 4U);
  ring->next_rptr_cpu_addr = adev->wb.wb + (unsigned long )ring->next_rptr_offs;
  r = amdgpu_fence_driver_start_ring(ring, irq_src, irq_type);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "failed initializing fences (%d).\n",
            r);
    return (r);
  } else {
  }
  ring->ring_lock = & adev->ring_lock;
  tmp___0 = __roundup_pow_of_two((unsigned long )(ring_size / 8U));
  tmp___1 = __ilog2_u64((u64 )tmp___0);
  rb_bufsz = (u32 )tmp___1;
  ring_size = (unsigned int )(4 << (int )(rb_bufsz + 1U));
  ring->ring_size = ring_size;
  ring->align_mask = align_mask;
  ring->nop = nop;
  ring->type = ring_type;
  if ((unsigned long )ring->ring_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, (unsigned long )ring->ring_size, 4096, 1, 2U, 0ULL,
                         (struct sg_table *)0, & ring->ring_obj);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "(%d) ring create failed\n", r);
      return (r);
    } else {
    }
    r = amdgpu_bo_reserve(ring->ring_obj, 0);
    tmp___2 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___2 != 0L) {
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(ring->ring_obj, 2U, & ring->gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(ring->ring_obj);
      dev_err((struct device const *)adev->dev, "(%d) ring pin failed\n", r);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(ring->ring_obj, (void **)(& ring->ring));
    amdgpu_bo_unreserve(ring->ring_obj);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "(%d) ring map failed\n", r);
      return (r);
    } else {
    }
  } else {
  }
  ring->ptr_mask = ring->ring_size / 4U - 1U;
  ring->ring_free_dw = ring->ring_size / 4U;
  tmp___3 = amdgpu_debugfs_ring_init(adev, ring);
  if (tmp___3 != 0) {
    drm_err("Failed to register debugfs file for rings !\n");
  } else {
  }
  amdgpu_ring_lockup_update(ring);
  return (0);
}
}
void amdgpu_ring_fini(struct amdgpu_ring *ring )
{
  int r ;
  struct amdgpu_bo *ring_obj ;
  long tmp ;
  {
  if ((unsigned long )ring->ring_lock == (unsigned long )((struct mutex *)0)) {
    return;
  } else {
  }
  mutex_lock_nested(ring->ring_lock, 0U);
  ring_obj = ring->ring_obj;
  ring->ready = 0;
  ring->ring = (u32 volatile *)0U;
  ring->ring_obj = (struct amdgpu_bo *)0;
  mutex_unlock(ring->ring_lock);
  amdgpu_wb_free(ring->adev, ring->fence_offs);
  amdgpu_wb_free(ring->adev, ring->rptr_offs);
  amdgpu_wb_free(ring->adev, ring->wptr_offs);
  amdgpu_wb_free(ring->adev, ring->next_rptr_offs);
  if ((unsigned long )ring_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(ring_obj, 0);
    tmp = ldv__builtin_expect(r == 0, 1L);
    if (tmp != 0L) {
      amdgpu_bo_kunmap(ring_obj);
      amdgpu_bo_unpin(ring_obj);
      amdgpu_bo_unreserve(ring_obj);
    } else {
    }
    amdgpu_bo_unref(& ring_obj);
  } else {
  }
  return;
}
}
static int amdgpu_debugfs_ring_info(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int roffset ;
  struct amdgpu_ring *ring ;
  u32 rptr ;
  u32 wptr ;
  u32 rptr_next ;
  unsigned int count ;
  unsigned int i ;
  unsigned int j ;
  {
  node = (struct drm_info_node *)m->private;
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  roffset = *((int *)(node->info_ent)->data);
  ring = (struct amdgpu_ring *)adev + (unsigned long )roffset;
  amdgpu_ring_free_size(ring);
  count = ring->ring_size / 4U - ring->ring_free_dw;
  wptr = (*((ring->funcs)->get_wptr))(ring);
  seq_printf(m, "wptr: 0x%08x [%5d]\n", wptr, wptr);
  rptr = (*((ring->funcs)->get_rptr))(ring);
  seq_printf(m, "rptr: 0x%08x [%5d]\n", rptr, rptr);
  rptr_next = 4294967295U;
  seq_printf(m, "driver\'s copy of the wptr: 0x%08x [%5d]\n", ring->wptr, ring->wptr);
  seq_printf(m, "last semaphore signal addr : 0x%016llx\n", ring->last_semaphore_signal_addr);
  seq_printf(m, "last semaphore wait addr   : 0x%016llx\n", ring->last_semaphore_wait_addr);
  seq_printf(m, "%u free dwords in ring\n", ring->ring_free_dw);
  seq_printf(m, "%u dwords in ring\n", count);
  if (! ring->ready) {
    return (0);
  } else {
  }
  i = ((ring->ptr_mask + rptr) - 31U) & ring->ptr_mask;
  j = 0U;
  goto ldv_47849;
  ldv_47848:
  seq_printf(m, "r[%5d]=0x%08x", i, *(ring->ring + (unsigned long )i));
  if (rptr == i) {
    seq_puts(m, " *");
  } else {
  }
  if (rptr_next == i) {
    seq_puts(m, " #");
  } else {
  }
  seq_puts(m, "\n");
  i = (i + 1U) & ring->ptr_mask;
  j = j + 1U;
  ldv_47849: ;
  if (count + 32U >= j) {
    goto ldv_47848;
  } else {
  }
  return (0);
}
}
static int amdgpu_gfx_index = 11792;
static int cayman_cp1_index = 12432;
static int cayman_cp2_index = 13064;
static int amdgpu_dma1_index = 17600;
static int amdgpu_dma2_index = 18248;
static int r600_uvd_index = 19320;
static int si_vce1_index = 20496;
static int si_vce2_index = 21128;
static struct drm_info_list amdgpu_debugfs_ring_info_list[8U] =
  { {"amdgpu_ring_gfx", & amdgpu_debugfs_ring_info, 0U, (void *)(& amdgpu_gfx_index)},
        {"amdgpu_ring_cp1",
      & amdgpu_debugfs_ring_info, 0U, (void *)(& cayman_cp1_index)},
        {"amdgpu_ring_cp2", & amdgpu_debugfs_ring_info, 0U, (void *)(& cayman_cp2_index)},
        {"amdgpu_ring_dma1",
      & amdgpu_debugfs_ring_info, 0U, (void *)(& amdgpu_dma1_index)},
        {"amdgpu_ring_dma2", & amdgpu_debugfs_ring_info, 0U, (void *)(& amdgpu_dma2_index)},
        {"amdgpu_ring_uvd",
      & amdgpu_debugfs_ring_info, 0U, (void *)(& r600_uvd_index)},
        {"amdgpu_ring_vce1", & amdgpu_debugfs_ring_info, 0U, (void *)(& si_vce1_index)},
        {"amdgpu_ring_vce2",
      & amdgpu_debugfs_ring_info, 0U, (void *)(& si_vce2_index)}};
static int amdgpu_debugfs_ring_init(struct amdgpu_device *adev , struct amdgpu_ring *ring )
{
  unsigned int i ;
  struct drm_info_list *info ;
  int roffset ;
  struct amdgpu_ring *other ;
  unsigned int r ;
  int tmp ;
  {
  i = 0U;
  goto ldv_47873;
  ldv_47872:
  info = (struct drm_info_list *)(& amdgpu_debugfs_ring_info_list) + (unsigned long )i;
  roffset = *((int *)amdgpu_debugfs_ring_info_list[i].data);
  other = (struct amdgpu_ring *)adev + (unsigned long )roffset;
  if ((unsigned long )other != (unsigned long )ring) {
    goto ldv_47871;
  } else {
  }
  tmp = amdgpu_debugfs_add_files(adev, info, 1U);
  r = (unsigned int )tmp;
  if (r != 0U) {
    return ((int )r);
  } else {
  }
  ldv_47871:
  i = i + 1U;
  ldv_47873: ;
  if (i <= 7U) {
    goto ldv_47872;
  } else {
  }
  return (0);
}
}
bool ldv_queue_work_on_229(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_230(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_231(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_232(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_233(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void __list_splice(struct list_head const *list , struct list_head *prev ,
                                   struct list_head *next )
{
  struct list_head *first ;
  struct list_head *last ;
  {
  first = list->next;
  last = list->prev;
  first->prev = prev;
  prev->next = first;
  last->next = next;
  next->prev = last;
  return;
}
}
__inline static void list_splice(struct list_head const *list , struct list_head *head )
{
  int tmp ;
  {
  tmp = list_empty(list);
  if (tmp == 0) {
    __list_splice(list, head, head->next);
  } else {
  }
  return;
}
}
__inline static unsigned long arch_local_save_flags___2(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static int __atomic_add_unless___2(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___2(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___2(v, a, u);
  return (tmp != u);
}
}
__inline static bool static_key_false___1(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int rcu_read_lock_sched_held___1(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___2();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
bool ldv_queue_work_on_243(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_245(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_244(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_247(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_246(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static int kref_put_mutex___2(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___2(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
extern void list_sort(void * , struct list_head * , int (*)(void * , struct list_head * ,
                                                            struct list_head * ) ) ;
extern unsigned long _copy_from_user(void * , void const * , unsigned int ) ;
extern void __copy_from_user_overflow(void) ;
__inline static unsigned long copy_from_user(void *to , void const *from , unsigned long n )
{
  int sz ;
  unsigned long tmp ;
  long tmp___0 ;
  {
  tmp = __builtin_object_size((void const *)to, 0);
  sz = (int )tmp;
  __might_fault("./arch/x86/include/asm/uaccess.h", 697);
  tmp___0 = ldv__builtin_expect((long )(sz < 0 || (unsigned long )sz >= n), 1L);
  if (tmp___0 != 0L) {
    n = _copy_from_user(to, from, (unsigned int )n);
  } else {
    __copy_from_user_overflow();
  }
  return (n);
}
}
extern void *__vmalloc(unsigned long , gfp_t , pgprot_t ) ;
__inline static void *drm_malloc_ab(size_t nmemb , size_t size )
{
  void *tmp ;
  pgprot_t __constr_expr_0 ;
  void *tmp___0 ;
  {
  if (size != 0UL && 0xffffffffffffffffUL / size < nmemb) {
    return ((void *)0);
  } else {
  }
  if (size * nmemb <= 4096UL) {
    tmp = kmalloc(nmemb * size, 208U);
    return (tmp);
  } else {
  }
  __constr_expr_0.pgprot = 0x8000000000000163UL;
  tmp___0 = __vmalloc(size * nmemb, 210U, __constr_expr_0);
  return (tmp___0);
}
}
extern void ttm_eu_fence_buffer_objects(struct ww_acquire_ctx * , struct list_head * ,
                                        struct fence * ) ;
__inline static void drm_gem_object_unreference_unlocked___2(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___2(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
void amdgpu_sync_fence(struct amdgpu_sync *sync , struct amdgpu_fence *fence ) ;
struct amdgpu_bo_list *amdgpu_bo_list_get(struct amdgpu_fpriv *fpriv , int id ) ;
void amdgpu_bo_list_put(struct amdgpu_bo_list *list ) ;
int amdgpu_ib_get(struct amdgpu_ring *ring , struct amdgpu_vm *vm , unsigned int size ,
                  struct amdgpu_ib *ib ) ;
void amdgpu_ib_free(struct amdgpu_device *adev , struct amdgpu_ib *ib ) ;
int amdgpu_ib_schedule(struct amdgpu_device *adev , unsigned int num_ibs , struct amdgpu_ib *ibs ,
                       void *owner ) ;
struct amdgpu_ctx *amdgpu_ctx_get(struct amdgpu_fpriv *fpriv , u32 id ) ;
int amdgpu_ctx_put(struct amdgpu_ctx *ctx ) ;
int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p , void *data ) ;
int amdgpu_cs_get_ring(struct amdgpu_device *adev , u32 ip_type , u32 ip_instance ,
                       u32 ring , struct amdgpu_ring **out_ring ) ;
int amdgpu_vm_update_page_directory(struct amdgpu_device *adev , struct amdgpu_vm *vm ) ;
int amdgpu_vm_clear_invalids(struct amdgpu_device *adev , struct amdgpu_vm *vm , struct amdgpu_sync *sync ) ;
struct amdgpu_bo_va_mapping *amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser ,
                                                    uint64_t addr , struct amdgpu_bo **bo ) ;
__inline static unsigned int amdgpu_mem_type_to_domain___0(u32 mem_type )
{
  {
  switch (mem_type) {
  case 2U: ;
  return (4U);
  case 1U: ;
  return (2U);
  case 0U: ;
  return (1U);
  case 3U: ;
  return (8U);
  case 4U: ;
  return (16U);
  case 5U: ;
  return (32U);
  default: ;
  goto ldv_43512;
  }
  ldv_43512: ;
  return (0U);
}
}
struct tracepoint __tracepoint_amdgpu_cs ;
__inline static void trace_amdgpu_cs(struct amdgpu_cs_parser *p , int i )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_279 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_281 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___1(& __tracepoint_amdgpu_cs.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_cs.funcs), (void *)(& __u.__c),
                     8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___1();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               49, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43761:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_cs_parser * , int ))it_func))(__data, p,
                                                                         i);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43761;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_cs.funcs), (void *)(& __u___0.__c),
                   8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___1();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             49, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
static void amdgpu_cs_buckets_init(struct amdgpu_cs_buckets *b )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_44505;
  ldv_44504:
  INIT_LIST_HEAD((struct list_head *)(& b->bucket) + (unsigned long )i);
  i = i + 1U;
  ldv_44505: ;
  if (i <= 32U) {
    goto ldv_44504;
  } else {
  }
  return;
}
}
static void amdgpu_cs_buckets_add(struct amdgpu_cs_buckets *b , struct list_head *item ,
                                  unsigned int priority )
{
  unsigned int _min1 ;
  unsigned int _min2 ;
  {
  _min1 = priority;
  _min2 = 32U;
  list_add_tail(item, (struct list_head *)(& b->bucket) + (unsigned long )(_min1 < _min2 ? _min1 : _min2));
  return;
}
}
static void amdgpu_cs_buckets_get_list(struct amdgpu_cs_buckets *b , struct list_head *out_list )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_44521;
  ldv_44520:
  list_splice((struct list_head const *)(& b->bucket) + (unsigned long )i, out_list);
  i = i + 1U;
  ldv_44521: ;
  if (i <= 32U) {
    goto ldv_44520;
  } else {
  }
  return;
}
}
int amdgpu_cs_get_ring(struct amdgpu_device *adev , u32 ip_type , u32 ip_instance ,
                       u32 ring , struct amdgpu_ring **out_ring )
{
  {
  if (ip_instance != 0U) {
    drm_err("invalid ip instance: %d\n", ip_instance);
    return (-22);
  } else {
  }
  switch (ip_type) {
  default:
  drm_err("unknown ip type: %d\n", ip_type);
  return (-22);
  case 0U: ;
  if (adev->gfx.num_gfx_rings > ring) {
    *out_ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring) + (unsigned long )ring;
  } else {
    drm_err("only %d gfx rings are supported now\n", adev->gfx.num_gfx_rings);
    return (-22);
  }
  goto ldv_44532;
  case 1U: ;
  if (adev->gfx.num_compute_rings > ring) {
    *out_ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )ring;
  } else {
    drm_err("only %d compute rings are supported now\n", adev->gfx.num_compute_rings);
    return (-22);
  }
  goto ldv_44532;
  case 2U: ;
  if (ring <= 1U) {
    *out_ring = & adev->sdma[ring].ring;
  } else {
    drm_err("only two SDMA rings are supported\n");
    return (-22);
  }
  goto ldv_44532;
  case 3U:
  *out_ring = & adev->uvd.ring;
  goto ldv_44532;
  case 4U: ;
  if (ring <= 1U) {
    *out_ring = (struct amdgpu_ring *)(& adev->vce.ring) + (unsigned long )ring;
  } else {
    drm_err("only two VCE rings are supported\n");
    return (-22);
  }
  goto ldv_44532;
  }
  ldv_44532: ;
  return (0);
}
}
int amdgpu_cs_parser_init(struct amdgpu_cs_parser *p , void *data )
{
  union drm_amdgpu_cs *cs ;
  uint64_t *chunk_array_user ;
  uint64_t *chunk_array ;
  struct amdgpu_fpriv *fpriv ;
  unsigned int size ;
  unsigned int i ;
  int r ;
  void *tmp ;
  unsigned long tmp___0 ;
  void *tmp___1 ;
  struct drm_amdgpu_cs_chunk **chunk_ptr ;
  struct drm_amdgpu_cs_chunk user_chunk ;
  u32 *cdata ;
  unsigned long tmp___2 ;
  void *tmp___3 ;
  unsigned long tmp___4 ;
  u32 handle ;
  struct drm_gem_object *gobj ;
  struct drm_amdgpu_cs_chunk_fence *fence_data ;
  struct drm_gem_object const *__mptr ;
  void *tmp___5 ;
  {
  cs = (union drm_amdgpu_cs *)data;
  chunk_array = (uint64_t *)0ULL;
  fpriv = (struct amdgpu_fpriv *)(p->filp)->driver_priv;
  r = 0;
  if (cs->in.num_chunks == 0U) {
    goto out;
  } else {
  }
  p->ctx = amdgpu_ctx_get(fpriv, cs->in.ctx_id);
  if ((unsigned long )p->ctx == (unsigned long )((struct amdgpu_ctx *)0)) {
    r = -22;
    goto out;
  } else {
  }
  p->bo_list = amdgpu_bo_list_get(fpriv, (int )cs->in.bo_list_handle);
  INIT_LIST_HEAD(& p->validated);
  tmp = kcalloc((size_t )cs->in.num_chunks, 8UL, 208U);
  chunk_array = (uint64_t *)tmp;
  if ((unsigned long )chunk_array == (unsigned long )((uint64_t *)0ULL)) {
    r = -12;
    goto out;
  } else {
  }
  chunk_array_user = (uint64_t *)cs->in.chunks;
  tmp___0 = copy_from_user((void *)chunk_array, (void const *)chunk_array_user,
                           (unsigned long )cs->in.num_chunks * 8UL);
  if (tmp___0 != 0UL) {
    r = -14;
    goto out;
  } else {
  }
  p->nchunks = cs->in.num_chunks;
  tmp___1 = kcalloc((size_t )p->nchunks, 24UL, 208U);
  p->chunks = (struct amdgpu_cs_chunk *)tmp___1;
  if ((unsigned long )p->chunks == (unsigned long )((struct amdgpu_cs_chunk *)0)) {
    r = -12;
    goto out;
  } else {
  }
  i = 0U;
  goto ldv_44563;
  ldv_44562:
  chunk_ptr = (struct drm_amdgpu_cs_chunk **)0;
  chunk_ptr = (struct drm_amdgpu_cs_chunk **)*(chunk_array + (unsigned long )i);
  tmp___2 = copy_from_user((void *)(& user_chunk), (void const *)chunk_ptr, 16UL);
  if (tmp___2 != 0UL) {
    r = -14;
    goto out;
  } else {
  }
  (p->chunks + (unsigned long )i)->chunk_id = user_chunk.chunk_id;
  (p->chunks + (unsigned long )i)->length_dw = user_chunk.length_dw;
  size = (p->chunks + (unsigned long )i)->length_dw;
  cdata = (u32 *)user_chunk.chunk_data;
  (p->chunks + (unsigned long )i)->user_ptr = (void *)cdata;
  tmp___3 = drm_malloc_ab((size_t )size, 4UL);
  (p->chunks + (unsigned long )i)->kdata = (u32 *)tmp___3;
  if ((unsigned long )(p->chunks + (unsigned long )i)->kdata == (unsigned long )((u32 *)0U)) {
    r = -12;
    goto out;
  } else {
  }
  size = size * 4U;
  tmp___4 = copy_from_user((void *)(p->chunks + (unsigned long )i)->kdata, (void const *)cdata,
                           (unsigned long )size);
  if (tmp___4 != 0UL) {
    r = -14;
    goto out;
  } else {
  }
  switch ((p->chunks + (unsigned long )i)->chunk_id) {
  case 1U:
  p->num_ibs = p->num_ibs + 1U;
  goto ldv_44553;
  case 2U:
  size = 8U;
  if ((unsigned long )(p->chunks + (unsigned long )i)->length_dw * 4UL >= (unsigned long )size) {
    fence_data = (struct drm_amdgpu_cs_chunk_fence *)(p->chunks + (unsigned long )i)->kdata;
    handle = fence_data->handle;
    gobj = drm_gem_object_lookup((p->adev)->ddev, p->filp, handle);
    if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
      r = -22;
      goto out;
    } else {
    }
    __mptr = (struct drm_gem_object const *)gobj;
    p->uf.bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
    p->uf.offset = fence_data->offset;
  } else {
    r = -22;
    goto out;
  }
  goto ldv_44553;
  case 3U: ;
  goto ldv_44553;
  default:
  r = -22;
  goto out;
  }
  ldv_44553:
  i = i + 1U;
  ldv_44563: ;
  if (p->nchunks > i) {
    goto ldv_44562;
  } else {
  }
  tmp___5 = kcalloc((size_t )p->num_ibs, 272UL, 208U);
  p->ibs = (struct amdgpu_ib *)tmp___5;
  if ((unsigned long )p->ibs == (unsigned long )((struct amdgpu_ib *)0)) {
    r = -12;
    goto out;
  } else {
  }
  out:
  kfree((void const *)chunk_array);
  return (r);
}
}
static u64 amdgpu_cs_get_threshold_for_moves(struct amdgpu_device *adev )
{
  u64 real_vram_size ;
  u64 vram_usage ;
  long tmp ;
  u64 half_vram ;
  u64 half_free_vram ;
  u64 bytes_moved_threshold ;
  u64 _max1 ;
  unsigned long long _max2 ;
  {
  real_vram_size = adev->mc.real_vram_size;
  tmp = atomic64_read((atomic64_t const *)(& adev->vram_usage));
  vram_usage = (u64 )tmp;
  half_vram = real_vram_size >> 1;
  half_free_vram = vram_usage < half_vram ? half_vram - vram_usage : 0ULL;
  bytes_moved_threshold = half_free_vram >> 1;
  _max1 = bytes_moved_threshold;
  _max2 = 1048576ULL;
  return (_max1 > _max2 ? _max1 : _max2);
}
}
int amdgpu_cs_list_validate(struct amdgpu_cs_parser *p )
{
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_vm *vm ;
  struct amdgpu_device *adev ;
  struct amdgpu_bo_list_entry *lobj ;
  struct list_head duplicates ;
  struct amdgpu_bo *bo ;
  u64 bytes_moved ;
  u64 initial_bytes_moved ;
  u64 bytes_moved_threshold ;
  u64 tmp ;
  int r ;
  long tmp___0 ;
  struct list_head const *__mptr ;
  u32 domain ;
  u32 current_domain ;
  unsigned int tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  struct list_head const *__mptr___0 ;
  {
  fpriv = (struct amdgpu_fpriv *)(p->filp)->driver_priv;
  vm = & fpriv->vm;
  adev = p->adev;
  bytes_moved = 0ULL;
  tmp = amdgpu_cs_get_threshold_for_moves(adev);
  bytes_moved_threshold = tmp;
  INIT_LIST_HEAD(& duplicates);
  r = ttm_eu_reserve_buffers(& p->ticket, & p->validated, 1, & duplicates);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    return (r);
  } else {
  }
  __mptr = (struct list_head const *)p->validated.next;
  lobj = (struct amdgpu_bo_list_entry *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_44597;
  ldv_44596:
  bo = lobj->robj;
  if (bo->pin_count == 0U) {
    domain = lobj->prefered_domains;
    tmp___1 = amdgpu_mem_type_to_domain___0(bo->tbo.mem.mem_type);
    current_domain = tmp___1;
    if ((current_domain != 1U && (domain & current_domain) == 0U) && bytes_moved > bytes_moved_threshold) {
      domain = current_domain;
    } else {
    }
    retry:
    amdgpu_ttm_placement_from_domain(bo, domain);
    tmp___2 = atomic64_read((atomic64_t const *)(& adev->num_bytes_moved));
    initial_bytes_moved = (u64 )tmp___2;
    r = ttm_bo_validate(& bo->tbo, & bo->placement, 1, 0);
    tmp___3 = atomic64_read((atomic64_t const *)(& adev->num_bytes_moved));
    bytes_moved = ((unsigned long long )tmp___3 - initial_bytes_moved) + bytes_moved;
    tmp___4 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___4 != 0L) {
      if (r != -512 && lobj->allowed_domains != domain) {
        domain = lobj->allowed_domains;
        goto retry;
      } else {
      }
      ttm_eu_backoff_reservation(& p->ticket, & p->validated);
      return (r);
    } else {
    }
  } else {
  }
  lobj->bo_va = amdgpu_vm_bo_find(vm, bo);
  __mptr___0 = (struct list_head const *)lobj->tv.head.next;
  lobj = (struct amdgpu_bo_list_entry *)__mptr___0 + 0xfffffffffffffff8UL;
  ldv_44597: ;
  if ((unsigned long )(& lobj->tv.head) != (unsigned long )(& p->validated)) {
    goto ldv_44596;
  } else {
  }
  return (0);
}
}
static int amdgpu_cs_parser_relocs(struct amdgpu_cs_parser *p )
{
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_cs_buckets buckets ;
  bool need_mmap_lock ;
  int i ;
  int r ;
  struct task_struct *tmp ;
  struct task_struct *tmp___0 ;
  {
  fpriv = (struct amdgpu_fpriv *)(p->filp)->driver_priv;
  need_mmap_lock = 0;
  if ((unsigned long )p->bo_list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    need_mmap_lock = (p->bo_list)->has_userptr;
    amdgpu_cs_buckets_init(& buckets);
    i = 0;
    goto ldv_44608;
    ldv_44607:
    amdgpu_cs_buckets_add(& buckets, & ((p->bo_list)->array + (unsigned long )i)->tv.head,
                          ((p->bo_list)->array + (unsigned long )i)->priority);
    i = i + 1;
    ldv_44608: ;
    if ((unsigned int )i < (p->bo_list)->num_entries) {
      goto ldv_44607;
    } else {
    }
    amdgpu_cs_buckets_get_list(& buckets, & p->validated);
  } else {
  }
  p->vm_bos = amdgpu_vm_get_bos(p->adev, & fpriv->vm, & p->validated);
  if ((int )need_mmap_lock) {
    tmp = get_current();
    down_read(& (tmp->mm)->mmap_sem);
  } else {
  }
  r = amdgpu_cs_list_validate(p);
  if ((int )need_mmap_lock) {
    tmp___0 = get_current();
    up_read(& (tmp___0->mm)->mmap_sem);
  } else {
  }
  return (r);
}
}
static int amdgpu_cs_sync_rings(struct amdgpu_cs_parser *p )
{
  struct amdgpu_bo_list_entry *e ;
  int r ;
  struct list_head const *__mptr ;
  struct reservation_object *resv ;
  struct list_head const *__mptr___0 ;
  {
  __mptr = (struct list_head const *)p->validated.next;
  e = (struct amdgpu_bo_list_entry *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_44621;
  ldv_44620:
  resv = (e->robj)->tbo.resv;
  r = amdgpu_sync_resv(p->adev, & (p->ibs)->sync, resv, (void *)p->filp);
  if (r != 0) {
    return (r);
  } else {
  }
  __mptr___0 = (struct list_head const *)e->tv.head.next;
  e = (struct amdgpu_bo_list_entry *)__mptr___0 + 0xfffffffffffffff8UL;
  ldv_44621: ;
  if ((unsigned long )(& e->tv.head) != (unsigned long )(& p->validated)) {
    goto ldv_44620;
  } else {
  }
  return (0);
}
}
static int cmp_size_smaller_first(void *priv , struct list_head *a , struct list_head *b )
{
  struct amdgpu_bo_list_entry *la ;
  struct list_head const *__mptr ;
  struct amdgpu_bo_list_entry *lb ;
  struct list_head const *__mptr___0 ;
  {
  __mptr = (struct list_head const *)a;
  la = (struct amdgpu_bo_list_entry *)__mptr + 0xfffffffffffffff8UL;
  __mptr___0 = (struct list_head const *)b;
  lb = (struct amdgpu_bo_list_entry *)__mptr___0 + 0xfffffffffffffff8UL;
  return ((int )(la->robj)->tbo.num_pages - (int )(lb->robj)->tbo.num_pages);
}
}
static void amdgpu_cs_parser_fini(struct amdgpu_cs_parser *parser , int error , bool backoff )
{
  unsigned int i ;
  {
  if (error == 0) {
    list_sort((void *)0, & parser->validated, & cmp_size_smaller_first);
    ttm_eu_fence_buffer_objects(& parser->ticket, & parser->validated, & ((parser->ibs + (unsigned long )(parser->num_ibs - 1U))->fence)->base);
  } else
  if ((int )backoff) {
    ttm_eu_backoff_reservation(& parser->ticket, & parser->validated);
  } else {
  }
  if ((unsigned long )parser->ctx != (unsigned long )((struct amdgpu_ctx *)0)) {
    amdgpu_ctx_put(parser->ctx);
  } else {
  }
  if ((unsigned long )parser->bo_list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    amdgpu_bo_list_put(parser->bo_list);
  } else {
  }
  drm_free_large((void *)parser->vm_bos);
  i = 0U;
  goto ldv_44641;
  ldv_44640:
  drm_free_large((void *)(parser->chunks + (unsigned long )i)->kdata);
  i = i + 1U;
  ldv_44641: ;
  if (parser->nchunks > i) {
    goto ldv_44640;
  } else {
  }
  kfree((void const *)parser->chunks);
  if ((unsigned long )parser->ibs != (unsigned long )((struct amdgpu_ib *)0)) {
    i = 0U;
    goto ldv_44644;
    ldv_44643:
    amdgpu_ib_free(parser->adev, parser->ibs + (unsigned long )i);
    i = i + 1U;
    ldv_44644: ;
    if (parser->num_ibs > i) {
      goto ldv_44643;
    } else {
    }
  } else {
  }
  kfree((void const *)parser->ibs);
  if ((unsigned long )parser->uf.bo != (unsigned long )((struct amdgpu_bo *)0)) {
    drm_gem_object_unreference_unlocked___2(& (parser->uf.bo)->gem_base);
  } else {
  }
  return;
}
}
static int amdgpu_bo_vm_update_pte(struct amdgpu_cs_parser *p , struct amdgpu_vm *vm )
{
  struct amdgpu_device *adev ;
  struct amdgpu_bo_va *bo_va ;
  struct amdgpu_bo *bo ;
  int i ;
  int r ;
  int tmp ;
  {
  adev = p->adev;
  r = amdgpu_vm_update_page_directory(adev, vm);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vm_clear_freed(adev, vm);
  if (r != 0) {
    return (r);
  } else {
  }
  if ((unsigned long )p->bo_list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    i = 0;
    goto ldv_44657;
    ldv_44656:
    bo = ((p->bo_list)->array + (unsigned long )i)->robj;
    if ((unsigned long )bo == (unsigned long )((struct amdgpu_bo *)0)) {
      goto ldv_44655;
    } else {
    }
    bo_va = ((p->bo_list)->array + (unsigned long )i)->bo_va;
    if ((unsigned long )bo_va == (unsigned long )((struct amdgpu_bo_va *)0)) {
      goto ldv_44655;
    } else {
    }
    r = amdgpu_vm_bo_update(adev, bo_va, & bo->tbo.mem);
    if (r != 0) {
      return (r);
    } else {
    }
    amdgpu_sync_fence(& (p->ibs)->sync, bo_va->last_pt_update);
    ldv_44655:
    i = i + 1;
    ldv_44657: ;
    if ((unsigned int )i < (p->bo_list)->num_entries) {
      goto ldv_44656;
    } else {
    }
  } else {
  }
  tmp = amdgpu_vm_clear_invalids(adev, vm, & (p->ibs)->sync);
  return (tmp);
}
}
static int amdgpu_cs_ib_vm_chunk(struct amdgpu_device *adev , struct amdgpu_cs_parser *parser )
{
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_vm *vm ;
  struct amdgpu_ring *ring ;
  int i ;
  int r ;
  {
  fpriv = (struct amdgpu_fpriv *)(parser->filp)->driver_priv;
  vm = & fpriv->vm;
  if (parser->num_ibs == 0U) {
    return (0);
  } else {
  }
  i = 0;
  goto ldv_44669;
  ldv_44668:
  ring = (parser->ibs + (unsigned long )i)->ring;
  if ((unsigned long )(ring->funcs)->parse_cs != (unsigned long )((int (* )(struct amdgpu_cs_parser * ,
                                                                                       u32 ))0)) {
    r = (*((ring->funcs)->parse_cs))(parser, (u32 )i);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
  }
  i = i + 1;
  ldv_44669: ;
  if ((u32 )i < parser->num_ibs) {
    goto ldv_44668;
  } else {
  }
  mutex_lock_nested(& vm->mutex, 0U);
  r = amdgpu_bo_vm_update_pte(parser, vm);
  if (r != 0) {
    goto out;
  } else {
  }
  amdgpu_cs_sync_rings(parser);
  r = amdgpu_ib_schedule(adev, parser->num_ibs, parser->ibs, (void *)parser->filp);
  out:
  mutex_unlock(& vm->mutex);
  return (r);
}
}
static int amdgpu_cs_handle_lockup(struct amdgpu_device *adev , int r )
{
  {
  if (r == -35) {
    r = amdgpu_gpu_reset(adev);
    if (r == 0) {
      r = -11;
    } else {
    }
  } else {
  }
  return (r);
}
}
static int amdgpu_cs_ib_fill(struct amdgpu_device *adev , struct amdgpu_cs_parser *parser )
{
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_vm *vm ;
  int i ;
  int j ;
  int r ;
  struct amdgpu_cs_chunk *chunk ;
  struct amdgpu_ib *ib ;
  struct drm_amdgpu_cs_chunk_ib *chunk_ib ;
  struct amdgpu_ring *ring ;
  struct amdgpu_bo_va_mapping *m ;
  struct amdgpu_bo *aobj ;
  uint64_t offset ;
  uint8_t *kptr ;
  struct amdgpu_bo *gds ;
  struct amdgpu_bo *gws ;
  struct amdgpu_bo *oa ;
  struct amdgpu_ib *ib___0 ;
  u64 tmp ;
  unsigned long tmp___0 ;
  u64 tmp___1 ;
  unsigned long tmp___2 ;
  u64 tmp___3 ;
  unsigned long tmp___4 ;
  struct amdgpu_ib *ib___1 ;
  {
  fpriv = (struct amdgpu_fpriv *)(parser->filp)->driver_priv;
  vm = & fpriv->vm;
  i = 0;
  j = 0;
  goto ldv_44695;
  ldv_44694:
  chunk = parser->chunks + (unsigned long )i;
  ib = parser->ibs + (unsigned long )j;
  chunk_ib = (struct drm_amdgpu_cs_chunk_ib *)chunk->kdata;
  if (chunk->chunk_id != 1U) {
    goto ldv_44689;
  } else {
  }
  r = amdgpu_cs_get_ring(adev, chunk_ib->ip_type, chunk_ib->ip_instance, chunk_ib->ring,
                         & ring);
  if (r != 0) {
    return (r);
  } else {
  }
  if ((unsigned long )(ring->funcs)->parse_cs != (unsigned long )((int (* )(struct amdgpu_cs_parser * ,
                                                                                       u32 ))0)) {
    aobj = (struct amdgpu_bo *)0;
    m = amdgpu_cs_find_mapping(parser, chunk_ib->va_start, & aobj);
    if ((unsigned long )aobj == (unsigned long )((struct amdgpu_bo *)0)) {
      drm_err("IB va_start is invalid\n");
      return (-22);
    } else {
    }
    if (chunk_ib->va_start + (uint64_t )chunk_ib->ib_bytes > (unsigned long long )((m->it.last + 1UL) * 4096UL)) {
      drm_err("IB va_start+ib_bytes is invalid\n");
      return (-22);
    } else {
    }
    r = amdgpu_bo_kmap(aobj, (void **)(& kptr));
    if (r != 0) {
      return (r);
    } else {
    }
    offset = (unsigned long long )m->it.start * 4096ULL;
    kptr = kptr + (chunk_ib->va_start - offset);
    r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, chunk_ib->ib_bytes, ib);
    if (r != 0) {
      drm_err("Failed to get ib !\n");
      return (r);
    } else {
    }
    memcpy((void *)ib->ptr, (void const *)kptr, (size_t )chunk_ib->ib_bytes);
    amdgpu_bo_kunmap(aobj);
  } else {
    r = amdgpu_ib_get(ring, vm, 0U, ib);
    if (r != 0) {
      drm_err("Failed to get ib !\n");
      return (r);
    } else {
    }
    ib->gpu_addr = chunk_ib->va_start;
  }
  ib->length_dw = chunk_ib->ib_bytes / 4U;
  ib->flags = chunk_ib->flags;
  ib->ctx = parser->ctx;
  j = j + 1;
  ldv_44689:
  i = i + 1;
  ldv_44695: ;
  if ((unsigned int )i < parser->nchunks && (u32 )j < parser->num_ibs) {
    goto ldv_44694;
  } else {
  }
  if (parser->num_ibs == 0U) {
    return (0);
  } else {
  }
  if ((unsigned long )parser->bo_list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    gds = (parser->bo_list)->gds_obj;
    gws = (parser->bo_list)->gws_obj;
    oa = (parser->bo_list)->oa_obj;
    ib___0 = parser->ibs;
    if ((unsigned long )gds != (unsigned long )((struct amdgpu_bo *)0)) {
      tmp = amdgpu_bo_gpu_offset(gds);
      ib___0->gds_base = (u32 )tmp;
      tmp___0 = amdgpu_bo_size(gds);
      ib___0->gds_size = (u32 )tmp___0;
    } else {
    }
    if ((unsigned long )gws != (unsigned long )((struct amdgpu_bo *)0)) {
      tmp___1 = amdgpu_bo_gpu_offset(gws);
      ib___0->gws_base = (u32 )tmp___1;
      tmp___2 = amdgpu_bo_size(gws);
      ib___0->gws_size = (u32 )tmp___2;
    } else {
    }
    if ((unsigned long )oa != (unsigned long )((struct amdgpu_bo *)0)) {
      tmp___3 = amdgpu_bo_gpu_offset(oa);
      ib___0->oa_base = (u32 )tmp___3;
      tmp___4 = amdgpu_bo_size(oa);
      ib___0->oa_size = (u32 )tmp___4;
    } else {
    }
  } else {
  }
  if ((unsigned long )parser->uf.bo != (unsigned long )((struct amdgpu_bo *)0)) {
    ib___1 = parser->ibs + (unsigned long )(parser->num_ibs - 1U);
    if ((unsigned int )(ib___1->ring)->type == 3U || (unsigned int )(ib___1->ring)->type == 4U) {
      return (-22);
    } else {
    }
    ib___1->user = & parser->uf;
  } else {
  }
  return (0);
}
}
static int amdgpu_cs_dependencies(struct amdgpu_device *adev , struct amdgpu_cs_parser *p )
{
  struct amdgpu_ib *ib ;
  int i ;
  int j ;
  int r ;
  struct drm_amdgpu_cs_chunk_dep *deps ;
  struct amdgpu_cs_chunk *chunk ;
  unsigned int num_deps ;
  struct amdgpu_fence *fence ;
  struct amdgpu_ring *ring ;
  {
  if (p->num_ibs == 0U) {
    return (0);
  } else {
  }
  ib = p->ibs;
  i = 0;
  goto ldv_44720;
  ldv_44719:
  chunk = p->chunks + (unsigned long )i;
  if (chunk->chunk_id != 3U) {
    goto ldv_44713;
  } else {
  }
  deps = (struct drm_amdgpu_cs_chunk_dep *)chunk->kdata;
  num_deps = (chunk->length_dw * 4U) / 24U;
  j = 0;
  goto ldv_44717;
  ldv_44716:
  r = amdgpu_cs_get_ring(adev, (deps + (unsigned long )j)->ip_type, (deps + (unsigned long )j)->ip_instance,
                         (deps + (unsigned long )j)->ring, & ring);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_fence_recreate(ring, (void *)p->filp, (deps + (unsigned long )j)->handle,
                            & fence);
  if (r != 0) {
    return (r);
  } else {
  }
  amdgpu_sync_fence(& ib->sync, fence);
  amdgpu_fence_unref(& fence);
  j = j + 1;
  ldv_44717: ;
  if ((unsigned int )j < num_deps) {
    goto ldv_44716;
  } else {
  }
  ldv_44713:
  i = i + 1;
  ldv_44720: ;
  if ((unsigned int )i < p->nchunks) {
    goto ldv_44719;
  } else {
  }
  return (0);
}
}
int amdgpu_cs_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  struct amdgpu_device *adev ;
  union drm_amdgpu_cs *cs ;
  struct amdgpu_cs_parser parser ;
  int r ;
  int i ;
  bool reserved_buffers ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  cs = (union drm_amdgpu_cs *)data;
  reserved_buffers = 0;
  down_read(& adev->exclusive_lock);
  if (! adev->accel_working) {
    up_read(& adev->exclusive_lock);
    return (-16);
  } else {
  }
  memset((void *)(& parser), 0, 200UL);
  parser.filp = filp;
  parser.adev = adev;
  r = amdgpu_cs_parser_init(& parser, data);
  if (r != 0) {
    drm_err("Failed to initialize parser !\n");
    amdgpu_cs_parser_fini(& parser, r, 0);
    up_read(& adev->exclusive_lock);
    r = amdgpu_cs_handle_lockup(adev, r);
    return (r);
  } else {
  }
  r = amdgpu_cs_parser_relocs(& parser);
  if (r != 0) {
    if (r != -512) {
      if (r == -12) {
        drm_err("Not enough memory for command submission!\n");
      } else {
        drm_err("Failed to process the buffer list %d!\n", r);
      }
    } else {
    }
  } else {
  }
  if (r == 0) {
    reserved_buffers = 1;
    r = amdgpu_cs_ib_fill(adev, & parser);
  } else {
  }
  if (r == 0) {
    r = amdgpu_cs_dependencies(adev, & parser);
  } else {
  }
  if (r != 0) {
    amdgpu_cs_parser_fini(& parser, r, (int )reserved_buffers);
    up_read(& adev->exclusive_lock);
    r = amdgpu_cs_handle_lockup(adev, r);
    return (r);
  } else {
  }
  i = 0;
  goto ldv_44734;
  ldv_44733:
  trace_amdgpu_cs(& parser, i);
  i = i + 1;
  ldv_44734: ;
  if ((u32 )i < parser.num_ibs) {
    goto ldv_44733;
  } else {
  }
  r = amdgpu_cs_ib_vm_chunk(adev, & parser);
  if (r != 0) {
    goto out;
  } else {
  }
  cs->out.handle = ((parser.ibs + (unsigned long )(parser.num_ibs - 1U))->fence)->seq;
  out:
  amdgpu_cs_parser_fini(& parser, r, 1);
  up_read(& adev->exclusive_lock);
  r = amdgpu_cs_handle_lockup(adev, r);
  return (r);
}
}
int amdgpu_cs_wait_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  union drm_amdgpu_wait_cs *wait ;
  struct amdgpu_device *adev ;
  unsigned long timeout ;
  unsigned long tmp ;
  struct amdgpu_fence *fence ;
  struct amdgpu_ring *ring ;
  struct amdgpu_ctx *ctx ;
  long r ;
  int tmp___0 ;
  int tmp___1 ;
  {
  wait = (union drm_amdgpu_wait_cs *)data;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_gem_timeout(wait->in.timeout);
  timeout = tmp;
  fence = (struct amdgpu_fence *)0;
  ring = (struct amdgpu_ring *)0;
  ctx = amdgpu_ctx_get((struct amdgpu_fpriv *)filp->driver_priv, wait->in.ctx_id);
  if ((unsigned long )ctx == (unsigned long )((struct amdgpu_ctx *)0)) {
    return (-22);
  } else {
  }
  tmp___0 = amdgpu_cs_get_ring(adev, wait->in.ip_type, wait->in.ip_instance, wait->in.ring,
                               & ring);
  r = (long )tmp___0;
  if (r != 0L) {
    return ((int )r);
  } else {
  }
  tmp___1 = amdgpu_fence_recreate(ring, (void *)filp, wait->in.handle, & fence);
  r = (long )tmp___1;
  if (r != 0L) {
    return ((int )r);
  } else {
  }
  r = fence_wait_timeout(& fence->base, 1, (long )timeout);
  amdgpu_fence_unref(& fence);
  amdgpu_ctx_put(ctx);
  if (r < 0L) {
    return ((int )r);
  } else {
  }
  memset((void *)wait, 0, 32UL);
  wait->out.status = r == 0L;
  return (0);
}
}
struct amdgpu_bo_va_mapping *amdgpu_cs_find_mapping(struct amdgpu_cs_parser *parser ,
                                                    uint64_t addr , struct amdgpu_bo **bo )
{
  struct amdgpu_bo_list_entry *reloc ;
  struct amdgpu_bo_va_mapping *mapping ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  addr = addr / 4096ULL;
  __mptr = (struct list_head const *)parser->validated.next;
  reloc = (struct amdgpu_bo_list_entry *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_44770;
  ldv_44769: ;
  if ((unsigned long )reloc->bo_va == (unsigned long )((struct amdgpu_bo_va *)0)) {
    goto ldv_44760;
  } else {
  }
  __mptr___0 = (struct list_head const *)(reloc->bo_va)->mappings.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr___0;
  goto ldv_44767;
  ldv_44766: ;
  if ((unsigned long long )mapping->it.start > addr || (unsigned long long )mapping->it.last < addr) {
    goto ldv_44765;
  } else {
  }
  *bo = (reloc->bo_va)->bo;
  return (mapping);
  ldv_44765:
  __mptr___1 = (struct list_head const *)mapping->list.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr___1;
  ldv_44767: ;
  if ((unsigned long )(& mapping->list) != (unsigned long )(& (reloc->bo_va)->mappings)) {
    goto ldv_44766;
  } else {
  }
  ldv_44760:
  __mptr___2 = (struct list_head const *)reloc->tv.head.next;
  reloc = (struct amdgpu_bo_list_entry *)__mptr___2 + 0xfffffffffffffff8UL;
  ldv_44770: ;
  if ((unsigned long )(& reloc->tv.head) != (unsigned long )(& parser->validated)) {
    goto ldv_44769;
  } else {
  }
  return ((struct amdgpu_bo_va_mapping *)0);
}
}
bool ldv_queue_work_on_243(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_244(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_245(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_246(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_247(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern int memcmp(void const * , void const * , size_t ) ;
bool ldv_queue_work_on_257(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_259(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_258(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_261(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_260(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static unsigned char readb(void const volatile *addr )
{
  unsigned char ret ;
  {
  __asm__ volatile ("movb %1,%0": "=q" (ret): "m" (*((unsigned char volatile *)addr)): "memory");
  return (ret);
}
}
__inline static void memcpy_fromio(void *dst , void const volatile *src , size_t count )
{
  {
  memcpy(dst, (void const *)src, count);
  return;
}
}
extern struct pci_dev *pci_get_class(unsigned int , struct pci_dev * ) ;
extern void *pci_map_rom(struct pci_dev * , size_t * ) ;
extern void pci_unmap_rom(struct pci_dev * , void * ) ;
extern void *pci_platform_rom(struct pci_dev * , size_t * ) ;
bool amdgpu_read_bios(struct amdgpu_device *adev ) ;
extern char const *acpi_format_exception(acpi_status ) ;
extern acpi_status acpi_get_handle(acpi_handle , acpi_string , acpi_handle ** ) ;
extern acpi_status acpi_evaluate_object(acpi_handle , acpi_string , struct acpi_object_list * ,
                                        struct acpi_buffer * ) ;
extern acpi_status acpi_get_table_with_size(acpi_string , u32 , struct acpi_table_header ** ,
                                            acpi_size * ) ;
__inline static bool is_acpi_node(struct fwnode_handle *fwnode )
{
  {
  return ((bool )((unsigned long )fwnode != (unsigned long )((struct fwnode_handle *)0) && (unsigned int )fwnode->type == 2U));
}
}
__inline static struct acpi_device *to_acpi_node(struct fwnode_handle *fwnode )
{
  struct fwnode_handle const *__mptr ;
  struct acpi_device *tmp___0 ;
  bool tmp___1 ;
  {
  tmp___1 = is_acpi_node(fwnode);
  if ((int )tmp___1) {
    __mptr = (struct fwnode_handle const *)fwnode;
    tmp___0 = (struct acpi_device *)__mptr + 0xfffffffffffffff0UL;
  } else {
    tmp___0 = (struct acpi_device *)0;
  }
  return (tmp___0);
}
}
__inline static acpi_handle acpi_device_handle(struct acpi_device *adev )
{
  {
  return ((unsigned long )adev != (unsigned long )((struct acpi_device *)0) ? adev->handle : (acpi_handle )0);
}
}
static bool igp_read_bios_from_vram(struct amdgpu_device *adev )
{
  uint8_t *bios ;
  resource_size_t vram_base ;
  resource_size_t size ;
  bool tmp ;
  int tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  {
  size = 262144ULL;
  if ((adev->flags & 131072UL) == 0UL) {
    tmp = amdgpu_card_posted(adev);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    if (tmp___0) {
      return (0);
    } else {
    }
  } else {
  }
  adev->bios = (uint8_t *)0U;
  vram_base = (adev->pdev)->resource[0].start;
  tmp___1 = ioremap(vram_base, (unsigned long )size);
  bios = (uint8_t *)tmp___1;
  if ((unsigned long )bios == (unsigned long )((uint8_t *)0U)) {
    return (0);
  } else {
  }
  if ((size == 0ULL || (unsigned int )*bios != 85U) || (unsigned int )*(bios + 1UL) != 170U) {
    iounmap((void volatile *)bios);
    return (0);
  } else {
  }
  tmp___2 = kmalloc((size_t )size, 208U);
  adev->bios = (uint8_t *)tmp___2;
  if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
    iounmap((void volatile *)bios);
    return (0);
  } else {
  }
  memcpy_fromio((void *)adev->bios, (void const volatile *)bios, (size_t )size);
  iounmap((void volatile *)bios);
  return (1);
}
}
bool amdgpu_read_bios(struct amdgpu_device *adev )
{
  uint8_t *bios ;
  uint8_t val1 ;
  uint8_t val2 ;
  size_t size ;
  void *tmp ;
  void *tmp___0 ;
  {
  adev->bios = (uint8_t *)0U;
  tmp = pci_map_rom(adev->pdev, & size);
  bios = (uint8_t *)tmp;
  if ((unsigned long )bios == (unsigned long )((uint8_t *)0U)) {
    return (0);
  } else {
  }
  val1 = readb((void const volatile *)bios);
  val2 = readb((void const volatile *)bios + 1U);
  if ((size == 0UL || (unsigned int )val1 != 85U) || (unsigned int )val2 != 170U) {
    pci_unmap_rom(adev->pdev, (void *)bios);
    return (0);
  } else {
  }
  tmp___0 = kzalloc(size, 208U);
  adev->bios = (uint8_t *)tmp___0;
  if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
    pci_unmap_rom(adev->pdev, (void *)bios);
    return (0);
  } else {
  }
  memcpy_fromio((void *)adev->bios, (void const volatile *)bios, size);
  pci_unmap_rom(adev->pdev, (void *)bios);
  return (1);
}
}
static bool amdgpu_read_platform_bios(struct amdgpu_device *adev )
{
  uint8_t *bios ;
  size_t size ;
  void *tmp ;
  void *tmp___0 ;
  {
  adev->bios = (uint8_t *)0U;
  tmp = pci_platform_rom(adev->pdev, & size);
  bios = (uint8_t *)tmp;
  if ((unsigned long )bios == (unsigned long )((uint8_t *)0U)) {
    return (0);
  } else {
  }
  if ((size == 0UL || (unsigned int )*bios != 85U) || (unsigned int )*(bios + 1UL) != 170U) {
    return (0);
  } else {
  }
  tmp___0 = kmemdup((void const *)bios, size, 208U);
  adev->bios = (uint8_t *)tmp___0;
  if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
    return (0);
  } else {
  }
  return (1);
}
}
static int amdgpu_atrm_call(acpi_handle atrm_handle , uint8_t *bios , int offset ,
                            int len )
{
  acpi_status status ;
  union acpi_object atrm_arg_elements[2U] ;
  union acpi_object *obj ;
  struct acpi_object_list atrm_arg ;
  struct acpi_buffer buffer ;
  char const *tmp ;
  {
  buffer.length = 0xffffffffffffffffULL;
  buffer.pointer = (void *)0;
  atrm_arg.count = 2U;
  atrm_arg.pointer = (union acpi_object *)(& atrm_arg_elements);
  atrm_arg_elements[0].type = 1U;
  atrm_arg_elements[0].integer.value = (u64 )offset;
  atrm_arg_elements[1].type = 1U;
  atrm_arg_elements[1].integer.value = (u64 )len;
  status = acpi_evaluate_object(atrm_handle, (acpi_string )0, & atrm_arg, & buffer);
  if (status != 0U) {
    tmp = acpi_format_exception(status);
    printk("failed to evaluate ATRM got %s\n", tmp);
    return (-19);
  } else {
  }
  obj = (union acpi_object *)buffer.pointer;
  memcpy((void *)bios + (unsigned long )offset, (void const *)obj->buffer.pointer,
           (size_t )obj->buffer.length);
  len = (int )obj->buffer.length;
  kfree((void const *)buffer.pointer);
  return (len);
}
}
static bool amdgpu_atrm_get_bios(struct amdgpu_device *adev )
{
  int ret ;
  int size ;
  int i ;
  struct pci_dev *pdev ;
  acpi_handle dhandle ;
  acpi_handle atrm_handle ;
  acpi_status status ;
  bool found ;
  struct acpi_device *tmp ;
  struct acpi_device *tmp___0 ;
  void *tmp___1 ;
  {
  size = 262144;
  pdev = (struct pci_dev *)0;
  found = 0;
  if ((adev->flags & 131072UL) != 0UL) {
    return (0);
  } else {
  }
  goto ldv_51561;
  ldv_51563:
  tmp = to_acpi_node(pdev->dev.fwnode);
  dhandle = acpi_device_handle(tmp);
  if ((unsigned long )dhandle == (unsigned long )((acpi_handle )0)) {
    goto ldv_51561;
  } else {
  }
  status = acpi_get_handle(dhandle, (char *)"ATRM", & atrm_handle);
  if (status == 0U) {
    found = 1;
    goto ldv_51562;
  } else {
  }
  ldv_51561:
  pdev = pci_get_class(196608U, pdev);
  if ((unsigned long )pdev != (unsigned long )((struct pci_dev *)0)) {
    goto ldv_51563;
  } else {
  }
  ldv_51562: ;
  if (! found) {
    goto ldv_51564;
    ldv_51566:
    tmp___0 = to_acpi_node(pdev->dev.fwnode);
    dhandle = acpi_device_handle(tmp___0);
    if ((unsigned long )dhandle == (unsigned long )((acpi_handle )0)) {
      goto ldv_51564;
    } else {
    }
    status = acpi_get_handle(dhandle, (char *)"ATRM", & atrm_handle);
    if (status == 0U) {
      found = 1;
      goto ldv_51565;
    } else {
    }
    ldv_51564:
    pdev = pci_get_class(229376U, pdev);
    if ((unsigned long )pdev != (unsigned long )((struct pci_dev *)0)) {
      goto ldv_51566;
    } else {
    }
    ldv_51565: ;
  } else {
  }
  if (! found) {
    return (0);
  } else {
  }
  tmp___1 = kmalloc((size_t )size, 208U);
  adev->bios = (uint8_t *)tmp___1;
  if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
    drm_err("Unable to allocate bios\n");
    return (0);
  } else {
  }
  i = 0;
  goto ldv_51569;
  ldv_51568:
  ret = amdgpu_atrm_call(atrm_handle, adev->bios, i * 4096, 4096);
  if (ret <= 4095) {
    goto ldv_51567;
  } else {
  }
  i = i + 1;
  ldv_51569: ;
  if (size / 4096 > i) {
    goto ldv_51568;
  } else {
  }
  ldv_51567: ;
  if ((i == 0 || (unsigned int )*(adev->bios) != 85U) || (unsigned int )*(adev->bios + 1UL) != 170U) {
    kfree((void const *)adev->bios);
    return (0);
  } else {
  }
  return (1);
}
}
static bool amdgpu_read_disabled_bios(struct amdgpu_device *adev )
{
  bool tmp ;
  bool tmp___0 ;
  {
  if ((adev->flags & 131072UL) != 0UL) {
    tmp = igp_read_bios_from_vram(adev);
    return (tmp);
  } else {
    tmp___0 = (*((adev->asic_funcs)->read_disabled_bios))(adev);
    return (tmp___0);
  }
}
}
static bool amdgpu_acpi_vfct_bios(struct amdgpu_device *adev )
{
  bool ret ;
  struct acpi_table_header *hdr ;
  acpi_size tbl_size ;
  UEFI_ACPI_VFCT *vfct ;
  GOP_VBIOS_CONTENT *vbios ;
  VFCT_IMAGE_HEADER *vhdr ;
  acpi_status tmp ;
  void *tmp___0 ;
  {
  ret = 0;
  tmp = acpi_get_table_with_size((char *)"VFCT", 1U, & hdr, & tbl_size);
  if (tmp != 0U) {
    return (0);
  } else {
  }
  if (tbl_size <= 75ULL) {
    drm_err("ACPI VFCT table present but broken (too short #1)\n");
    goto out_unmap;
  } else {
  }
  vfct = (UEFI_ACPI_VFCT *)hdr;
  if ((unsigned long long )((unsigned long )vfct->VBIOSImageOffset + 28UL) > tbl_size) {
    drm_err("ACPI VFCT table present but broken (too short #2)\n");
    goto out_unmap;
  } else {
  }
  vbios = (GOP_VBIOS_CONTENT *)hdr + (unsigned long )vfct->VBIOSImageOffset;
  vhdr = & vbios->VbiosHeader;
  printk("\016[drm] ACPI VFCT contains a BIOS for %02x:%02x.%d %04x:%04x, size %d\n",
         vhdr->PCIBus, vhdr->PCIDevice, vhdr->PCIFunction, (int )vhdr->VendorID, (int )vhdr->DeviceID,
         vhdr->ImageLength);
  if ((((vhdr->PCIBus != (ULONG )((adev->pdev)->bus)->number || vhdr->PCIDevice != (((adev->pdev)->devfn >> 3) & 31U)) || vhdr->PCIFunction != ((adev->pdev)->devfn & 7U)) || (int )vhdr->VendorID != (int )(adev->pdev)->vendor) || (int )vhdr->DeviceID != (int )(adev->pdev)->device) {
    printk("\016[drm] ACPI VFCT table is not for this card\n");
    goto out_unmap;
  } else {
  }
  if ((unsigned long long )(((unsigned long )vfct->VBIOSImageOffset + (unsigned long )vhdr->ImageLength) + 28UL) > tbl_size) {
    drm_err("ACPI VFCT image truncated\n");
    goto out_unmap;
  } else {
  }
  tmp___0 = kmemdup((void const *)(& vbios->VbiosContent), (size_t )vhdr->ImageLength,
                    208U);
  adev->bios = (uint8_t *)tmp___0;
  ret = (unsigned long )adev->bios != (unsigned long )((uint8_t *)0U);
  out_unmap: ;
  return (ret);
}
}
bool amdgpu_get_bios(struct amdgpu_device *adev )
{
  bool r ;
  u16 tmp ;
  int tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  r = amdgpu_atrm_get_bios(adev);
  if (! r) {
    r = amdgpu_acpi_vfct_bios(adev);
  } else {
  }
  if (! r) {
    r = igp_read_bios_from_vram(adev);
  } else {
  }
  if (! r) {
    r = amdgpu_read_bios(adev);
  } else {
  }
  if (! r) {
    r = amdgpu_read_disabled_bios(adev);
  } else {
  }
  if (! r) {
    r = amdgpu_read_platform_bios(adev);
  } else {
  }
  if (! r || (unsigned long )adev->bios == (unsigned long )((uint8_t *)0U)) {
    drm_err("Unable to locate a BIOS ROM\n");
    adev->bios = (uint8_t *)0U;
    return (0);
  } else {
  }
  if ((unsigned int )*(adev->bios) != 85U || (unsigned int )*(adev->bios + 1UL) != 170U) {
    printk("BIOS signature incorrect %x %x\n", (int )*(adev->bios), (int )*(adev->bios + 1UL));
    goto free_bios;
  } else {
  }
  tmp = (u16 )((int )((short )*(adev->bios + 24UL)) | (int )((short )((int )*(adev->bios + 25UL) << 8)));
  if ((unsigned int )*(adev->bios + ((unsigned long )tmp + 20UL)) != 0U) {
    printk("\016[drm] Not an x86 BIOS ROM, not using.\n");
    goto free_bios;
  } else {
  }
  adev->bios_header_start = (u16 )((int )((short )*(adev->bios + 72UL)) | (int )((short )((int )*(adev->bios + 73UL) << 8)));
  if ((unsigned int )adev->bios_header_start == 0U) {
    goto free_bios;
  } else {
  }
  tmp = (unsigned int )adev->bios_header_start + 4U;
  tmp___0 = memcmp((void const *)adev->bios + (unsigned long )tmp, (void const *)"ATOM",
                   4UL);
  if (tmp___0 == 0) {
    adev->is_atom_bios = 1;
  } else {
    tmp___1 = memcmp((void const *)adev->bios + (unsigned long )tmp, (void const *)"MOTA",
                     4UL);
    if (tmp___1 == 0) {
      adev->is_atom_bios = 1;
    } else {
      adev->is_atom_bios = 0;
    }
  }
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("amdgpu_get_bios", "%sBIOS detected\n", (int )adev->is_atom_bios ? (char *)"ATOM" : (char *)"COM");
  } else {
  }
  return (1);
  free_bios:
  kfree((void const *)adev->bios);
  adev->bios = (uint8_t *)0U;
  return (0);
}
}
bool ldv_queue_work_on_257(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_258(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_259(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_260(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_261(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_271(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_273(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_272(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_275(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_274(struct workqueue_struct *ldv_func_arg1 ) ;
static int amdgpu_benchmark_do_move(struct amdgpu_device *adev , unsigned int size ,
                                    uint64_t saddr , uint64_t daddr , int n )
{
  unsigned long start_jiffies ;
  unsigned long end_jiffies ;
  struct amdgpu_fence *fence ;
  int i ;
  int r ;
  struct amdgpu_ring *ring ;
  unsigned int tmp ;
  {
  fence = (struct amdgpu_fence *)0;
  start_jiffies = jiffies;
  i = 0;
  goto ldv_43655;
  ldv_43654:
  ring = adev->mman.buffer_funcs_ring;
  r = amdgpu_copy_buffer(ring, saddr, daddr, size, (struct reservation_object *)0,
                         & fence);
  if (r != 0) {
    goto exit_do_move;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    goto exit_do_move;
  } else {
  }
  amdgpu_fence_unref(& fence);
  i = i + 1;
  ldv_43655: ;
  if (i < n) {
    goto ldv_43654;
  } else {
  }
  end_jiffies = jiffies;
  tmp = jiffies_to_msecs(end_jiffies - start_jiffies);
  r = (int )tmp;
  exit_do_move: ;
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fence);
  } else {
  }
  return (r);
}
}
static void amdgpu_benchmark_log_results(int n , unsigned int size , unsigned int time ,
                                         unsigned int sdomain , unsigned int ddomain ,
                                         char *kind )
{
  unsigned int throughput ;
  {
  throughput = ((size >> 10) * (unsigned int )n) / time;
  printk("\016[drm] amdgpu: %s %u bo moves of %u kB from %d to %d in %u ms, throughput: %u Mb/s or %u MB/s\n",
         kind, n, size >> 10, sdomain, ddomain, time, throughput * 8U, throughput);
  return;
}
}
static void amdgpu_benchmark_move(struct amdgpu_device *adev , unsigned int size ,
                                  unsigned int sdomain , unsigned int ddomain )
{
  struct amdgpu_bo *dobj ;
  struct amdgpu_bo *sobj ;
  uint64_t saddr ;
  uint64_t daddr ;
  int r ;
  int n ;
  int time ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  dobj = (struct amdgpu_bo *)0;
  sobj = (struct amdgpu_bo *)0;
  n = 1024;
  r = amdgpu_bo_create(adev, (unsigned long )size, 4096, 1, sdomain, 0ULL, (struct sg_table *)0,
                       & sobj);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_reserve(sobj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_pin(sobj, sdomain, & saddr);
  amdgpu_bo_unreserve(sobj);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )size, 4096, 1, ddomain, 0ULL, (struct sg_table *)0,
                       & dobj);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_reserve(dobj, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_pin(dobj, ddomain, & daddr);
  amdgpu_bo_unreserve(dobj);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  if ((unsigned long )adev->mman.buffer_funcs != (unsigned long )((struct amdgpu_buffer_funcs const *)0)) {
    time = amdgpu_benchmark_do_move(adev, size, saddr, daddr, n);
    if (time < 0) {
      goto out_cleanup;
    } else {
    }
    if (time > 0) {
      amdgpu_benchmark_log_results(n, size, (unsigned int )time, sdomain, ddomain,
                                   (char *)"dma");
    } else {
    }
  } else {
  }
  out_cleanup: ;
  if ((unsigned long )sobj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(sobj, 0);
    tmp___1 = ldv__builtin_expect(r == 0, 1L);
    if (tmp___1 != 0L) {
      amdgpu_bo_unpin(sobj);
      amdgpu_bo_unreserve(sobj);
    } else {
    }
    amdgpu_bo_unref(& sobj);
  } else {
  }
  if ((unsigned long )dobj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(dobj, 0);
    tmp___2 = ldv__builtin_expect(r == 0, 1L);
    if (tmp___2 != 0L) {
      amdgpu_bo_unpin(dobj);
      amdgpu_bo_unreserve(dobj);
    } else {
    }
    amdgpu_bo_unref(& dobj);
  } else {
  }
  if (r != 0) {
    drm_err("Error while benchmarking BO move.\n");
  } else {
  }
  return;
}
}
void amdgpu_benchmark(struct amdgpu_device *adev , int test_number )
{
  int i ;
  int common_modes[17U] ;
  {
  common_modes[0] = 1228800;
  common_modes[1] = 1382400;
  common_modes[2] = 1920000;
  common_modes[3] = 1628160;
  common_modes[4] = 3145728;
  common_modes[5] = 3538944;
  common_modes[6] = 3686400;
  common_modes[7] = 4096000;
  common_modes[8] = 4372480;
  common_modes[9] = 4915200;
  common_modes[10] = 5242880;
  common_modes[11] = 5184000;
  common_modes[12] = 5880000;
  common_modes[13] = 7056000;
  common_modes[14] = 7680000;
  common_modes[15] = 8294400;
  common_modes[16] = 9216000;
  switch (test_number) {
  case 1:
  amdgpu_benchmark_move(adev, 1048576U, 2U, 4U);
  amdgpu_benchmark_move(adev, 1048576U, 4U, 2U);
  goto ldv_43687;
  case 2:
  amdgpu_benchmark_move(adev, 1048576U, 4U, 4U);
  goto ldv_43687;
  case 3:
  i = 1;
  goto ldv_43691;
  ldv_43690:
  amdgpu_benchmark_move(adev, (unsigned int )(i * 4096), 2U, 4U);
  i = i << 1;
  ldv_43691: ;
  if (i <= 16384) {
    goto ldv_43690;
  } else {
  }
  goto ldv_43687;
  case 4:
  i = 1;
  goto ldv_43695;
  ldv_43694:
  amdgpu_benchmark_move(adev, (unsigned int )(i * 4096), 4U, 2U);
  i = i << 1;
  ldv_43695: ;
  if (i <= 16384) {
    goto ldv_43694;
  } else {
  }
  goto ldv_43687;
  case 5:
  i = 1;
  goto ldv_43699;
  ldv_43698:
  amdgpu_benchmark_move(adev, (unsigned int )(i * 4096), 4U, 4U);
  i = i << 1;
  ldv_43699: ;
  if (i <= 16384) {
    goto ldv_43698;
  } else {
  }
  goto ldv_43687;
  case 6:
  i = 0;
  goto ldv_43703;
  ldv_43702:
  amdgpu_benchmark_move(adev, (unsigned int )common_modes[i], 2U, 4U);
  i = i + 1;
  ldv_43703: ;
  if (i <= 16) {
    goto ldv_43702;
  } else {
  }
  goto ldv_43687;
  case 7:
  i = 0;
  goto ldv_43707;
  ldv_43706:
  amdgpu_benchmark_move(adev, (unsigned int )common_modes[i], 4U, 2U);
  i = i + 1;
  ldv_43707: ;
  if (i <= 16) {
    goto ldv_43706;
  } else {
  }
  goto ldv_43687;
  case 8:
  i = 0;
  goto ldv_43711;
  ldv_43710:
  amdgpu_benchmark_move(adev, (unsigned int )common_modes[i], 4U, 4U);
  i = i + 1;
  ldv_43711: ;
  if (i <= 16) {
    goto ldv_43710;
  } else {
  }
  goto ldv_43687;
  default:
  drm_err("Unknown benchmark\n");
  }
  ldv_43687: ;
  return;
}
}
bool ldv_queue_work_on_271(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_272(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_273(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_274(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_275(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_285(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_287(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_286(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_289(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_288(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_semaphore_create(struct amdgpu_device *adev , struct amdgpu_semaphore **semaphore ) ;
bool amdgpu_semaphore_emit_signal(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ) ;
bool amdgpu_semaphore_emit_wait(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ) ;
void amdgpu_semaphore_free(struct amdgpu_device *adev , struct amdgpu_semaphore **semaphore ,
                           struct amdgpu_fence *fence ) ;
void amdgpu_test_ring_sync(struct amdgpu_device *adev , struct amdgpu_ring *ringA ,
                           struct amdgpu_ring *ringB ) ;
int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence ) ;
int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence ) ;
int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence ) ;
int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence ) ;
static void amdgpu_do_test_moves(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_bo *vram_obj ;
  struct amdgpu_bo **gtt_obj ;
  uint64_t gtt_addr ;
  uint64_t vram_addr ;
  unsigned int n ;
  unsigned int size ;
  int i ;
  int r ;
  void *tmp ;
  long tmp___0 ;
  void *gtt_map ;
  void *vram_map ;
  void **gtt_start ;
  void **gtt_end ;
  void **vram_start ;
  void **vram_end ;
  struct amdgpu_fence *fence ;
  long tmp___1 ;
  {
  ring = adev->mman.buffer_funcs_ring;
  vram_obj = (struct amdgpu_bo *)0;
  gtt_obj = (struct amdgpu_bo **)0;
  size = 1048576U;
  n = (unsigned int )adev->mc.gtt_size - 1048576U;
  i = 0;
  goto ldv_43714;
  ldv_43713: ;
  if ((unsigned long )adev->rings[i] != (unsigned long )((struct amdgpu_ring *)0)) {
    n = n - (adev->rings[i])->ring_size;
  } else {
  }
  i = i + 1;
  ldv_43714: ;
  if (i <= 15) {
    goto ldv_43713;
  } else {
  }
  if ((unsigned long )adev->wb.wb_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    n = n - 4096U;
  } else {
  }
  if ((unsigned long )adev->irq.ih.ring_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    n = n - adev->irq.ih.ring_size;
  } else {
  }
  n = n / size;
  tmp = kzalloc((unsigned long )n * 8UL, 208U);
  gtt_obj = (struct amdgpu_bo **)tmp;
  if ((unsigned long )gtt_obj == (unsigned long )((struct amdgpu_bo **)0)) {
    drm_err("Failed to allocate %d pointers\n", n);
    r = 1;
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )size, 4096, 1, 4U, 0ULL, (struct sg_table *)0,
                       & vram_obj);
  if (r != 0) {
    drm_err("Failed to create VRAM object\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_bo_reserve(vram_obj, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    goto out_unref;
  } else {
  }
  r = amdgpu_bo_pin(vram_obj, 4U, & vram_addr);
  if (r != 0) {
    drm_err("Failed to pin VRAM object\n");
    goto out_unres;
  } else {
  }
  i = 0;
  goto ldv_43745;
  ldv_43744:
  fence = (struct amdgpu_fence *)0;
  r = amdgpu_bo_create(adev, (unsigned long )size, 4096, 1, 2U, 0ULL, (struct sg_table *)0,
                       gtt_obj + (unsigned long )i);
  if (r != 0) {
    drm_err("Failed to create GTT object %d\n", i);
    goto out_lclean;
  } else {
  }
  r = amdgpu_bo_reserve(*(gtt_obj + (unsigned long )i), 0);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    goto out_lclean_unref;
  } else {
  }
  r = amdgpu_bo_pin(*(gtt_obj + (unsigned long )i), 2U, & gtt_addr);
  if (r != 0) {
    drm_err("Failed to pin GTT object %d\n", i);
    goto out_lclean_unres;
  } else {
  }
  r = amdgpu_bo_kmap(*(gtt_obj + (unsigned long )i), & gtt_map);
  if (r != 0) {
    drm_err("Failed to map GTT object %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  gtt_start = (void **)gtt_map;
  gtt_end = (void **)gtt_map + (unsigned long )size;
  goto ldv_43731;
  ldv_43730:
  *gtt_start = (void *)gtt_start;
  gtt_start = gtt_start + 1;
  ldv_43731: ;
  if ((unsigned long )gtt_start < (unsigned long )gtt_end) {
    goto ldv_43730;
  } else {
  }
  amdgpu_bo_kunmap(*(gtt_obj + (unsigned long )i));
  r = amdgpu_copy_buffer(ring, gtt_addr, vram_addr, size, (struct reservation_object *)0,
                         & fence);
  if (r != 0) {
    drm_err("Failed GTT->VRAM copy %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("Failed to wait for GTT->VRAM fence %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  amdgpu_fence_unref(& fence);
  r = amdgpu_bo_kmap(vram_obj, & vram_map);
  if (r != 0) {
    drm_err("Failed to map VRAM object after copy %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  gtt_start = (void **)gtt_map;
  gtt_end = (void **)gtt_map + (unsigned long )size;
  vram_start = (void **)vram_map;
  vram_end = (void **)vram_map + (unsigned long )size;
  goto ldv_43734;
  ldv_43733: ;
  if ((unsigned long )*vram_start != (unsigned long )((void *)gtt_start)) {
    drm_err("Incorrect GTT->VRAM copy %d: Got 0x%p, expected 0x%p (GTT/VRAM offset 0x%16llx/0x%16llx)\n",
            i, *vram_start, gtt_start, (unsigned long long )((long )((void *)gtt_start + (gtt_addr - adev->mc.gtt_start)) - (long )gtt_map),
            (unsigned long long )((long )((void *)gtt_start + (vram_addr - adev->mc.vram_start)) - (long )gtt_map));
    amdgpu_bo_kunmap(vram_obj);
    goto out_lclean_unpin;
  } else {
  }
  *vram_start = (void *)vram_start;
  gtt_start = gtt_start + 1;
  vram_start = vram_start + 1;
  ldv_43734: ;
  if ((unsigned long )vram_start < (unsigned long )vram_end) {
    goto ldv_43733;
  } else {
  }
  amdgpu_bo_kunmap(vram_obj);
  r = amdgpu_copy_buffer(ring, vram_addr, gtt_addr, size, (struct reservation_object *)0,
                         & fence);
  if (r != 0) {
    drm_err("Failed VRAM->GTT copy %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("Failed to wait for VRAM->GTT fence %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  amdgpu_fence_unref(& fence);
  r = amdgpu_bo_kmap(*(gtt_obj + (unsigned long )i), & gtt_map);
  if (r != 0) {
    drm_err("Failed to map GTT object after copy %d\n", i);
    goto out_lclean_unpin;
  } else {
  }
  gtt_start = (void **)gtt_map;
  gtt_end = (void **)gtt_map + (unsigned long )size;
  vram_start = (void **)vram_map;
  vram_end = (void **)vram_map + (unsigned long )size;
  goto ldv_43737;
  ldv_43736: ;
  if ((unsigned long )*gtt_start != (unsigned long )((void *)vram_start)) {
    drm_err("Incorrect VRAM->GTT copy %d: Got 0x%p, expected 0x%p (VRAM/GTT offset 0x%16llx/0x%16llx)\n",
            i, *gtt_start, vram_start, (unsigned long long )((long )((void *)vram_start + (vram_addr - adev->mc.vram_start)) - (long )vram_map),
            (unsigned long long )((long )((void *)vram_start + (gtt_addr - adev->mc.gtt_start)) - (long )vram_map));
    amdgpu_bo_kunmap(*(gtt_obj + (unsigned long )i));
    goto out_lclean_unpin;
  } else {
  }
  gtt_start = gtt_start + 1;
  vram_start = vram_start + 1;
  ldv_43737: ;
  if ((unsigned long )gtt_start < (unsigned long )gtt_end) {
    goto ldv_43736;
  } else {
  }
  amdgpu_bo_kunmap(*(gtt_obj + (unsigned long )i));
  printk("\016[drm] Tested GTT->VRAM and VRAM->GTT copy for GTT offset 0x%llx\n",
         gtt_addr - adev->mc.gtt_start);
  goto ldv_43739;
  out_lclean_unpin:
  amdgpu_bo_unpin(*(gtt_obj + (unsigned long )i));
  out_lclean_unres:
  amdgpu_bo_unreserve(*(gtt_obj + (unsigned long )i));
  out_lclean_unref:
  amdgpu_bo_unref(gtt_obj + (unsigned long )i);
  out_lclean:
  i = i - 1;
  goto ldv_43741;
  ldv_43740:
  amdgpu_bo_unpin(*(gtt_obj + (unsigned long )i));
  amdgpu_bo_unreserve(*(gtt_obj + (unsigned long )i));
  amdgpu_bo_unref(gtt_obj + (unsigned long )i);
  i = i - 1;
  ldv_43741: ;
  if (i >= 0) {
    goto ldv_43740;
  } else {
  }
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fence);
  } else {
  }
  goto ldv_43743;
  ldv_43739:
  i = i + 1;
  ldv_43745: ;
  if ((unsigned int )i < n) {
    goto ldv_43744;
  } else {
  }
  ldv_43743:
  amdgpu_bo_unpin(vram_obj);
  out_unres:
  amdgpu_bo_unreserve(vram_obj);
  out_unref:
  amdgpu_bo_unref(& vram_obj);
  out_cleanup:
  kfree((void const *)gtt_obj);
  if (r != 0) {
    printk("\fError while testing BO move.\n");
  } else {
  }
  return;
}
}
void amdgpu_test_moves(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mman.buffer_funcs != (unsigned long )((struct amdgpu_buffer_funcs const *)0)) {
    amdgpu_do_test_moves(adev);
  } else {
  }
  return;
}
}
static int amdgpu_test_create_and_emit_fence(struct amdgpu_device *adev , struct amdgpu_ring *ring ,
                                             struct amdgpu_fence **fence )
{
  u32 handle ;
  int r ;
  {
  handle = ring->idx ^ 3736059631U;
  if ((unsigned long )(& adev->uvd.ring) == (unsigned long )ring) {
    r = amdgpu_uvd_get_create_msg(ring, handle, (struct amdgpu_fence **)0);
    if (r != 0) {
      drm_err("Failed to get dummy create msg\n");
      return (r);
    } else {
    }
    r = amdgpu_uvd_get_destroy_msg(ring, handle, fence);
    if (r != 0) {
      drm_err("Failed to get dummy destroy msg\n");
      return (r);
    } else {
    }
  } else
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring || (unsigned long )((struct amdgpu_ring *)(& adev->vce.ring) + 1UL) == (unsigned long )ring) {
    r = amdgpu_vce_get_create_msg(ring, handle, (struct amdgpu_fence **)0);
    if (r != 0) {
      drm_err("Failed to get dummy create msg\n");
      return (r);
    } else {
    }
    r = amdgpu_vce_get_destroy_msg(ring, handle, fence);
    if (r != 0) {
      drm_err("Failed to get dummy destroy msg\n");
      return (r);
    } else {
    }
  } else {
    r = amdgpu_ring_lock(ring, 64U);
    if (r != 0) {
      drm_err("Failed to lock ring A %d\n", ring->idx);
      return (r);
    } else {
    }
    amdgpu_fence_emit(ring, (void *)0, fence);
    amdgpu_ring_unlock_commit(ring);
  }
  return (0);
}
}
void amdgpu_test_ring_sync(struct amdgpu_device *adev , struct amdgpu_ring *ringA ,
                           struct amdgpu_ring *ringB )
{
  struct amdgpu_fence *fence1 ;
  struct amdgpu_fence *fence2 ;
  struct amdgpu_semaphore *semaphore ;
  int r ;
  unsigned long __ms ;
  unsigned long tmp ;
  bool tmp___0 ;
  unsigned long __ms___0 ;
  unsigned long tmp___1 ;
  bool tmp___2 ;
  {
  fence1 = (struct amdgpu_fence *)0;
  fence2 = (struct amdgpu_fence *)0;
  semaphore = (struct amdgpu_semaphore *)0;
  r = amdgpu_semaphore_create(adev, & semaphore);
  if (r != 0) {
    drm_err("Failed to create semaphore\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringA, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring A %d\n", ringA->idx);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_wait(ringA, semaphore);
  amdgpu_ring_unlock_commit(ringA);
  r = amdgpu_test_create_and_emit_fence(adev, ringA, & fence1);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringA, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring A %d\n", ringA->idx);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_wait(ringA, semaphore);
  amdgpu_ring_unlock_commit(ringA);
  r = amdgpu_test_create_and_emit_fence(adev, ringA, & fence2);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  __ms = 1000UL;
  goto ldv_43768;
  ldv_43767:
  __const_udelay(4295000UL);
  ldv_43768:
  tmp = __ms;
  __ms = __ms - 1UL;
  if (tmp != 0UL) {
    goto ldv_43767;
  } else {
  }
  tmp___0 = amdgpu_fence_signaled(fence1);
  if ((int )tmp___0) {
    drm_err("Fence 1 signaled without waiting for semaphore.\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringB, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring B %p\n", ringB);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_signal(ringB, semaphore);
  amdgpu_ring_unlock_commit(ringB);
  r = amdgpu_fence_wait(fence1, 0);
  if (r != 0) {
    drm_err("Failed to wait for sync fence 1\n");
    goto out_cleanup;
  } else {
  }
  __ms___0 = 1000UL;
  goto ldv_43772;
  ldv_43771:
  __const_udelay(4295000UL);
  ldv_43772:
  tmp___1 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___1 != 0UL) {
    goto ldv_43771;
  } else {
  }
  tmp___2 = amdgpu_fence_signaled(fence2);
  if ((int )tmp___2) {
    drm_err("Fence 2 signaled without waiting for semaphore.\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringB, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring B %p\n", ringB);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_signal(ringB, semaphore);
  amdgpu_ring_unlock_commit(ringB);
  r = amdgpu_fence_wait(fence2, 0);
  if (r != 0) {
    drm_err("Failed to wait for sync fence 1\n");
    goto out_cleanup;
  } else {
  }
  out_cleanup:
  amdgpu_semaphore_free(adev, & semaphore, (struct amdgpu_fence *)0);
  if ((unsigned long )fence1 != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fence1);
  } else {
  }
  if ((unsigned long )fence2 != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fence2);
  } else {
  }
  if (r != 0) {
    printk("\fError while testing ring sync (%d).\n", r);
  } else {
  }
  return;
}
}
static void amdgpu_test_ring_sync2(struct amdgpu_device *adev , struct amdgpu_ring *ringA ,
                                   struct amdgpu_ring *ringB , struct amdgpu_ring *ringC )
{
  struct amdgpu_fence *fenceA ;
  struct amdgpu_fence *fenceB ;
  struct amdgpu_semaphore *semaphore ;
  bool sigA ;
  bool sigB ;
  int i ;
  int r ;
  unsigned long __ms ;
  unsigned long tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  unsigned long __ms___0 ;
  unsigned long tmp___2 ;
  unsigned long __ms___1 ;
  unsigned long tmp___3 ;
  {
  fenceA = (struct amdgpu_fence *)0;
  fenceB = (struct amdgpu_fence *)0;
  semaphore = (struct amdgpu_semaphore *)0;
  r = amdgpu_semaphore_create(adev, & semaphore);
  if (r != 0) {
    drm_err("Failed to create semaphore\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringA, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring A %d\n", ringA->idx);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_wait(ringA, semaphore);
  amdgpu_ring_unlock_commit(ringA);
  r = amdgpu_test_create_and_emit_fence(adev, ringA, & fenceA);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringB, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring B %d\n", ringB->idx);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_wait(ringB, semaphore);
  amdgpu_ring_unlock_commit(ringB);
  r = amdgpu_test_create_and_emit_fence(adev, ringB, & fenceB);
  if (r != 0) {
    goto out_cleanup;
  } else {
  }
  __ms = 1000UL;
  goto ldv_43790;
  ldv_43789:
  __const_udelay(4295000UL);
  ldv_43790:
  tmp = __ms;
  __ms = __ms - 1UL;
  if (tmp != 0UL) {
    goto ldv_43789;
  } else {
  }
  tmp___0 = amdgpu_fence_signaled(fenceA);
  if ((int )tmp___0) {
    drm_err("Fence A signaled without waiting for semaphore.\n");
    goto out_cleanup;
  } else {
  }
  tmp___1 = amdgpu_fence_signaled(fenceB);
  if ((int )tmp___1) {
    drm_err("Fence B signaled without waiting for semaphore.\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_ring_lock(ringC, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring B %p\n", ringC);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_signal(ringC, semaphore);
  amdgpu_ring_unlock_commit(ringC);
  i = 0;
  goto ldv_43798;
  ldv_43797:
  __ms___0 = 100UL;
  goto ldv_43794;
  ldv_43793:
  __const_udelay(4295000UL);
  ldv_43794:
  tmp___2 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___2 != 0UL) {
    goto ldv_43793;
  } else {
  }
  sigA = amdgpu_fence_signaled(fenceA);
  sigB = amdgpu_fence_signaled(fenceB);
  if ((int )sigA || (int )sigB) {
    goto ldv_43796;
  } else {
  }
  i = i + 1;
  ldv_43798: ;
  if (i <= 29) {
    goto ldv_43797;
  } else {
  }
  ldv_43796: ;
  if (! sigA && ! sigB) {
    drm_err("Neither fence A nor B has been signaled\n");
    goto out_cleanup;
  } else
  if ((int )sigA && (int )sigB) {
    drm_err("Both fence A and B has been signaled\n");
    goto out_cleanup;
  } else {
  }
  printk("\016[drm] Fence %c was first signaled\n", (int )sigA ? 65 : 66);
  r = amdgpu_ring_lock(ringC, 64U);
  if (r != 0) {
    drm_err("Failed to lock ring B %p\n", ringC);
    goto out_cleanup;
  } else {
  }
  amdgpu_semaphore_emit_signal(ringC, semaphore);
  amdgpu_ring_unlock_commit(ringC);
  __ms___1 = 1000UL;
  goto ldv_43801;
  ldv_43800:
  __const_udelay(4295000UL);
  ldv_43801:
  tmp___3 = __ms___1;
  __ms___1 = __ms___1 - 1UL;
  if (tmp___3 != 0UL) {
    goto ldv_43800;
  } else {
  }
  r = amdgpu_fence_wait(fenceA, 0);
  if (r != 0) {
    drm_err("Failed to wait for sync fence A\n");
    goto out_cleanup;
  } else {
  }
  r = amdgpu_fence_wait(fenceB, 0);
  if (r != 0) {
    drm_err("Failed to wait for sync fence B\n");
    goto out_cleanup;
  } else {
  }
  out_cleanup:
  amdgpu_semaphore_free(adev, & semaphore, (struct amdgpu_fence *)0);
  if ((unsigned long )fenceA != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fenceA);
  } else {
  }
  if ((unsigned long )fenceB != (unsigned long )((struct amdgpu_fence *)0)) {
    amdgpu_fence_unref(& fenceB);
  } else {
  }
  if (r != 0) {
    printk("\fError while testing ring sync (%d).\n", r);
  } else {
  }
  return;
}
}
static bool amdgpu_test_sync_possible(struct amdgpu_ring *ringA , struct amdgpu_ring *ringB )
{
  {
  if ((unsigned long )((struct amdgpu_ring *)(& (ringA->adev)->vce.ring)) == (unsigned long )ringA && (unsigned long )((struct amdgpu_ring *)(& (ringB->adev)->vce.ring) + 1UL) == (unsigned long )ringB) {
    return (0);
  } else {
  }
  return (1);
}
}
void amdgpu_test_syncing(struct amdgpu_device *adev )
{
  int i ;
  int j ;
  int k ;
  struct amdgpu_ring *ringA ;
  struct amdgpu_ring *ringB ;
  bool tmp ;
  int tmp___0 ;
  struct amdgpu_ring *ringC ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  {
  i = 1;
  goto ldv_43826;
  ldv_43825:
  ringA = adev->rings[i];
  if ((unsigned long )ringA == (unsigned long )((struct amdgpu_ring *)0) || ! ringA->ready) {
    goto ldv_43814;
  } else {
  }
  j = 0;
  goto ldv_43823;
  ldv_43822:
  ringB = adev->rings[j];
  if ((unsigned long )ringB == (unsigned long )((struct amdgpu_ring *)0) || ! ringB->ready) {
    goto ldv_43816;
  } else {
  }
  tmp = amdgpu_test_sync_possible(ringA, ringB);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    goto ldv_43816;
  } else {
  }
  printk("\016[drm] Testing syncing between rings %d and %d...\n", i, j);
  amdgpu_test_ring_sync(adev, ringA, ringB);
  printk("\016[drm] Testing syncing between rings %d and %d...\n", j, i);
  amdgpu_test_ring_sync(adev, ringB, ringA);
  k = 0;
  goto ldv_43820;
  ldv_43819:
  ringC = adev->rings[k];
  if ((unsigned long )ringC == (unsigned long )((struct amdgpu_ring *)0) || ! ringC->ready) {
    goto ldv_43818;
  } else {
  }
  tmp___1 = amdgpu_test_sync_possible(ringA, ringC);
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2) {
    goto ldv_43818;
  } else {
  }
  tmp___3 = amdgpu_test_sync_possible(ringB, ringC);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
    goto ldv_43818;
  } else {
  }
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", i, j, k);
  amdgpu_test_ring_sync2(adev, ringA, ringB, ringC);
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", i, k, j);
  amdgpu_test_ring_sync2(adev, ringA, ringC, ringB);
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", j, i, k);
  amdgpu_test_ring_sync2(adev, ringB, ringA, ringC);
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", j, k, i);
  amdgpu_test_ring_sync2(adev, ringB, ringC, ringA);
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", k, i, j);
  amdgpu_test_ring_sync2(adev, ringC, ringA, ringB);
  printk("\016[drm] Testing syncing between rings %d, %d and %d...\n", k, j, i);
  amdgpu_test_ring_sync2(adev, ringC, ringB, ringA);
  ldv_43818:
  k = k + 1;
  ldv_43820: ;
  if (k < j) {
    goto ldv_43819;
  } else {
  }
  ldv_43816:
  j = j + 1;
  ldv_43823: ;
  if (j < i) {
    goto ldv_43822;
  } else {
  }
  ldv_43814:
  i = i + 1;
  ldv_43826: ;
  if (i <= 15) {
    goto ldv_43825;
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_285(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_286(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_287(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_288(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_289(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
extern int kstrtouint(char const * , unsigned int , unsigned int * ) ;
extern int kstrtoint(char const * , unsigned int , int * ) ;
__inline static int kstrtou32(char const *s , unsigned int base , u32 *res )
{
  int tmp ;
  {
  tmp = kstrtouint(s, base, res);
  return (tmp);
}
}
__inline static long PTR_ERR(void const *ptr ) ;
__inline static bool IS_ERR(void const *ptr ) ;
bool ldv_queue_work_on_299(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_301(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_300(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_303(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_302(struct workqueue_struct *ldv_func_arg1 ) ;
extern int device_create_file(struct device * , struct device_attribute const * ) ;
extern void device_remove_file(struct device * , struct device_attribute const * ) ;
int amdgpu_pm_sysfs_init(struct amdgpu_device *adev ) ;
void amdgpu_pm_sysfs_fini(struct amdgpu_device *adev ) ;
void amdgpu_pm_print_power_states(struct amdgpu_device *adev ) ;
void amdgpu_pm_compute_clocks(struct amdgpu_device *adev ) ;
void amdgpu_dpm_thermal_work_handler(struct work_struct *work ) ;
void amdgpu_dpm_enable_uvd(struct amdgpu_device *adev , bool enable ) ;
void amdgpu_dpm_enable_vce(struct amdgpu_device *adev , bool enable ) ;
extern int power_supply_is_system_supplied(void) ;
extern struct device *hwmon_device_register_with_groups(struct device * , char const * ,
                                                        void * , struct attribute_group const ** ) ;
extern void hwmon_device_unregister(struct device * ) ;
static int amdgpu_debugfs_pm_init(struct amdgpu_device *adev ) ;
void amdgpu_pm_acpi_event_handler(struct amdgpu_device *adev )
{
  int tmp ;
  {
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    tmp = power_supply_is_system_supplied();
    if (tmp > 0) {
      adev->pm.dpm.ac_power = 1;
    } else {
      adev->pm.dpm.ac_power = 0;
    }
    if ((unsigned long )(adev->pm.funcs)->enable_bapm != (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                bool ))0)) {
      (*((adev->pm.funcs)->enable_bapm))(adev, (int )adev->pm.dpm.ac_power);
    } else {
    }
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return;
}
}
static ssize_t amdgpu_get_dpm_state(struct device *dev , struct device_attribute *attr ,
                                    char *buf )
{
  struct drm_device *ddev ;
  void *tmp ;
  struct amdgpu_device *adev ;
  enum amdgpu_pm_state_type pm ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  ddev = (struct drm_device *)tmp;
  adev = (struct amdgpu_device *)ddev->dev_private;
  pm = adev->pm.dpm.user_state;
  tmp___0 = snprintf(buf, 4096UL, "%s\n", (unsigned int )pm != 2U ? ((unsigned int )pm == 3U ? (char *)"balanced" : (char *)"performance") : (char *)"battery");
  return ((ssize_t )tmp___0);
}
}
static ssize_t amdgpu_set_dpm_state(struct device *dev , struct device_attribute *attr ,
                                    char const *buf , size_t count )
{
  struct drm_device *ddev ;
  void *tmp ;
  struct amdgpu_device *adev ;
  size_t tmp___0 ;
  int tmp___1 ;
  size_t tmp___2 ;
  int tmp___3 ;
  size_t tmp___4 ;
  int tmp___5 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  ddev = (struct drm_device *)tmp;
  adev = (struct amdgpu_device *)ddev->dev_private;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  tmp___4 = strlen("battery");
  tmp___5 = strncmp("battery", buf, tmp___4);
  if (tmp___5 == 0) {
    adev->pm.dpm.user_state = 2;
  } else {
    tmp___2 = strlen("balanced");
    tmp___3 = strncmp("balanced", buf, tmp___2);
    if (tmp___3 == 0) {
      adev->pm.dpm.user_state = 3;
    } else {
      tmp___0 = strlen("performance");
      tmp___1 = strncmp("performance", buf, tmp___0);
      if (tmp___1 == 0) {
        adev->pm.dpm.user_state = 4;
      } else {
        mutex_unlock(& adev->pm.mutex);
        count = 0xffffffffffffffeaUL;
        goto fail;
      }
    }
  }
  mutex_unlock(& adev->pm.mutex);
  if ((adev->flags & 262144UL) == 0UL || ddev->switch_power_state == 0) {
    amdgpu_pm_compute_clocks(adev);
  } else {
  }
  fail: ;
  return ((ssize_t )count);
}
}
static ssize_t amdgpu_get_dpm_forced_performance_level(struct device *dev , struct device_attribute *attr ,
                                                       char *buf )
{
  struct drm_device *ddev ;
  void *tmp ;
  struct amdgpu_device *adev ;
  enum amdgpu_dpm_forced_level level ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  ddev = (struct drm_device *)tmp;
  adev = (struct amdgpu_device *)ddev->dev_private;
  level = adev->pm.dpm.forced_level;
  tmp___0 = snprintf(buf, 4096UL, "%s\n", (unsigned int )level != 0U ? ((unsigned int )level == 1U ? (char *)"low" : (char *)"high") : (char *)"auto");
  return ((ssize_t )tmp___0);
}
}
static ssize_t amdgpu_set_dpm_forced_performance_level(struct device *dev , struct device_attribute *attr ,
                                                       char const *buf , size_t count )
{
  struct drm_device *ddev ;
  void *tmp ;
  struct amdgpu_device *adev ;
  enum amdgpu_dpm_forced_level level ;
  int ret ;
  size_t tmp___0 ;
  int tmp___1 ;
  size_t tmp___2 ;
  int tmp___3 ;
  size_t tmp___4 ;
  int tmp___5 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  ddev = (struct drm_device *)tmp;
  adev = (struct amdgpu_device *)ddev->dev_private;
  ret = 0;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  tmp___4 = strlen("low");
  tmp___5 = strncmp("low", buf, tmp___4);
  if (tmp___5 == 0) {
    level = 1;
  } else {
    tmp___2 = strlen("high");
    tmp___3 = strncmp("high", buf, tmp___2);
    if (tmp___3 == 0) {
      level = 2;
    } else {
      tmp___0 = strlen("auto");
      tmp___1 = strncmp("auto", buf, tmp___0);
      if (tmp___1 == 0) {
        level = 0;
      } else {
        count = 0xffffffffffffffeaUL;
        goto fail;
      }
    }
  }
  if ((unsigned long )(adev->pm.funcs)->force_performance_level != (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                         enum amdgpu_dpm_forced_level ))0)) {
    if ((int )adev->pm.dpm.thermal_active) {
      count = 0xffffffffffffffeaUL;
      goto fail;
    } else {
    }
    ret = (*((adev->pm.funcs)->force_performance_level))(adev, level);
    if (ret != 0) {
      count = 0xffffffffffffffeaUL;
    } else {
    }
  } else {
  }
  fail:
  mutex_unlock(& adev->pm.mutex);
  return ((ssize_t )count);
}
}
static struct device_attribute dev_attr_power_dpm_state = {{"power_dpm_state", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                             {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
    & amdgpu_get_dpm_state, & amdgpu_set_dpm_state};
static struct device_attribute dev_attr_power_dpm_force_performance_level = {{"power_dpm_force_performance_level", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0},
                                                               {(char)0}, {(char)0},
                                                               {(char)0}, {(char)0},
                                                               {(char)0}, {(char)0}}}},
    & amdgpu_get_dpm_forced_performance_level, & amdgpu_set_dpm_forced_performance_level};
static ssize_t amdgpu_hwmon_show_temp(struct device *dev , struct device_attribute *attr ,
                                      char *buf )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  int temp ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  if ((unsigned long )(adev->pm.funcs)->get_temperature != (unsigned long )((int (* )(struct amdgpu_device * ))0)) {
    temp = (*((adev->pm.funcs)->get_temperature))(adev);
  } else {
    temp = 0;
  }
  tmp___0 = snprintf(buf, 4096UL, "%d\n", temp);
  return ((ssize_t )tmp___0);
}
}
static ssize_t amdgpu_hwmon_show_temp_thresh(struct device *dev , struct device_attribute *attr ,
                                             char *buf )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  int hyst ;
  struct device_attribute const *__mptr ;
  int temp ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  __mptr = (struct device_attribute const *)attr;
  hyst = ((struct sensor_device_attribute *)__mptr)->index;
  if (hyst != 0) {
    temp = adev->pm.dpm.thermal.min_temp;
  } else {
    temp = adev->pm.dpm.thermal.max_temp;
  }
  tmp___0 = snprintf(buf, 4096UL, "%d\n", temp);
  return ((ssize_t )tmp___0);
}
}
static ssize_t amdgpu_hwmon_get_pwm1_enable(struct device *dev , struct device_attribute *attr ,
                                            char *buf )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  u32 pwm_mode ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  pwm_mode = 0U;
  if ((unsigned long )(adev->pm.funcs)->get_fan_control_mode != (unsigned long )((u32 (* )(struct amdgpu_device * ))0)) {
    pwm_mode = (*((adev->pm.funcs)->get_fan_control_mode))(adev);
  } else {
  }
  tmp___0 = sprintf(buf, "%i\n", pwm_mode == 1U ? 1 : 2);
  return ((ssize_t )tmp___0);
}
}
static ssize_t amdgpu_hwmon_set_pwm1_enable(struct device *dev , struct device_attribute *attr ,
                                            char const *buf , size_t count )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  int err ;
  int value ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  if ((unsigned long )(adev->pm.funcs)->set_fan_control_mode == (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                       u32 ))0)) {
    return (-22L);
  } else {
  }
  err = kstrtoint(buf, 10U, & value);
  if (err != 0) {
    return ((ssize_t )err);
  } else {
  }
  switch (value) {
  case 1:
  (*((adev->pm.funcs)->set_fan_control_mode))(adev, 1U);
  goto ldv_48434;
  default:
  (*((adev->pm.funcs)->set_fan_control_mode))(adev, 0U);
  goto ldv_48434;
  }
  ldv_48434: ;
  return ((ssize_t )count);
}
}
static ssize_t amdgpu_hwmon_get_pwm1_min(struct device *dev , struct device_attribute *attr ,
                                         char *buf )
{
  int tmp ;
  {
  tmp = sprintf(buf, "%i\n", 0);
  return ((ssize_t )tmp);
}
}
static ssize_t amdgpu_hwmon_get_pwm1_max(struct device *dev , struct device_attribute *attr ,
                                         char *buf )
{
  int tmp ;
  {
  tmp = sprintf(buf, "%i\n", 255);
  return ((ssize_t )tmp);
}
}
static ssize_t amdgpu_hwmon_set_pwm1(struct device *dev , struct device_attribute *attr ,
                                     char const *buf , size_t count )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  int err ;
  u32 value ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  err = kstrtou32(buf, 10U, & value);
  if (err != 0) {
    return ((ssize_t )err);
  } else {
  }
  value = (value * 100U) / 255U;
  err = (*((adev->pm.funcs)->set_fan_speed_percent))(adev, value);
  if (err != 0) {
    return ((ssize_t )err);
  } else {
  }
  return ((ssize_t )count);
}
}
static ssize_t amdgpu_hwmon_get_pwm1(struct device *dev , struct device_attribute *attr ,
                                     char *buf )
{
  struct amdgpu_device *adev ;
  void *tmp ;
  int err ;
  u32 speed ;
  int tmp___0 ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  err = (*((adev->pm.funcs)->get_fan_speed_percent))(adev, & speed);
  if (err != 0) {
    return ((ssize_t )err);
  } else {
  }
  speed = (speed * 255U) / 100U;
  tmp___0 = sprintf(buf, "%i\n", speed);
  return ((ssize_t )tmp___0);
}
}
static struct sensor_device_attribute sensor_dev_attr_temp1_input = {{{"temp1_input", 292U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                          {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_show_temp, (ssize_t (*)(struct device * , struct device_attribute * ,
                                            char const * , size_t ))0}, 0};
static struct sensor_device_attribute sensor_dev_attr_temp1_crit = {{{"temp1_crit", 292U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                         {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_show_temp_thresh, (ssize_t (*)(struct device * , struct device_attribute * ,
                                                   char const * , size_t ))0},
    0};
static struct sensor_device_attribute sensor_dev_attr_temp1_crit_hyst = {{{"temp1_crit_hyst", 292U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                              {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_show_temp_thresh, (ssize_t (*)(struct device * , struct device_attribute * ,
                                                   char const * , size_t ))0},
    1};
static struct sensor_device_attribute sensor_dev_attr_pwm1 = {{{"pwm1", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0}, {(char)0},
                                   {(char)0}, {(char)0}, {(char)0}}}}, & amdgpu_hwmon_get_pwm1,
     & amdgpu_hwmon_set_pwm1}, 0};
static struct sensor_device_attribute sensor_dev_attr_pwm1_enable = {{{"pwm1_enable", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                          {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_get_pwm1_enable, & amdgpu_hwmon_set_pwm1_enable}, 0};
static struct sensor_device_attribute sensor_dev_attr_pwm1_min = {{{"pwm1_min", 292U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                       {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_get_pwm1_min, (ssize_t (*)(struct device * , struct device_attribute * ,
                                               char const * , size_t ))0}, 0};
static struct sensor_device_attribute sensor_dev_attr_pwm1_max = {{{"pwm1_max", 292U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                       {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
     & amdgpu_hwmon_get_pwm1_max, (ssize_t (*)(struct device * , struct device_attribute * ,
                                               char const * , size_t ))0}, 0};
static struct attribute *hwmon_attributes[8U] =
  { & sensor_dev_attr_temp1_input.dev_attr.attr, & sensor_dev_attr_temp1_crit.dev_attr.attr, & sensor_dev_attr_temp1_crit_hyst.dev_attr.attr, & sensor_dev_attr_pwm1.dev_attr.attr,
        & sensor_dev_attr_pwm1_enable.dev_attr.attr, & sensor_dev_attr_pwm1_min.dev_attr.attr, & sensor_dev_attr_pwm1_max.dev_attr.attr, (struct attribute *)0};
static umode_t hwmon_attributes_visible(struct kobject *kobj , struct attribute *attr ,
                                        int index )
{
  struct device *dev ;
  struct kobject const *__mptr ;
  struct amdgpu_device *adev ;
  void *tmp ;
  umode_t effective_mode ;
  {
  __mptr = (struct kobject const *)kobj;
  dev = (struct device *)__mptr + 0xfffffffffffffff0UL;
  tmp = dev_get_drvdata((struct device const *)dev);
  adev = (struct amdgpu_device *)tmp;
  effective_mode = attr->mode;
  if (! adev->pm.dpm_enabled && ((unsigned long )attr == (unsigned long )(& sensor_dev_attr_temp1_crit.dev_attr.attr) || (unsigned long )attr == (unsigned long )(& sensor_dev_attr_temp1_crit_hyst.dev_attr.attr))) {
    return (0U);
  } else {
  }
  if ((int )adev->pm.no_fan && ((((unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1.dev_attr.attr) || (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_enable.dev_attr.attr)) || (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_max.dev_attr.attr)) || (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_min.dev_attr.attr))) {
    return (0U);
  } else {
  }
  if (((unsigned long )(adev->pm.funcs)->get_fan_speed_percent == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                        u32 * ))0) && (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1.dev_attr.attr)) || ((unsigned long )(adev->pm.funcs)->get_fan_control_mode == (unsigned long )((u32 (* )(struct amdgpu_device * ))0) && (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_enable.dev_attr.attr))) {
    effective_mode = (unsigned int )effective_mode & 65243U;
  } else {
  }
  if (((unsigned long )(adev->pm.funcs)->set_fan_speed_percent == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                        u32 ))0) && (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1.dev_attr.attr)) || ((unsigned long )(adev->pm.funcs)->set_fan_control_mode == (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                                                                                                                                                                                                                          u32 ))0) && (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_enable.dev_attr.attr))) {
    effective_mode = (unsigned int )effective_mode & 65407U;
  } else {
  }
  if (((unsigned long )(adev->pm.funcs)->set_fan_speed_percent == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                        u32 ))0) && (unsigned long )(adev->pm.funcs)->get_fan_speed_percent == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                                                                                                                                      u32 * ))0)) && ((unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_max.dev_attr.attr) || (unsigned long )attr == (unsigned long )(& sensor_dev_attr_pwm1_min.dev_attr.attr))) {
    return (0U);
  } else {
  }
  return (effective_mode);
}
}
static struct attribute_group const hwmon_attrgroup = {0, & hwmon_attributes_visible, (struct attribute **)(& hwmon_attributes), 0};
static struct attribute_group const *hwmon_groups[2U] = { & hwmon_attrgroup, (struct attribute_group const *)0};
void amdgpu_dpm_thermal_work_handler(struct work_struct *work )
{
  struct amdgpu_device *adev ;
  struct work_struct const *__mptr ;
  enum amdgpu_pm_state_type dpm_state ;
  int temp ;
  int tmp ;
  {
  __mptr = (struct work_struct const *)work;
  adev = (struct amdgpu_device *)__mptr + 0xffffffffffffd658UL;
  dpm_state = 11;
  if (! adev->pm.dpm_enabled) {
    return;
  } else {
  }
  if ((unsigned long )(adev->pm.funcs)->get_temperature != (unsigned long )((int (* )(struct amdgpu_device * ))0)) {
    tmp = (*((adev->pm.funcs)->get_temperature))(adev);
    temp = tmp;
    if (adev->pm.dpm.thermal.min_temp > temp) {
      dpm_state = adev->pm.dpm.user_state;
    } else {
    }
  } else
  if ((int )adev->pm.dpm.thermal.high_to_low) {
    dpm_state = adev->pm.dpm.user_state;
  } else {
  }
  mutex_lock_nested(& adev->pm.mutex, 0U);
  if ((unsigned int )dpm_state == 11U) {
    adev->pm.dpm.thermal_active = 1;
  } else {
    adev->pm.dpm.thermal_active = 0;
  }
  adev->pm.dpm.state = dpm_state;
  mutex_unlock(& adev->pm.mutex);
  amdgpu_pm_compute_clocks(adev);
  return;
}
}
static struct amdgpu_ps *amdgpu_dpm_pick_power_state(struct amdgpu_device *adev ,
                                                     enum amdgpu_pm_state_type dpm_state )
{
  int i ;
  struct amdgpu_ps *ps ;
  u32 ui_class ;
  bool single_display ;
  bool tmp ;
  {
  single_display = adev->pm.dpm.new_active_crtc_count <= 1;
  if ((int )single_display && (unsigned long )(adev->pm.funcs)->vblank_too_short != (unsigned long )((bool (* )(struct amdgpu_device * ))0)) {
    tmp = (*((adev->pm.funcs)->vblank_too_short))(adev);
    if ((int )tmp) {
      single_display = 0;
    } else {
    }
  } else {
  }
  if ((unsigned int )dpm_state == 4U) {
    dpm_state = 14;
  } else {
  }
  if ((unsigned int )dpm_state == 3U) {
    dpm_state = 4;
  } else {
  }
  restart_search:
  i = 0;
  goto ldv_48600;
  ldv_48599:
  ps = adev->pm.dpm.ps + (unsigned long )i;
  ui_class = ps->class & 7U;
  switch ((unsigned int )dpm_state) {
  case 2U: ;
  if (ui_class == 1U) {
    if ((int )ps->caps & 1) {
      if ((int )single_display) {
        return (ps);
      } else {
      }
    } else {
      return (ps);
    }
  } else {
  }
  goto ldv_48585;
  case 3U: ;
  if (ui_class == 3U) {
    if ((int )ps->caps & 1) {
      if ((int )single_display) {
        return (ps);
      } else {
      }
    } else {
      return (ps);
    }
  } else {
  }
  goto ldv_48585;
  case 4U: ;
  if (ui_class == 5U) {
    if ((int )ps->caps & 1) {
      if ((int )single_display) {
        return (ps);
      } else {
      }
    } else {
      return (ps);
    }
  } else {
  }
  goto ldv_48585;
  case 5U: ;
  if ((unsigned long )adev->pm.dpm.uvd_ps != (unsigned long )((struct amdgpu_ps *)0)) {
    return (adev->pm.dpm.uvd_ps);
  } else {
    goto ldv_48585;
  }
  case 6U: ;
  if ((ps->class & 32768U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 7U: ;
  if ((ps->class & 16384U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 8U: ;
  if ((ps->class & 8192U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 9U: ;
  if ((ps->class2 & 4U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 10U: ;
  return (adev->pm.dpm.boot_ps);
  case 11U: ;
  if ((ps->class & 16U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 12U: ;
  if ((ps->class & 4096U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 13U: ;
  if ((ps->class2 & 2U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  case 14U: ;
  if ((ps->class & 256U) != 0U) {
    return (ps);
  } else {
  }
  goto ldv_48585;
  default: ;
  goto ldv_48585;
  }
  ldv_48585:
  i = i + 1;
  ldv_48600: ;
  if (adev->pm.dpm.num_ps > i) {
    goto ldv_48599;
  } else {
  }
  switch ((unsigned int )dpm_state) {
  case 6U:
  dpm_state = 7;
  goto restart_search;
  case 7U: ;
  case 8U: ;
  case 9U: ;
  if ((unsigned long )adev->pm.dpm.uvd_ps != (unsigned long )((struct amdgpu_ps *)0)) {
    return (adev->pm.dpm.uvd_ps);
  } else {
    dpm_state = 4;
    goto restart_search;
  }
  case 11U:
  dpm_state = 12;
  goto restart_search;
  case 12U:
  dpm_state = 2;
  goto restart_search;
  case 2U: ;
  case 3U: ;
  case 14U:
  dpm_state = 4;
  goto restart_search;
  default: ;
  goto ldv_48612;
  }
  ldv_48612: ;
  return ((struct amdgpu_ps *)0);
}
}
static void amdgpu_dpm_change_power_state_locked(struct amdgpu_device *adev )
{
  int i ;
  struct amdgpu_ps *ps ;
  enum amdgpu_pm_state_type dpm_state ;
  int ret ;
  struct amdgpu_ring *ring ;
  enum amdgpu_dpm_forced_level level ;
  {
  if (! adev->pm.dpm_enabled) {
    return;
  } else {
  }
  if ((unsigned int )adev->pm.dpm.user_state != (unsigned int )adev->pm.dpm.state) {
    if (! adev->pm.dpm.thermal_active && ! adev->pm.dpm.uvd_active) {
      adev->pm.dpm.state = adev->pm.dpm.user_state;
    } else {
    }
  } else {
  }
  dpm_state = adev->pm.dpm.state;
  ps = amdgpu_dpm_pick_power_state(adev, dpm_state);
  if ((unsigned long )ps != (unsigned long )((struct amdgpu_ps *)0)) {
    adev->pm.dpm.requested_ps = ps;
  } else {
    return;
  }
  if ((unsigned long )adev->pm.dpm.current_ps == (unsigned long )adev->pm.dpm.requested_ps) {
    if ((int )ps->vce_active != (int )adev->pm.dpm.vce_active) {
      goto force;
    } else {
    }
    if ((adev->flags & 131072UL) != 0UL) {
      if (adev->pm.dpm.new_active_crtcs != adev->pm.dpm.current_active_crtcs) {
        (*((adev->mode_info.funcs)->bandwidth_update))(adev);
        (*((adev->pm.funcs)->display_configuration_changed))(adev);
        adev->pm.dpm.current_active_crtcs = adev->pm.dpm.new_active_crtcs;
        adev->pm.dpm.current_active_crtc_count = adev->pm.dpm.new_active_crtc_count;
      } else {
      }
      return;
    } else
    if (adev->pm.dpm.new_active_crtcs == adev->pm.dpm.current_active_crtcs) {
      return;
    } else
    if (adev->pm.dpm.current_active_crtc_count > 1 && adev->pm.dpm.new_active_crtc_count > 1) {
      (*((adev->mode_info.funcs)->bandwidth_update))(adev);
      (*((adev->pm.funcs)->display_configuration_changed))(adev);
      adev->pm.dpm.current_active_crtcs = adev->pm.dpm.new_active_crtcs;
      adev->pm.dpm.current_active_crtc_count = adev->pm.dpm.new_active_crtc_count;
      return;
    } else {
    }
  } else {
  }
  force: ;
  if (amdgpu_dpm == 1) {
    printk("switching from power state:\n");
    (*((adev->pm.funcs)->print_power_state))(adev, adev->pm.dpm.current_ps);
    printk("switching to power state:\n");
    (*((adev->pm.funcs)->print_power_state))(adev, adev->pm.dpm.requested_ps);
  } else {
  }
  mutex_lock_nested(& (adev->ddev)->struct_mutex, 0U);
  mutex_lock_nested(& adev->ring_lock, 0U);
  ps->vce_active = adev->pm.dpm.vce_active;
  ret = (*((adev->pm.funcs)->pre_set_power_state))(adev);
  if (ret != 0) {
    goto done;
  } else {
  }
  (*((adev->mode_info.funcs)->bandwidth_update))(adev);
  (*((adev->pm.funcs)->display_configuration_changed))(adev);
  adev->pm.dpm.current_active_crtcs = adev->pm.dpm.new_active_crtcs;
  adev->pm.dpm.current_active_crtc_count = adev->pm.dpm.new_active_crtc_count;
  i = 0;
  goto ldv_48624;
  ldv_48623:
  ring = adev->rings[i];
  if ((unsigned long )ring != (unsigned long )((struct amdgpu_ring *)0) && (int )ring->ready) {
    amdgpu_fence_wait_empty(ring);
  } else {
  }
  i = i + 1;
  ldv_48624: ;
  if (i <= 15) {
    goto ldv_48623;
  } else {
  }
  (*((adev->pm.funcs)->set_power_state))(adev);
  adev->pm.dpm.current_ps = adev->pm.dpm.requested_ps;
  (*((adev->pm.funcs)->post_set_power_state))(adev);
  if ((unsigned long )(adev->pm.funcs)->force_performance_level != (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                         enum amdgpu_dpm_forced_level ))0)) {
    if ((int )adev->pm.dpm.thermal_active) {
      level = adev->pm.dpm.forced_level;
      (*((adev->pm.funcs)->force_performance_level))(adev, 1);
      adev->pm.dpm.forced_level = level;
    } else {
      (*((adev->pm.funcs)->force_performance_level))(adev, adev->pm.dpm.forced_level);
    }
  } else {
  }
  done:
  mutex_unlock(& adev->ring_lock);
  mutex_unlock(& (adev->ddev)->struct_mutex);
  return;
}
}
void amdgpu_dpm_enable_uvd(struct amdgpu_device *adev , bool enable )
{
  {
  if ((unsigned long )(adev->pm.funcs)->powergate_uvd != (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                bool ))0)) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    (*((adev->pm.funcs)->powergate_uvd))(adev, (int )((bool )(! ((int )enable != 0))));
    mutex_unlock(& adev->pm.mutex);
  } else {
    if ((int )enable) {
      mutex_lock_nested(& adev->pm.mutex, 0U);
      adev->pm.dpm.uvd_active = 1;
      adev->pm.dpm.state = 5;
      mutex_unlock(& adev->pm.mutex);
    } else {
      mutex_lock_nested(& adev->pm.mutex, 0U);
      adev->pm.dpm.uvd_active = 0;
      mutex_unlock(& adev->pm.mutex);
    }
    amdgpu_pm_compute_clocks(adev);
  }
  return;
}
}
void amdgpu_dpm_enable_vce(struct amdgpu_device *adev , bool enable )
{
  {
  if ((unsigned long )(adev->pm.funcs)->powergate_vce != (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                bool ))0)) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    (*((adev->pm.funcs)->powergate_vce))(adev, (int )((bool )(! ((int )enable != 0))));
    mutex_unlock(& adev->pm.mutex);
  } else {
    if ((int )enable) {
      mutex_lock_nested(& adev->pm.mutex, 0U);
      adev->pm.dpm.vce_active = 1;
      adev->pm.dpm.vce_level = 0;
      mutex_unlock(& adev->pm.mutex);
    } else {
      mutex_lock_nested(& adev->pm.mutex, 0U);
      adev->pm.dpm.vce_active = 0;
      mutex_unlock(& adev->pm.mutex);
    }
    amdgpu_pm_compute_clocks(adev);
  }
  return;
}
}
void amdgpu_pm_print_power_states(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_48640;
  ldv_48639:
  printk("== power state %d ==\n", i);
  (*((adev->pm.funcs)->print_power_state))(adev, adev->pm.dpm.ps + (unsigned long )i);
  i = i + 1;
  ldv_48640: ;
  if (adev->pm.dpm.num_ps > i) {
    goto ldv_48639;
  } else {
  }
  return;
}
}
int amdgpu_pm_sysfs_init(struct amdgpu_device *adev )
{
  int ret ;
  long tmp ;
  bool tmp___0 ;
  {
  if ((unsigned long )(adev->pm.funcs)->get_temperature == (unsigned long )((int (* )(struct amdgpu_device * ))0)) {
    return (0);
  } else {
  }
  adev->pm.int_hwmon_dev = hwmon_device_register_with_groups(adev->dev, "amdgpu",
                                                             (void *)adev, (struct attribute_group const **)(& hwmon_groups));
  tmp___0 = IS_ERR((void const *)adev->pm.int_hwmon_dev);
  if ((int )tmp___0) {
    tmp = PTR_ERR((void const *)adev->pm.int_hwmon_dev);
    ret = (int )tmp;
    dev_err((struct device const *)adev->dev, "Unable to register hwmon device: %d\n",
            ret);
    return (ret);
  } else {
  }
  ret = device_create_file(adev->dev, (struct device_attribute const *)(& dev_attr_power_dpm_state));
  if (ret != 0) {
    drm_err("failed to create device file for dpm state\n");
    return (ret);
  } else {
  }
  ret = device_create_file(adev->dev, (struct device_attribute const *)(& dev_attr_power_dpm_force_performance_level));
  if (ret != 0) {
    drm_err("failed to create device file for dpm state\n");
    return (ret);
  } else {
  }
  ret = amdgpu_debugfs_pm_init(adev);
  if (ret != 0) {
    drm_err("Failed to register debugfs file for dpm!\n");
    return (ret);
  } else {
  }
  return (0);
}
}
void amdgpu_pm_sysfs_fini(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.int_hwmon_dev != (unsigned long )((struct device *)0)) {
    hwmon_device_unregister(adev->pm.int_hwmon_dev);
  } else {
  }
  device_remove_file(adev->dev, (struct device_attribute const *)(& dev_attr_power_dpm_state));
  device_remove_file(adev->dev, (struct device_attribute const *)(& dev_attr_power_dpm_force_performance_level));
  return;
}
}
void amdgpu_pm_compute_clocks(struct amdgpu_device *adev )
{
  struct drm_device *ddev ;
  struct drm_crtc *crtc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct list_head const *__mptr ;
  struct drm_crtc const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  int tmp ;
  {
  ddev = adev->ddev;
  if (! adev->pm.dpm_enabled) {
    return;
  } else {
  }
  mutex_lock_nested(& adev->pm.mutex, 0U);
  adev->pm.dpm.new_active_crtcs = 0U;
  adev->pm.dpm.new_active_crtc_count = 0;
  if (adev->mode_info.num_crtc != 0 && (int )adev->mode_info.mode_config_initialized) {
    __mptr = (struct list_head const *)ddev->mode_config.crtc_list.next;
    crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
    goto ldv_48662;
    ldv_48661:
    __mptr___0 = (struct drm_crtc const *)crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    if ((int )crtc->enabled) {
      adev->pm.dpm.new_active_crtcs = adev->pm.dpm.new_active_crtcs | (u32 )(1 << amdgpu_crtc->crtc_id);
      adev->pm.dpm.new_active_crtc_count = adev->pm.dpm.new_active_crtc_count + 1;
    } else {
    }
    __mptr___1 = (struct list_head const *)crtc->head.next;
    crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
    ldv_48662: ;
    if ((unsigned long )(& crtc->head) != (unsigned long )(& ddev->mode_config.crtc_list)) {
      goto ldv_48661;
    } else {
    }
  } else {
  }
  tmp = power_supply_is_system_supplied();
  if (tmp > 0) {
    adev->pm.dpm.ac_power = 1;
  } else {
    adev->pm.dpm.ac_power = 0;
  }
  amdgpu_dpm_change_power_state_locked(adev);
  mutex_unlock(& adev->pm.mutex);
  return;
}
}
static int amdgpu_debugfs_pm_info(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  node = (struct drm_info_node *)m->private;
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    if ((unsigned long )(adev->pm.funcs)->debugfs_print_current_performance_level != (unsigned long )((void (* )(struct amdgpu_device * ,
                                                                                                                            struct seq_file * ))0)) {
      (*((adev->pm.funcs)->debugfs_print_current_performance_level))(adev, m);
    } else {
      seq_printf(m, "Debugfs support not implemented for this asic\n");
    }
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (0);
}
}
static struct drm_info_list amdgpu_pm_info_list[1U] = { {"amdgpu_pm_info", & amdgpu_debugfs_pm_info, 0U, (void *)0}};
static int amdgpu_debugfs_pm_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = amdgpu_debugfs_add_files(adev, (struct drm_info_list *)(& amdgpu_pm_info_list),
                                 1U);
  return (tmp);
}
}
void ldv_initialize_device_attribute_153(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(48UL);
  dev_attr_power_dpm_force_performance_level_group0 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  dev_attr_power_dpm_force_performance_level_group1 = (struct device *)tmp___0;
  return;
}
}
void ldv_initialize_sensor_device_attribute_148(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(48UL);
  sensor_dev_attr_pwm1_enable_group0 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  sensor_dev_attr_pwm1_enable_group1 = (struct device *)tmp___0;
  return;
}
}
void ldv_initialize_sensor_device_attribute_149(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(48UL);
  sensor_dev_attr_pwm1_group0 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  sensor_dev_attr_pwm1_group1 = (struct device *)tmp___0;
  return;
}
}
void ldv_initialize_device_attribute_154(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(48UL);
  dev_attr_power_dpm_state_group0 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  dev_attr_power_dpm_state_group1 = (struct device *)tmp___0;
  return;
}
}
void ldv_main_exported_152(void)
{
  char *ldvarg652 ;
  void *tmp ;
  struct device *ldvarg654 ;
  void *tmp___0 ;
  struct device_attribute *ldvarg653 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg652 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  ldvarg654 = (struct device *)tmp___0;
  tmp___1 = ldv_init_zalloc(48UL);
  ldvarg653 = (struct device_attribute *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_152 == 1) {
    amdgpu_hwmon_show_temp(ldvarg654, ldvarg653, ldvarg652);
    ldv_state_variable_152 = 1;
  } else {
  }
  goto ldv_48696;
  default:
  ldv_stop();
  }
  ldv_48696: ;
  return;
}
}
void ldv_main_exported_149(void)
{
  size_t ldvarg732 ;
  char *ldvarg730 ;
  void *tmp ;
  char *ldvarg731 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg730 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg731 = (char *)tmp___0;
  ldv_memset((void *)(& ldvarg732), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_149 == 1) {
    amdgpu_hwmon_set_pwm1(sensor_dev_attr_pwm1_group1, sensor_dev_attr_pwm1_group0,
                          (char const *)ldvarg731, ldvarg732);
    ldv_state_variable_149 = 1;
  } else {
  }
  goto ldv_48705;
  case 1: ;
  if (ldv_state_variable_149 == 1) {
    amdgpu_hwmon_get_pwm1(sensor_dev_attr_pwm1_group1, sensor_dev_attr_pwm1_group0,
                          ldvarg730);
    ldv_state_variable_149 = 1;
  } else {
  }
  goto ldv_48705;
  default:
  ldv_stop();
  }
  ldv_48705: ;
  return;
}
}
void ldv_main_exported_147(void)
{
  struct device_attribute *ldvarg335 ;
  void *tmp ;
  struct device *ldvarg336 ;
  void *tmp___0 ;
  char *ldvarg334 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg335 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1416UL);
  ldvarg336 = (struct device *)tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg334 = (char *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_147 == 1) {
    amdgpu_hwmon_get_pwm1_min(ldvarg336, ldvarg335, ldvarg334);
    ldv_state_variable_147 = 1;
  } else {
  }
  goto ldv_48715;
  default:
  ldv_stop();
  }
  ldv_48715: ;
  return;
}
}
void ldv_main_exported_146(void)
{
  struct device_attribute *ldvarg1064 ;
  void *tmp ;
  char *ldvarg1063 ;
  void *tmp___0 ;
  struct device *ldvarg1065 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg1064 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg1063 = (char *)tmp___0;
  tmp___1 = ldv_init_zalloc(1416UL);
  ldvarg1065 = (struct device *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_146 == 1) {
    amdgpu_hwmon_get_pwm1_max(ldvarg1065, ldvarg1064, ldvarg1063);
    ldv_state_variable_146 = 1;
  } else {
  }
  goto ldv_48724;
  default:
  ldv_stop();
  }
  ldv_48724: ;
  return;
}
}
void ldv_main_exported_153(void)
{
  char *ldvarg189 ;
  void *tmp ;
  char *ldvarg190 ;
  void *tmp___0 ;
  size_t ldvarg191 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg189 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg190 = (char *)tmp___0;
  ldv_memset((void *)(& ldvarg191), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_153 == 1) {
    amdgpu_set_dpm_forced_performance_level(dev_attr_power_dpm_force_performance_level_group1,
                                            dev_attr_power_dpm_force_performance_level_group0,
                                            (char const *)ldvarg190, ldvarg191);
    ldv_state_variable_153 = 1;
  } else {
  }
  goto ldv_48733;
  case 1: ;
  if (ldv_state_variable_153 == 1) {
    amdgpu_get_dpm_forced_performance_level(dev_attr_power_dpm_force_performance_level_group1,
                                            dev_attr_power_dpm_force_performance_level_group0,
                                            ldvarg189);
    ldv_state_variable_153 = 1;
  } else {
  }
  goto ldv_48733;
  default:
  ldv_stop();
  }
  ldv_48733: ;
  return;
}
}
void ldv_main_exported_154(void)
{
  char *ldvarg420 ;
  void *tmp ;
  char *ldvarg421 ;
  void *tmp___0 ;
  size_t ldvarg422 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg420 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg421 = (char *)tmp___0;
  ldv_memset((void *)(& ldvarg422), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_154 == 1) {
    amdgpu_set_dpm_state(dev_attr_power_dpm_state_group1, dev_attr_power_dpm_state_group0,
                         (char const *)ldvarg421, ldvarg422);
    ldv_state_variable_154 = 1;
  } else {
  }
  goto ldv_48743;
  case 1: ;
  if (ldv_state_variable_154 == 1) {
    amdgpu_get_dpm_state(dev_attr_power_dpm_state_group1, dev_attr_power_dpm_state_group0,
                         ldvarg420);
    ldv_state_variable_154 = 1;
  } else {
  }
  goto ldv_48743;
  default:
  ldv_stop();
  }
  ldv_48743: ;
  return;
}
}
void ldv_main_exported_145(void)
{
  struct attribute *ldvarg365 ;
  void *tmp ;
  struct kobject *ldvarg366 ;
  void *tmp___0 ;
  int ldvarg364 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(32UL);
  ldvarg365 = (struct attribute *)tmp;
  tmp___0 = ldv_init_zalloc(296UL);
  ldvarg366 = (struct kobject *)tmp___0;
  ldv_memset((void *)(& ldvarg364), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_145 == 1) {
    hwmon_attributes_visible(ldvarg366, ldvarg365, ldvarg364);
    ldv_state_variable_145 = 1;
  } else {
  }
  goto ldv_48753;
  default:
  ldv_stop();
  }
  ldv_48753: ;
  return;
}
}
void ldv_main_exported_151(void)
{
  struct device_attribute *ldvarg107 ;
  void *tmp ;
  char *ldvarg106 ;
  void *tmp___0 ;
  struct device *ldvarg108 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg107 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg106 = (char *)tmp___0;
  tmp___1 = ldv_init_zalloc(1416UL);
  ldvarg108 = (struct device *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_151 == 1) {
    amdgpu_hwmon_show_temp_thresh(ldvarg108, ldvarg107, ldvarg106);
    ldv_state_variable_151 = 1;
  } else {
  }
  goto ldv_48762;
  default:
  ldv_stop();
  }
  ldv_48762: ;
  return;
}
}
void ldv_main_exported_148(void)
{
  size_t ldvarg115 ;
  char *ldvarg114 ;
  void *tmp ;
  char *ldvarg113 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg114 = (char *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg113 = (char *)tmp___0;
  ldv_memset((void *)(& ldvarg115), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_148 == 1) {
    amdgpu_hwmon_set_pwm1_enable(sensor_dev_attr_pwm1_enable_group1, sensor_dev_attr_pwm1_enable_group0,
                                 (char const *)ldvarg114, ldvarg115);
    ldv_state_variable_148 = 1;
  } else {
  }
  goto ldv_48771;
  case 1: ;
  if (ldv_state_variable_148 == 1) {
    amdgpu_hwmon_get_pwm1_enable(sensor_dev_attr_pwm1_enable_group1, sensor_dev_attr_pwm1_enable_group0,
                                 ldvarg113);
    ldv_state_variable_148 = 1;
  } else {
  }
  goto ldv_48771;
  default:
  ldv_stop();
  }
  ldv_48771: ;
  return;
}
}
void ldv_main_exported_150(void)
{
  struct device_attribute *ldvarg994 ;
  void *tmp ;
  char *ldvarg993 ;
  void *tmp___0 ;
  struct device *ldvarg995 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg994 = (struct device_attribute *)tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg993 = (char *)tmp___0;
  tmp___1 = ldv_init_zalloc(1416UL);
  ldvarg995 = (struct device *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_150 == 1) {
    amdgpu_hwmon_show_temp_thresh(ldvarg995, ldvarg994, ldvarg993);
    ldv_state_variable_150 = 1;
  } else {
  }
  goto ldv_48781;
  default:
  ldv_stop();
  }
  ldv_48781: ;
  return;
}
}
bool ldv_queue_work_on_299(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_300(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_301(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_302(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_303(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_313(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_315(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_314(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_317(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_316(struct workqueue_struct *ldv_func_arg1 ) ;
extern void usleep_range(unsigned long , unsigned long ) ;
extern bool drm_dp_channel_eq_ok(u8 const * , int ) ;
extern bool drm_dp_clock_recovery_ok(u8 const * , int ) ;
extern u8 drm_dp_get_adjust_request_voltage(u8 const * , int ) ;
extern u8 drm_dp_get_adjust_request_pre_emphasis(u8 const * , int ) ;
extern void drm_dp_link_train_clock_recovery_delay(u8 const * ) ;
extern void drm_dp_link_train_channel_eq_delay(u8 const * ) ;
extern u8 drm_dp_link_rate_to_bw_code(int ) ;
extern int drm_dp_bw_code_to_link_rate(u8 ) ;
__inline static int drm_dp_max_link_rate(u8 const *dpcd )
{
  int tmp ;
  {
  tmp = drm_dp_bw_code_to_link_rate((int )*(dpcd + 1UL));
  return (tmp);
}
}
__inline static u8 drm_dp_max_lane_count(u8 const *dpcd )
{
  {
  return ((unsigned int )((u8 )*(dpcd + 2UL)) & 31U);
}
}
__inline static bool drm_dp_enhanced_frame_cap(u8 const *dpcd )
{
  {
  return ((bool )((unsigned int )((unsigned char )*dpcd) > 16U && (int )((signed char )*(dpcd + 2UL)) < 0));
}
}
extern ssize_t drm_dp_dpcd_read(struct drm_dp_aux * , unsigned int , void * , size_t ) ;
extern ssize_t drm_dp_dpcd_write(struct drm_dp_aux * , unsigned int , void * , size_t ) ;
__inline static ssize_t drm_dp_dpcd_readb(struct drm_dp_aux *aux , unsigned int offset ,
                                          u8 *valuep )
{
  ssize_t tmp ;
  {
  tmp = drm_dp_dpcd_read(aux, offset, (void *)valuep, 1UL);
  return (tmp);
}
}
__inline static ssize_t drm_dp_dpcd_writeb(struct drm_dp_aux *aux , unsigned int offset ,
                                           u8 value )
{
  ssize_t tmp ;
  {
  tmp = drm_dp_dpcd_write(aux, offset, (void *)(& value), 1UL);
  return (tmp);
}
}
extern int drm_dp_dpcd_read_link_status(struct drm_dp_aux * , u8 * ) ;
extern int drm_dp_aux_register(struct drm_dp_aux * ) ;
void amdgpu_atombios_encoder_setup_dig_encoder(struct drm_encoder *encoder , int action ,
                                               int panel_mode ) ;
void amdgpu_atombios_encoder_setup_dig_transmitter(struct drm_encoder *encoder , int action ,
                                                   uint8_t lane_num , uint8_t lane_set ) ;
int amdgpu_atombios_dp_get_panel_mode(struct drm_encoder *encoder , struct drm_connector *connector ) ;
void amdgpu_atombios_dp_set_link_config(struct drm_connector *connector , struct drm_display_mode const *mode ) ;
void amdgpu_atombios_dp_set_rx_power_state(struct drm_connector *connector , u8 power_state ) ;
void amdgpu_atombios_dp_link_train(struct drm_encoder *encoder , struct drm_connector *connector ) ;
static char *voltage_names[4U] = { (char *)"0.4V", (char *)"0.6V", (char *)"0.8V", (char *)"1.2V"};
static char *pre_emph_names[4U] = { (char *)"0dB", (char *)"3.5dB", (char *)"6dB", (char *)"9.5dB"};
static int amdgpu_atombios_dp_process_aux_ch(struct amdgpu_i2c_chan *chan , u8 *send ,
                                             int send_bytes , u8 *recv , int recv_size ,
                                             u8 delay , u8 *ack )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  union aux_channel_transaction args ;
  int index ;
  unsigned char *base ;
  int recv_bytes ;
  int r ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  dev = chan->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 78;
  r = 0;
  memset((void *)(& args), 0, 8UL);
  mutex_lock_nested(& chan->mutex, 0U);
  base = (unsigned char *)(adev->mode_info.atom_context)->scratch + 1U;
  amdgpu_atombios_copy_swap(base, send, (int )((u8 )send_bytes), 1);
  args.v2.lpAuxRequest = 4U;
  args.v2.lpDataOut = 20U;
  args.v2.ucDataOutLen = 0U;
  args.v2.ucChannelID = chan->rec.i2c_id;
  args.v2.__annonCompField117.ucDelay = (UCHAR )((unsigned int )delay / 10U);
  args.v2.ucHPD_ID = (UCHAR )chan->rec.hpd;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  *ack = args.v2.__annonCompField117.ucReplyStatus;
  if ((unsigned int )args.v2.__annonCompField117.ucReplyStatus == 1U) {
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_process_aux_ch", "dp_aux_ch timeout\n");
    } else {
    }
    r = -110;
    goto done;
  } else {
  }
  if ((unsigned int )args.v2.__annonCompField117.ucReplyStatus == 2U) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_process_aux_ch", "dp_aux_ch flags not zero\n");
    } else {
    }
    r = -5;
    goto done;
  } else {
  }
  if ((unsigned int )args.v2.__annonCompField117.ucReplyStatus == 3U) {
    tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_process_aux_ch", "dp_aux_ch error\n");
    } else {
    }
    r = -5;
    goto done;
  } else {
  }
  recv_bytes = (int )args.v1.ucDataOutLen;
  if (recv_bytes > recv_size) {
    recv_bytes = recv_size;
  } else {
  }
  if ((unsigned long )recv != (unsigned long )((u8 *)0U) && recv_size != 0) {
    amdgpu_atombios_copy_swap(recv, base + 16U, (int )((u8 )recv_bytes), 0);
  } else {
  }
  r = recv_bytes;
  done:
  mutex_unlock(& chan->mutex);
  return (r);
}
}
static ssize_t amdgpu_atombios_dp_aux_transfer(struct drm_dp_aux *aux , struct drm_dp_aux_msg *msg )
{
  struct amdgpu_i2c_chan *chan ;
  struct drm_dp_aux const *__mptr ;
  int ret ;
  u8 tx_buf[20U] ;
  size_t tx_size ;
  u8 ack ;
  u8 delay ;
  int __ret_warn_on ;
  long tmp ;
  long tmp___0 ;
  {
  __mptr = (struct drm_dp_aux const *)aux;
  chan = (struct amdgpu_i2c_chan *)__mptr + 0xfffffffffffff7d8UL;
  delay = 0U;
  __ret_warn_on = msg->size > 16UL;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/atombios_dp.c",
                       137);
  } else {
  }
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    return (-7L);
  } else {
  }
  tx_buf[0] = (u8 )msg->address;
  tx_buf[1] = (u8 )(msg->address >> 8);
  tx_buf[2] = (int )msg->request << 4U;
  tx_buf[3] = msg->size != 0UL ? (unsigned int )((u8 )msg->size) - 1U : 0U;
  switch ((int )msg->request & -5) {
  case 8: ;
  case 0:
  tx_size = msg->size + 4UL;
  if (msg->size == 0UL) {
    tx_buf[3] = (u8 )((unsigned int )tx_buf[3] | 48U);
  } else {
    tx_buf[3] = (int )tx_buf[3] | ((int )((u8 )tx_size) << 4U);
  }
  memcpy((void *)(& tx_buf) + 4U, (void const *)msg->buffer, msg->size);
  ret = amdgpu_atombios_dp_process_aux_ch(chan, (u8 *)(& tx_buf), (int )tx_size, (u8 *)0U,
                                          0, (int )delay, & ack);
  if (ret >= 0) {
    ret = (int )msg->size;
  } else {
  }
  goto ldv_48043;
  case 9: ;
  case 1:
  tx_size = 4UL;
  if (msg->size == 0UL) {
    tx_buf[3] = (u8 )((unsigned int )tx_buf[3] | 48U);
  } else {
    tx_buf[3] = (int )tx_buf[3] | ((int )((u8 )tx_size) << 4U);
  }
  ret = amdgpu_atombios_dp_process_aux_ch(chan, (u8 *)(& tx_buf), (int )tx_size, (u8 *)msg->buffer,
                                          (int )msg->size, (int )delay, & ack);
  goto ldv_48043;
  default:
  ret = -22;
  goto ldv_48043;
  }
  ldv_48043: ;
  if (ret >= 0) {
    msg->reply = (u8 )((int )ack >> 4);
  } else {
  }
  return ((ssize_t )ret);
}
}
void amdgpu_atombios_dp_aux_init(struct amdgpu_connector *amdgpu_connector )
{
  int ret ;
  int __ret_warn_on ;
  long tmp ;
  {
  (amdgpu_connector->ddc_bus)->rec.hpd = amdgpu_connector->hpd.hpd;
  (amdgpu_connector->ddc_bus)->aux.dev = amdgpu_connector->base.kdev;
  (amdgpu_connector->ddc_bus)->aux.transfer = & amdgpu_atombios_dp_aux_transfer;
  ret = drm_dp_aux_register(& (amdgpu_connector->ddc_bus)->aux);
  if (ret == 0) {
    (amdgpu_connector->ddc_bus)->has_aux = 1;
  } else {
  }
  __ret_warn_on = ret != 0;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/atombios_dp.c",
                      198, "drm_dp_aux_register_i2c_bus() failed with error %d\n",
                      ret);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  return;
}
}
static void amdgpu_atombios_dp_get_adjust_train(u8 const *link_status , int lane_count ,
                                                u8 *train_set )
{
  u8 v ;
  u8 p ;
  int lane ;
  u8 this_v ;
  u8 tmp ;
  u8 this_p ;
  u8 tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  v = 0U;
  p = 0U;
  lane = 0;
  goto ldv_48065;
  ldv_48064:
  tmp = drm_dp_get_adjust_request_voltage(link_status, lane);
  this_v = tmp;
  tmp___0 = drm_dp_get_adjust_request_pre_emphasis(link_status, lane);
  this_p = tmp___0;
  tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___1 != 0L) {
    drm_ut_debug_printk("amdgpu_atombios_dp_get_adjust_train", "requested signal parameters: lane %d voltage %s pre_emph %s\n",
                        lane, voltage_names[(int )this_v], pre_emph_names[(int )this_p >> 3]);
  } else {
  }
  if ((int )this_v > (int )v) {
    v = this_v;
  } else {
  }
  if ((int )this_p > (int )p) {
    p = this_p;
  } else {
  }
  lane = lane + 1;
  ldv_48065: ;
  if (lane < lane_count) {
    goto ldv_48064;
  } else {
  }
  if ((unsigned int )v > 2U) {
    v = (u8 )((unsigned int )v | 4U);
  } else {
  }
  if ((unsigned int )p > 23U) {
    p = (u8 )((unsigned int )p | 32U);
  } else {
  }
  tmp___2 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("amdgpu_atombios_dp_get_adjust_train", "using signal parameters: voltage %s pre_emph %s\n",
                        voltage_names[(int )v & 3], pre_emph_names[((int )p & 24) >> 3]);
  } else {
  }
  lane = 0;
  goto ldv_48068;
  ldv_48067:
  *(train_set + (unsigned long )lane) = (u8 )((int )v | (int )p);
  lane = lane + 1;
  ldv_48068: ;
  if (lane <= 3) {
    goto ldv_48067;
  } else {
  }
  return;
}
}
static int amdgpu_atombios_dp_convert_bpc_to_bpp(int bpc )
{
  {
  if (bpc == 0) {
    return (24);
  } else {
    return (bpc * 3);
  }
}
}
static int amdgpu_atombios_dp_get_max_dp_pix_clock(int link_rate , int lane_num ,
                                                   int bpp )
{
  {
  return (((link_rate * lane_num) * 8) / bpp);
}
}
static int amdgpu_atombios_dp_get_dp_lane_number(struct drm_connector *connector ,
                                                 u8 const *dpcd , int pix_clock )
{
  int bpp ;
  int tmp ;
  int tmp___0 ;
  int max_link_rate ;
  int tmp___1 ;
  int max_lane_num ;
  u8 tmp___2 ;
  int lane_num ;
  int max_dp_pix_clock ;
  {
  tmp = amdgpu_connector_get_monitor_bpc(connector);
  tmp___0 = amdgpu_atombios_dp_convert_bpc_to_bpp(tmp);
  bpp = tmp___0;
  tmp___1 = drm_dp_max_link_rate(dpcd);
  max_link_rate = tmp___1;
  tmp___2 = drm_dp_max_lane_count(dpcd);
  max_lane_num = (int )tmp___2;
  lane_num = 1;
  goto ldv_48090;
  ldv_48089:
  max_dp_pix_clock = amdgpu_atombios_dp_get_max_dp_pix_clock(max_link_rate, lane_num,
                                                             bpp);
  if (pix_clock <= max_dp_pix_clock) {
    goto ldv_48088;
  } else {
  }
  lane_num = lane_num << 1;
  ldv_48090: ;
  if (lane_num < max_lane_num) {
    goto ldv_48089;
  } else {
  }
  ldv_48088: ;
  return (lane_num);
}
}
static int amdgpu_atombios_dp_get_dp_link_clock(struct drm_connector *connector ,
                                                u8 const *dpcd , int pix_clock )
{
  int bpp ;
  int tmp ;
  int tmp___0 ;
  int lane_num ;
  int max_pix_clock ;
  u16 tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  tmp = amdgpu_connector_get_monitor_bpc(connector);
  tmp___0 = amdgpu_atombios_dp_convert_bpc_to_bpp(tmp);
  bpp = tmp___0;
  tmp___1 = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
  if ((unsigned int )tmp___1 == 34U) {
    return (270000);
  } else {
  }
  lane_num = amdgpu_atombios_dp_get_dp_lane_number(connector, dpcd, pix_clock);
  max_pix_clock = amdgpu_atombios_dp_get_max_dp_pix_clock(162000, lane_num, bpp);
  if (pix_clock <= max_pix_clock) {
    return (162000);
  } else {
  }
  max_pix_clock = amdgpu_atombios_dp_get_max_dp_pix_clock(270000, lane_num, bpp);
  if (pix_clock <= max_pix_clock) {
    return (270000);
  } else {
  }
  tmp___2 = amdgpu_connector_is_dp12_capable(connector);
  if ((int )tmp___2) {
    max_pix_clock = amdgpu_atombios_dp_get_max_dp_pix_clock(540000, lane_num, bpp);
    if (pix_clock <= max_pix_clock) {
      return (540000);
    } else {
    }
  } else {
  }
  tmp___3 = drm_dp_max_link_rate(dpcd);
  return (tmp___3);
}
}
static u8 amdgpu_atombios_dp_encoder_service(struct amdgpu_device *adev , int action ,
                                             int dp_clock , u8 ucconfig , u8 lane_num )
{
  DP_ENCODER_SERVICE_PARAMETERS args ;
  int index ;
  {
  index = 79;
  memset((void *)(& args), 0, 8UL);
  args.ucLinkClock = (USHORT )(dp_clock / 10);
  args.__annonCompField118.ucConfig = ucconfig;
  args.ucAction = (UCHAR )action;
  args.ucLaneNum = lane_num;
  args.ucStatus = 0U;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return (args.ucStatus);
}
}
u8 amdgpu_atombios_dp_get_sinktype(struct amdgpu_connector *amdgpu_connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u8 tmp ;
  {
  dev = amdgpu_connector->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_atombios_dp_encoder_service(adev, 1, 0, (int )(amdgpu_connector->ddc_bus)->rec.i2c_id,
                                           0);
  return (tmp);
}
}
static void amdgpu_atombios_dp_probe_oui(struct amdgpu_connector *amdgpu_connector )
{
  struct amdgpu_connector_atom_dig *dig_connector ;
  u8 buf[3U] ;
  long tmp ;
  ssize_t tmp___0 ;
  long tmp___1 ;
  ssize_t tmp___2 ;
  {
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((int )((signed char )dig_connector->dpcd[7]) >= 0) {
    return;
  } else {
  }
  tmp___0 = drm_dp_dpcd_read(& (amdgpu_connector->ddc_bus)->aux, 1024U, (void *)(& buf),
                             3UL);
  if (tmp___0 == 3L) {
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_probe_oui", "Sink OUI: %02hx%02hx%02hx\n",
                          (int )buf[0], (int )buf[1], (int )buf[2]);
    } else {
    }
  } else {
  }
  tmp___2 = drm_dp_dpcd_read(& (amdgpu_connector->ddc_bus)->aux, 1280U, (void *)(& buf),
                             3UL);
  if (tmp___2 == 3L) {
    tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_probe_oui", "Branch OUI: %02hx%02hx%02hx\n",
                          (int )buf[0], (int )buf[1], (int )buf[2]);
    } else {
    }
  } else {
  }
  return;
}
}
int amdgpu_atombios_dp_get_dpcd(struct amdgpu_connector *amdgpu_connector )
{
  struct amdgpu_connector_atom_dig *dig_connector ;
  u8 msg[15U] ;
  int ret ;
  int i ;
  ssize_t tmp ;
  long tmp___0 ;
  {
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  i = 0;
  goto ldv_48128;
  ldv_48127:
  tmp = drm_dp_dpcd_read(& (amdgpu_connector->ddc_bus)->aux, 0U, (void *)(& msg),
                         15UL);
  ret = (int )tmp;
  if (ret == 15) {
    memcpy((void *)(& dig_connector->dpcd), (void const *)(& msg), 15UL);
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_get_dpcd", "DPCD: %*ph\n", 15, (u8 *)(& dig_connector->dpcd));
    } else {
    }
    amdgpu_atombios_dp_probe_oui(amdgpu_connector);
    return (0);
  } else {
  }
  i = i + 1;
  ldv_48128: ;
  if (i <= 6) {
    goto ldv_48127;
  } else {
  }
  dig_connector->dpcd[0] = 0U;
  return (-22);
}
}
int amdgpu_atombios_dp_get_panel_mode(struct drm_encoder *encoder , struct drm_connector *connector )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  int panel_mode ;
  u16 dp_bridge ;
  u16 tmp ;
  u8 tmp___0 ;
  ssize_t tmp___1 ;
  ssize_t tmp___2 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  panel_mode = 0;
  tmp = amdgpu_connector_encoder_get_dp_bridge_encoder_id(connector);
  dp_bridge = tmp;
  if ((unsigned long )amdgpu_connector->con_priv == (unsigned long )((void *)0)) {
    return (panel_mode);
  } else {
  }
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dp_bridge != 0U) {
    tmp___1 = drm_dp_dpcd_readb(& (amdgpu_connector->ddc_bus)->aux, 13U, & tmp___0);
    if (tmp___1 == 1L) {
      if ((int )tmp___0 & 1) {
        panel_mode = 1;
      } else
      if ((unsigned int )dp_bridge == 34U || (unsigned int )dp_bridge == 35U) {
        panel_mode = 17;
      } else {
        panel_mode = 0;
      }
    } else {
    }
  } else
  if (connector->connector_type == 14) {
    tmp___2 = drm_dp_dpcd_readb(& (amdgpu_connector->ddc_bus)->aux, 13U, & tmp___0);
    if (tmp___2 == 1L) {
      if ((int )tmp___0 & 1) {
        panel_mode = 1;
      } else {
      }
    } else {
    }
  } else {
  }
  return (panel_mode);
}
}
void amdgpu_atombios_dp_set_link_config(struct drm_connector *connector , struct drm_display_mode const *mode )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->con_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dp_sink_type == 19U || (unsigned int )dig_connector->dp_sink_type == 20U) {
    dig_connector->dp_clock = amdgpu_atombios_dp_get_dp_link_clock(connector, (u8 const *)(& dig_connector->dpcd),
                                                                   mode->clock);
    dig_connector->dp_lane_count = amdgpu_atombios_dp_get_dp_lane_number(connector,
                                                                         (u8 const *)(& dig_connector->dpcd),
                                                                         mode->clock);
  } else {
  }
  return;
}
}
int amdgpu_atombios_dp_mode_valid_helper(struct drm_connector *connector , struct drm_display_mode *mode )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  int dp_clock ;
  bool tmp ;
  int tmp___0 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->con_priv == (unsigned long )((void *)0)) {
    return (15);
  } else {
  }
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  dp_clock = amdgpu_atombios_dp_get_dp_link_clock(connector, (u8 const *)(& dig_connector->dpcd),
                                                  mode->clock);
  if (dp_clock == 540000) {
    tmp = amdgpu_connector_is_dp12_capable(connector);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    if (tmp___0) {
      return (15);
    } else {
    }
  } else {
  }
  return (0);
}
}
bool amdgpu_atombios_dp_needs_link_train(struct amdgpu_connector *amdgpu_connector )
{
  u8 link_status[6U] ;
  struct amdgpu_connector_atom_dig *dig ;
  int tmp ;
  bool tmp___0 ;
  {
  dig = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  tmp = drm_dp_dpcd_read_link_status(& (amdgpu_connector->ddc_bus)->aux, (u8 *)(& link_status));
  if (tmp <= 0) {
    return (0);
  } else {
  }
  tmp___0 = drm_dp_channel_eq_ok((u8 const *)(& link_status), dig->dp_lane_count);
  if ((int )tmp___0) {
    return (0);
  } else {
  }
  return (1);
}
}
void amdgpu_atombios_dp_set_rx_power_state(struct drm_connector *connector , u8 power_state )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  if ((unsigned long )amdgpu_connector->con_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dpcd[0] > 16U) {
    drm_dp_dpcd_writeb(& (amdgpu_connector->ddc_bus)->aux, 1536U, (int )power_state);
    usleep_range(1000UL, 2000UL);
  } else {
  }
  return;
}
}
static void amdgpu_atombios_dp_update_vs_emph(struct amdgpu_atombios_dp_link_train_info *dp_info )
{
  {
  amdgpu_atombios_encoder_setup_dig_transmitter(dp_info->encoder, 11, 0, (int )dp_info->train_set[0]);
  drm_dp_dpcd_write(dp_info->aux, 259U, (void *)(& dp_info->train_set), (size_t )dp_info->dp_lane_count);
  return;
}
}
static void amdgpu_atombios_dp_set_tp(struct amdgpu_atombios_dp_link_train_info *dp_info ,
                                      int tp )
{
  int rtp ;
  {
  rtp = 0;
  switch (tp) {
  case 1:
  rtp = 9;
  goto ldv_48192;
  case 2:
  rtp = 10;
  goto ldv_48192;
  case 3:
  rtp = 19;
  goto ldv_48192;
  }
  ldv_48192:
  amdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder, rtp, 0);
  drm_dp_dpcd_writeb(dp_info->aux, 258U, (int )((u8 )tp));
  return;
}
}
static int amdgpu_atombios_dp_link_train_init(struct amdgpu_atombios_dp_link_train_info *dp_info )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u8 tmp ;
  bool tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)dp_info->encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_atombios_dp_set_rx_power_state(dp_info->connector, 1);
  if ((int )dp_info->dpcd[3] & 1) {
    drm_dp_dpcd_writeb(dp_info->aux, 263U, 16);
  } else {
    drm_dp_dpcd_writeb(dp_info->aux, 263U, 0);
  }
  if (dig->panel_mode == 1) {
    drm_dp_dpcd_writeb(dp_info->aux, 266U, 1);
  } else {
  }
  tmp = (u8 )dp_info->dp_lane_count;
  tmp___0 = drm_dp_enhanced_frame_cap((u8 const *)(& dp_info->dpcd));
  if ((int )tmp___0) {
    tmp = (u8 )((unsigned int )tmp | 128U);
  } else {
  }
  drm_dp_dpcd_writeb(dp_info->aux, 257U, (int )tmp);
  tmp = drm_dp_link_rate_to_bw_code(dp_info->dp_clock);
  drm_dp_dpcd_writeb(dp_info->aux, 256U, (int )tmp);
  amdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder, 8, 0);
  drm_dp_dpcd_writeb(dp_info->aux, 258U, 0);
  return (0);
}
}
static int amdgpu_atombios_dp_link_train_finish(struct amdgpu_atombios_dp_link_train_info *dp_info )
{
  {
  __const_udelay(1718000UL);
  drm_dp_dpcd_writeb(dp_info->aux, 258U, 0);
  amdgpu_atombios_encoder_setup_dig_encoder(dp_info->encoder, 11, 0);
  return (0);
}
}
static int amdgpu_atombios_dp_link_train_cr(struct amdgpu_atombios_dp_link_train_info *dp_info )
{
  bool clock_recovery ;
  u8 voltage ;
  int i ;
  int tmp ;
  bool tmp___0 ;
  long tmp___1 ;
  {
  amdgpu_atombios_dp_set_tp(dp_info, 1);
  memset((void *)(& dp_info->train_set), 0, 4UL);
  amdgpu_atombios_dp_update_vs_emph(dp_info);
  __const_udelay(1718000UL);
  clock_recovery = 0;
  dp_info->tries = 0U;
  voltage = 255U;
  ldv_48216:
  drm_dp_link_train_clock_recovery_delay((u8 const *)(& dp_info->dpcd));
  tmp = drm_dp_dpcd_read_link_status(dp_info->aux, (u8 *)(& dp_info->link_status));
  if (tmp <= 0) {
    drm_err("displayport link status failed\n");
    goto ldv_48212;
  } else {
  }
  tmp___0 = drm_dp_clock_recovery_ok((u8 const *)(& dp_info->link_status), dp_info->dp_lane_count);
  if ((int )tmp___0) {
    clock_recovery = 1;
    goto ldv_48212;
  } else {
  }
  i = 0;
  goto ldv_48215;
  ldv_48214: ;
  if (((int )dp_info->train_set[i] & 4) == 0) {
    goto ldv_48213;
  } else {
  }
  i = i + 1;
  ldv_48215: ;
  if (dp_info->dp_lane_count > i) {
    goto ldv_48214;
  } else {
  }
  ldv_48213: ;
  if (dp_info->dp_lane_count == i) {
    drm_err("clock recovery reached max voltage\n");
    goto ldv_48212;
  } else {
  }
  if (((int )dp_info->train_set[0] & 3) == (int )voltage) {
    dp_info->tries = (u8 )((int )dp_info->tries + 1);
    if ((unsigned int )dp_info->tries == 5U) {
      drm_err("clock recovery tried 5 times\n");
      goto ldv_48212;
    } else {
    }
  } else {
    dp_info->tries = 0U;
  }
  voltage = (unsigned int )dp_info->train_set[0] & 3U;
  amdgpu_atombios_dp_get_adjust_train((u8 const *)(& dp_info->link_status), dp_info->dp_lane_count,
                                      (u8 *)(& dp_info->train_set));
  amdgpu_atombios_dp_update_vs_emph(dp_info);
  goto ldv_48216;
  ldv_48212: ;
  if (! clock_recovery) {
    drm_err("clock recovery failed\n");
    return (-1);
  } else {
    tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_link_train_cr", "clock recovery at voltage %d pre-emphasis %d\n",
                          (int )dp_info->train_set[0] & 3, ((int )dp_info->train_set[0] & 24) >> 3);
    } else {
    }
    return (0);
  }
}
}
static int amdgpu_atombios_dp_link_train_ce(struct amdgpu_atombios_dp_link_train_info *dp_info )
{
  bool channel_eq ;
  int tmp ;
  bool tmp___0 ;
  long tmp___1 ;
  {
  if ((int )dp_info->tp3_supported) {
    amdgpu_atombios_dp_set_tp(dp_info, 3);
  } else {
    amdgpu_atombios_dp_set_tp(dp_info, 2);
  }
  dp_info->tries = 0U;
  channel_eq = 0;
  ldv_48223:
  drm_dp_link_train_channel_eq_delay((u8 const *)(& dp_info->dpcd));
  tmp = drm_dp_dpcd_read_link_status(dp_info->aux, (u8 *)(& dp_info->link_status));
  if (tmp <= 0) {
    drm_err("displayport link status failed\n");
    goto ldv_48222;
  } else {
  }
  tmp___0 = drm_dp_channel_eq_ok((u8 const *)(& dp_info->link_status), dp_info->dp_lane_count);
  if ((int )tmp___0) {
    channel_eq = 1;
    goto ldv_48222;
  } else {
  }
  if ((unsigned int )dp_info->tries > 5U) {
    drm_err("channel eq failed: 5 tries\n");
    goto ldv_48222;
  } else {
  }
  amdgpu_atombios_dp_get_adjust_train((u8 const *)(& dp_info->link_status), dp_info->dp_lane_count,
                                      (u8 *)(& dp_info->train_set));
  amdgpu_atombios_dp_update_vs_emph(dp_info);
  dp_info->tries = (u8 )((int )dp_info->tries + 1);
  goto ldv_48223;
  ldv_48222: ;
  if (! channel_eq) {
    drm_err("channel eq failed\n");
    return (-1);
  } else {
    tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_dp_link_train_ce", "channel eq at voltage %d pre-emphasis %d\n",
                          (int )dp_info->train_set[0] & 3, ((int )dp_info->train_set[0] & 24) >> 3);
    } else {
    }
    return (0);
  }
}
}
void amdgpu_atombios_dp_link_train(struct drm_encoder *encoder , struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct amdgpu_connector *amdgpu_connector ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  struct amdgpu_atombios_dp_link_train_info dp_info ;
  u8 tmp ;
  struct drm_connector const *__mptr___0 ;
  ssize_t tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if ((unsigned long )amdgpu_encoder->enc_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if ((unsigned long )amdgpu_connector->con_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dp_sink_type != 19U && (unsigned int )dig_connector->dp_sink_type != 20U) {
    return;
  } else {
  }
  tmp___0 = drm_dp_dpcd_readb(& (amdgpu_connector->ddc_bus)->aux, 2U, & tmp);
  if (tmp___0 == 1L) {
    if (((int )tmp & 64) != 0) {
      dp_info.tp3_supported = 1;
    } else {
      dp_info.tp3_supported = 0;
    }
  } else {
    dp_info.tp3_supported = 0;
  }
  memcpy((void *)(& dp_info.dpcd), (void const *)(& dig_connector->dpcd), 15UL);
  dp_info.adev = adev;
  dp_info.encoder = encoder;
  dp_info.connector = connector;
  dp_info.dp_lane_count = dig_connector->dp_lane_count;
  dp_info.dp_clock = dig_connector->dp_clock;
  dp_info.aux = & (amdgpu_connector->ddc_bus)->aux;
  tmp___1 = amdgpu_atombios_dp_link_train_init(& dp_info);
  if (tmp___1 != 0) {
    goto done;
  } else {
  }
  tmp___2 = amdgpu_atombios_dp_link_train_cr(& dp_info);
  if (tmp___2 != 0) {
    goto done;
  } else {
  }
  tmp___3 = amdgpu_atombios_dp_link_train_ce(& dp_info);
  if (tmp___3 != 0) {
  } else {
  }
  done:
  tmp___4 = amdgpu_atombios_dp_link_train_finish(& dp_info);
  if (tmp___4 != 0) {
    return;
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_313(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_314(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_315(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_316(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_317(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_327(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_329(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_328(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_331(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_330(struct workqueue_struct *ldv_func_arg1 ) ;
extern unsigned long gcd(unsigned long , unsigned long ) ;
struct amdgpu_afmt_acr amdgpu_afmt_acr(u32 clock ) ;
static struct amdgpu_afmt_acr const amdgpu_afmt_predefined_acr[10U] =
  { {25175U, 4096, 25175, 28224, 125875, 6144, 25175},
        {25200U, 4096, 25200, 6272, 28000, 6144, 25200},
        {27000U, 4096, 27000, 6272, 30000, 6144, 27000},
        {27027U, 4096, 27027, 6272, 30030, 6144, 27027},
        {54000U, 4096, 54000, 6272, 60000, 6144, 54000},
        {54054U, 4096, 54054, 6272, 60060, 6144, 54054},
        {74176U, 4096, 74176, 5733, 75335, 6144, 74176},
        {74250U, 4096, 74250, 6272, 82500, 6144, 74250},
        {148352U, 4096, 148352, 5733, 150670, 6144, 148352},
        {148500U, 4096, 148500, 6272, 165000, 6144, 148500}};
static void amdgpu_afmt_calc_cts(u32 clock , int *CTS , int *N , int freq )
{
  int n ;
  int cts ;
  unsigned long div ;
  unsigned long mul ;
  long tmp ;
  {
  n = freq * 128;
  cts = (int )(clock * 1000U);
  div = gcd((unsigned long )n, (unsigned long )cts);
  n = (int )((unsigned long )n / div);
  cts = (int )((unsigned long )cts / div);
  mul = (unsigned long )(((freq * 128) / 1000 + (n + -1)) / n);
  n = (int )((unsigned int )((unsigned long )n) * (unsigned int )mul);
  cts = (int )((unsigned int )((unsigned long )cts) * (unsigned int )mul);
  if ((freq * 128) / 1500 > n) {
    printk("\fCalculated ACR N value is too small. You may experience audio problems.\n");
  } else {
  }
  if ((freq * 128) / 300 < n) {
    printk("\fCalculated ACR N value is too large. You may experience audio problems.\n");
  } else {
  }
  *N = n;
  *CTS = cts;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_afmt_calc_cts", "Calculated ACR timing N=%d CTS=%d for frequency %d\n",
                        *N, *CTS, freq);
  } else {
  }
  return;
}
}
struct amdgpu_afmt_acr amdgpu_afmt_acr(u32 clock )
{
  struct amdgpu_afmt_acr res ;
  u8 i ;
  {
  i = 0U;
  goto ldv_43663;
  ldv_43662: ;
  if ((unsigned int )amdgpu_afmt_predefined_acr[(int )i].clock == clock) {
    return ((struct amdgpu_afmt_acr )amdgpu_afmt_predefined_acr[(int )i]);
  } else {
  }
  i = (u8 )((int )i + 1);
  ldv_43663: ;
  if ((unsigned int )i <= 9U) {
    goto ldv_43662;
  } else {
  }
  amdgpu_afmt_calc_cts(clock, & res.cts_32khz, & res.n_32khz, 32000);
  amdgpu_afmt_calc_cts(clock, & res.cts_44_1khz, & res.n_44_1khz, 44100);
  amdgpu_afmt_calc_cts(clock, & res.cts_48khz, & res.n_48khz, 48000);
  return (res);
}
}
bool ldv_queue_work_on_327(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_328(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_329(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_330(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_331(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_341(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_343(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_342(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_345(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_344(struct workqueue_struct *ldv_func_arg1 ) ;
struct tracepoint __tracepoint_amdgpu_vm_grab_id ;
struct tracepoint __tracepoint_amdgpu_vm_bo_map ;
struct tracepoint __tracepoint_amdgpu_vm_bo_unmap ;
struct tracepoint __tracepoint_amdgpu_vm_bo_update ;
struct tracepoint __tracepoint_amdgpu_vm_set_page ;
struct tracepoint __tracepoint_amdgpu_vm_flush ;
struct tracepoint __tracepoint_amdgpu_bo_list_set ;
struct tracepoint __tracepoint_amdgpu_semaphore_signale ;
struct tracepoint __tracepoint_amdgpu_semaphore_wait ;
static char const __tpstrtab_amdgpu_bo_create[17U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'b',
        'o', '_', 'c', 'r',
        'e', 'a', 't', 'e',
        '\000'};
struct tracepoint __tracepoint_amdgpu_bo_create = {(char const *)(& __tpstrtab_amdgpu_bo_create), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_cs[10U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'c',
        's', '\000'};
struct tracepoint __tracepoint_amdgpu_cs = {(char const *)(& __tpstrtab_amdgpu_cs), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_grab_id[18U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 'g', 'r',
        'a', 'b', '_', 'i',
        'd', '\000'};
struct tracepoint __tracepoint_amdgpu_vm_grab_id = {(char const *)(& __tpstrtab_amdgpu_vm_grab_id), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_bo_map[17U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 'b', 'o',
        '_', 'm', 'a', 'p',
        '\000'};
struct tracepoint __tracepoint_amdgpu_vm_bo_map = {(char const *)(& __tpstrtab_amdgpu_vm_bo_map), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_bo_unmap[19U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 'b', 'o',
        '_', 'u', 'n', 'm',
        'a', 'p', '\000'};
struct tracepoint __tracepoint_amdgpu_vm_bo_unmap = {(char const *)(& __tpstrtab_amdgpu_vm_bo_unmap), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_bo_update[20U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 'b', 'o',
        '_', 'u', 'p', 'd',
        'a', 't', 'e', '\000'};
struct tracepoint __tracepoint_amdgpu_vm_bo_update = {(char const *)(& __tpstrtab_amdgpu_vm_bo_update), {{0}}, (void (*)(void))0,
    (void (*)(void))0, (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_set_page[19U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 's', 'e',
        't', '_', 'p', 'a',
        'g', 'e', '\000'};
struct tracepoint __tracepoint_amdgpu_vm_set_page = {(char const *)(& __tpstrtab_amdgpu_vm_set_page), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_vm_flush[16U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'v',
        'm', '_', 'f', 'l',
        'u', 's', 'h', '\000'};
struct tracepoint __tracepoint_amdgpu_vm_flush = {(char const *)(& __tpstrtab_amdgpu_vm_flush), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_bo_list_set[19U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'b',
        'o', '_', 'l', 'i',
        's', 't', '_', 's',
        'e', 't', '\000'};
struct tracepoint __tracepoint_amdgpu_bo_list_set = {(char const *)(& __tpstrtab_amdgpu_bo_list_set), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_fence_emit[18U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'f',
        'e', 'n', 'c', 'e',
        '_', 'e', 'm', 'i',
        't', '\000'};
struct tracepoint __tracepoint_amdgpu_fence_emit = {(char const *)(& __tpstrtab_amdgpu_fence_emit), {{0}}, (void (*)(void))0, (void (*)(void))0,
    (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_fence_wait_begin[24U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'f',
        'e', 'n', 'c', 'e',
        '_', 'w', 'a', 'i',
        't', '_', 'b', 'e',
        'g', 'i', 'n', '\000'};
struct tracepoint __tracepoint_amdgpu_fence_wait_begin = {(char const *)(& __tpstrtab_amdgpu_fence_wait_begin), {{0}}, (void (*)(void))0,
    (void (*)(void))0, (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_fence_wait_end[22U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 'f',
        'e', 'n', 'c', 'e',
        '_', 'w', 'a', 'i',
        't', '_', 'e', 'n',
        'd', '\000'};
struct tracepoint __tracepoint_amdgpu_fence_wait_end = {(char const *)(& __tpstrtab_amdgpu_fence_wait_end), {{0}}, (void (*)(void))0,
    (void (*)(void))0, (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_semaphore_signale[25U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 's',
        'e', 'm', 'a', 'p',
        'h', 'o', 'r', 'e',
        '_', 's', 'i', 'g',
        'n', 'a', 'l', 'e',
        '\000'};
struct tracepoint __tracepoint_amdgpu_semaphore_signale = {(char const *)(& __tpstrtab_amdgpu_semaphore_signale), {{0}}, (void (*)(void))0,
    (void (*)(void))0, (struct tracepoint_func *)0};
static char const __tpstrtab_amdgpu_semaphore_wait[22U] =
  { 'a', 'm', 'd', 'g',
        'p', 'u', '_', 's',
        'e', 'm', 'a', 'p',
        'h', 'o', 'r', 'e',
        '_', 'w', 'a', 'i',
        't', '\000'};
struct tracepoint __tracepoint_amdgpu_semaphore_wait = {(char const *)(& __tpstrtab_amdgpu_semaphore_wait), {{0}}, (void (*)(void))0,
    (void (*)(void))0, (struct tracepoint_func *)0};
__inline static bool seq_buf_has_overflowed(struct seq_buf *s )
{
  {
  return (s->len > s->size);
}
}
__inline static bool trace_seq_has_overflowed(struct trace_seq *s )
{
  bool tmp ;
  int tmp___0 ;
  {
  if (s->full != 0) {
    tmp___0 = 1;
  } else {
    tmp = seq_buf_has_overflowed(& s->seq);
    if ((int )tmp) {
      tmp___0 = 1;
    } else {
      tmp___0 = 0;
    }
  }
  return ((bool )tmp___0);
}
}
extern void trace_seq_printf(struct trace_seq * , char const * , ...) ;
extern int trace_raw_output_prep(struct trace_iterator * , struct trace_event * ) ;
__inline static enum print_line_t trace_handle_return(struct trace_seq *s )
{
  bool tmp ;
  {
  tmp = trace_seq_has_overflowed(s);
  return ((int )tmp ? 0 : 1);
}
}
extern int trace_event_reg(struct trace_event_call * , enum trace_reg , void * ) ;
extern int trace_event_raw_init(struct trace_event_call * ) ;
extern int trace_define_field(struct trace_event_call * , char const * , char const * ,
                              int , int , int , int ) ;
static enum print_line_t trace_raw_output_amdgpu_bo_create(struct trace_iterator *iter ,
                                                           int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_bo_create *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_bo_create *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "bo=%p, pages=%u\n", field->bo, field->pages);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_cs(struct trace_iterator *iter ,
                                                    int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_cs *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_cs *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "bo_list=%p, ring=%u, dw=%u, fences=%u\n", field->bo_list, field->ring,
                   field->dw, field->fences);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_grab_id(struct trace_iterator *iter ,
                                                            int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_grab_id *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_grab_id *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "vmid=%u, ring=%u\n", field->vmid, field->ring);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_bo_map(struct trace_iterator *iter ,
                                                           int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_bo_map *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_bo_map *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "bo=%p, start=%lx, last=%lx, offset=%010llx, flags=%08x\n",
                   field->bo, field->start, field->last, field->offset, field->flags);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_bo_unmap(struct trace_iterator *iter ,
                                                             int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_bo_unmap *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_bo_unmap *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "bo=%p, start=%lx, last=%lx, offset=%010llx, flags=%08x\n",
                   field->bo, field->start, field->last, field->offset, field->flags);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_bo_update(struct trace_iterator *iter ,
                                                              int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_bo_update *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_bo_update *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "soffs=%010llx, eoffs=%010llx, flags=%08x\n", field->soffset,
                   field->eoffset, field->flags);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_set_page(struct trace_iterator *iter ,
                                                             int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_set_page *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_set_page *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "pe=%010Lx, addr=%010Lx, incr=%u, flags=%08x, count=%u\n", field->pe,
                   field->addr, field->incr, field->flags, field->count);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_vm_flush(struct trace_iterator *iter ,
                                                          int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_vm_flush *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_vm_flush *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "pd_addr=%010Lx, ring=%u, id=%u\n", field->pd_addr, field->ring,
                   field->id);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_bo_list_set(struct trace_iterator *iter ,
                                                             int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_bo_list_set *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_bo_list_set *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "list=%p, bo=%p\n", field->list, field->bo);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_fence_request(struct trace_iterator *iter ,
                                                               int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_fence_request *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_fence_request *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "dev=%u, ring=%d, seqno=%u\n", field->dev, field->ring, field->seqno);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static enum print_line_t trace_raw_output_amdgpu_semaphore_request(struct trace_iterator *iter ,
                                                                   int flags , struct trace_event *trace_event )
{
  struct trace_seq *s ;
  struct trace_seq *p ;
  struct trace_event_raw_amdgpu_semaphore_request *field ;
  int ret ;
  enum print_line_t tmp ;
  {
  s = & iter->seq;
  p = & iter->tmp_seq;
  field = (struct trace_event_raw_amdgpu_semaphore_request *)iter->ent;
  ret = trace_raw_output_prep(iter, trace_event);
  if (ret != 1) {
    return ((enum print_line_t )ret);
  } else {
  }
  trace_seq_printf(s, "ring=%u, waiters=%d, addr=%010Lx\n", field->ring, field->waiters,
                   field->gpu_addr);
  tmp = trace_handle_return(s);
  return (tmp);
}
}
static int trace_event_define_fields_amdgpu_bo_create(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "struct amdgpu_bo *", "bo", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "pages", 16, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_cs(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "struct amdgpu_bo_list *", "bo_list", 8, 8,
                           0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "ring", 16, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "dw", 20, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "fences", 24, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_grab_id(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "u32", "vmid", 8, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "ring", 12, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_bo_map(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "struct amdgpu_bo *", "bo", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "long", "start", 16, 8, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "long", "last", 24, 8, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u64", "offset", 32, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "flags", 40, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_bo_unmap(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "struct amdgpu_bo *", "bo", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "long", "start", 16, 8, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "long", "last", 24, 8, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u64", "offset", 32, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "flags", 40, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_bo_update(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "u64", "soffset", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u64", "eoffset", 16, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "flags", 24, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_set_page(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "u64", "pe", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u64", "addr", 16, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "count", 24, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "incr", 28, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "flags", 32, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_vm_flush(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "u64", "pd_addr", 8, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "ring", 16, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "id", 20, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_bo_list_set(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "struct amdgpu_bo_list *", "list", 8, 8, 0,
                           0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "struct amdgpu_bo *", "bo", 16, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_fence_request(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "u32", "dev", 8, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "int", "ring", 12, 4, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "u32", "seqno", 16, 4, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int trace_event_define_fields_amdgpu_semaphore_request(struct trace_event_call *event_call )
{
  int ret ;
  {
  ret = trace_define_field(event_call, "int", "ring", 8, 4, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "signed", "waiters", 12, 4, 1, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = trace_define_field(event_call, "uint64_t", "gpu_addr", 16, 8, 0, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
void ldv_initialize_trace_event_class_127(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_set_page_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_133(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_bo_create_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_129(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_bo_unmap_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_126(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_flush_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_128(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_bo_update_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_130(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_bo_map_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_123(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_semaphore_request_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_125(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_bo_list_set_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_131(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_vm_grab_id_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_124(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_fence_request_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_initialize_trace_event_class_132(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(144UL);
  event_class_amdgpu_cs_group0 = (struct trace_event_call *)tmp;
  return;
}
}
void ldv_main_exported_124(void)
{
  void *ldvarg392 ;
  void *tmp ;
  enum trace_reg ldvarg393 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg392 = tmp;
  ldv_memset((void *)(& ldvarg393), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_124 == 1) {
    trace_event_reg(event_class_amdgpu_fence_request_group0, ldvarg393, ldvarg392);
    ldv_state_variable_124 = 1;
  } else {
  }
  goto ldv_48694;
  case 1: ;
  if (ldv_state_variable_124 == 1) {
    trace_event_raw_init(event_class_amdgpu_fence_request_group0);
    ldv_state_variable_124 = 1;
  } else {
  }
  goto ldv_48694;
  case 2: ;
  if (ldv_state_variable_124 == 1) {
    trace_event_define_fields_amdgpu_fence_request(event_class_amdgpu_fence_request_group0);
    ldv_state_variable_124 = 1;
  } else {
  }
  goto ldv_48694;
  default:
  ldv_stop();
  }
  ldv_48694: ;
  return;
}
}
void ldv_main_exported_131(void)
{
  void *ldvarg398 ;
  void *tmp ;
  enum trace_reg ldvarg399 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg398 = tmp;
  ldv_memset((void *)(& ldvarg399), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_131 == 1) {
    trace_event_reg(event_class_amdgpu_vm_grab_id_group0, ldvarg399, ldvarg398);
    ldv_state_variable_131 = 1;
  } else {
  }
  goto ldv_48704;
  case 1: ;
  if (ldv_state_variable_131 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_grab_id_group0);
    ldv_state_variable_131 = 1;
  } else {
  }
  goto ldv_48704;
  case 2: ;
  if (ldv_state_variable_131 == 1) {
    trace_event_define_fields_amdgpu_vm_grab_id(event_class_amdgpu_vm_grab_id_group0);
    ldv_state_variable_131 = 1;
  } else {
  }
  goto ldv_48704;
  default:
  ldv_stop();
  }
  ldv_48704: ;
  return;
}
}
void ldv_main_exported_130(void)
{
  enum trace_reg ldvarg1007 ;
  void *ldvarg1006 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg1006 = tmp;
  ldv_memset((void *)(& ldvarg1007), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_130 == 1) {
    trace_event_reg(event_class_amdgpu_vm_bo_map_group0, ldvarg1007, ldvarg1006);
    ldv_state_variable_130 = 1;
  } else {
  }
  goto ldv_48714;
  case 1: ;
  if (ldv_state_variable_130 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_bo_map_group0);
    ldv_state_variable_130 = 1;
  } else {
  }
  goto ldv_48714;
  case 2: ;
  if (ldv_state_variable_130 == 1) {
    trace_event_define_fields_amdgpu_vm_bo_map(event_class_amdgpu_vm_bo_map_group0);
    ldv_state_variable_130 = 1;
  } else {
  }
  goto ldv_48714;
  default:
  ldv_stop();
  }
  ldv_48714: ;
  return;
}
}
void ldv_main_exported_127(void)
{
  void *ldvarg0 ;
  void *tmp ;
  enum trace_reg ldvarg1 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg0 = tmp;
  ldv_memset((void *)(& ldvarg1), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_127 == 1) {
    trace_event_reg(event_class_amdgpu_vm_set_page_group0, ldvarg1, ldvarg0);
    ldv_state_variable_127 = 1;
  } else {
  }
  goto ldv_48724;
  case 1: ;
  if (ldv_state_variable_127 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_set_page_group0);
    ldv_state_variable_127 = 1;
  } else {
  }
  goto ldv_48724;
  case 2: ;
  if (ldv_state_variable_127 == 1) {
    trace_event_define_fields_amdgpu_vm_set_page(event_class_amdgpu_vm_set_page_group0);
    ldv_state_variable_127 = 1;
  } else {
  }
  goto ldv_48724;
  default:
  ldv_stop();
  }
  ldv_48724: ;
  return;
}
}
void ldv_main_exported_142(void)
{
  int ldvarg656 ;
  struct trace_event *ldvarg655 ;
  void *tmp ;
  struct trace_iterator *ldvarg657 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg655 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg657 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg656), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_142 == 1) {
    trace_raw_output_amdgpu_vm_grab_id(ldvarg657, ldvarg656, ldvarg655);
    ldv_state_variable_142 = 1;
  } else {
  }
  goto ldv_48735;
  default:
  ldv_stop();
  }
  ldv_48735: ;
  return;
}
}
void ldv_main_exported_139(void)
{
  int ldvarg298 ;
  struct trace_event *ldvarg297 ;
  void *tmp ;
  struct trace_iterator *ldvarg299 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg297 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg299 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg298), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_139 == 1) {
    trace_raw_output_amdgpu_vm_bo_update(ldvarg299, ldvarg298, ldvarg297);
    ldv_state_variable_139 = 1;
  } else {
  }
  goto ldv_48744;
  default:
  ldv_stop();
  }
  ldv_48744: ;
  return;
}
}
void ldv_main_exported_129(void)
{
  enum trace_reg ldvarg301 ;
  void *ldvarg300 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg300 = tmp;
  ldv_memset((void *)(& ldvarg301), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_129 == 1) {
    trace_event_reg(event_class_amdgpu_vm_bo_unmap_group0, ldvarg301, ldvarg300);
    ldv_state_variable_129 = 1;
  } else {
  }
  goto ldv_48752;
  case 1: ;
  if (ldv_state_variable_129 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_bo_unmap_group0);
    ldv_state_variable_129 = 1;
  } else {
  }
  goto ldv_48752;
  case 2: ;
  if (ldv_state_variable_129 == 1) {
    trace_event_define_fields_amdgpu_vm_bo_unmap(event_class_amdgpu_vm_bo_unmap_group0);
    ldv_state_variable_129 = 1;
  } else {
  }
  goto ldv_48752;
  default:
  ldv_stop();
  }
  ldv_48752: ;
  return;
}
}
void ldv_main_exported_143(void)
{
  int ldvarg1012 ;
  struct trace_event *ldvarg1011 ;
  void *tmp ;
  struct trace_iterator *ldvarg1013 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg1011 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg1013 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg1012), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_143 == 1) {
    trace_raw_output_amdgpu_cs(ldvarg1013, ldvarg1012, ldvarg1011);
    ldv_state_variable_143 = 1;
  } else {
  }
  goto ldv_48763;
  default:
  ldv_stop();
  }
  ldv_48763: ;
  return;
}
}
void ldv_main_exported_136(void)
{
  int ldvarg865 ;
  struct trace_event *ldvarg864 ;
  void *tmp ;
  struct trace_iterator *ldvarg866 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg864 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg866 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg865), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_136 == 1) {
    trace_raw_output_amdgpu_bo_list_set(ldvarg866, ldvarg865, ldvarg864);
    ldv_state_variable_136 = 1;
  } else {
  }
  goto ldv_48772;
  default:
  ldv_stop();
  }
  ldv_48772: ;
  return;
}
}
void ldv_main_exported_144(void)
{
  struct trace_event *ldvarg885 ;
  void *tmp ;
  int ldvarg886 ;
  struct trace_iterator *ldvarg887 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg885 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg887 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg886), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_144 == 1) {
    trace_raw_output_amdgpu_bo_create(ldvarg887, ldvarg886, ldvarg885);
    ldv_state_variable_144 = 1;
  } else {
  }
  goto ldv_48781;
  default:
  ldv_stop();
  }
  ldv_48781: ;
  return;
}
}
void ldv_main_exported_141(void)
{
  struct trace_event *ldvarg888 ;
  void *tmp ;
  struct trace_iterator *ldvarg890 ;
  void *tmp___0 ;
  int ldvarg889 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg888 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg890 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg889), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_141 == 1) {
    trace_raw_output_amdgpu_vm_bo_map(ldvarg890, ldvarg889, ldvarg888);
    ldv_state_variable_141 = 1;
  } else {
  }
  goto ldv_48790;
  default:
  ldv_stop();
  }
  ldv_48790: ;
  return;
}
}
void ldv_main_exported_125(void)
{
  void *ldvarg40 ;
  void *tmp ;
  enum trace_reg ldvarg41 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg40 = tmp;
  ldv_memset((void *)(& ldvarg41), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_125 == 1) {
    trace_event_reg(event_class_amdgpu_bo_list_set_group0, ldvarg41, ldvarg40);
    ldv_state_variable_125 = 1;
  } else {
  }
  goto ldv_48798;
  case 1: ;
  if (ldv_state_variable_125 == 1) {
    trace_event_raw_init(event_class_amdgpu_bo_list_set_group0);
    ldv_state_variable_125 = 1;
  } else {
  }
  goto ldv_48798;
  case 2: ;
  if (ldv_state_variable_125 == 1) {
    trace_event_define_fields_amdgpu_bo_list_set(event_class_amdgpu_bo_list_set_group0);
    ldv_state_variable_125 = 1;
  } else {
  }
  goto ldv_48798;
  default:
  ldv_stop();
  }
  ldv_48798: ;
  return;
}
}
void ldv_main_exported_133(void)
{
  void *ldvarg728 ;
  void *tmp ;
  enum trace_reg ldvarg729 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg728 = tmp;
  ldv_memset((void *)(& ldvarg729), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_133 == 1) {
    trace_event_reg(event_class_amdgpu_bo_create_group0, ldvarg729, ldvarg728);
    ldv_state_variable_133 = 1;
  } else {
  }
  goto ldv_48808;
  case 1: ;
  if (ldv_state_variable_133 == 1) {
    trace_event_raw_init(event_class_amdgpu_bo_create_group0);
    ldv_state_variable_133 = 1;
  } else {
  }
  goto ldv_48808;
  case 2: ;
  if (ldv_state_variable_133 == 1) {
    trace_event_define_fields_amdgpu_bo_create(event_class_amdgpu_bo_create_group0);
    ldv_state_variable_133 = 1;
  } else {
  }
  goto ldv_48808;
  default:
  ldv_stop();
  }
  ldv_48808: ;
  return;
}
}
void ldv_main_exported_126(void)
{
  enum trace_reg ldvarg445 ;
  void *ldvarg444 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg444 = tmp;
  ldv_memset((void *)(& ldvarg445), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_126 == 1) {
    trace_event_reg(event_class_amdgpu_vm_flush_group0, ldvarg445, ldvarg444);
    ldv_state_variable_126 = 1;
  } else {
  }
  goto ldv_48818;
  case 1: ;
  if (ldv_state_variable_126 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_flush_group0);
    ldv_state_variable_126 = 1;
  } else {
  }
  goto ldv_48818;
  case 2: ;
  if (ldv_state_variable_126 == 1) {
    trace_event_define_fields_amdgpu_vm_flush(event_class_amdgpu_vm_flush_group0);
    ldv_state_variable_126 = 1;
  } else {
  }
  goto ldv_48818;
  default:
  ldv_stop();
  }
  ldv_48818: ;
  return;
}
}
void ldv_main_exported_123(void)
{
  void *ldvarg733 ;
  void *tmp ;
  enum trace_reg ldvarg734 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg733 = tmp;
  ldv_memset((void *)(& ldvarg734), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_123 == 1) {
    trace_event_reg(event_class_amdgpu_semaphore_request_group0, ldvarg734, ldvarg733);
    ldv_state_variable_123 = 1;
  } else {
  }
  goto ldv_48828;
  case 1: ;
  if (ldv_state_variable_123 == 1) {
    trace_event_raw_init(event_class_amdgpu_semaphore_request_group0);
    ldv_state_variable_123 = 1;
  } else {
  }
  goto ldv_48828;
  case 2: ;
  if (ldv_state_variable_123 == 1) {
    trace_event_define_fields_amdgpu_semaphore_request(event_class_amdgpu_semaphore_request_group0);
    ldv_state_variable_123 = 1;
  } else {
  }
  goto ldv_48828;
  default:
  ldv_stop();
  }
  ldv_48828: ;
  return;
}
}
void ldv_main_exported_128(void)
{
  enum trace_reg ldvarg901 ;
  void *ldvarg900 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg900 = tmp;
  ldv_memset((void *)(& ldvarg901), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_128 == 1) {
    trace_event_reg(event_class_amdgpu_vm_bo_update_group0, ldvarg901, ldvarg900);
    ldv_state_variable_128 = 1;
  } else {
  }
  goto ldv_48838;
  case 1: ;
  if (ldv_state_variable_128 == 1) {
    trace_event_raw_init(event_class_amdgpu_vm_bo_update_group0);
    ldv_state_variable_128 = 1;
  } else {
  }
  goto ldv_48838;
  case 2: ;
  if (ldv_state_variable_128 == 1) {
    trace_event_define_fields_amdgpu_vm_bo_update(event_class_amdgpu_vm_bo_update_group0);
    ldv_state_variable_128 = 1;
  } else {
  }
  goto ldv_48838;
  default:
  ldv_stop();
  }
  ldv_48838: ;
  return;
}
}
void ldv_main_exported_134(void)
{
  struct trace_iterator *ldvarg937 ;
  void *tmp ;
  int ldvarg936 ;
  struct trace_event *ldvarg935 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(8560UL);
  ldvarg937 = (struct trace_iterator *)tmp;
  tmp___0 = ldv_init_zalloc(48UL);
  ldvarg935 = (struct trace_event *)tmp___0;
  ldv_memset((void *)(& ldvarg936), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_134 == 1) {
    trace_raw_output_amdgpu_semaphore_request(ldvarg937, ldvarg936, ldvarg935);
    ldv_state_variable_134 = 1;
  } else {
  }
  goto ldv_48849;
  default:
  ldv_stop();
  }
  ldv_48849: ;
  return;
}
}
void ldv_main_exported_138(void)
{
  struct trace_iterator *ldvarg169 ;
  void *tmp ;
  int ldvarg168 ;
  struct trace_event *ldvarg167 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(8560UL);
  ldvarg169 = (struct trace_iterator *)tmp;
  tmp___0 = ldv_init_zalloc(48UL);
  ldvarg167 = (struct trace_event *)tmp___0;
  ldv_memset((void *)(& ldvarg168), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_138 == 1) {
    trace_raw_output_amdgpu_vm_set_page(ldvarg169, ldvarg168, ldvarg167);
    ldv_state_variable_138 = 1;
  } else {
  }
  goto ldv_48858;
  default:
  ldv_stop();
  }
  ldv_48858: ;
  return;
}
}
void ldv_main_exported_135(void)
{
  struct trace_iterator *ldvarg345 ;
  void *tmp ;
  int ldvarg344 ;
  struct trace_event *ldvarg343 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(8560UL);
  ldvarg345 = (struct trace_iterator *)tmp;
  tmp___0 = ldv_init_zalloc(48UL);
  ldvarg343 = (struct trace_event *)tmp___0;
  ldv_memset((void *)(& ldvarg344), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_135 == 1) {
    trace_raw_output_amdgpu_fence_request(ldvarg345, ldvarg344, ldvarg343);
    ldv_state_variable_135 = 1;
  } else {
  }
  goto ldv_48867;
  default:
  ldv_stop();
  }
  ldv_48867: ;
  return;
}
}
void ldv_main_exported_137(void)
{
  struct trace_event *ldvarg207 ;
  void *tmp ;
  int ldvarg208 ;
  struct trace_iterator *ldvarg209 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg207 = (struct trace_event *)tmp;
  tmp___0 = ldv_init_zalloc(8560UL);
  ldvarg209 = (struct trace_iterator *)tmp___0;
  ldv_memset((void *)(& ldvarg208), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_137 == 1) {
    trace_raw_output_amdgpu_vm_flush(ldvarg209, ldvarg208, ldvarg207);
    ldv_state_variable_137 = 1;
  } else {
  }
  goto ldv_48876;
  default:
  ldv_stop();
  }
  ldv_48876: ;
  return;
}
}
void ldv_main_exported_132(void)
{
  void *ldvarg1106 ;
  void *tmp ;
  enum trace_reg ldvarg1107 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg1106 = tmp;
  ldv_memset((void *)(& ldvarg1107), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_132 == 1) {
    trace_event_reg(event_class_amdgpu_cs_group0, ldvarg1107, ldvarg1106);
    ldv_state_variable_132 = 1;
  } else {
  }
  goto ldv_48884;
  case 1: ;
  if (ldv_state_variable_132 == 1) {
    trace_event_raw_init(event_class_amdgpu_cs_group0);
    ldv_state_variable_132 = 1;
  } else {
  }
  goto ldv_48884;
  case 2: ;
  if (ldv_state_variable_132 == 1) {
    trace_event_define_fields_amdgpu_cs(event_class_amdgpu_cs_group0);
    ldv_state_variable_132 = 1;
  } else {
  }
  goto ldv_48884;
  default:
  ldv_stop();
  }
  ldv_48884: ;
  return;
}
}
void ldv_main_exported_140(void)
{
  struct trace_iterator *ldvarg391 ;
  void *tmp ;
  struct trace_event *ldvarg389 ;
  void *tmp___0 ;
  int ldvarg390 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(8560UL);
  ldvarg391 = (struct trace_iterator *)tmp;
  tmp___0 = ldv_init_zalloc(48UL);
  ldvarg389 = (struct trace_event *)tmp___0;
  ldv_memset((void *)(& ldvarg390), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_140 == 1) {
    trace_raw_output_amdgpu_vm_bo_unmap(ldvarg391, ldvarg390, ldvarg389);
    ldv_state_variable_140 = 1;
  } else {
  }
  goto ldv_48895;
  default:
  ldv_stop();
  }
  ldv_48895: ;
  return;
}
}
bool ldv_queue_work_on_341(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_342(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_343(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_344(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_345(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static bool IS_ERR(void const *ptr ) ;
bool ldv_queue_work_on_355(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_357(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_356(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_359(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_358(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static int backlight_update_status(struct backlight_device *bd )
{
  int ret ;
  {
  ret = -2;
  mutex_lock_nested(& bd->update_lock, 0U);
  if ((unsigned long )bd->ops != (unsigned long )((struct backlight_ops const *)0) && (unsigned long )(bd->ops)->update_status != (unsigned long )((int (* )(struct backlight_device * ))0)) {
    ret = (*((bd->ops)->update_status))(bd);
  } else {
  }
  mutex_unlock(& bd->update_lock);
  return (ret);
}
}
extern struct backlight_device *backlight_device_register(char const * , struct device * ,
                                                          void * , struct backlight_ops const * ,
                                                          struct backlight_properties const * ) ;
extern void backlight_device_unregister(struct backlight_device * ) ;
__inline static void *bl_get_data(struct backlight_device *bl_dev )
{
  void *tmp ;
  {
  tmp = dev_get_drvdata((struct device const *)(& bl_dev->dev));
  return (tmp);
}
}
extern bool drm_edid_is_valid(struct edid * ) ;
u8 amdgpu_atombios_encoder_get_backlight_level(struct amdgpu_encoder *amdgpu_encoder ) ;
void amdgpu_atombios_encoder_set_backlight_level(struct amdgpu_encoder *amdgpu_encoder ,
                                                 u8 level ) ;
void amdgpu_atombios_encoder_fini_backlight(struct amdgpu_encoder *amdgpu_encoder ) ;
bool amdgpu_atombios_encoder_is_digital(struct drm_encoder *encoder ) ;
bool amdgpu_atombios_encoder_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode const *mode ,
                                        struct drm_display_mode *adjusted_mode ) ;
void amdgpu_atombios_encoder_dpms(struct drm_encoder *encoder , int mode ) ;
void amdgpu_atombios_encoder_set_crtc_source(struct drm_encoder *encoder ) ;
void amdgpu_atombios_encoder_init_dig(struct amdgpu_device *adev ) ;
enum drm_connector_status amdgpu_atombios_encoder_dac_detect(struct drm_encoder *encoder ,
                                                             struct drm_connector *connector ) ;
enum drm_connector_status amdgpu_atombios_encoder_dig_detect(struct drm_encoder *encoder ,
                                                             struct drm_connector *connector ) ;
struct amdgpu_encoder_atom_dig *amdgpu_atombios_encoder_get_lcd_info(struct amdgpu_encoder *encoder ) ;
struct amdgpu_encoder_atom_dig *amdgpu_atombios_encoder_get_dig_info(struct amdgpu_encoder *amdgpu_encoder ) ;
static u8 amdgpu_atombios_encoder_get_backlight_level_from_reg(struct amdgpu_device *adev )
{
  u8 backlight_level ;
  u32 bios_2_scratch ;
  {
  bios_2_scratch = amdgpu_mm_rreg(adev, 1483U, 0);
  backlight_level = (u8 )(((long )bios_2_scratch & 65280L) >> 8);
  return (backlight_level);
}
}
static void amdgpu_atombios_encoder_set_backlight_level_to_reg(struct amdgpu_device *adev ,
                                                               u8 backlight_level )
{
  u32 bios_2_scratch ;
  {
  bios_2_scratch = amdgpu_mm_rreg(adev, 1483U, 0);
  bios_2_scratch = bios_2_scratch & 4294902015U;
  bios_2_scratch = (u32 )((((int )backlight_level << 8) & 65535) | (int )bios_2_scratch);
  amdgpu_mm_wreg(adev, 1483U, bios_2_scratch, 0);
  return;
}
}
u8 amdgpu_atombios_encoder_get_backlight_level(struct amdgpu_encoder *amdgpu_encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u8 tmp ;
  {
  dev = amdgpu_encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if (((int )adev->mode_info.firmware_flags & 32) == 0) {
    return (0U);
  } else {
  }
  tmp = amdgpu_atombios_encoder_get_backlight_level_from_reg(adev);
  return (tmp);
}
}
void amdgpu_atombios_encoder_set_backlight_level(struct amdgpu_encoder *amdgpu_encoder ,
                                                 u8 level )
{
  struct drm_encoder *encoder ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder_atom_dig *dig ;
  {
  encoder = & amdgpu_encoder->base;
  dev = amdgpu_encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if (((int )adev->mode_info.firmware_flags & 32) == 0) {
    return;
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L && (unsigned long )amdgpu_encoder->enc_priv != (unsigned long )((void *)0)) {
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    dig->backlight_level = level;
    amdgpu_atombios_encoder_set_backlight_level_to_reg(adev, (int )dig->backlight_level);
    switch (amdgpu_encoder->encoder_id) {
    case 30U: ;
    case 31U: ;
    case 32U: ;
    case 33U: ;
    if ((unsigned int )dig->backlight_level == 0U) {
      amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 2, 0, 0);
    } else {
      amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 4, 0, 0);
      amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 3, 0, 0);
    }
    goto ldv_47874;
    default: ;
    goto ldv_47874;
    }
    ldv_47874: ;
  } else {
  }
  return;
}
}
static u8 amdgpu_atombios_encoder_backlight_level(struct backlight_device *bd )
{
  u8 level ;
  {
  if (bd->props.brightness < 0) {
    level = 0U;
  } else
  if (bd->props.brightness > 255) {
    level = 255U;
  } else {
    level = (u8 )bd->props.brightness;
  }
  return (level);
}
}
static int amdgpu_atombios_encoder_update_backlight_status(struct backlight_device *bd )
{
  struct amdgpu_backlight_privdata *pdata ;
  void *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  u8 tmp___0 ;
  {
  tmp = bl_get_data(bd);
  pdata = (struct amdgpu_backlight_privdata *)tmp;
  amdgpu_encoder = pdata->encoder;
  tmp___0 = amdgpu_atombios_encoder_backlight_level(bd);
  amdgpu_atombios_encoder_set_backlight_level(amdgpu_encoder, (int )tmp___0);
  return (0);
}
}
static int amdgpu_atombios_encoder_get_backlight_brightness(struct backlight_device *bd )
{
  struct amdgpu_backlight_privdata *pdata ;
  void *tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u8 tmp___0 ;
  {
  tmp = bl_get_data(bd);
  pdata = (struct amdgpu_backlight_privdata *)tmp;
  amdgpu_encoder = pdata->encoder;
  dev = amdgpu_encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp___0 = amdgpu_atombios_encoder_get_backlight_level_from_reg(adev);
  return ((int )tmp___0);
}
}
static struct backlight_ops const amdgpu_atombios_encoder_backlight_ops = {0U, & amdgpu_atombios_encoder_update_backlight_status, & amdgpu_atombios_encoder_get_backlight_brightness,
    0};
void amdgpu_atombios_encoder_init_backlight(struct amdgpu_encoder *amdgpu_encoder ,
                                            struct drm_connector *drm_connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct backlight_device *bd ;
  struct backlight_properties props ;
  struct amdgpu_backlight_privdata *pdata ;
  struct amdgpu_encoder_atom_dig *dig ;
  u8 backlight_level ;
  char bl_name[16U] ;
  void *tmp ;
  bool tmp___0 ;
  {
  dev = amdgpu_encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((unsigned int )(adev->pdev)->subsystem_vendor == 4203U && (unsigned int )(adev->pdev)->device == 26433U) {
    return;
  } else {
  }
  if ((unsigned long )amdgpu_encoder->enc_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  if (! adev->is_atom_bios) {
    return;
  } else {
  }
  if (((int )adev->mode_info.firmware_flags & 32) == 0) {
    return;
  } else {
  }
  tmp = kmalloc(16UL, 208U);
  pdata = (struct amdgpu_backlight_privdata *)tmp;
  if ((unsigned long )pdata == (unsigned long )((struct amdgpu_backlight_privdata *)0)) {
    drm_err("Memory allocation failed\n");
    goto error;
  } else {
  }
  memset((void *)(& props), 0, 24UL);
  props.max_brightness = 255;
  props.type = 1;
  snprintf((char *)(& bl_name), 16UL, "amdgpu_bl%d", (dev->primary)->index);
  bd = backlight_device_register((char const *)(& bl_name), drm_connector->kdev,
                                 (void *)pdata, & amdgpu_atombios_encoder_backlight_ops,
                                 (struct backlight_properties const *)(& props));
  tmp___0 = IS_ERR((void const *)bd);
  if ((int )tmp___0) {
    drm_err("Backlight registration failed\n");
    goto error;
  } else {
  }
  pdata->encoder = amdgpu_encoder;
  backlight_level = amdgpu_atombios_encoder_get_backlight_level_from_reg(adev);
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  dig->bl_dev = bd;
  bd->props.brightness = amdgpu_atombios_encoder_get_backlight_brightness(bd);
  bd->props.power = 0;
  backlight_update_status(bd);
  printk("\016[drm] amdgpu atom DIG backlight initialized\n");
  return;
  error:
  kfree((void const *)pdata);
  return;
}
}
void amdgpu_atombios_encoder_fini_backlight(struct amdgpu_encoder *amdgpu_encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct backlight_device *bd ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct amdgpu_legacy_backlight_privdata *pdata ;
  void *tmp ;
  {
  dev = amdgpu_encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  bd = (struct backlight_device *)0;
  if ((unsigned long )amdgpu_encoder->enc_priv == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  if (! adev->is_atom_bios) {
    return;
  } else {
  }
  if (((int )adev->mode_info.firmware_flags & 32) == 0) {
    return;
  } else {
  }
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  bd = dig->bl_dev;
  dig->bl_dev = (struct backlight_device *)0;
  if ((unsigned long )bd != (unsigned long )((struct backlight_device *)0)) {
    tmp = bl_get_data(bd);
    pdata = (struct amdgpu_legacy_backlight_privdata *)tmp;
    backlight_device_unregister(bd);
    kfree((void const *)pdata);
    printk("\016[drm] amdgpu atom LVDS backlight unloaded\n");
  } else {
  }
  return;
}
}
bool amdgpu_atombios_encoder_is_digital(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  switch (amdgpu_encoder->encoder_id) {
  case 20U: ;
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  return (1);
  default: ;
  return (0);
  }
}
}
bool amdgpu_atombios_encoder_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode const *mode ,
                                        struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  u16 tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_encoder_set_active_device(encoder);
  drm_mode_set_crtcinfo(adjusted_mode, 0);
  if (((unsigned int )mode->flags & 16U) != 0U && (int )mode->crtc_vsync_start < (int )mode->crtc_vdisplay + 2) {
    adjusted_mode->crtc_vsync_start = adjusted_mode->crtc_vdisplay + 2;
  } else {
  }
  if (((long )amdgpu_encoder->active_device & 34L) != 0L) {
    amdgpu_panel_mode_fixup(encoder, adjusted_mode);
  } else
  if ((unsigned int )amdgpu_encoder->rmx_type != 0U) {
    amdgpu_panel_mode_fixup(encoder, adjusted_mode);
  } else {
  }
  if (((long )amdgpu_encoder->active_device & 3818L) != 0L) {
    tmp = amdgpu_get_connector_for_encoder(encoder);
    connector = tmp;
    amdgpu_atombios_dp_set_link_config(connector, (struct drm_display_mode const *)adjusted_mode);
  } else {
    tmp___0 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    if ((unsigned int )tmp___0 != 0U) {
      tmp = amdgpu_get_connector_for_encoder(encoder);
      connector = tmp;
      amdgpu_atombios_dp_set_link_config(connector, (struct drm_display_mode const *)adjusted_mode);
    } else {
    }
  }
  return (1);
}
}
static void amdgpu_atombios_encoder_setup_dac(struct drm_encoder *encoder , int action )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  DAC_ENCODER_CONTROL_PARAMETERS args ;
  int index ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  index = 0;
  memset((void *)(& args), 0, 4UL);
  switch (amdgpu_encoder->encoder_id) {
  case 4U: ;
  case 21U:
  index = 24;
  goto ldv_47949;
  case 5U: ;
  case 22U:
  index = 25;
  goto ldv_47949;
  }
  ldv_47949:
  args.ucAction = (UCHAR )action;
  args.ucDacStandard = 1U;
  args.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
static u8 amdgpu_atombios_encoder_get_bpc(struct drm_encoder *encoder )
{
  int bpc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  bpc = 8;
  if ((unsigned long )encoder->crtc != (unsigned long )((struct drm_crtc *)0)) {
    __mptr = (struct drm_crtc const *)encoder->crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
    bpc = amdgpu_crtc->bpc;
  } else {
  }
  switch (bpc) {
  case 0: ;
  return (0U);
  case 6: ;
  return (1U);
  case 8: ;
  default: ;
  return (2U);
  case 10: ;
  return (3U);
  case 12: ;
  return (4U);
  case 16: ;
  return (5U);
  }
}
}
static void amdgpu_atombios_encoder_setup_dvo(struct drm_encoder *encoder , int action )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  union dvo_encoder_control args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  index = 8;
  memset((void *)(& args), 0, 16UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1:
  args.ext_tmds.sXTmdsEncoder.ucEnable = (UCHAR )action;
  tmp___1 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
  if ((int )tmp___1) {
    args.ext_tmds.sXTmdsEncoder.ucMisc = (UCHAR )((unsigned int )args.ext_tmds.sXTmdsEncoder.ucMisc | 1U);
  } else {
  }
  args.ext_tmds.sXTmdsEncoder.ucMisc = (UCHAR )((unsigned int )args.ext_tmds.sXTmdsEncoder.ucMisc | 2U);
  goto ldv_47986;
  case 2:
  args.dvo.sDVOEncoder.ucAction = (UCHAR )action;
  args.dvo.sDVOEncoder.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  args.dvo.sDVOEncoder.ucDeviceType = 3U;
  tmp___2 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
  if ((int )tmp___2) {
    args.dvo.sDVOEncoder.usDevAttr.sDigAttrib.ucAttribute = (UCHAR )((unsigned int )args.dvo.sDVOEncoder.usDevAttr.sDigAttrib.ucAttribute | 1U);
  } else {
  }
  goto ldv_47986;
  case 3:
  args.dvo_v3.ucAction = (UCHAR )action;
  args.dvo_v3.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  args.dvo_v3.ucDVOConfig = 0U;
  goto ldv_47986;
  case 4:
  args.dvo_v4.ucAction = (UCHAR )action;
  args.dvo_v4.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  args.dvo_v4.ucDVOConfig = 0U;
  args.dvo_v4.ucBitPerColor = amdgpu_atombios_encoder_get_bpc(encoder);
  goto ldv_47986;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_47986;
  }
  ldv_47986: ;
  goto ldv_47991;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_47991;
  }
  ldv_47991:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
int amdgpu_atombios_encoder_get_encoder_mode(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  u16 tmp ;
  struct drm_connector const *__mptr___0 ;
  struct edid *tmp___0 ;
  bool tmp___1 ;
  struct edid *tmp___2 ;
  bool tmp___3 ;
  struct edid *tmp___4 ;
  bool tmp___5 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
  if ((unsigned int )tmp != 0U) {
    return (0);
  } else {
  }
  if (amdgpu_encoder->encoder_id == 11U || amdgpu_encoder->encoder_id == 20U) {
    return (16);
  } else {
  }
  connector = amdgpu_get_connector_for_encoder(encoder);
  if ((unsigned long )connector == (unsigned long )((struct drm_connector *)0)) {
    connector = amdgpu_get_connector_for_encoder_init(encoder);
  } else {
  }
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  switch (connector->connector_type) {
  case 2: ;
  case 12: ;
  if (amdgpu_audio != 0) {
    if ((int )amdgpu_connector->use_digital && (unsigned int )amdgpu_connector->audio == 1U) {
      return (3);
    } else {
      tmp___0 = amdgpu_connector_edid(connector);
      tmp___1 = drm_detect_hdmi_monitor(tmp___0);
      if ((int )tmp___1 && (unsigned int )amdgpu_connector->audio == 2U) {
        return (3);
      } else
      if ((int )amdgpu_connector->use_digital) {
        return (2);
      } else {
        return (15);
      }
    }
  } else
  if ((int )amdgpu_connector->use_digital) {
    return (2);
  } else {
    return (15);
  }
  case 3: ;
  case 11: ;
  default: ;
  if (amdgpu_audio != 0) {
    if ((unsigned int )amdgpu_connector->audio == 1U) {
      return (3);
    } else {
      tmp___2 = amdgpu_connector_edid(connector);
      tmp___3 = drm_detect_hdmi_monitor(tmp___2);
      if ((int )tmp___3 && (unsigned int )amdgpu_connector->audio == 2U) {
        return (3);
      } else {
        return (2);
      }
    }
  } else {
    return (2);
  }
  case 7: ;
  return (1);
  case 10:
  dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  if ((unsigned int )dig_connector->dp_sink_type == 19U || (unsigned int )dig_connector->dp_sink_type == 20U) {
    return (0);
  } else
  if (amdgpu_audio != 0) {
    if ((unsigned int )amdgpu_connector->audio == 1U) {
      return (3);
    } else {
      tmp___4 = amdgpu_connector_edid(connector);
      tmp___5 = drm_detect_hdmi_monitor(tmp___4);
      if ((int )tmp___5 && (unsigned int )amdgpu_connector->audio == 2U) {
        return (3);
      } else {
        return (2);
      }
    }
  } else {
    return (2);
  }
  case 14: ;
  return (0);
  case 4: ;
  case 1: ;
  return (15);
  case 5: ;
  case 6: ;
  case 9: ;
  return (13);
  }
}
}
void amdgpu_atombios_encoder_setup_dig_encoder(struct drm_encoder *encoder , int action ,
                                               int panel_mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  union dig_encoder_control args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  int dp_clock ;
  int dp_lane_count ;
  int hpd_id ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  bool tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  bool tmp___5 ;
  int tmp___6 ;
  bool tmp___7 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  index = 4;
  dp_clock = 0;
  dp_lane_count = 0;
  hpd_id = 255;
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
    dp_clock = dig_connector->dp_clock;
    dp_lane_count = dig_connector->dp_lane_count;
    hpd_id = (int )amdgpu_connector->hpd.hpd;
  } else {
  }
  if (dig->dig_encoder == -1) {
    return;
  } else {
  }
  memset((void *)(& args), 0, 8UL);
  tmp___0 = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                         & crev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1:
  args.v1.ucAction = (UCHAR )action;
  args.v1.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  if (action == 16) {
    args.v3.__annonCompField86.ucPanelMode = (UCHAR )panel_mode;
  } else {
    tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    args.v1.ucEncoderMode = (UCHAR )tmp___2;
  }
  if ((unsigned int )args.v1.ucEncoderMode == 0U || (unsigned int )args.v1.ucEncoderMode == 5U) {
    args.v1.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___3 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___3) {
      args.v1.ucLaneNum = 8U;
    } else {
      args.v1.ucLaneNum = 4U;
    }
  }
  if (((unsigned int )args.v1.ucEncoderMode == 0U || (unsigned int )args.v1.ucEncoderMode == 5U) && dp_clock == 270000) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 1U);
  } else {
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U:
  args.v1.ucConfig = 0U;
  goto ldv_48048;
  case 32U: ;
  case 31U:
  args.v1.ucConfig = 8U;
  goto ldv_48048;
  case 33U:
  args.v1.ucConfig = 16U;
  goto ldv_48048;
  }
  ldv_48048: ;
  if ((int )dig->linkb) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 4U);
  } else {
    args.v1.ucConfig = args.v1.ucConfig;
  }
  goto ldv_48052;
  case 2: ;
  case 3:
  args.v3.ucAction = (UCHAR )action;
  args.v3.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  if (action == 16) {
    args.v3.__annonCompField86.ucPanelMode = (UCHAR )panel_mode;
  } else {
    tmp___4 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    args.v3.__annonCompField86.ucEncoderMode = (UCHAR )tmp___4;
  }
  if ((unsigned int )args.v3.__annonCompField86.ucEncoderMode == 0U || (unsigned int )args.v3.__annonCompField86.ucEncoderMode == 5U) {
    args.v3.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___5 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___5) {
      args.v3.ucLaneNum = 8U;
    } else {
      args.v3.ucLaneNum = 4U;
    }
  }
  if (((unsigned int )args.v3.__annonCompField86.ucEncoderMode == 0U || (unsigned int )args.v3.__annonCompField86.ucEncoderMode == 5U) && dp_clock == 270000) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 1U);
  } else {
  }
  args.v3.acConfig.ucDigSel = (unsigned char )dig->dig_encoder;
  args.v3.ucBitPerColor = amdgpu_atombios_encoder_get_bpc(encoder);
  goto ldv_48052;
  case 4:
  args.v4.ucAction = (UCHAR )action;
  args.v4.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  if (action == 16) {
    args.v4.__annonCompField88.ucPanelMode = (UCHAR )panel_mode;
  } else {
    tmp___6 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    args.v4.__annonCompField88.ucEncoderMode = (UCHAR )tmp___6;
  }
  if ((unsigned int )args.v4.__annonCompField88.ucEncoderMode == 0U || (unsigned int )args.v4.__annonCompField88.ucEncoderMode == 5U) {
    args.v4.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___7 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___7) {
      args.v4.ucLaneNum = 8U;
    } else {
      args.v4.ucLaneNum = 4U;
    }
  }
  if ((unsigned int )args.v4.__annonCompField88.ucEncoderMode == 0U || (unsigned int )args.v4.__annonCompField88.ucEncoderMode == 5U) {
    if (dp_clock == 540000) {
      args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 2U);
    } else
    if (dp_clock == 324000) {
      args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 3U);
    } else
    if (dp_clock == 270000) {
      args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 1U);
    } else {
      args.v1.ucConfig = args.v1.ucConfig;
    }
  } else {
  }
  args.v4.__annonCompField87.acConfig.ucDigSel = (unsigned char )dig->dig_encoder;
  args.v4.ucBitPerColor = amdgpu_atombios_encoder_get_bpc(encoder);
  if (hpd_id == 255) {
    args.v4.ucHPD_ID = 0U;
  } else {
    args.v4.ucHPD_ID = (unsigned int )((UCHAR )hpd_id) + 1U;
  }
  goto ldv_48052;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_48052;
  }
  ldv_48052: ;
  goto ldv_48057;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_48057;
  }
  ldv_48057:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_encoder_setup_dig_transmitter(struct drm_encoder *encoder , int action ,
                                                   uint8_t lane_num , uint8_t lane_set )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  union dig_transmitter_control args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  bool is_dp ;
  int pll_id ;
  int dp_clock ;
  int dp_lane_count ;
  int connector_object_id ;
  int igp_lane_info ;
  int dig_encoder ;
  int hpd_id ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___1 ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  bool tmp___6 ;
  bool tmp___7 ;
  bool tmp___8 ;
  bool tmp___9 ;
  bool tmp___10 ;
  bool tmp___11 ;
  bool tmp___12 ;
  bool tmp___13 ;
  bool tmp___14 ;
  bool tmp___15 ;
  int tmp___16 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  index = 0;
  is_dp = 0;
  pll_id = 0;
  dp_clock = 0;
  dp_lane_count = 0;
  connector_object_id = 0;
  igp_lane_info = 0;
  dig_encoder = dig->dig_encoder;
  hpd_id = 255;
  if (action == 7) {
    connector = amdgpu_get_connector_for_encoder_init(encoder);
    dig_encoder = 0;
  } else {
    connector = amdgpu_get_connector_for_encoder(encoder);
  }
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
    hpd_id = (int )amdgpu_connector->hpd.hpd;
    dp_clock = dig_connector->dp_clock;
    dp_lane_count = dig_connector->dp_lane_count;
    connector_object_id = (int )amdgpu_connector->connector_object_id & 255;
  } else {
  }
  if ((unsigned long )encoder->crtc != (unsigned long )((struct drm_crtc *)0)) {
    __mptr___1 = (struct drm_crtc const *)encoder->crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___1;
    pll_id = (int )amdgpu_crtc->pll_id;
  } else {
  }
  if (dig_encoder == -1) {
    return;
  } else {
  }
  tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  if (tmp == 0) {
    is_dp = 1;
  } else {
    tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___0 == 5) {
      is_dp = 1;
    } else {
    }
  }
  memset((void *)(& args), 0, 12UL);
  switch (amdgpu_encoder->encoder_id) {
  case 20U:
  index = 26;
  goto ldv_48098;
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U:
  index = 76;
  goto ldv_48098;
  case 31U:
  index = 77;
  goto ldv_48098;
  }
  ldv_48098:
  tmp___1 = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                         & crev);
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1:
  args.v1.ucAction = (UCHAR )action;
  if (action == 7) {
    args.v1.__annonCompField89.usInitInfo = (unsigned short )connector_object_id;
  } else
  if (action == 11) {
    args.v1.__annonCompField89.asMode.ucLaneSel = lane_num;
    args.v1.__annonCompField89.asMode.ucLaneSet = lane_set;
  } else
  if ((int )is_dp) {
    args.v1.__annonCompField89.usPixelClock = (unsigned short )(dp_clock / 10);
  } else {
    tmp___3 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___3) {
      args.v1.__annonCompField89.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 20U);
    } else {
      args.v1.__annonCompField89.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
    }
  }
  args.v1.ucConfig = 0U;
  if (dig_encoder != 0) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 8U);
  } else {
    args.v1.ucConfig = args.v1.ucConfig;
  }
  if ((adev->flags & 131072UL) != 0UL && amdgpu_encoder->encoder_id == 30U) {
    if ((int )is_dp) {
      goto _L;
    } else {
      tmp___4 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
      if (tmp___4) {
        tmp___5 = 0;
      } else {
        tmp___5 = 1;
      }
      if (tmp___5) {
        _L:
        if (igp_lane_info & 1) {
          args.v1.ucConfig = args.v1.ucConfig;
        } else
        if ((igp_lane_info & 2) != 0) {
          args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 64U);
        } else
        if ((igp_lane_info & 4) != 0) {
          args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 128U);
        } else
        if ((igp_lane_info & 8) != 0) {
          args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 192U);
        } else {
        }
      } else
      if ((igp_lane_info & 3) != 0) {
        args.v1.ucConfig = args.v1.ucConfig;
      } else
      if ((igp_lane_info & 12) != 0) {
        args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 128U);
      } else {
      }
    }
  } else {
  }
  if ((int )dig->linkb) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 4U);
  } else {
    args.v1.ucConfig = args.v1.ucConfig;
  }
  if ((int )is_dp) {
    args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 2U);
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    if ((int )dig->coherent_mode) {
      args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 2U);
    } else {
    }
    tmp___6 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___6) {
      args.v1.ucConfig = (UCHAR )((unsigned int )args.v1.ucConfig | 1U);
    } else {
    }
  } else {
  }
  goto ldv_48106;
  case 2:
  args.v2.ucAction = (UCHAR )action;
  if (action == 7) {
    args.v2.__annonCompField90.usInitInfo = (unsigned short )connector_object_id;
  } else
  if (action == 11) {
    args.v2.__annonCompField90.asMode.ucLaneSel = lane_num;
    args.v2.__annonCompField90.asMode.ucLaneSet = lane_set;
  } else
  if ((int )is_dp) {
    args.v2.__annonCompField90.usPixelClock = (unsigned short )(dp_clock / 10);
  } else {
    tmp___7 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___7) {
      args.v2.__annonCompField90.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 20U);
    } else {
      args.v2.__annonCompField90.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
    }
  }
  args.v2.acConfig.ucEncoderSel = (unsigned char )dig_encoder;
  if ((int )dig->linkb) {
    args.v2.acConfig.ucLinkSel = 1U;
  } else {
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U:
  args.v2.acConfig.ucTransmitterSel = 0U;
  goto ldv_48109;
  case 32U:
  args.v2.acConfig.ucTransmitterSel = 1U;
  goto ldv_48109;
  case 33U:
  args.v2.acConfig.ucTransmitterSel = 2U;
  goto ldv_48109;
  }
  ldv_48109: ;
  if ((int )is_dp) {
    args.v2.acConfig.fCoherentMode = 1U;
    args.v2.acConfig.fDPConnector = 1U;
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    if ((int )dig->coherent_mode) {
      args.v2.acConfig.fCoherentMode = 1U;
    } else {
    }
    tmp___8 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___8) {
      args.v2.acConfig.fDualLinkConnector = 1U;
    } else {
    }
  } else {
  }
  goto ldv_48106;
  case 3:
  args.v3.ucAction = (UCHAR )action;
  if (action == 7) {
    args.v3.__annonCompField91.usInitInfo = (unsigned short )connector_object_id;
  } else
  if (action == 11) {
    args.v3.__annonCompField91.asMode.ucLaneSel = lane_num;
    args.v3.__annonCompField91.asMode.ucLaneSet = lane_set;
  } else
  if ((int )is_dp) {
    args.v3.__annonCompField91.usPixelClock = (unsigned short )(dp_clock / 10);
  } else {
    tmp___9 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___9) {
      args.v3.__annonCompField91.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 20U);
    } else {
      args.v3.__annonCompField91.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
    }
  }
  if ((int )is_dp) {
    args.v3.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___10 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___10) {
      args.v3.ucLaneNum = 8U;
    } else {
      args.v3.ucLaneNum = 4U;
    }
  }
  if ((int )dig->linkb) {
    args.v3.acConfig.ucLinkSel = 1U;
  } else {
  }
  if (dig_encoder & 1) {
    args.v3.acConfig.ucEncoderSel = 1U;
  } else {
  }
  if ((int )is_dp && adev->clock.dp_extclk != 0U) {
    args.v3.acConfig.ucRefClkSource = 2U;
  } else {
    args.v3.acConfig.ucRefClkSource = (unsigned char )pll_id;
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U:
  args.v3.acConfig.ucTransmitterSel = 0U;
  goto ldv_48114;
  case 32U:
  args.v3.acConfig.ucTransmitterSel = 1U;
  goto ldv_48114;
  case 33U:
  args.v3.acConfig.ucTransmitterSel = 2U;
  goto ldv_48114;
  }
  ldv_48114: ;
  if ((int )is_dp) {
    args.v3.acConfig.fCoherentMode = 1U;
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    if ((int )dig->coherent_mode) {
      args.v3.acConfig.fCoherentMode = 1U;
    } else {
    }
    tmp___11 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___11) {
      args.v3.acConfig.fDualLinkConnector = 1U;
    } else {
    }
  } else {
  }
  goto ldv_48106;
  case 4:
  args.v4.ucAction = (UCHAR )action;
  if (action == 7) {
    args.v4.__annonCompField94.usInitInfo = (unsigned short )connector_object_id;
  } else
  if (action == 11) {
    args.v4.__annonCompField94.asMode.ucLaneSel = lane_num;
    args.v4.__annonCompField94.asMode.__annonCompField93.ucLaneSet = lane_set;
  } else
  if ((int )is_dp) {
    args.v4.__annonCompField94.usPixelClock = (unsigned short )(dp_clock / 10);
  } else {
    tmp___12 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___12) {
      args.v4.__annonCompField94.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 20U);
    } else {
      args.v4.__annonCompField94.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
    }
  }
  if ((int )is_dp) {
    args.v4.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___13 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___13) {
      args.v4.ucLaneNum = 8U;
    } else {
      args.v4.ucLaneNum = 4U;
    }
  }
  if ((int )dig->linkb) {
    args.v4.__annonCompField95.acConfig.ucLinkSel = 1U;
  } else {
  }
  if (dig_encoder & 1) {
    args.v4.__annonCompField95.acConfig.ucEncoderSel = 1U;
  } else {
  }
  if ((int )is_dp) {
    if (adev->clock.dp_extclk != 0U) {
      args.v4.__annonCompField95.acConfig.ucRefClkSource = 3U;
    } else {
      args.v4.__annonCompField95.acConfig.ucRefClkSource = 2U;
    }
  } else {
    args.v4.__annonCompField95.acConfig.ucRefClkSource = (unsigned char )pll_id;
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U:
  args.v4.__annonCompField95.acConfig.ucTransmitterSel = 0U;
  goto ldv_48119;
  case 32U:
  args.v4.__annonCompField95.acConfig.ucTransmitterSel = 1U;
  goto ldv_48119;
  case 33U:
  args.v4.__annonCompField95.acConfig.ucTransmitterSel = 2U;
  goto ldv_48119;
  }
  ldv_48119: ;
  if ((int )is_dp) {
    args.v4.__annonCompField95.acConfig.fCoherentMode = 1U;
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    if ((int )dig->coherent_mode) {
      args.v4.__annonCompField95.acConfig.fCoherentMode = 1U;
    } else {
    }
    tmp___14 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___14) {
      args.v4.__annonCompField95.acConfig.fDualLinkConnector = 1U;
    } else {
    }
  } else {
  }
  goto ldv_48106;
  case 5:
  args.v5.ucAction = (UCHAR )action;
  if ((int )is_dp) {
    args.v5.usSymClock = (unsigned short )(dp_clock / 10);
  } else {
    args.v5.usSymClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  if ((int )dig->linkb) {
    args.v5.ucPhyId = 1U;
  } else {
    args.v5.ucPhyId = 0U;
  }
  goto ldv_48124;
  case 32U: ;
  if ((int )dig->linkb) {
    args.v5.ucPhyId = 3U;
  } else {
    args.v5.ucPhyId = 2U;
  }
  goto ldv_48124;
  case 33U: ;
  if ((int )dig->linkb) {
    args.v5.ucPhyId = 5U;
  } else {
    args.v5.ucPhyId = 4U;
  }
  goto ldv_48124;
  case 37U:
  args.v5.ucPhyId = 6U;
  goto ldv_48124;
  }
  ldv_48124: ;
  if ((int )is_dp) {
    args.v5.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___15 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___15) {
      args.v5.ucLaneNum = 8U;
    } else {
      args.v5.ucLaneNum = 4U;
    }
  }
  args.v5.ucConnObjId = (UCHAR )connector_object_id;
  tmp___16 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  args.v5.ucDigMode = (UCHAR )tmp___16;
  if ((int )is_dp && adev->clock.dp_extclk != 0U) {
    args.v5.__annonCompField96.asConfig.ucPhyClkSrcId = 3U;
  } else {
    args.v5.__annonCompField96.asConfig.ucPhyClkSrcId = (unsigned char )pll_id;
  }
  if ((int )is_dp) {
    args.v5.__annonCompField96.asConfig.ucCoherentMode = 1U;
  } else
  if (((long )amdgpu_encoder->devices & 3784L) != 0L) {
    if ((int )dig->coherent_mode) {
      args.v5.__annonCompField96.asConfig.ucCoherentMode = 1U;
    } else {
    }
  } else {
  }
  if (hpd_id == 255) {
    args.v5.__annonCompField96.asConfig.ucHPDSel = 0U;
  } else {
    args.v5.__annonCompField96.asConfig.ucHPDSel = (unsigned char )((unsigned int )((unsigned char )hpd_id) + 1U);
  }
  args.v5.ucDigEncoderSel = (UCHAR )(1 << dig_encoder);
  args.v5.ucDPLaneSet = lane_set;
  goto ldv_48106;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_48106;
  }
  ldv_48106: ;
  goto ldv_48129;
  default:
  drm_err("Unknown table version %d, %d\n", (int )frev, (int )crev);
  goto ldv_48129;
  }
  ldv_48129:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
bool amdgpu_atombios_encoder_set_edp_panel_power(struct drm_connector *connector ,
                                                 int action )
{
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  union dig_transmitter_control args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  bool tmp ;
  int tmp___0 ;
  int i ;
  bool tmp___1 ;
  unsigned long __ms ;
  unsigned long tmp___2 ;
  {
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  dev = amdgpu_connector->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 76;
  if (connector->connector_type != 14) {
    goto done;
  } else {
  }
  if (action != 12 && action != 13) {
    goto done;
  } else {
  }
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    goto done;
  } else {
  }
  memset((void *)(& args), 0, 12UL);
  args.v1.ucAction = (UCHAR )action;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  if (action == 12) {
    i = 0;
    goto ldv_48151;
    ldv_48150:
    tmp___1 = (*((adev->mode_info.funcs)->hpd_sense))(adev, amdgpu_connector->hpd.hpd);
    if ((int )tmp___1) {
      return (1);
    } else {
    }
    if (1) {
      __const_udelay(4295000UL);
    } else {
      __ms = 1UL;
      goto ldv_48148;
      ldv_48147:
      __const_udelay(4295000UL);
      ldv_48148:
      tmp___2 = __ms;
      __ms = __ms - 1UL;
      if (tmp___2 != 0UL) {
        goto ldv_48147;
      } else {
      }
    }
    i = i + 1;
    ldv_48151: ;
    if (i <= 299) {
      goto ldv_48150;
    } else {
    }
    return (0);
  } else {
  }
  done: ;
  return (1);
}
}
static void amdgpu_atombios_encoder_setup_external_encoder(struct drm_encoder *encoder ,
                                                           struct drm_encoder *ext_encoder ,
                                                           int action )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder *ext_amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  union external_encoder_control args ;
  struct drm_connector *connector ;
  int index ;
  u8 frev ;
  u8 crev ;
  int dp_clock ;
  int dp_lane_count ;
  int connector_object_id ;
  u32 ext_enum ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  struct amdgpu_connector_atom_dig *dig_connector ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_encoder const *)ext_encoder;
  ext_amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  index = 50;
  dp_clock = 0;
  dp_lane_count = 0;
  connector_object_id = 0;
  ext_enum = (ext_amdgpu_encoder->encoder_enum & 1792U) >> 8;
  if (action == 7) {
    connector = amdgpu_get_connector_for_encoder_init(encoder);
  } else {
    connector = amdgpu_get_connector_for_encoder(encoder);
  }
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
    dp_clock = dig_connector->dp_clock;
    dp_lane_count = dig_connector->dp_lane_count;
    connector_object_id = (int )amdgpu_connector->connector_object_id & 255;
  } else {
  }
  memset((void *)(& args), 0, 16UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  goto ldv_48183;
  case 2: ;
  switch ((int )crev) {
  case 1: ;
  case 2:
  args.v1.sDigEncoder.ucAction = (UCHAR )action;
  args.v1.sDigEncoder.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  tmp___1 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  args.v1.sDigEncoder.ucEncoderMode = (UCHAR )tmp___1;
  if ((unsigned int )args.v1.sDigEncoder.ucEncoderMode == 0U || (unsigned int )args.v1.sDigEncoder.ucEncoderMode == 5U) {
    if (dp_clock == 270000) {
      args.v1.sDigEncoder.ucConfig = (UCHAR )((unsigned int )args.v1.sDigEncoder.ucConfig | 1U);
    } else {
    }
    args.v1.sDigEncoder.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___2 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___2) {
      args.v1.sDigEncoder.ucLaneNum = 8U;
    } else {
      args.v1.sDigEncoder.ucLaneNum = 4U;
    }
  }
  goto ldv_48187;
  case 3:
  args.v3.sExtEncoder.ucAction = (UCHAR )action;
  if (action == 7) {
    args.v3.sExtEncoder.__annonCompField97.usConnectorId = (unsigned short )connector_object_id;
  } else {
    args.v3.sExtEncoder.__annonCompField97.usPixelClock = (unsigned short )(amdgpu_encoder->pixel_clock / 10U);
  }
  tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  args.v3.sExtEncoder.ucEncoderMode = (UCHAR )tmp___3;
  if ((unsigned int )args.v3.sExtEncoder.ucEncoderMode == 0U || (unsigned int )args.v3.sExtEncoder.ucEncoderMode == 5U) {
    if (dp_clock == 270000) {
      args.v3.sExtEncoder.ucConfig = (UCHAR )((unsigned int )args.v3.sExtEncoder.ucConfig | 1U);
    } else
    if (dp_clock == 540000) {
      args.v3.sExtEncoder.ucConfig = (UCHAR )((unsigned int )args.v3.sExtEncoder.ucConfig | 2U);
    } else {
    }
    args.v3.sExtEncoder.ucLaneNum = (UCHAR )dp_lane_count;
  } else {
    tmp___4 = amdgpu_dig_monitor_is_duallink(encoder, amdgpu_encoder->pixel_clock);
    if ((int )tmp___4) {
      args.v3.sExtEncoder.ucLaneNum = 8U;
    } else {
      args.v3.sExtEncoder.ucLaneNum = 4U;
    }
  }
  switch (ext_enum) {
  case 1U:
  args.v3.sExtEncoder.ucConfig = args.v3.sExtEncoder.ucConfig;
  goto ldv_48190;
  case 2U:
  args.v3.sExtEncoder.ucConfig = (UCHAR )((unsigned int )args.v3.sExtEncoder.ucConfig | 16U);
  goto ldv_48190;
  case 3U:
  args.v3.sExtEncoder.ucConfig = (UCHAR )((unsigned int )args.v3.sExtEncoder.ucConfig | 32U);
  goto ldv_48190;
  }
  ldv_48190:
  args.v3.sExtEncoder.ucBitPerColor = amdgpu_atombios_encoder_get_bpc(encoder);
  goto ldv_48187;
  default:
  drm_err("Unknown table version: %d, %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48187: ;
  goto ldv_48183;
  default:
  drm_err("Unknown table version: %d, %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48183:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
static void amdgpu_atombios_encoder_setup_dig(struct drm_encoder *encoder , int action )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_encoder *ext_encoder ;
  struct drm_encoder *tmp ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct amdgpu_connector_atom_dig *amdgpu_dig_connector ;
  struct drm_connector const *__mptr___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  int tmp___5 ;
  int tmp___6 ;
  int tmp___7 ;
  int tmp___8 ;
  int tmp___9 ;
  int tmp___10 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = amdgpu_get_external_encoder(encoder);
  ext_encoder = tmp;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp___0 = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp___0;
  amdgpu_connector = (struct amdgpu_connector *)0;
  amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)0;
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    amdgpu_dig_connector = (struct amdgpu_connector_atom_dig *)amdgpu_connector->con_priv;
  } else {
  }
  if (action == 1) {
    if ((unsigned long )connector == (unsigned long )((struct drm_connector *)0)) {
      dig->panel_mode = 0;
    } else {
      dig->panel_mode = amdgpu_atombios_dp_get_panel_mode(encoder, connector);
    }
    amdgpu_atombios_encoder_setup_dig_encoder(encoder, 15, 0);
    amdgpu_atombios_encoder_setup_dig_encoder(encoder, 16, dig->panel_mode);
    if ((unsigned long )ext_encoder != (unsigned long )((struct drm_encoder *)0)) {
      amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 15);
    } else {
    }
    tmp___1 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___1 == 0) {
      goto _L;
    } else {
      tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      if (tmp___2 == 5) {
        _L:
        if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
          if (connector->connector_type == 14) {
            amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
            amdgpu_dig_connector->edp_on = 1;
          } else {
          }
        } else {
        }
      } else {
      }
    }
    amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 1, 0, 0);
    tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___3 == 0) {
      goto _L___0;
    } else {
      tmp___4 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      if (tmp___4 == 5) {
        _L___0:
        if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
          amdgpu_atombios_dp_link_train(encoder, connector);
          amdgpu_atombios_encoder_setup_dig_encoder(encoder, 13, 0);
        } else {
        }
      } else {
      }
    }
    if (((long )amdgpu_encoder->devices & 34L) != 0L) {
      amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 3, 0, 0);
    } else {
    }
    if ((unsigned long )ext_encoder != (unsigned long )((struct drm_encoder *)0)) {
      amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 1);
    } else {
    }
  } else {
    tmp___5 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___5 == 0) {
      goto _L___1;
    } else {
      tmp___6 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      if (tmp___6 == 5) {
        _L___1:
        if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
          amdgpu_atombios_encoder_setup_dig_encoder(encoder, 12, 0);
        } else {
        }
      } else {
      }
    }
    if ((unsigned long )ext_encoder != (unsigned long )((struct drm_encoder *)0)) {
      amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 0);
    } else {
    }
    if (((long )amdgpu_encoder->devices & 34L) != 0L) {
      amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 2, 0, 0);
    } else {
    }
    tmp___7 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___7 == 0) {
      goto _L___2;
    } else {
      tmp___8 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      if (tmp___8 == 5) {
        _L___2:
        if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
          amdgpu_atombios_dp_set_rx_power_state(connector, 2);
        } else {
        }
      } else {
      }
    }
    amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 0, 0, 0);
    tmp___9 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp___9 == 0) {
      goto _L___3;
    } else {
      tmp___10 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      if (tmp___10 == 5) {
        _L___3:
        if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
          if (connector->connector_type == 14) {
            amdgpu_atombios_encoder_set_edp_panel_power(connector, 13);
            amdgpu_dig_connector->edp_on = 0;
          } else {
          }
        } else {
        }
      } else {
      }
    }
  }
  return;
}
}
void amdgpu_atombios_encoder_dpms(struct drm_encoder *encoder , int mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  long tmp ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atombios_encoder_dpms", "encoder dpms %d to mode %d, devices %08x, active_devices %08x\n",
                        amdgpu_encoder->encoder_id, mode, amdgpu_encoder->devices,
                        amdgpu_encoder->active_device);
  } else {
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  switch (mode) {
  case 0:
  amdgpu_atombios_encoder_setup_dig(encoder, 1);
  goto ldv_48222;
  case 1: ;
  case 2: ;
  case 3:
  amdgpu_atombios_encoder_setup_dig(encoder, 0);
  goto ldv_48222;
  }
  ldv_48222: ;
  goto ldv_48226;
  case 20U: ;
  switch (mode) {
  case 0:
  amdgpu_atombios_encoder_setup_dvo(encoder, 1);
  goto ldv_48229;
  case 1: ;
  case 2: ;
  case 3:
  amdgpu_atombios_encoder_setup_dvo(encoder, 0);
  goto ldv_48229;
  }
  ldv_48229: ;
  goto ldv_48226;
  case 21U: ;
  switch (mode) {
  case 0:
  amdgpu_atombios_encoder_setup_dac(encoder, 1);
  goto ldv_48235;
  case 1: ;
  case 2: ;
  case 3:
  amdgpu_atombios_encoder_setup_dac(encoder, 0);
  goto ldv_48235;
  }
  ldv_48235: ;
  goto ldv_48226;
  default: ;
  return;
  }
  ldv_48226: ;
  return;
}
}
void amdgpu_atombios_encoder_set_crtc_source(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  union crtc_source_param args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  struct amdgpu_encoder_atom_dig *dig ;
  bool tmp ;
  int tmp___0 ;
  struct drm_connector *connector ;
  struct drm_connector *tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  u16 tmp___4 ;
  struct drm_connector *connector___0 ;
  struct drm_connector *tmp___5 ;
  int tmp___6 ;
  int tmp___7 ;
  u16 tmp___8 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  index = 42;
  memset((void *)(& args), 0, 4UL);
  tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                     & crev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  switch ((int )frev) {
  case 1: ;
  switch ((int )crev) {
  case 1: ;
  default:
  args.v1.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  switch (amdgpu_encoder->encoder_id) {
  case 2U: ;
  case 19U:
  args.v1.ucDevice = 3U;
  goto ldv_48265;
  case 1U: ;
  case 15U: ;
  if (((long )amdgpu_encoder->devices & 2L) != 0L) {
    args.v1.ucDevice = 1U;
  } else {
    args.v1.ucDevice = 9U;
  }
  goto ldv_48265;
  case 11U: ;
  case 25U: ;
  case 20U:
  args.v1.ucDevice = 7U;
  goto ldv_48265;
  case 4U: ;
  case 21U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v1.ucDevice = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v1.ucDevice = 8U;
  } else {
    args.v1.ucDevice = 0U;
  }
  goto ldv_48265;
  case 5U: ;
  case 22U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v1.ucDevice = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v1.ucDevice = 8U;
  } else {
    args.v1.ucDevice = 4U;
  }
  goto ldv_48265;
  }
  ldv_48265: ;
  goto ldv_48275;
  case 2:
  args.v2.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  tmp___4 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
  if ((unsigned int )tmp___4 != 0U) {
    tmp___1 = amdgpu_get_connector_for_encoder(encoder);
    connector = tmp___1;
    if (connector->connector_type == 7) {
      args.v2.ucEncodeMode = 1U;
    } else
    if (connector->connector_type == 1) {
      args.v2.ucEncodeMode = 15U;
    } else {
      tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      args.v2.ucEncodeMode = (UCHAR )tmp___2;
    }
  } else
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    args.v2.ucEncodeMode = 1U;
  } else {
    tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    args.v2.ucEncodeMode = (UCHAR )tmp___3;
  }
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  case 31U:
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  switch (dig->dig_encoder) {
  case 0:
  args.v2.ucEncoderID = 3U;
  goto ldv_48284;
  case 1:
  args.v2.ucEncoderID = 9U;
  goto ldv_48284;
  case 2:
  args.v2.ucEncoderID = 10U;
  goto ldv_48284;
  case 3:
  args.v2.ucEncoderID = 11U;
  goto ldv_48284;
  case 4:
  args.v2.ucEncoderID = 12U;
  goto ldv_48284;
  case 5:
  args.v2.ucEncoderID = 13U;
  goto ldv_48284;
  case 6:
  args.v2.ucEncoderID = 14U;
  goto ldv_48284;
  }
  ldv_48284: ;
  goto ldv_48291;
  case 20U:
  args.v2.ucEncoderID = 7U;
  goto ldv_48291;
  case 21U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v2.ucEncoderID = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v2.ucEncoderID = 2U;
  } else {
    args.v2.ucEncoderID = 0U;
  }
  goto ldv_48291;
  case 22U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v2.ucEncoderID = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v2.ucEncoderID = 2U;
  } else {
    args.v2.ucEncoderID = 4U;
  }
  goto ldv_48291;
  }
  ldv_48291: ;
  goto ldv_48275;
  case 3:
  args.v3.ucCRTC = (UCHAR )amdgpu_crtc->crtc_id;
  tmp___8 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
  if ((unsigned int )tmp___8 != 0U) {
    tmp___5 = amdgpu_get_connector_for_encoder(encoder);
    connector___0 = tmp___5;
    if (connector___0->connector_type == 7) {
      args.v2.ucEncodeMode = 1U;
    } else
    if (connector___0->connector_type == 1) {
      args.v2.ucEncodeMode = 15U;
    } else {
      tmp___6 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
      args.v2.ucEncodeMode = (UCHAR )tmp___6;
    }
  } else
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    args.v2.ucEncodeMode = 1U;
  } else {
    tmp___7 = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    args.v2.ucEncodeMode = (UCHAR )tmp___7;
  }
  args.v3.ucDstBpc = amdgpu_atombios_encoder_get_bpc(encoder);
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  case 31U:
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  switch (dig->dig_encoder) {
  case 0:
  args.v3.ucEncoderID = 3U;
  goto ldv_48303;
  case 1:
  args.v3.ucEncoderID = 9U;
  goto ldv_48303;
  case 2:
  args.v3.ucEncoderID = 10U;
  goto ldv_48303;
  case 3:
  args.v3.ucEncoderID = 11U;
  goto ldv_48303;
  case 4:
  args.v3.ucEncoderID = 12U;
  goto ldv_48303;
  case 5:
  args.v3.ucEncoderID = 13U;
  goto ldv_48303;
  case 6:
  args.v3.ucEncoderID = 14U;
  goto ldv_48303;
  }
  ldv_48303: ;
  goto ldv_48310;
  case 20U:
  args.v3.ucEncoderID = 7U;
  goto ldv_48310;
  case 21U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v3.ucEncoderID = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v3.ucEncoderID = 2U;
  } else {
    args.v3.ucEncoderID = 0U;
  }
  goto ldv_48310;
  case 22U: ;
  if (((long )amdgpu_encoder->active_device & 4L) != 0L) {
    args.v3.ucEncoderID = 2U;
  } else
  if (((long )amdgpu_encoder->active_device & 256L) != 0L) {
    args.v3.ucEncoderID = 2U;
  } else {
    args.v3.ucEncoderID = 4U;
  }
  goto ldv_48310;
  }
  ldv_48310: ;
  goto ldv_48275;
  }
  ldv_48275: ;
  goto ldv_48314;
  default:
  drm_err("Unknown table version: %d, %d\n", (int )frev, (int )crev);
  return;
  }
  ldv_48314:
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  return;
}
}
void amdgpu_atombios_encoder_init_dig(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct list_head const *__mptr ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  struct drm_encoder *ext_encoder ;
  struct drm_encoder *tmp ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_48335;
  ldv_48334:
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  tmp = amdgpu_get_external_encoder(encoder);
  ext_encoder = tmp;
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U:
  amdgpu_atombios_encoder_setup_dig_transmitter(encoder, 7, 0, 0);
  goto ldv_48333;
  }
  ldv_48333: ;
  if ((unsigned long )ext_encoder != (unsigned long )((struct drm_encoder *)0)) {
    amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 7);
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_48335: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_48334;
  } else {
  }
  return;
}
}
static bool amdgpu_atombios_encoder_dac_load_detect(struct drm_encoder *encoder ,
                                                    struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  DAC_LOAD_DETECTION_PS_ALLOCATION args ;
  int index ;
  uint8_t frev ;
  uint8_t crev ;
  bool tmp ;
  int tmp___0 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if (((long )amdgpu_encoder->devices & 277L) != 0L) {
    index = 21;
    memset((void *)(& args), 0, 12UL);
    tmp = amdgpu_atom_parse_cmd_header(adev->mode_info.atom_context, index, & frev,
                                       & crev);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    if (tmp___0) {
      return (0);
    } else {
    }
    args.sDacload.ucMisc = 0U;
    if (amdgpu_encoder->encoder_id == 4U || amdgpu_encoder->encoder_id == 21U) {
      args.sDacload.ucDacType = 0U;
    } else {
      args.sDacload.ucDacType = 1U;
    }
    if ((int )amdgpu_connector->devices & 1) {
      args.sDacload.usDeviceID = 1U;
    } else
    if (((long )amdgpu_connector->devices & 16L) != 0L) {
      args.sDacload.usDeviceID = 16U;
    } else
    if (((long )amdgpu_connector->devices & 256L) != 0L) {
      args.sDacload.usDeviceID = 256U;
      if ((unsigned int )crev > 2U) {
        args.sDacload.ucMisc = 1U;
      } else {
      }
    } else
    if (((long )amdgpu_connector->devices & 4L) != 0L) {
      args.sDacload.usDeviceID = 4U;
      if ((unsigned int )crev > 2U) {
        args.sDacload.ucMisc = 1U;
      } else {
      }
    } else {
    }
    amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
    return (1);
  } else {
    return (0);
  }
}
}
enum drm_connector_status amdgpu_atombios_encoder_dac_detect(struct drm_encoder *encoder ,
                                                             struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  u32 bios_0_scratch ;
  long tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  tmp___0 = amdgpu_atombios_encoder_dac_load_detect(encoder, connector);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_encoder_dac_detect", "detect returned false \n");
    } else {
    }
    return (3);
  } else {
  }
  bios_0_scratch = amdgpu_mm_rreg(adev, 1481U, 0);
  tmp___2 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("amdgpu_atombios_encoder_dac_detect", "Bios 0 scratch %x %08x\n",
                        bios_0_scratch, amdgpu_encoder->devices);
  } else {
  }
  if ((int )amdgpu_connector->devices & 1) {
    if (((long )bios_0_scratch & 3L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 16L) != 0L) {
    if (((long )bios_0_scratch & 768L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 256L) != 0L) {
    if (((long )bios_0_scratch & 12336L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 4L) != 0L) {
    if (((long )bios_0_scratch & 1028L) != 0L) {
      return (1);
    } else
    if (((long )bios_0_scratch & 2056L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  return (2);
}
}
enum drm_connector_status amdgpu_atombios_encoder_dig_detect(struct drm_encoder *encoder ,
                                                             struct drm_connector *connector )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct drm_encoder *ext_encoder ;
  struct drm_encoder *tmp ;
  u32 bios_0_scratch ;
  long tmp___0 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  tmp = amdgpu_get_external_encoder(encoder);
  ext_encoder = tmp;
  if ((unsigned long )ext_encoder == (unsigned long )((struct drm_encoder *)0)) {
    return (3);
  } else {
  }
  if (((long )amdgpu_connector->devices & 17L) == 0L) {
    return (3);
  } else {
  }
  amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 18);
  bios_0_scratch = amdgpu_mm_rreg(adev, 1481U, 0);
  tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("amdgpu_atombios_encoder_dig_detect", "Bios 0 scratch %x %08x\n",
                        bios_0_scratch, amdgpu_encoder->devices);
  } else {
  }
  if ((int )amdgpu_connector->devices & 1) {
    if (((long )bios_0_scratch & 3L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 16L) != 0L) {
    if (((long )bios_0_scratch & 768L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 256L) != 0L) {
    if (((long )bios_0_scratch & 12336L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  if (((long )amdgpu_connector->devices & 4L) != 0L) {
    if (((long )bios_0_scratch & 1028L) != 0L) {
      return (1);
    } else
    if (((long )bios_0_scratch & 2056L) != 0L) {
      return (1);
    } else {
    }
  } else {
  }
  return (2);
}
}
void amdgpu_atombios_encoder_setup_ext_encoder_ddc(struct drm_encoder *encoder )
{
  struct drm_encoder *ext_encoder ;
  struct drm_encoder *tmp ;
  {
  tmp = amdgpu_get_external_encoder(encoder);
  ext_encoder = tmp;
  if ((unsigned long )ext_encoder != (unsigned long )((struct drm_encoder *)0)) {
    amdgpu_atombios_encoder_setup_external_encoder(encoder, ext_encoder, 20);
  } else {
  }
  return;
}
}
void amdgpu_atombios_encoder_set_bios_scratch_regs(struct drm_connector *connector ,
                                                   struct drm_encoder *encoder , bool connected )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr___0 ;
  u32 bios_0_scratch ;
  u32 bios_3_scratch ;
  u32 bios_6_scratch ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  long tmp___5 ;
  long tmp___6 ;
  long tmp___7 ;
  long tmp___8 ;
  long tmp___9 ;
  long tmp___10 ;
  long tmp___11 ;
  long tmp___12 ;
  long tmp___13 ;
  long tmp___14 ;
  long tmp___15 ;
  long tmp___16 ;
  {
  dev = connector->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr;
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  bios_0_scratch = amdgpu_mm_rreg(adev, 1481U, 0);
  bios_3_scratch = amdgpu_mm_rreg(adev, 1484U, 0);
  bios_6_scratch = amdgpu_mm_rreg(adev, 1487U, 0);
  if (((long )amdgpu_encoder->devices & 2L) != 0L && ((long )amdgpu_connector->devices & 2L) != 0L) {
    if ((int )connected) {
      tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "LCD1 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 262144U;
      bios_3_scratch = bios_3_scratch | 2U;
      bios_6_scratch = bios_6_scratch | 131072U;
    } else {
      tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "LCD1 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4294705151U;
      bios_3_scratch = bios_3_scratch & 4294967293U;
      bios_6_scratch = bios_6_scratch & 4294836223U;
    }
  } else {
  }
  if ((int )amdgpu_encoder->devices & 1 && (int )amdgpu_connector->devices & 1) {
    if ((int )connected) {
      tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___1 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "CRT1 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 2U;
      bios_3_scratch = bios_3_scratch | 1U;
      bios_6_scratch = bios_6_scratch | 65536U;
    } else {
      tmp___2 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___2 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "CRT1 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4294967292U;
      bios_3_scratch = bios_3_scratch & 4294967294U;
      bios_6_scratch = bios_6_scratch & 4294901759U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 16L) != 0L && ((long )amdgpu_connector->devices & 16L) != 0L) {
    if ((int )connected) {
      tmp___3 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___3 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "CRT2 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 512U;
      bios_3_scratch = bios_3_scratch | 16U;
      bios_6_scratch = bios_6_scratch | 1048576U;
    } else {
      tmp___4 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___4 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "CRT2 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4294966527U;
      bios_3_scratch = bios_3_scratch & 4294967279U;
      bios_6_scratch = bios_6_scratch & 4293918719U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 8L) != 0L && ((long )amdgpu_connector->devices & 8L) != 0L) {
    if ((int )connected) {
      tmp___5 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___5 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP1 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 65536U;
      bios_3_scratch = bios_3_scratch | 8U;
      bios_6_scratch = bios_6_scratch | 524288U;
    } else {
      tmp___6 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___6 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP1 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4294901759U;
      bios_3_scratch = bios_3_scratch & 4294967287U;
      bios_6_scratch = bios_6_scratch & 4294443007U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 128L) != 0L && ((long )amdgpu_connector->devices & 128L) != 0L) {
    if ((int )connected) {
      tmp___7 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___7 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP2 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 131072U;
      bios_3_scratch = bios_3_scratch | 128U;
      bios_6_scratch = bios_6_scratch | 8388608U;
    } else {
      tmp___8 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___8 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP2 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4294836223U;
      bios_3_scratch = bios_3_scratch & 4294967167U;
      bios_6_scratch = bios_6_scratch & 4286578687U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 512L) != 0L && ((long )amdgpu_connector->devices & 512L) != 0L) {
    if ((int )connected) {
      tmp___9 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___9 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP3 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 2097152U;
      bios_3_scratch = bios_3_scratch | 512U;
      bios_6_scratch = bios_6_scratch | 33554432U;
    } else {
      tmp___10 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___10 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP3 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4292870143U;
      bios_3_scratch = bios_3_scratch & 4294966783U;
      bios_6_scratch = bios_6_scratch & 4261412863U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 1024L) != 0L && ((long )amdgpu_connector->devices & 1024L) != 0L) {
    if ((int )connected) {
      tmp___11 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___11 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP4 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 4194304U;
      bios_3_scratch = bios_3_scratch | 1024U;
      bios_6_scratch = bios_6_scratch | 67108864U;
    } else {
      tmp___12 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___12 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP4 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4290772991U;
      bios_3_scratch = bios_3_scratch & 4294966271U;
      bios_6_scratch = bios_6_scratch & 4227858431U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 2048L) != 0L && ((long )amdgpu_connector->devices & 2048L) != 0L) {
    if ((int )connected) {
      tmp___13 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___13 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP5 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 8388608U;
      bios_3_scratch = bios_3_scratch | 2048U;
      bios_6_scratch = bios_6_scratch | 134217728U;
    } else {
      tmp___14 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___14 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP5 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4286578687U;
      bios_3_scratch = bios_3_scratch & 4294965247U;
      bios_6_scratch = bios_6_scratch & 4160749567U;
    }
  } else {
  }
  if (((long )amdgpu_encoder->devices & 64L) != 0L && ((long )amdgpu_connector->devices & 64L) != 0L) {
    if ((int )connected) {
      tmp___15 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___15 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP6 connected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch | 1048576U;
      bios_3_scratch = bios_3_scratch | 64U;
      bios_6_scratch = bios_6_scratch | 4194304U;
    } else {
      tmp___16 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___16 != 0L) {
        drm_ut_debug_printk("amdgpu_atombios_encoder_set_bios_scratch_regs", "DFP6 disconnected\n");
      } else {
      }
      bios_0_scratch = bios_0_scratch & 4293918719U;
      bios_3_scratch = bios_3_scratch & 4294967231U;
      bios_6_scratch = bios_6_scratch & 4290772991U;
    }
  } else {
  }
  amdgpu_mm_wreg(adev, 1481U, bios_0_scratch, 0);
  amdgpu_mm_wreg(adev, 1484U, bios_3_scratch, 0);
  amdgpu_mm_wreg(adev, 1487U, bios_6_scratch, 0);
  return;
}
}
struct amdgpu_encoder_atom_dig *amdgpu_atombios_encoder_get_lcd_info(struct amdgpu_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_mode_info *mode_info ;
  int index ;
  u16 data_offset ;
  u16 misc ;
  union lvds_info *lvds_info ;
  uint8_t frev ;
  uint8_t crev ;
  struct amdgpu_encoder_atom_dig *lvds ;
  int encoder_enum ;
  void *tmp ;
  ATOM_FAKE_EDID_PATCH_RECORD *fake_edid_record ;
  ATOM_PANEL_RESOLUTION_PATCH_RECORD *panel_res_record ;
  bool bad_record ;
  u8 *record ;
  struct edid *edid ;
  int edid_size ;
  int _max1 ;
  int _max2 ;
  void *tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  {
  dev = encoder->base.dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  mode_info = & adev->mode_info;
  index = 6;
  lvds = (struct amdgpu_encoder_atom_dig *)0;
  encoder_enum = (int )((encoder->encoder_enum & 1792U) >> 8);
  tmp___2 = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___2) {
    lvds_info = (union lvds_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    tmp = kzalloc(264UL, 208U);
    lvds = (struct amdgpu_encoder_atom_dig *)tmp;
    if ((unsigned long )lvds == (unsigned long )((struct amdgpu_encoder_atom_dig *)0)) {
      return ((struct amdgpu_encoder_atom_dig *)0);
    } else {
    }
    lvds->native_mode.clock = (int )lvds_info->info.sLCDTiming.usPixClk * 10;
    lvds->native_mode.hdisplay = (int )lvds_info->info.sLCDTiming.usHActive;
    lvds->native_mode.vdisplay = (int )lvds_info->info.sLCDTiming.usVActive;
    lvds->native_mode.htotal = lvds->native_mode.hdisplay + (int )lvds_info->info.sLCDTiming.usHBlanking_Time;
    lvds->native_mode.hsync_start = lvds->native_mode.hdisplay + (int )lvds_info->info.sLCDTiming.usHSyncOffset;
    lvds->native_mode.hsync_end = lvds->native_mode.hsync_start + (int )lvds_info->info.sLCDTiming.usHSyncWidth;
    lvds->native_mode.vtotal = lvds->native_mode.vdisplay + (int )lvds_info->info.sLCDTiming.usVBlanking_Time;
    lvds->native_mode.vsync_start = lvds->native_mode.vdisplay + (int )lvds_info->info.sLCDTiming.usVSyncOffset;
    lvds->native_mode.vsync_end = lvds->native_mode.vsync_start + (int )lvds_info->info.sLCDTiming.usVSyncWidth;
    lvds->panel_pwr_delay = lvds_info->info.usOffDelayInMs;
    lvds->lcd_misc = (u32 )lvds_info->info.ucLVDS_Misc;
    misc = lvds_info->info.sLCDTiming.susModeMiscInfo.usAccess;
    if (((int )misc & 4) != 0) {
      lvds->native_mode.flags = lvds->native_mode.flags | 8U;
    } else {
    }
    if (((int )misc & 2) != 0) {
      lvds->native_mode.flags = lvds->native_mode.flags | 2U;
    } else {
    }
    if (((int )misc & 64) != 0) {
      lvds->native_mode.flags = lvds->native_mode.flags | 64U;
    } else {
    }
    if (((int )misc & 128) != 0) {
      lvds->native_mode.flags = lvds->native_mode.flags | 16U;
    } else {
    }
    if (((int )misc & 256) != 0) {
      lvds->native_mode.flags = lvds->native_mode.flags | 32U;
    } else {
    }
    lvds->native_mode.width_mm = (int )lvds_info->info.sLCDTiming.usImageHSize;
    lvds->native_mode.height_mm = (int )lvds_info->info.sLCDTiming.usImageVSize;
    drm_mode_set_crtcinfo(& lvds->native_mode, 1);
    lvds->lcd_ss_id = (u32 )lvds_info->info.ucSS_Id;
    encoder->native_mode = lvds->native_mode;
    if (encoder_enum == 2) {
      lvds->linkb = 1;
    } else {
      lvds->linkb = 0;
    }
    if ((unsigned int )lvds_info->info.usModePatchTableOffset != 0U) {
      bad_record = 0;
      if ((unsigned int )frev == 1U && (unsigned int )crev <= 1U) {
        record = (u8 *)(mode_info->atom_context)->bios + (unsigned long )lvds_info->info.usModePatchTableOffset;
      } else {
        record = (u8 *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )lvds_info->info.usModePatchTableOffset));
      }
      goto ldv_48438;
      ldv_48437: ;
      switch ((int )*record) {
      case 1:
      record = record + 5UL;
      goto ldv_48425;
      case 2:
      record = record + 2UL;
      goto ldv_48425;
      case 3:
      record = record + 3UL;
      goto ldv_48425;
      case 4:
      fake_edid_record = (ATOM_FAKE_EDID_PATCH_RECORD *)record;
      if ((unsigned int )fake_edid_record->ucFakeEDIDLength != 0U) {
        _max1 = 128;
        _max2 = (int )fake_edid_record->ucFakeEDIDLength;
        edid_size = _max1 > _max2 ? _max1 : _max2;
        tmp___0 = kmalloc((size_t )edid_size, 208U);
        edid = (struct edid *)tmp___0;
        if ((unsigned long )edid != (unsigned long )((struct edid *)0)) {
          memcpy((void *)edid, (void const *)(& fake_edid_record->ucFakeEDIDString),
                   (size_t )fake_edid_record->ucFakeEDIDLength);
          tmp___1 = drm_edid_is_valid(edid);
          if ((int )tmp___1) {
            adev->mode_info.bios_hardcoded_edid = edid;
            adev->mode_info.bios_hardcoded_edid_size = edid_size;
          } else {
            kfree((void const *)edid);
          }
        } else {
        }
      } else {
      }
      record = record + ((unsigned int )fake_edid_record->ucFakeEDIDLength != 0U ? (unsigned long )((int )fake_edid_record->ucFakeEDIDLength + 2) : 3UL);
      goto ldv_48425;
      case 5:
      panel_res_record = (ATOM_PANEL_RESOLUTION_PATCH_RECORD *)record;
      lvds->native_mode.width_mm = (int )panel_res_record->usHSize;
      lvds->native_mode.height_mm = (int )panel_res_record->usVSize;
      record = record + 5UL;
      goto ldv_48425;
      default:
      drm_err("Bad LCD record %d\n", (int )*record);
      bad_record = 1;
      goto ldv_48425;
      }
      ldv_48425: ;
      if ((int )bad_record) {
        goto ldv_48436;
      } else {
      }
      ldv_48438: ;
      if ((unsigned int )*record != 255U) {
        goto ldv_48437;
      } else {
      }
      ldv_48436: ;
    } else {
    }
  } else {
  }
  return (lvds);
}
}
struct amdgpu_encoder_atom_dig *amdgpu_atombios_encoder_get_dig_info(struct amdgpu_encoder *amdgpu_encoder )
{
  int encoder_enum ;
  struct amdgpu_encoder_atom_dig *dig ;
  void *tmp ;
  {
  encoder_enum = (int )((amdgpu_encoder->encoder_enum & 1792U) >> 8);
  tmp = kzalloc(264UL, 208U);
  dig = (struct amdgpu_encoder_atom_dig *)tmp;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0)) {
    return ((struct amdgpu_encoder_atom_dig *)0);
  } else {
  }
  dig->coherent_mode = 1;
  dig->dig_encoder = -1;
  if (encoder_enum == 2) {
    dig->linkb = 1;
  } else {
    dig->linkb = 0;
  }
  return (dig);
}
}
void ldv_initialize_backlight_ops_122(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1848UL);
  amdgpu_atombios_encoder_backlight_ops_group0 = (struct backlight_device *)tmp;
  return;
}
}
void ldv_main_exported_122(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_122 == 1) {
    amdgpu_atombios_encoder_update_backlight_status(amdgpu_atombios_encoder_backlight_ops_group0);
    ldv_state_variable_122 = 1;
  } else {
  }
  goto ldv_48451;
  case 1: ;
  if (ldv_state_variable_122 == 1) {
    amdgpu_atombios_encoder_get_backlight_brightness(amdgpu_atombios_encoder_backlight_ops_group0);
    ldv_state_variable_122 = 1;
  } else {
  }
  goto ldv_48451;
  default:
  ldv_stop();
  }
  ldv_48451: ;
  return;
}
}
bool ldv_queue_work_on_355(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_356(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_357(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_358(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_359(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static unsigned long arch_local_save_flags___3(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static bool static_key_false___2(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int rcu_read_lock_sched_held___2(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___3();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
bool ldv_queue_work_on_369(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_371(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_370(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_373(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_372(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static uint64_t amdgpu_sa_bo_gpu_addr(struct amdgpu_sa_bo *sa_bo )
{
  {
  return ((sa_bo->manager)->gpu_addr + (uint64_t )sa_bo->soffset);
}
}
__inline static void *amdgpu_sa_bo_cpu_addr(struct amdgpu_sa_bo *sa_bo )
{
  {
  return ((sa_bo->manager)->cpu_ptr + (unsigned long )sa_bo->soffset);
}
}
int amdgpu_sa_bo_new(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ,
                     struct amdgpu_sa_bo **sa_bo , unsigned int size , unsigned int align ) ;
void amdgpu_sa_bo_free(struct amdgpu_device *adev , struct amdgpu_sa_bo **sa_bo ,
                       struct amdgpu_fence *fence ) ;
__inline static void trace_amdgpu_semaphore_signale(int ring , struct amdgpu_semaphore *sem )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_323 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_325 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___2(& __tracepoint_amdgpu_semaphore_signale.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_semaphore_signale.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___2();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               259, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44400:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , int , struct amdgpu_semaphore * ))it_func))(__data, ring,
                                                                         sem);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44400;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_semaphore_signale.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___2();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             259, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_semaphore_wait(int ring , struct amdgpu_semaphore *sem )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_327 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_329 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___2(& __tracepoint_amdgpu_semaphore_wait.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_semaphore_wait.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___2();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               266, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44456:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , int , struct amdgpu_semaphore * ))it_func))(__data, ring,
                                                                         sem);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44456;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_semaphore_wait.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___2();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             266, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
int amdgpu_semaphore_create(struct amdgpu_device *adev , struct amdgpu_semaphore **semaphore )
{
  int r ;
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = kmalloc(24UL, 208U);
  *semaphore = (struct amdgpu_semaphore *)tmp;
  if ((unsigned long )*semaphore == (unsigned long )((struct amdgpu_semaphore *)0)) {
    return (-12);
  } else {
  }
  r = amdgpu_sa_bo_new(adev, & adev->ring_tmp_bo, & (*semaphore)->sa_bo, 8U, 8U);
  if (r != 0) {
    kfree((void const *)*semaphore);
    *semaphore = (struct amdgpu_semaphore *)0;
    return (r);
  } else {
  }
  (*semaphore)->waiters = 0;
  (*semaphore)->gpu_addr = amdgpu_sa_bo_gpu_addr((*semaphore)->sa_bo);
  tmp___0 = amdgpu_sa_bo_cpu_addr((*semaphore)->sa_bo);
  *((uint64_t *)tmp___0) = 0ULL;
  return (0);
}
}
bool amdgpu_semaphore_emit_signal(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore )
{
  bool tmp ;
  {
  trace_amdgpu_semaphore_signale((int )ring->idx, semaphore);
  tmp = (*((ring->funcs)->emit_semaphore))(ring, semaphore, 0);
  if ((int )tmp) {
    semaphore->waiters = semaphore->waiters - 1;
    ring->last_semaphore_signal_addr = semaphore->gpu_addr;
    return (1);
  } else {
  }
  return (0);
}
}
bool amdgpu_semaphore_emit_wait(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore )
{
  bool tmp ;
  {
  trace_amdgpu_semaphore_wait((int )ring->idx, semaphore);
  tmp = (*((ring->funcs)->emit_semaphore))(ring, semaphore, 1);
  if ((int )tmp) {
    semaphore->waiters = semaphore->waiters + 1;
    ring->last_semaphore_wait_addr = semaphore->gpu_addr;
    return (1);
  } else {
  }
  return (0);
}
}
void amdgpu_semaphore_free(struct amdgpu_device *adev , struct amdgpu_semaphore **semaphore ,
                           struct amdgpu_fence *fence )
{
  {
  if ((unsigned long )semaphore == (unsigned long )((struct amdgpu_semaphore **)0) || (unsigned long )*semaphore == (unsigned long )((struct amdgpu_semaphore *)0)) {
    return;
  } else {
  }
  if ((*semaphore)->waiters > 0) {
    dev_err((struct device const *)adev->dev, "semaphore %p has more waiters than signalers, hardware lockup imminent!\n",
            *semaphore);
  } else {
  }
  amdgpu_sa_bo_free(adev, & (*semaphore)->sa_bo, fence);
  kfree((void const *)*semaphore);
  *semaphore = (struct amdgpu_semaphore *)0;
  return;
}
}
bool ldv_queue_work_on_369(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_370(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_371(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_372(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_373(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void __add_wait_queue_tail(wait_queue_head_t *head , wait_queue_t *new )
{
  {
  list_add_tail(& new->task_list, & head->task_list);
  return;
}
}
extern int autoremove_wake_function(wait_queue_t * , unsigned int , int , void * ) ;
bool ldv_queue_work_on_383(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_385(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_384(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_387(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_386(struct workqueue_struct *ldv_func_arg1 ) ;
extern void schedule(void) ;
__inline static int signal_pending___0(struct task_struct *p )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = test_tsk_thread_flag(p, 2);
  tmp___0 = ldv__builtin_expect(tmp != 0, 0L);
  return ((int )tmp___0);
}
}
int amdgpu_sa_bo_manager_init(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ,
                              unsigned int size , u32 align , u32 domain ) ;
void amdgpu_sa_bo_manager_fini(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ) ;
int amdgpu_sa_bo_manager_start(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ) ;
int amdgpu_sa_bo_manager_suspend(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ) ;
void amdgpu_sa_bo_dump_debug_info(struct amdgpu_sa_manager *sa_manager , struct seq_file *m ) ;
static void amdgpu_sa_bo_remove_locked(struct amdgpu_sa_bo *sa_bo ) ;
static void amdgpu_sa_bo_try_free(struct amdgpu_sa_manager *sa_manager ) ;
int amdgpu_sa_bo_manager_init(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ,
                              unsigned int size , u32 align , u32 domain )
{
  int i ;
  int r ;
  struct lock_class_key __key ;
  {
  __init_waitqueue_head(& sa_manager->wq, "&sa_manager->wq", & __key);
  sa_manager->bo = (struct amdgpu_bo *)0;
  sa_manager->size = size;
  sa_manager->domain = domain;
  sa_manager->align = align;
  sa_manager->hole = & sa_manager->olist;
  INIT_LIST_HEAD(& sa_manager->olist);
  i = 0;
  goto ldv_43655;
  ldv_43654:
  INIT_LIST_HEAD((struct list_head *)(& sa_manager->flist) + (unsigned long )i);
  i = i + 1;
  ldv_43655: ;
  if (i <= 15) {
    goto ldv_43654;
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )size, (int )align, 1, domain, 0ULL, (struct sg_table *)0,
                       & sa_manager->bo);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate bo for manager\n",
            r);
    return (r);
  } else {
  }
  return (r);
}
}
void amdgpu_sa_bo_manager_fini(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager )
{
  struct amdgpu_sa_bo *sa_bo ;
  struct amdgpu_sa_bo *tmp ;
  int tmp___0 ;
  int tmp___1 ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  tmp___1 = list_empty((struct list_head const *)(& sa_manager->olist));
  if (tmp___1 == 0) {
    sa_manager->hole = & sa_manager->olist;
    amdgpu_sa_bo_try_free(sa_manager);
    tmp___0 = list_empty((struct list_head const *)(& sa_manager->olist));
    if (tmp___0 == 0) {
      dev_err((struct device const *)adev->dev, "sa_manager is not empty, clearing anyway\n");
    } else {
    }
  } else {
  }
  __mptr = (struct list_head const *)sa_manager->olist.next;
  sa_bo = (struct amdgpu_sa_bo *)__mptr;
  __mptr___0 = (struct list_head const *)sa_bo->olist.next;
  tmp = (struct amdgpu_sa_bo *)__mptr___0;
  goto ldv_43670;
  ldv_43669:
  amdgpu_sa_bo_remove_locked(sa_bo);
  sa_bo = tmp;
  __mptr___1 = (struct list_head const *)tmp->olist.next;
  tmp = (struct amdgpu_sa_bo *)__mptr___1;
  ldv_43670: ;
  if ((unsigned long )(& sa_bo->olist) != (unsigned long )(& sa_manager->olist)) {
    goto ldv_43669;
  } else {
  }
  amdgpu_bo_unref(& sa_manager->bo);
  sa_manager->size = 0U;
  return;
}
}
int amdgpu_sa_bo_manager_start(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager )
{
  int r ;
  {
  if ((unsigned long )sa_manager->bo == (unsigned long )((struct amdgpu_bo *)0)) {
    dev_err((struct device const *)adev->dev, "no bo for sa manager\n");
    return (-22);
  } else {
  }
  r = amdgpu_bo_reserve(sa_manager->bo, 0);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to reserve manager bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(sa_manager->bo, sa_manager->domain, & sa_manager->gpu_addr);
  if (r != 0) {
    amdgpu_bo_unreserve(sa_manager->bo);
    dev_err((struct device const *)adev->dev, "(%d) failed to pin manager bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(sa_manager->bo, & sa_manager->cpu_ptr);
  amdgpu_bo_unreserve(sa_manager->bo);
  return (r);
}
}
int amdgpu_sa_bo_manager_suspend(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager )
{
  int r ;
  {
  if ((unsigned long )sa_manager->bo == (unsigned long )((struct amdgpu_bo *)0)) {
    dev_err((struct device const *)adev->dev, "no bo for sa manager\n");
    return (-22);
  } else {
  }
  r = amdgpu_bo_reserve(sa_manager->bo, 0);
  if (r == 0) {
    amdgpu_bo_kunmap(sa_manager->bo);
    amdgpu_bo_unpin(sa_manager->bo);
    amdgpu_bo_unreserve(sa_manager->bo);
  } else {
  }
  return (r);
}
}
static void amdgpu_sa_bo_remove_locked(struct amdgpu_sa_bo *sa_bo )
{
  struct amdgpu_sa_manager *sa_manager ;
  {
  sa_manager = sa_bo->manager;
  if ((unsigned long )sa_manager->hole == (unsigned long )(& sa_bo->olist)) {
    sa_manager->hole = sa_bo->olist.prev;
  } else {
  }
  list_del_init(& sa_bo->olist);
  list_del_init(& sa_bo->flist);
  amdgpu_fence_unref(& sa_bo->fence);
  kfree((void const *)sa_bo);
  return;
}
}
static void amdgpu_sa_bo_try_free(struct amdgpu_sa_manager *sa_manager )
{
  struct amdgpu_sa_bo *sa_bo ;
  struct amdgpu_sa_bo *tmp ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  bool tmp___0 ;
  int tmp___1 ;
  struct list_head const *__mptr___1 ;
  {
  if ((unsigned long )(sa_manager->hole)->next == (unsigned long )(& sa_manager->olist)) {
    return;
  } else {
  }
  __mptr = (struct list_head const *)(sa_manager->hole)->next;
  sa_bo = (struct amdgpu_sa_bo *)__mptr;
  __mptr___0 = (struct list_head const *)sa_bo->olist.next;
  tmp = (struct amdgpu_sa_bo *)__mptr___0;
  goto ldv_43698;
  ldv_43697: ;
  if ((unsigned long )sa_bo->fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return;
  } else {
    tmp___0 = amdgpu_fence_signaled(sa_bo->fence);
    if (tmp___0) {
      tmp___1 = 0;
    } else {
      tmp___1 = 1;
    }
    if (tmp___1) {
      return;
    } else {
    }
  }
  amdgpu_sa_bo_remove_locked(sa_bo);
  sa_bo = tmp;
  __mptr___1 = (struct list_head const *)tmp->olist.next;
  tmp = (struct amdgpu_sa_bo *)__mptr___1;
  ldv_43698: ;
  if ((unsigned long )(& sa_bo->olist) != (unsigned long )(& sa_manager->olist)) {
    goto ldv_43697;
  } else {
  }
  return;
}
}
__inline static unsigned int amdgpu_sa_bo_hole_soffset(struct amdgpu_sa_manager *sa_manager )
{
  struct list_head *hole ;
  struct list_head const *__mptr ;
  {
  hole = sa_manager->hole;
  if ((unsigned long )(& sa_manager->olist) != (unsigned long )hole) {
    __mptr = (struct list_head const *)hole;
    return (((struct amdgpu_sa_bo *)__mptr)->eoffset);
  } else {
  }
  return (0U);
}
}
__inline static unsigned int amdgpu_sa_bo_hole_eoffset(struct amdgpu_sa_manager *sa_manager )
{
  struct list_head *hole ;
  struct list_head const *__mptr ;
  {
  hole = sa_manager->hole;
  if ((unsigned long )hole->next != (unsigned long )(& sa_manager->olist)) {
    __mptr = (struct list_head const *)hole->next;
    return (((struct amdgpu_sa_bo *)__mptr)->soffset);
  } else {
  }
  return (sa_manager->size);
}
}
static bool amdgpu_sa_bo_try_alloc(struct amdgpu_sa_manager *sa_manager , struct amdgpu_sa_bo *sa_bo ,
                                   unsigned int size , unsigned int align )
{
  unsigned int soffset ;
  unsigned int eoffset ;
  unsigned int wasted ;
  {
  soffset = amdgpu_sa_bo_hole_soffset(sa_manager);
  eoffset = amdgpu_sa_bo_hole_eoffset(sa_manager);
  wasted = (align - soffset % align) % align;
  if (eoffset - soffset >= size + wasted) {
    soffset = soffset + wasted;
    sa_bo->manager = sa_manager;
    sa_bo->soffset = soffset;
    sa_bo->eoffset = soffset + size;
    list_add(& sa_bo->olist, sa_manager->hole);
    INIT_LIST_HEAD(& sa_bo->flist);
    sa_manager->hole = & sa_bo->olist;
    return (1);
  } else {
  }
  return (0);
}
}
static bool amdgpu_sa_event(struct amdgpu_sa_manager *sa_manager , unsigned int size ,
                            unsigned int align )
{
  unsigned int soffset ;
  unsigned int eoffset ;
  unsigned int wasted ;
  int i ;
  int tmp ;
  {
  i = 0;
  goto ldv_43731;
  ldv_43730:
  tmp = list_empty((struct list_head const *)(& sa_manager->flist) + (unsigned long )i);
  if (tmp == 0) {
    return (1);
  } else {
  }
  i = i + 1;
  ldv_43731: ;
  if (i <= 15) {
    goto ldv_43730;
  } else {
  }
  soffset = amdgpu_sa_bo_hole_soffset(sa_manager);
  eoffset = amdgpu_sa_bo_hole_eoffset(sa_manager);
  wasted = (align - soffset % align) % align;
  if (eoffset - soffset >= size + wasted) {
    return (1);
  } else {
  }
  return (0);
}
}
static bool amdgpu_sa_bo_next_hole(struct amdgpu_sa_manager *sa_manager , struct amdgpu_fence **fences ,
                                   unsigned int *tries )
{
  struct amdgpu_sa_bo *best_bo ;
  unsigned int i ;
  unsigned int soffset ;
  unsigned int best ;
  unsigned int tmp ;
  struct amdgpu_sa_bo *sa_bo ;
  int tmp___0 ;
  struct list_head const *__mptr ;
  bool tmp___1 ;
  int tmp___2 ;
  {
  best_bo = (struct amdgpu_sa_bo *)0;
  if ((unsigned long )(sa_manager->hole)->next == (unsigned long )(& sa_manager->olist)) {
    sa_manager->hole = & sa_manager->olist;
    return (1);
  } else {
  }
  soffset = amdgpu_sa_bo_hole_soffset(sa_manager);
  best = sa_manager->size * 2U;
  i = 0U;
  goto ldv_43748;
  ldv_43747:
  tmp___0 = list_empty((struct list_head const *)(& sa_manager->flist) + (unsigned long )i);
  if (tmp___0 != 0) {
    goto ldv_43744;
  } else {
  }
  __mptr = (struct list_head const *)((struct list_head *)(& sa_manager->flist) + (unsigned long )i)->next;
  sa_bo = (struct amdgpu_sa_bo *)__mptr + 0xfffffffffffffff0UL;
  tmp___1 = amdgpu_fence_signaled(sa_bo->fence);
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2) {
    *(fences + (unsigned long )i) = sa_bo->fence;
    goto ldv_43744;
  } else {
  }
  if (*(tries + (unsigned long )i) > 2U) {
    goto ldv_43744;
  } else {
  }
  tmp = sa_bo->soffset;
  if (tmp < soffset) {
    tmp = sa_manager->size + tmp;
  } else {
  }
  tmp = tmp - soffset;
  if (tmp < best) {
    best = tmp;
    best_bo = sa_bo;
  } else {
  }
  ldv_43744:
  i = i + 1U;
  ldv_43748: ;
  if (i <= 15U) {
    goto ldv_43747;
  } else {
  }
  if ((unsigned long )best_bo != (unsigned long )((struct amdgpu_sa_bo *)0)) {
    *(tries + (unsigned long )((best_bo->fence)->ring)->idx) = *(tries + (unsigned long )((best_bo->fence)->ring)->idx) + 1U;
    sa_manager->hole = best_bo->olist.prev;
    amdgpu_sa_bo_remove_locked(best_bo);
    return (1);
  } else {
  }
  return (0);
}
}
int amdgpu_sa_bo_new(struct amdgpu_device *adev , struct amdgpu_sa_manager *sa_manager ,
                     struct amdgpu_sa_bo **sa_bo , unsigned int size , unsigned int align )
{
  struct amdgpu_fence *fences[16U] ;
  unsigned int tries[16U] ;
  int i ;
  int r ;
  long tmp ;
  long tmp___0 ;
  void *tmp___1 ;
  bool tmp___2 ;
  bool tmp___3 ;
  int __ret ;
  wait_queue_t __wait ;
  struct task_struct *tmp___5 ;
  int tmp___6 ;
  long tmp___7 ;
  struct task_struct *tmp___8 ;
  long volatile __ret___0 ;
  struct task_struct *tmp___9 ;
  struct task_struct *tmp___10 ;
  struct task_struct *tmp___11 ;
  struct task_struct *tmp___12 ;
  struct task_struct *tmp___13 ;
  int tmp___14 ;
  bool tmp___15 ;
  int tmp___16 ;
  struct task_struct *tmp___17 ;
  struct task_struct *tmp___18 ;
  bool tmp___19 ;
  {
  tmp = ldv__builtin_expect(sa_manager->align < align, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c"),
                         "i" (321), "i" (12UL));
    ldv_43761: ;
    goto ldv_43761;
  } else {
  }
  tmp___0 = ldv__builtin_expect(sa_manager->size < size, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_sa.c"),
                         "i" (322), "i" (12UL));
    ldv_43762: ;
    goto ldv_43762;
  } else {
  }
  tmp___1 = kmalloc(56UL, 208U);
  *sa_bo = (struct amdgpu_sa_bo *)tmp___1;
  if ((unsigned long )*sa_bo == (unsigned long )((struct amdgpu_sa_bo *)0)) {
    return (-12);
  } else {
  }
  (*sa_bo)->manager = sa_manager;
  (*sa_bo)->fence = (struct amdgpu_fence *)0;
  INIT_LIST_HEAD(& (*sa_bo)->olist);
  INIT_LIST_HEAD(& (*sa_bo)->flist);
  spin_lock(& sa_manager->wq.lock);
  ldv_43781:
  i = 0;
  goto ldv_43764;
  ldv_43763:
  fences[i] = (struct amdgpu_fence *)0;
  tries[i] = 0U;
  i = i + 1;
  ldv_43764: ;
  if (i <= 15) {
    goto ldv_43763;
  } else {
  }
  ldv_43766:
  amdgpu_sa_bo_try_free(sa_manager);
  tmp___2 = amdgpu_sa_bo_try_alloc(sa_manager, *sa_bo, size, align);
  if ((int )tmp___2) {
    spin_unlock(& sa_manager->wq.lock);
    return (0);
  } else {
  }
  tmp___3 = amdgpu_sa_bo_next_hole(sa_manager, (struct amdgpu_fence **)(& fences),
                                   (unsigned int *)(& tries));
  if ((int )tmp___3) {
    goto ldv_43766;
  } else {
  }
  spin_unlock(& sa_manager->wq.lock);
  r = amdgpu_fence_wait_any(adev, (struct amdgpu_fence **)(& fences), 0);
  spin_lock(& sa_manager->wq.lock);
  if (r == -2) {
    tmp___19 = amdgpu_sa_event(sa_manager, size, align);
    if ((int )tmp___19) {
      r = 0;
    } else {
      __ret = 0;
      tmp___5 = get_current();
      __wait.flags = 0U;
      __wait.private = (void *)tmp___5;
      __wait.func = & autoremove_wake_function;
      __wait.task_list.next = & __wait.task_list;
      __wait.task_list.prev = & __wait.task_list;
      ldv_43779:
      tmp___6 = list_empty((struct list_head const *)(& __wait.task_list));
      tmp___7 = ldv__builtin_expect(tmp___6 != 0, 1L);
      if (tmp___7 != 0L) {
        __add_wait_queue_tail(& sa_manager->wq, & __wait);
      } else {
      }
      tmp___8 = get_current();
      tmp___8->task_state_change = 0UL;
      __ret___0 = 1L;
      switch (8UL) {
      case 1UL:
      tmp___9 = get_current();
      __asm__ volatile ("xchgb %b0, %1\n": "+q" (__ret___0), "+m" (tmp___9->state): : "memory",
                           "cc");
      goto ldv_43772;
      case 2UL:
      tmp___10 = get_current();
      __asm__ volatile ("xchgw %w0, %1\n": "+r" (__ret___0), "+m" (tmp___10->state): : "memory",
                           "cc");
      goto ldv_43772;
      case 4UL:
      tmp___11 = get_current();
      __asm__ volatile ("xchgl %0, %1\n": "+r" (__ret___0), "+m" (tmp___11->state): : "memory",
                           "cc");
      goto ldv_43772;
      case 8UL:
      tmp___12 = get_current();
      __asm__ volatile ("xchgq %q0, %1\n": "+r" (__ret___0), "+m" (tmp___12->state): : "memory",
                           "cc");
      goto ldv_43772;
      default:
      __xchg_wrong_size();
      }
      ldv_43772:
      tmp___13 = get_current();
      tmp___14 = signal_pending___0(tmp___13);
      if (tmp___14 != 0) {
        __ret = -512;
        goto ldv_43778;
      } else {
      }
      spin_unlock(& sa_manager->wq.lock);
      schedule();
      spin_lock(& sa_manager->wq.lock);
      tmp___15 = amdgpu_sa_event(sa_manager, size, align);
      if (tmp___15) {
        tmp___16 = 0;
      } else {
        tmp___16 = 1;
      }
      if (tmp___16) {
        goto ldv_43779;
      } else {
      }
      ldv_43778:
      __remove_wait_queue(& sa_manager->wq, & __wait);
      tmp___17 = get_current();
      tmp___17->task_state_change = 0UL;
      tmp___18 = get_current();
      tmp___18->state = 0L;
      r = __ret;
    }
  } else {
  }
  if (r == 0) {
    goto ldv_43781;
  } else {
  }
  spin_unlock(& sa_manager->wq.lock);
  kfree((void const *)*sa_bo);
  *sa_bo = (struct amdgpu_sa_bo *)0;
  return (r);
}
}
void amdgpu_sa_bo_free(struct amdgpu_device *adev , struct amdgpu_sa_bo **sa_bo ,
                       struct amdgpu_fence *fence )
{
  struct amdgpu_sa_manager *sa_manager ;
  bool tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )sa_bo == (unsigned long )((struct amdgpu_sa_bo **)0) || (unsigned long )*sa_bo == (unsigned long )((struct amdgpu_sa_bo *)0)) {
    return;
  } else {
  }
  sa_manager = (*sa_bo)->manager;
  spin_lock(& sa_manager->wq.lock);
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0)) {
    tmp = amdgpu_fence_signaled(fence);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    if (tmp___0) {
      (*sa_bo)->fence = amdgpu_fence_ref(fence);
      list_add_tail(& (*sa_bo)->flist, (struct list_head *)(& sa_manager->flist) + (unsigned long )(fence->ring)->idx);
    } else {
      amdgpu_sa_bo_remove_locked(*sa_bo);
    }
  } else {
    amdgpu_sa_bo_remove_locked(*sa_bo);
  }
  __wake_up_locked(& sa_manager->wq, 3U, 0);
  spin_unlock(& sa_manager->wq.lock);
  *sa_bo = (struct amdgpu_sa_bo *)0;
  return;
}
}
void amdgpu_sa_bo_dump_debug_info(struct amdgpu_sa_manager *sa_manager , struct seq_file *m )
{
  struct amdgpu_sa_bo *i ;
  struct list_head const *__mptr ;
  uint64_t soffset ;
  uint64_t eoffset ;
  struct list_head const *__mptr___0 ;
  {
  spin_lock(& sa_manager->wq.lock);
  __mptr = (struct list_head const *)sa_manager->olist.next;
  i = (struct amdgpu_sa_bo *)__mptr;
  goto ldv_43801;
  ldv_43800:
  soffset = (uint64_t )i->soffset + sa_manager->gpu_addr;
  eoffset = (uint64_t )i->eoffset + sa_manager->gpu_addr;
  if ((unsigned long )(& i->olist) == (unsigned long )sa_manager->hole) {
    seq_printf(m, ">");
  } else {
    seq_printf(m, " ");
  }
  seq_printf(m, "[0x%010llx 0x%010llx] size %8lld", soffset, eoffset, eoffset - soffset);
  if ((unsigned long )i->fence != (unsigned long )((struct amdgpu_fence *)0)) {
    seq_printf(m, " protected by 0x%016llx on ring %d", (i->fence)->seq, ((i->fence)->ring)->idx);
  } else {
  }
  seq_printf(m, "\n");
  __mptr___0 = (struct list_head const *)i->olist.next;
  i = (struct amdgpu_sa_bo *)__mptr___0;
  ldv_43801: ;
  if ((unsigned long )(& i->olist) != (unsigned long )(& sa_manager->olist)) {
    goto ldv_43800;
  } else {
  }
  spin_unlock(& sa_manager->wq.lock);
  return;
}
}
bool ldv_queue_work_on_383(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_384(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_385(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_386(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_387(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_397(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_399(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_398(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_401(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_400(struct workqueue_struct *ldv_func_arg1 ) ;
static int amdgpu_atombios_i2c_process_i2c_ch(struct amdgpu_i2c_chan *chan , u8 slave_addr ,
                                              u8 flags , u8 *buf , u8 num )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  PROCESS_I2C_CHANNEL_TRANSACTION_PARAMETERS args ;
  int index ;
  unsigned char *base ;
  u16 out ;
  int r ;
  long tmp ;
  {
  dev = chan->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  index = 54;
  out = 0U;
  r = 0;
  memset((void *)(& args), 0, 8UL);
  mutex_lock_nested(& chan->mutex, 0U);
  base = (unsigned char *)(adev->mode_info.atom_context)->scratch;
  if ((int )flags & 1) {
    if ((unsigned int )num > 3U) {
      drm_err("hw i2c: tried to write too many bytes (%d vs 3)\n", (int )num);
      r = -22;
      goto done;
    } else {
    }
    if ((unsigned long )buf == (unsigned long )((u8 *)0U)) {
      args.__annonCompField119.ucRegIndex = 0U;
    } else {
      args.__annonCompField119.ucRegIndex = *buf;
    }
    if ((unsigned int )num != 0U) {
      num = (u8 )((int )num - 1);
    } else {
    }
    if ((unsigned int )num != 0U) {
      memcpy((void *)(& out), (void const *)buf + 1U, (size_t )num);
    } else {
    }
    args.lpI2CDataOut = out;
  } else {
    args.__annonCompField119.ucRegIndex = 0U;
    args.lpI2CDataOut = 0U;
  }
  args.ucFlag = flags;
  args.ucI2CSpeed = 50U;
  args.ucTransBytes = num;
  args.ucSlaveAddr = (int )slave_addr << 1U;
  args.ucLineNumber = chan->rec.i2c_id;
  amdgpu_atom_execute_table(adev->mode_info.atom_context, index, (u32 *)(& args));
  if ((unsigned int )args.__annonCompField119.ucStatus != 1U) {
    tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_atombios_i2c_process_i2c_ch", "hw_i2c error\n");
    } else {
    }
    r = -5;
    goto done;
  } else {
  }
  if (((int )flags & 1) == 0) {
    amdgpu_atombios_copy_swap(buf, base, (int )num, 0);
  } else {
  }
  done:
  mutex_unlock(& chan->mutex);
  return (r);
}
}
int amdgpu_atombios_i2c_xfer(struct i2c_adapter *i2c_adap , struct i2c_msg *msgs ,
                             int num )
{
  struct amdgpu_i2c_chan *i2c ;
  void *tmp ;
  struct i2c_msg *p ;
  int i ;
  int remaining ;
  int current_count ;
  int buffer_offset ;
  int max_bytes ;
  int ret ;
  u8 flags ;
  {
  tmp = i2c_get_adapdata((struct i2c_adapter const *)i2c_adap);
  i2c = (struct amdgpu_i2c_chan *)tmp;
  p = msgs;
  if (num == 1 && (unsigned int )p->len == 0U) {
    ret = amdgpu_atombios_i2c_process_i2c_ch(i2c, (int )((u8 )p->addr), 1, (u8 *)0U,
                                             0);
    if (ret != 0) {
      return (ret);
    } else {
      return (num);
    }
  } else {
  }
  i = 0;
  goto ldv_47931;
  ldv_47930:
  p = msgs + (unsigned long )i;
  remaining = (int )p->len;
  buffer_offset = 0;
  if ((int )p->flags & 1) {
    max_bytes = 255;
    flags = 0U;
  } else {
    max_bytes = 3;
    flags = 1U;
  }
  goto ldv_47928;
  ldv_47927: ;
  if (remaining > max_bytes) {
    current_count = max_bytes;
  } else {
    current_count = remaining;
  }
  ret = amdgpu_atombios_i2c_process_i2c_ch(i2c, (int )((u8 )p->addr), (int )flags,
                                           p->buf + (unsigned long )buffer_offset,
                                           (int )((u8 )current_count));
  if (ret != 0) {
    return (ret);
  } else {
  }
  remaining = remaining - current_count;
  buffer_offset = buffer_offset + current_count;
  ldv_47928: ;
  if (remaining != 0) {
    goto ldv_47927;
  } else {
  }
  i = i + 1;
  ldv_47931: ;
  if (i < num) {
    goto ldv_47930;
  } else {
  }
  return (num);
}
}
u32 amdgpu_atombios_i2c_func(struct i2c_adapter *adap )
{
  {
  return (251592713U);
}
}
bool ldv_queue_work_on_397(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_398(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_399(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_400(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_401(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void *ERR_PTR(long error ) ;
bool ldv_queue_work_on_411(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_413(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_412(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_415(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_414(struct workqueue_struct *ldv_func_arg1 ) ;
extern struct dma_buf *drm_gem_prime_export(struct drm_device * , struct drm_gem_object * ,
                                            int ) ;
extern struct sg_table *drm_prime_pages_to_sg(struct page ** , unsigned int ) ;
struct sg_table *amdgpu_gem_prime_get_sg_table(struct drm_gem_object *obj )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  int npages ;
  struct sg_table *tmp ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  npages = (int )bo->tbo.num_pages;
  tmp = drm_prime_pages_to_sg((bo->tbo.ttm)->pages, (unsigned int )npages);
  return (tmp);
}
}
void *amdgpu_gem_prime_vmap(struct drm_gem_object *obj )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  int ret ;
  void *tmp ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  ret = ttm_bo_kmap(& bo->tbo, 0UL, bo->tbo.num_pages, & bo->dma_buf_vmap);
  if (ret != 0) {
    tmp = ERR_PTR((long )ret);
    return (tmp);
  } else {
  }
  return (bo->dma_buf_vmap.virtual);
}
}
void amdgpu_gem_prime_vunmap(struct drm_gem_object *obj , void *vaddr )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  ttm_bo_kunmap(& bo->dma_buf_vmap);
  return;
}
}
struct drm_gem_object *amdgpu_gem_prime_import_sg_table(struct drm_device *dev , struct dma_buf_attachment *attach ,
                                                        struct sg_table *sg )
{
  struct amdgpu_device *adev ;
  struct amdgpu_bo *bo ;
  int ret ;
  void *tmp ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  ret = amdgpu_bo_create(adev, (attach->dmabuf)->size, 4096, 0, 2U, 0ULL, sg, & bo);
  if (ret != 0) {
    tmp = ERR_PTR((long )ret);
    return ((struct drm_gem_object *)tmp);
  } else {
  }
  mutex_lock_nested(& adev->gem.mutex, 0U);
  list_add_tail(& bo->list, & adev->gem.objects);
  mutex_unlock(& adev->gem.mutex);
  return (& bo->gem_base);
}
}
int amdgpu_gem_prime_pin(struct drm_gem_object *obj )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  int ret ;
  long tmp ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  ret = 0;
  ret = amdgpu_bo_reserve(bo, 0);
  tmp = ldv__builtin_expect(ret != 0, 0L);
  if (tmp != 0L) {
    return (ret);
  } else {
  }
  ret = amdgpu_bo_pin(bo, 2U, (u64 *)0ULL);
  amdgpu_bo_unreserve(bo);
  return (ret);
}
}
void amdgpu_gem_prime_unpin(struct drm_gem_object *obj )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  int ret ;
  long tmp ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  ret = 0;
  ret = amdgpu_bo_reserve(bo, 0);
  tmp = ldv__builtin_expect(ret != 0, 0L);
  if (tmp != 0L) {
    return;
  } else {
  }
  amdgpu_bo_unpin(bo);
  amdgpu_bo_unreserve(bo);
  return;
}
}
struct reservation_object *amdgpu_gem_prime_res_obj(struct drm_gem_object *obj )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  {
  __mptr = (struct drm_gem_object const *)obj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  return (bo->tbo.resv);
}
}
struct dma_buf *amdgpu_gem_prime_export(struct drm_device *dev , struct drm_gem_object *gobj ,
                                        int flags )
{
  struct amdgpu_bo *bo ;
  struct drm_gem_object const *__mptr ;
  void *tmp ;
  bool tmp___0 ;
  struct dma_buf *tmp___1 ;
  {
  __mptr = (struct drm_gem_object const *)gobj;
  bo = (struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL;
  tmp___0 = amdgpu_ttm_tt_has_userptr(bo->tbo.ttm);
  if ((int )tmp___0) {
    tmp = ERR_PTR(-1L);
    return ((struct dma_buf *)tmp);
  } else {
  }
  tmp___1 = drm_gem_prime_export(dev, gobj, flags);
  return (tmp___1);
}
}
bool ldv_queue_work_on_411(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_412(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_413(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_414(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_415(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static unsigned long arch_local_save_flags___4(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static bool static_key_false___3(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int rcu_read_lock_sched_held___3(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___4();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
extern struct rb_node *rb_first_postorder(struct rb_root const * ) ;
extern struct rb_node *rb_next_postorder(struct rb_node const * ) ;
bool ldv_queue_work_on_425(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_427(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_426(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_429(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_428(struct workqueue_struct *ldv_func_arg1 ) ;
extern void interval_tree_insert(struct interval_tree_node * , struct rb_root * ) ;
extern void interval_tree_remove(struct interval_tree_node * , struct rb_root * ) ;
extern struct interval_tree_node *interval_tree_iter_first(struct rb_root * , unsigned long ,
                                                           unsigned long ) ;
extern int reservation_object_reserve_shared(struct reservation_object * ) ;
__inline static bool amdgpu_fence_is_earlier(struct amdgpu_fence *a , struct amdgpu_fence *b )
{
  long tmp ;
  {
  if ((unsigned long )a == (unsigned long )((struct amdgpu_fence *)0)) {
    return (0);
  } else {
  }
  if ((unsigned long )b == (unsigned long )((struct amdgpu_fence *)0)) {
    return (1);
  } else {
  }
  tmp = ldv__builtin_expect((unsigned long )a->ring != (unsigned long )b->ring, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu.h"),
                         "i" (479), "i" (12UL));
    ldv_42050: ;
    goto ldv_42050;
  } else {
  }
  return (a->seq < b->seq);
}
}
struct amdgpu_fence *amdgpu_vm_grab_id(struct amdgpu_ring *ring , struct amdgpu_vm *vm ) ;
void amdgpu_vm_flush(struct amdgpu_ring *ring , struct amdgpu_vm *vm , struct amdgpu_fence *updates ) ;
void amdgpu_vm_fence(struct amdgpu_device *adev , struct amdgpu_vm *vm , struct amdgpu_fence *fence ) ;
uint64_t amdgpu_vm_map_gart(struct amdgpu_device *adev , uint64_t addr ) ;
__inline static void trace_amdgpu_vm_grab_id(unsigned int vmid , int ring )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_283 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_285 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_grab_id.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_grab_id.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               64, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43810:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , unsigned int , int ))it_func))(__data, vmid, ring);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43810;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_grab_id.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             64, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_vm_bo_map(struct amdgpu_bo_va *bo_va , struct amdgpu_bo_va_mapping *mapping )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_287 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_289 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_bo_map.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_map.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               88, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43866:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_bo_va * , struct amdgpu_bo_va_mapping * ))it_func))(__data,
                                                                                              bo_va,
                                                                                              mapping);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43866;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_map.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             88, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_vm_bo_unmap(struct amdgpu_bo_va *bo_va , struct amdgpu_bo_va_mapping *mapping )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_291 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_293 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_bo_unmap.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_unmap.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               112, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43922:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_bo_va * , struct amdgpu_bo_va_mapping * ))it_func))(__data,
                                                                                              bo_va,
                                                                                              mapping);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43922;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_unmap.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             112, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_vm_bo_update(struct amdgpu_bo_va_mapping *mapping )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_295 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_297 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_bo_update.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_update.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               130, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_43976:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_bo_va_mapping * ))it_func))(__data, mapping);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_43976;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_bo_update.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             130, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_vm_set_page(uint64_t pe , uint64_t addr , unsigned int count ,
                                              u32 incr , u32 flags )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_299 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_301 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_set_page.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_set_page.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               154, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44035:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , uint64_t , uint64_t , unsigned int , u32 , u32 ))it_func))(__data,
                                                                                            pe,
                                                                                            addr,
                                                                                            count,
                                                                                            incr,
                                                                                            flags);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44035;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_set_page.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             154, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
__inline static void trace_amdgpu_vm_flush(uint64_t pd_addr , unsigned int ring ,
                                           unsigned int id )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_303 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_305 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___3(& __tracepoint_amdgpu_vm_flush.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_flush.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___3();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               172, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44102:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , uint64_t , unsigned int , unsigned int ))it_func))(__data,
                                                                                  pd_addr,
                                                                                  ring,
                                                                                  id);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44102;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_vm_flush.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___3();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             172, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
static unsigned int amdgpu_vm_num_pdes(struct amdgpu_device *adev )
{
  {
  return (adev->vm_manager.max_pfn >> amdgpu_vm_block_size);
}
}
static unsigned int amdgpu_vm_directory_size(struct amdgpu_device *adev )
{
  unsigned int tmp ;
  {
  tmp = amdgpu_vm_num_pdes(adev);
  return ((tmp * 8U + 4095U) & 4294963200U);
}
}
struct amdgpu_bo_list_entry *amdgpu_vm_get_bos(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                               struct list_head *head )
{
  struct amdgpu_bo_list_entry *list ;
  unsigned int i ;
  unsigned int idx ;
  void *tmp ;
  unsigned int tmp___0 ;
  {
  mutex_lock_nested(& vm->mutex, 0U);
  tmp = drm_malloc_ab((size_t )(vm->max_pde_used + 2U), 64UL);
  list = (struct amdgpu_bo_list_entry *)tmp;
  if ((unsigned long )list == (unsigned long )((struct amdgpu_bo_list_entry *)0)) {
    mutex_unlock(& vm->mutex);
    return ((struct amdgpu_bo_list_entry *)0);
  } else {
  }
  list->robj = vm->page_directory;
  list->prefered_domains = 4U;
  list->allowed_domains = 4U;
  list->priority = 0U;
  list->tv.bo = & (vm->page_directory)->tbo;
  list->tv.shared = 1;
  list_add(& list->tv.head, head);
  i = 0U;
  idx = 1U;
  goto ldv_44507;
  ldv_44506: ;
  if ((unsigned long )(vm->page_tables + (unsigned long )i)->bo == (unsigned long )((struct amdgpu_bo *)0)) {
    goto ldv_44505;
  } else {
  }
  (list + (unsigned long )idx)->robj = (vm->page_tables + (unsigned long )i)->bo;
  (list + (unsigned long )idx)->prefered_domains = 4U;
  (list + (unsigned long )idx)->allowed_domains = 4U;
  (list + (unsigned long )idx)->priority = 0U;
  (list + (unsigned long )idx)->tv.bo = & ((list + (unsigned long )idx)->robj)->tbo;
  (list + (unsigned long )idx)->tv.shared = 1;
  tmp___0 = idx;
  idx = idx + 1U;
  list_add(& (list + (unsigned long )tmp___0)->tv.head, head);
  ldv_44505:
  i = i + 1U;
  ldv_44507: ;
  if (vm->max_pde_used >= i) {
    goto ldv_44506;
  } else {
  }
  mutex_unlock(& vm->mutex);
  return (list);
}
}
struct amdgpu_fence *amdgpu_vm_grab_id(struct amdgpu_ring *ring , struct amdgpu_vm *vm )
{
  struct amdgpu_fence *best[16U] ;
  struct amdgpu_vm_id *vm_id ;
  struct amdgpu_device *adev ;
  unsigned int choices[2U] ;
  unsigned int i ;
  struct amdgpu_fence *fence ;
  bool tmp ;
  {
  best[0] = 0;
  best[1] = 0;
  best[2] = 0;
  best[3] = 0;
  best[4] = 0;
  best[5] = 0;
  best[6] = 0;
  best[7] = 0;
  best[8] = 0;
  best[9] = 0;
  best[10] = 0;
  best[11] = 0;
  best[12] = 0;
  best[13] = 0;
  best[14] = 0;
  best[15] = 0;
  vm_id = (struct amdgpu_vm_id *)(& vm->ids) + (unsigned long )ring->idx;
  adev = ring->adev;
  choices[0] = 0U;
  choices[1] = 0U;
  if ((vm_id->id != 0U && (unsigned long )vm_id->last_id_use != (unsigned long )((struct amdgpu_fence *)0)) && (unsigned long )vm_id->last_id_use == (unsigned long )adev->vm_manager.active[vm_id->id]) {
    return ((struct amdgpu_fence *)0);
  } else {
  }
  vm_id->pd_gpu_addr = 0xffffffffffffffffULL;
  i = 1U;
  goto ldv_44520;
  ldv_44519:
  fence = adev->vm_manager.active[i];
  if ((unsigned long )fence == (unsigned long )((struct amdgpu_fence *)0)) {
    vm_id->id = i;
    trace_amdgpu_vm_grab_id(i, (int )ring->idx);
    return ((struct amdgpu_fence *)0);
  } else {
  }
  tmp = amdgpu_fence_is_earlier(fence, best[(fence->ring)->idx]);
  if ((int )tmp) {
    best[(fence->ring)->idx] = fence;
    choices[(unsigned long )fence->ring != (unsigned long )ring] = i;
  } else {
  }
  i = i + 1U;
  ldv_44520: ;
  if (adev->vm_manager.nvm > i) {
    goto ldv_44519;
  } else {
  }
  i = 0U;
  goto ldv_44523;
  ldv_44522: ;
  if (choices[i] != 0U) {
    vm_id->id = choices[i];
    trace_amdgpu_vm_grab_id(choices[i], (int )ring->idx);
    return (adev->vm_manager.active[choices[i]]);
  } else {
  }
  i = i + 1U;
  ldv_44523: ;
  if (i <= 1U) {
    goto ldv_44522;
  } else {
  }
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c"),
                       "i" (182), "i" (12UL));
  ldv_44525: ;
  goto ldv_44525;
  return ((struct amdgpu_fence *)0);
}
}
void amdgpu_vm_flush(struct amdgpu_ring *ring , struct amdgpu_vm *vm , struct amdgpu_fence *updates )
{
  uint64_t pd_addr ;
  u64 tmp ;
  struct amdgpu_vm_id *vm_id ;
  bool tmp___0 ;
  {
  tmp = amdgpu_bo_gpu_offset(vm->page_directory);
  pd_addr = tmp;
  vm_id = (struct amdgpu_vm_id *)(& vm->ids) + (unsigned long )ring->idx;
  if (vm_id->pd_gpu_addr != pd_addr || (unsigned long )vm_id->flushed_updates == (unsigned long )((struct amdgpu_fence *)0)) {
    trace_amdgpu_vm_flush(pd_addr, ring->idx, vm_id->id);
    amdgpu_fence_unref(& vm_id->flushed_updates);
    vm_id->flushed_updates = amdgpu_fence_ref(updates);
    vm_id->pd_gpu_addr = pd_addr;
    (*((ring->funcs)->emit_vm_flush))(ring, vm_id->id, vm_id->pd_gpu_addr);
  } else {
    tmp___0 = amdgpu_fence_is_earlier(vm_id->flushed_updates, updates);
    if ((int )tmp___0) {
      trace_amdgpu_vm_flush(pd_addr, ring->idx, vm_id->id);
      amdgpu_fence_unref(& vm_id->flushed_updates);
      vm_id->flushed_updates = amdgpu_fence_ref(updates);
      vm_id->pd_gpu_addr = pd_addr;
      (*((ring->funcs)->emit_vm_flush))(ring, vm_id->id, vm_id->pd_gpu_addr);
    } else {
    }
  }
  return;
}
}
void amdgpu_vm_fence(struct amdgpu_device *adev , struct amdgpu_vm *vm , struct amdgpu_fence *fence )
{
  unsigned int ridx ;
  unsigned int vm_id ;
  {
  ridx = (fence->ring)->idx;
  vm_id = vm->ids[ridx].id;
  amdgpu_fence_unref((struct amdgpu_fence **)(& adev->vm_manager.active) + (unsigned long )vm_id);
  adev->vm_manager.active[vm_id] = amdgpu_fence_ref(fence);
  amdgpu_fence_unref(& vm->ids[ridx].last_id_use);
  vm->ids[ridx].last_id_use = amdgpu_fence_ref(fence);
  return;
}
}
struct amdgpu_bo_va *amdgpu_vm_bo_find(struct amdgpu_vm *vm , struct amdgpu_bo *bo )
{
  struct amdgpu_bo_va *bo_va ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  __mptr = (struct list_head const *)bo->va.next;
  bo_va = (struct amdgpu_bo_va *)__mptr;
  goto ldv_44550;
  ldv_44549: ;
  if ((unsigned long )bo_va->vm == (unsigned long )vm) {
    return (bo_va);
  } else {
  }
  __mptr___0 = (struct list_head const *)bo_va->bo_list.next;
  bo_va = (struct amdgpu_bo_va *)__mptr___0;
  ldv_44550: ;
  if ((unsigned long )(& bo_va->bo_list) != (unsigned long )(& bo->va)) {
    goto ldv_44549;
  } else {
  }
  return ((struct amdgpu_bo_va *)0);
}
}
static void amdgpu_vm_update_pages(struct amdgpu_device *adev , struct amdgpu_ib *ib ,
                                   uint64_t pe , uint64_t addr , unsigned int count ,
                                   u32 incr , u32 flags , u32 gtt_flags )
{
  uint64_t src ;
  {
  trace_amdgpu_vm_set_page(pe, addr, count, incr, flags);
  if ((flags & 2U) != 0U && flags == gtt_flags) {
    src = adev->gart.table_addr + (addr >> 12) * 8ULL;
    (*((adev->vm_manager.vm_pte_funcs)->copy_pte))(ib, pe, src, count);
  } else
  if ((flags & 2U) != 0U || count <= 2U) {
    (*((adev->vm_manager.vm_pte_funcs)->write_pte))(ib, pe, addr, count, incr, flags);
  } else {
    (*((adev->vm_manager.vm_pte_funcs)->set_pte_pde))(ib, pe, addr, count, incr, flags);
  }
  return;
}
}
static int amdgpu_vm_clear_bo(struct amdgpu_device *adev , struct amdgpu_bo *bo )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_ib ib ;
  unsigned int entries ;
  uint64_t addr ;
  int r ;
  unsigned long tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  {
  ring = adev->vm_manager.vm_pte_funcs_ring;
  r = amdgpu_bo_reserve(bo, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  r = reservation_object_reserve_shared(bo->tbo.resv);
  if (r != 0) {
    return (r);
  } else {
  }
  r = ttm_bo_validate(& bo->tbo, & bo->placement, 1, 0);
  if (r != 0) {
    goto error_unreserve;
  } else {
  }
  addr = amdgpu_bo_gpu_offset(bo);
  tmp = amdgpu_bo_size(bo);
  entries = (unsigned int )(tmp / 8UL);
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, (entries + 32U) * 2U, & ib);
  if (r != 0) {
    goto error_unreserve;
  } else {
  }
  ib.length_dw = 0U;
  amdgpu_vm_update_pages(adev, & ib, addr, 0ULL, entries, 0U, 0U, 0U);
  (*((adev->vm_manager.vm_pte_funcs)->pad_ib))(& ib);
  __ret_warn_on = ib.length_dw > 64U;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c",
                       341);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)1);
  if (r != 0) {
    goto error_free;
  } else {
  }
  amdgpu_bo_fence(bo, ib.fence, 1);
  error_free:
  amdgpu_ib_free(adev, & ib);
  error_unreserve:
  amdgpu_bo_unreserve(bo);
  return (r);
}
}
uint64_t amdgpu_vm_map_gart(struct amdgpu_device *adev , uint64_t addr )
{
  uint64_t result ;
  {
  result = *(adev->gart.pages_addr + (addr >> 12));
  result = (addr & 4095ULL) | result;
  return (result);
}
}
int amdgpu_vm_update_page_directory(struct amdgpu_device *adev , struct amdgpu_vm *vm )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_bo *pd ;
  uint64_t pd_addr ;
  u64 tmp ;
  u32 incr ;
  uint64_t last_pde ;
  uint64_t last_pt ;
  unsigned int count ;
  unsigned int pt_idx ;
  unsigned int ndw ;
  struct amdgpu_ib ib ;
  int r ;
  struct amdgpu_bo *bo ;
  uint64_t pde ;
  uint64_t pt ;
  int __ret_warn_on ;
  long tmp___0 ;
  {
  ring = adev->vm_manager.vm_pte_funcs_ring;
  pd = vm->page_directory;
  tmp = amdgpu_bo_gpu_offset(pd);
  pd_addr = tmp;
  incr = (u32 )(8 << amdgpu_vm_block_size);
  last_pde = 0xffffffffffffffffULL;
  last_pt = 0xffffffffffffffffULL;
  count = 0U;
  ndw = 64U;
  ndw = vm->max_pde_used * 6U + ndw;
  if (ndw > 1048575U) {
    return (-12);
  } else {
  }
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, ndw * 4U, & ib);
  if (r != 0) {
    return (r);
  } else {
  }
  ib.length_dw = 0U;
  pt_idx = 0U;
  goto ldv_44601;
  ldv_44600:
  bo = (vm->page_tables + (unsigned long )pt_idx)->bo;
  if ((unsigned long )bo == (unsigned long )((struct amdgpu_bo *)0)) {
    goto ldv_44599;
  } else {
  }
  pt = amdgpu_bo_gpu_offset(bo);
  if ((vm->page_tables + (unsigned long )pt_idx)->addr == pt) {
    goto ldv_44599;
  } else {
  }
  (vm->page_tables + (unsigned long )pt_idx)->addr = pt;
  pde = (uint64_t )(pt_idx * 8U) + pd_addr;
  if ((uint64_t )(count * 8U) + last_pde != pde || (uint64_t )(incr * count) + last_pt != pt) {
    if (count != 0U) {
      amdgpu_vm_update_pages(adev, & ib, last_pde, last_pt, count, incr, 1U, 0U);
    } else {
    }
    count = 1U;
    last_pde = pde;
    last_pt = pt;
  } else {
    count = count + 1U;
  }
  ldv_44599:
  pt_idx = pt_idx + 1U;
  ldv_44601: ;
  if (vm->max_pde_used >= pt_idx) {
    goto ldv_44600;
  } else {
  }
  if (count != 0U) {
    amdgpu_vm_update_pages(adev, & ib, last_pde, last_pt, count, incr, 1U, 0U);
  } else {
  }
  if (ib.length_dw != 0U) {
    (*((adev->vm_manager.vm_pte_funcs)->pad_ib))(& ib);
    amdgpu_sync_resv(adev, & ib.sync, pd->tbo.resv, (void *)1);
    __ret_warn_on = ib.length_dw > ndw;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c",
                         459);
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
    r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)1);
    if (r != 0) {
      amdgpu_ib_free(adev, & ib);
      return (r);
    } else {
    }
    amdgpu_bo_fence(pd, ib.fence, 1);
  } else {
  }
  amdgpu_ib_free(adev, & ib);
  return (0);
}
}
static void amdgpu_vm_frag_ptes(struct amdgpu_device *adev , struct amdgpu_ib *ib ,
                                uint64_t pe_start , uint64_t pe_end , uint64_t addr ,
                                u32 flags , u32 gtt_flags )
{
  uint64_t frag_flags ;
  uint64_t frag_align ;
  uint64_t frag_start ;
  uint64_t frag_end ;
  unsigned int count ;
  {
  frag_flags = 512ULL;
  frag_align = 128ULL;
  frag_start = ((frag_align + pe_start) - 1ULL) & - frag_align;
  frag_end = - frag_align & pe_end;
  if (((flags & 2U) != 0U || (flags & 1U) == 0U) || frag_start >= frag_end) {
    count = (unsigned int )((pe_end - pe_start) / 8ULL);
    amdgpu_vm_update_pages(adev, ib, pe_start, addr, count, 4096U, flags, gtt_flags);
    return;
  } else {
  }
  if (pe_start != frag_start) {
    count = (unsigned int )((frag_start - pe_start) / 8ULL);
    amdgpu_vm_update_pages(adev, ib, pe_start, addr, count, 4096U, flags, gtt_flags);
    addr = (uint64_t )(count * 4096U) + addr;
  } else {
  }
  count = (unsigned int )((frag_end - frag_start) / 8ULL);
  amdgpu_vm_update_pages(adev, ib, frag_start, addr, count, 4096U, (u32 )frag_flags | flags,
                         gtt_flags);
  if (frag_end != pe_end) {
    addr = (uint64_t )(count * 4096U) + addr;
    count = (unsigned int )((pe_end - frag_end) / 8ULL);
    amdgpu_vm_update_pages(adev, ib, frag_end, addr, count, 4096U, flags, gtt_flags);
  } else {
  }
  return;
}
}
static int amdgpu_vm_update_ptes(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                 struct amdgpu_ib *ib , uint64_t start , uint64_t end ,
                                 uint64_t dst , u32 flags , u32 gtt_flags )
{
  uint64_t mask ;
  uint64_t last_pte ;
  uint64_t last_dst ;
  unsigned int count ;
  uint64_t addr ;
  uint64_t pt_idx ;
  struct amdgpu_bo *pt ;
  unsigned int nptes ;
  uint64_t pte ;
  int r ;
  {
  mask = (uint64_t )((1 << amdgpu_vm_block_size) + -1);
  last_pte = 0xffffffffffffffffULL;
  last_dst = 0xffffffffffffffffULL;
  count = 0U;
  addr = start;
  goto ldv_44640;
  ldv_44639:
  pt_idx = addr >> amdgpu_vm_block_size;
  pt = (vm->page_tables + pt_idx)->bo;
  amdgpu_sync_resv(adev, & ib->sync, pt->tbo.resv, (void *)1);
  r = reservation_object_reserve_shared(pt->tbo.resv);
  if (r != 0) {
    return (r);
  } else {
  }
  if (((addr ^ end) & ~ mask) == 0ULL) {
    nptes = (unsigned int )end - (unsigned int )addr;
  } else {
    nptes = (unsigned int )(1 << amdgpu_vm_block_size) - ((unsigned int )addr & (unsigned int )mask);
  }
  pte = amdgpu_bo_gpu_offset(pt);
  pte = (addr & mask) * 8ULL + pte;
  if ((uint64_t )(count * 8U) + last_pte != pte) {
    if (count != 0U) {
      amdgpu_vm_frag_ptes(adev, ib, last_pte, (uint64_t )(count * 8U) + last_pte,
                          last_dst, flags, gtt_flags);
    } else {
    }
    count = nptes;
    last_pte = pte;
    last_dst = dst;
  } else {
    count = count + nptes;
  }
  addr = (uint64_t )nptes + addr;
  dst = (uint64_t )(nptes * 4096U) + dst;
  ldv_44640: ;
  if (addr < end) {
    goto ldv_44639;
  } else {
  }
  if (count != 0U) {
    amdgpu_vm_frag_ptes(adev, ib, last_pte, (uint64_t )(count * 8U) + last_pte, last_dst,
                        flags, gtt_flags);
  } else {
  }
  return (0);
}
}
static void amdgpu_vm_fence_pts(struct amdgpu_vm *vm , uint64_t start , uint64_t end ,
                                struct amdgpu_fence *fence )
{
  unsigned int i ;
  {
  start = start >> amdgpu_vm_block_size;
  end = end >> amdgpu_vm_block_size;
  i = (unsigned int )start;
  goto ldv_44650;
  ldv_44649:
  amdgpu_bo_fence((vm->page_tables + (unsigned long )i)->bo, fence, 1);
  i = i + 1U;
  ldv_44650: ;
  if ((uint64_t )i <= end) {
    goto ldv_44649;
  } else {
  }
  return;
}
}
static int amdgpu_vm_bo_update_mapping(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                       struct amdgpu_bo_va_mapping *mapping , uint64_t addr ,
                                       u32 gtt_flags , struct amdgpu_fence **fence )
{
  struct amdgpu_ring *ring ;
  unsigned int nptes ;
  unsigned int ncmds ;
  unsigned int ndw ;
  u32 flags ;
  struct amdgpu_ib ib ;
  int r ;
  int _min1 ;
  int _min2 ;
  unsigned int i ;
  struct amdgpu_fence *f ;
  int __ret_warn_on ;
  long tmp ;
  {
  ring = adev->vm_manager.vm_pte_funcs_ring;
  flags = gtt_flags;
  if ((mapping->flags & 32U) == 0U) {
    flags = flags & 4294967263U;
  } else {
  }
  if ((mapping->flags & 64U) == 0U) {
    flags = flags & 4294967231U;
  } else {
  }
  trace_amdgpu_vm_bo_update(mapping);
  nptes = ((unsigned int )mapping->it.last - (unsigned int )mapping->it.start) + 1U;
  _min1 = amdgpu_vm_block_size;
  _min2 = 11;
  ncmds = (nptes >> (_min1 < _min2 ? _min1 : _min2)) + 1U;
  ndw = 64U;
  if ((flags & 2U) != 0U && flags == gtt_flags) {
    ndw = ncmds * 7U + ndw;
  } else
  if ((flags & 2U) != 0U) {
    ndw = ncmds * 4U + ndw;
    ndw = nptes * 2U + ndw;
  } else {
    ndw = ncmds * 10U + ndw;
    ndw = ndw + 20U;
  }
  if (ndw > 1048575U) {
    return (-12);
  } else {
  }
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, ndw * 4U, & ib);
  if (r != 0) {
    return (r);
  } else {
  }
  ib.length_dw = 0U;
  if ((flags & 1U) == 0U) {
    i = 0U;
    goto ldv_44673;
    ldv_44672:
    f = vm->ids[i].last_id_use;
    amdgpu_sync_fence(& ib.sync, f);
    i = i + 1U;
    ldv_44673: ;
    if (i <= 15U) {
      goto ldv_44672;
    } else {
    }
  } else {
  }
  r = amdgpu_vm_update_ptes(adev, vm, & ib, (uint64_t )mapping->it.start, (uint64_t )(mapping->it.last + 1UL),
                            mapping->offset + addr, flags, gtt_flags);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    return (r);
  } else {
  }
  (*((adev->vm_manager.vm_pte_funcs)->pad_ib))(& ib);
  __ret_warn_on = ib.length_dw > ndw;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c",
                       749);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)1);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    return (r);
  } else {
  }
  amdgpu_vm_fence_pts(vm, (uint64_t )mapping->it.start, (uint64_t )(mapping->it.last + 1UL),
                      ib.fence);
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence **)0)) {
    amdgpu_fence_unref(fence);
    *fence = amdgpu_fence_ref(ib.fence);
  } else {
  }
  amdgpu_ib_free(adev, & ib);
  return (0);
}
}
int amdgpu_vm_bo_update(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va ,
                        struct ttm_mem_reg *mem )
{
  struct amdgpu_vm *vm ;
  struct amdgpu_bo_va_mapping *mapping ;
  u32 flags ;
  uint64_t addr ;
  int r ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  vm = bo_va->vm;
  if ((unsigned long )mem != (unsigned long )((struct ttm_mem_reg *)0)) {
    addr = (uint64_t )(mem->start << 12);
    if (mem->mem_type != 1U) {
      addr = adev->vm_manager.vram_base_offset + addr;
    } else {
    }
  } else {
    addr = 0ULL;
  }
  if (bo_va->addr == addr) {
    return (0);
  } else {
  }
  flags = amdgpu_ttm_tt_pte_flags(adev, (bo_va->bo)->tbo.ttm, mem);
  __mptr = (struct list_head const *)bo_va->mappings.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr;
  goto ldv_44692;
  ldv_44691:
  r = amdgpu_vm_bo_update_mapping(adev, vm, mapping, addr, flags, & bo_va->last_pt_update);
  if (r != 0) {
    return (r);
  } else {
  }
  __mptr___0 = (struct list_head const *)mapping->list.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr___0;
  ldv_44692: ;
  if ((unsigned long )(& mapping->list) != (unsigned long )(& bo_va->mappings)) {
    goto ldv_44691;
  } else {
  }
  bo_va->addr = addr;
  spin_lock(& vm->status_lock);
  list_del_init(& bo_va->vm_status);
  spin_unlock(& vm->status_lock);
  return (0);
}
}
int amdgpu_vm_clear_freed(struct amdgpu_device *adev , struct amdgpu_vm *vm )
{
  struct amdgpu_bo_va_mapping *mapping ;
  int r ;
  struct list_head const *__mptr ;
  int tmp ;
  {
  goto ldv_44703;
  ldv_44702:
  __mptr = (struct list_head const *)vm->freed.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr;
  list_del(& mapping->list);
  r = amdgpu_vm_bo_update_mapping(adev, vm, mapping, 0ULL, 0U, (struct amdgpu_fence **)0);
  kfree((void const *)mapping);
  if (r != 0) {
    return (r);
  } else {
  }
  ldv_44703:
  tmp = list_empty((struct list_head const *)(& vm->freed));
  if (tmp == 0) {
    goto ldv_44702;
  } else {
  }
  return (0);
}
}
int amdgpu_vm_clear_invalids(struct amdgpu_device *adev , struct amdgpu_vm *vm , struct amdgpu_sync *sync )
{
  struct amdgpu_bo_va *bo_va ;
  int r ;
  struct list_head const *__mptr ;
  int tmp ;
  {
  bo_va = (struct amdgpu_bo_va *)0;
  spin_lock(& vm->status_lock);
  goto ldv_44715;
  ldv_44714:
  __mptr = (struct list_head const *)vm->invalidated.next;
  bo_va = (struct amdgpu_bo_va *)__mptr + 0xffffffffffffffc8UL;
  spin_unlock(& vm->status_lock);
  r = amdgpu_vm_bo_update(adev, bo_va, (struct ttm_mem_reg *)0);
  if (r != 0) {
    return (r);
  } else {
  }
  spin_lock(& vm->status_lock);
  ldv_44715:
  tmp = list_empty((struct list_head const *)(& vm->invalidated));
  if (tmp == 0) {
    goto ldv_44714;
  } else {
  }
  spin_unlock(& vm->status_lock);
  if ((unsigned long )bo_va != (unsigned long )((struct amdgpu_bo_va *)0)) {
    amdgpu_sync_fence(sync, bo_va->last_pt_update);
  } else {
  }
  return (0);
}
}
struct amdgpu_bo_va *amdgpu_vm_bo_add(struct amdgpu_device *adev , struct amdgpu_vm *vm ,
                                      struct amdgpu_bo *bo )
{
  struct amdgpu_bo_va *bo_va ;
  void *tmp ;
  {
  tmp = kzalloc(88UL, 208U);
  bo_va = (struct amdgpu_bo_va *)tmp;
  if ((unsigned long )bo_va == (unsigned long )((struct amdgpu_bo_va *)0)) {
    return ((struct amdgpu_bo_va *)0);
  } else {
  }
  bo_va->vm = vm;
  bo_va->bo = bo;
  bo_va->addr = 0ULL;
  bo_va->ref_count = 1U;
  INIT_LIST_HEAD(& bo_va->bo_list);
  INIT_LIST_HEAD(& bo_va->mappings);
  INIT_LIST_HEAD(& bo_va->vm_status);
  mutex_lock_nested(& vm->mutex, 0U);
  list_add_tail(& bo_va->bo_list, & bo->va);
  mutex_unlock(& vm->mutex);
  return (bo_va);
}
}
int amdgpu_vm_bo_map(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va , uint64_t saddr ,
                     uint64_t offset , uint64_t size , u32 flags )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_vm *vm ;
  struct interval_tree_node *it ;
  unsigned int last_pfn ;
  unsigned int pt_idx ;
  uint64_t eaddr ;
  int r ;
  unsigned long tmp ;
  struct amdgpu_bo_va_mapping *tmp___0 ;
  struct interval_tree_node const *__mptr ;
  void *tmp___1 ;
  unsigned int tmp___2 ;
  long tmp___3 ;
  struct amdgpu_bo *pt ;
  {
  vm = bo_va->vm;
  if ((((saddr & 4095ULL) != 0ULL || (offset & 4095ULL) != 0ULL) || size == 0ULL) || (size & 4095ULL) != 0ULL) {
    amdgpu_bo_unreserve(bo_va->bo);
    return (-22);
  } else {
  }
  eaddr = saddr + size;
  if (saddr >= eaddr) {
    amdgpu_bo_unreserve(bo_va->bo);
    return (-22);
  } else {
    tmp = amdgpu_bo_size(bo_va->bo);
    if (offset + size > (unsigned long long )tmp) {
      amdgpu_bo_unreserve(bo_va->bo);
      return (-22);
    } else {
    }
  }
  last_pfn = (unsigned int )(eaddr / 4096ULL);
  if (adev->vm_manager.max_pfn < last_pfn) {
    dev_err((struct device const *)adev->dev, "va above limit (0x%08X > 0x%08X)\n",
            last_pfn, adev->vm_manager.max_pfn);
    amdgpu_bo_unreserve(bo_va->bo);
    return (-22);
  } else {
  }
  mutex_lock_nested(& vm->mutex, 0U);
  saddr = saddr / 4096ULL;
  eaddr = eaddr / 4096ULL;
  it = interval_tree_iter_first(& vm->va, (unsigned long )saddr, (unsigned long )(eaddr - 1ULL));
  if ((unsigned long )it != (unsigned long )((struct interval_tree_node *)0)) {
    __mptr = (struct interval_tree_node const *)it;
    tmp___0 = (struct amdgpu_bo_va_mapping *)__mptr + 0xfffffffffffffff0UL;
    dev_err((struct device const *)adev->dev, "bo %p va 0x%010Lx-0x%010Lx conflict with 0x%010lx-0x%010lx\n",
            bo_va->bo, saddr, eaddr, tmp___0->it.start, tmp___0->it.last + 1UL);
    amdgpu_bo_unreserve(bo_va->bo);
    r = -22;
    goto error_unlock;
  } else {
  }
  tmp___1 = kmalloc(80UL, 208U);
  mapping = (struct amdgpu_bo_va_mapping *)tmp___1;
  if ((unsigned long )mapping == (unsigned long )((struct amdgpu_bo_va_mapping *)0)) {
    amdgpu_bo_unreserve(bo_va->bo);
    r = -12;
    goto error_unlock;
  } else {
  }
  INIT_LIST_HEAD(& mapping->list);
  mapping->it.start = (unsigned long )saddr;
  mapping->it.last = (unsigned long )(eaddr - 1ULL);
  mapping->offset = offset;
  mapping->flags = flags;
  list_add(& mapping->list, & bo_va->mappings);
  interval_tree_insert(& mapping->it, & vm->va);
  trace_amdgpu_vm_bo_map(bo_va, mapping);
  bo_va->addr = 0ULL;
  saddr = saddr >> amdgpu_vm_block_size;
  eaddr = eaddr >> amdgpu_vm_block_size;
  tmp___2 = amdgpu_vm_num_pdes(adev);
  tmp___3 = ldv__builtin_expect((uint64_t )tmp___2 <= eaddr, 0L);
  if (tmp___3 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vm.c"),
                         "i" (1012), "i" (12UL));
    ldv_44742: ;
    goto ldv_44742;
  } else {
  }
  if ((uint64_t )vm->max_pde_used < eaddr) {
    vm->max_pde_used = (unsigned int )eaddr;
  } else {
  }
  amdgpu_bo_unreserve(bo_va->bo);
  pt_idx = (unsigned int )saddr;
  goto ldv_44747;
  ldv_44746: ;
  if ((unsigned long )(vm->page_tables + (unsigned long )pt_idx)->bo != (unsigned long )((struct amdgpu_bo *)0)) {
    goto ldv_44744;
  } else {
  }
  mutex_unlock(& vm->mutex);
  r = amdgpu_bo_create(adev, (unsigned long )(8 << amdgpu_vm_block_size), 4096, 1,
                       4U, 0ULL, (struct sg_table *)0, & pt);
  if (r != 0) {
    goto error_free;
  } else {
  }
  r = amdgpu_vm_clear_bo(adev, pt);
  if (r != 0) {
    amdgpu_bo_unref(& pt);
    goto error_free;
  } else {
  }
  mutex_lock_nested(& vm->mutex, 0U);
  if ((unsigned long )(vm->page_tables + (unsigned long )pt_idx)->bo != (unsigned long )((struct amdgpu_bo *)0)) {
    mutex_unlock(& vm->mutex);
    amdgpu_bo_unref(& pt);
    mutex_lock_nested(& vm->mutex, 0U);
    goto ldv_44744;
  } else {
  }
  (vm->page_tables + (unsigned long )pt_idx)->addr = 0ULL;
  (vm->page_tables + (unsigned long )pt_idx)->bo = pt;
  ldv_44744:
  pt_idx = pt_idx + 1U;
  ldv_44747: ;
  if ((uint64_t )pt_idx <= eaddr) {
    goto ldv_44746;
  } else {
  }
  mutex_unlock(& vm->mutex);
  return (0);
  error_free:
  mutex_lock_nested(& vm->mutex, 0U);
  list_del(& mapping->list);
  interval_tree_remove(& mapping->it, & vm->va);
  trace_amdgpu_vm_bo_unmap(bo_va, mapping);
  kfree((void const *)mapping);
  error_unlock:
  mutex_unlock(& vm->mutex);
  return (r);
}
}
int amdgpu_vm_bo_unmap(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va , uint64_t saddr )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_vm *vm ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  vm = bo_va->vm;
  saddr = saddr / 4096ULL;
  __mptr = (struct list_head const *)bo_va->mappings.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr;
  goto ldv_44762;
  ldv_44761: ;
  if ((unsigned long long )mapping->it.start == saddr) {
    goto ldv_44760;
  } else {
  }
  __mptr___0 = (struct list_head const *)mapping->list.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr___0;
  ldv_44762: ;
  if ((unsigned long )(& mapping->list) != (unsigned long )(& bo_va->mappings)) {
    goto ldv_44761;
  } else {
  }
  ldv_44760: ;
  if ((unsigned long )(& mapping->list) == (unsigned long )(& bo_va->mappings)) {
    amdgpu_bo_unreserve(bo_va->bo);
    return (-2);
  } else {
  }
  mutex_lock_nested(& vm->mutex, 0U);
  list_del(& mapping->list);
  interval_tree_remove(& mapping->it, & vm->va);
  trace_amdgpu_vm_bo_unmap(bo_va, mapping);
  if (bo_va->addr != 0ULL) {
    list_add(& mapping->list, & vm->freed);
  } else {
    kfree((void const *)mapping);
  }
  mutex_unlock(& vm->mutex);
  amdgpu_bo_unreserve(bo_va->bo);
  return (0);
}
}
void amdgpu_vm_bo_rmv(struct amdgpu_device *adev , struct amdgpu_bo_va *bo_va )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_bo_va_mapping *next ;
  struct amdgpu_vm *vm ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  vm = bo_va->vm;
  list_del(& bo_va->bo_list);
  mutex_lock_nested(& vm->mutex, 0U);
  spin_lock(& vm->status_lock);
  list_del(& bo_va->vm_status);
  spin_unlock(& vm->status_lock);
  __mptr = (struct list_head const *)bo_va->mappings.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr;
  __mptr___0 = (struct list_head const *)mapping->list.next;
  next = (struct amdgpu_bo_va_mapping *)__mptr___0;
  goto ldv_44777;
  ldv_44776:
  list_del(& mapping->list);
  interval_tree_remove(& mapping->it, & vm->va);
  trace_amdgpu_vm_bo_unmap(bo_va, mapping);
  if (bo_va->addr != 0ULL) {
    list_add(& mapping->list, & vm->freed);
  } else {
    kfree((void const *)mapping);
  }
  mapping = next;
  __mptr___1 = (struct list_head const *)next->list.next;
  next = (struct amdgpu_bo_va_mapping *)__mptr___1;
  ldv_44777: ;
  if ((unsigned long )(& mapping->list) != (unsigned long )(& bo_va->mappings)) {
    goto ldv_44776;
  } else {
  }
  amdgpu_fence_unref(& bo_va->last_pt_update);
  kfree((void const *)bo_va);
  mutex_unlock(& vm->mutex);
  return;
}
}
void amdgpu_vm_bo_invalidate(struct amdgpu_device *adev , struct amdgpu_bo *bo )
{
  struct amdgpu_bo_va *bo_va ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  __mptr = (struct list_head const *)bo->va.next;
  bo_va = (struct amdgpu_bo_va *)__mptr;
  goto ldv_44789;
  ldv_44788: ;
  if (bo_va->addr != 0ULL) {
    spin_lock(& (bo_va->vm)->status_lock);
    list_del(& bo_va->vm_status);
    list_add(& bo_va->vm_status, & (bo_va->vm)->invalidated);
    spin_unlock(& (bo_va->vm)->status_lock);
  } else {
  }
  __mptr___0 = (struct list_head const *)bo_va->bo_list.next;
  bo_va = (struct amdgpu_bo_va *)__mptr___0;
  ldv_44789: ;
  if ((unsigned long )(& bo_va->bo_list) != (unsigned long )(& bo->va)) {
    goto ldv_44788;
  } else {
  }
  return;
}
}
int amdgpu_vm_init(struct amdgpu_device *adev , struct amdgpu_vm *vm )
{
  unsigned int align ;
  int _min1 ;
  int _min2 ;
  unsigned int pd_size ;
  unsigned int pd_entries ;
  unsigned int pts_size ;
  int i ;
  int r ;
  struct lock_class_key __key ;
  struct rb_root __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  void *tmp ;
  {
  _min1 = 32768;
  _min2 = 8 << amdgpu_vm_block_size;
  align = (unsigned int const )(_min1 < _min2 ? _min1 : _min2);
  i = 0;
  goto ldv_44805;
  ldv_44804:
  vm->ids[i].id = 0U;
  vm->ids[i].flushed_updates = (struct amdgpu_fence *)0;
  vm->ids[i].last_id_use = (struct amdgpu_fence *)0;
  i = i + 1;
  ldv_44805: ;
  if (i <= 15) {
    goto ldv_44804;
  } else {
  }
  __mutex_init(& vm->mutex, "&vm->mutex", & __key);
  __constr_expr_0.rb_node = (struct rb_node *)0;
  vm->va = __constr_expr_0;
  spinlock_check(& vm->status_lock);
  __raw_spin_lock_init(& vm->status_lock.__annonCompField18.rlock, "&(&vm->status_lock)->rlock",
                       & __key___0);
  INIT_LIST_HEAD(& vm->invalidated);
  INIT_LIST_HEAD(& vm->freed);
  pd_size = amdgpu_vm_directory_size(adev);
  pd_entries = amdgpu_vm_num_pdes(adev);
  pts_size = pd_entries * 16U;
  tmp = kzalloc((size_t )pts_size, 208U);
  vm->page_tables = (struct amdgpu_vm_pt *)tmp;
  if ((unsigned long )vm->page_tables == (unsigned long )((struct amdgpu_vm_pt *)0)) {
    drm_err("Cannot allocate memory for page table array\n");
    return (-12);
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )pd_size, (int )align, 1, 4U, 0ULL, (struct sg_table *)0,
                       & vm->page_directory);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vm_clear_bo(adev, vm->page_directory);
  if (r != 0) {
    amdgpu_bo_unref(& vm->page_directory);
    vm->page_directory = (struct amdgpu_bo *)0;
    return (r);
  } else {
  }
  return (0);
}
}
void amdgpu_vm_fini(struct amdgpu_device *adev , struct amdgpu_vm *vm )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_bo_va_mapping *tmp ;
  int i ;
  struct rb_node *____ptr ;
  struct rb_node *tmp___0 ;
  struct rb_node const *__mptr ;
  struct amdgpu_bo_va_mapping *tmp___1 ;
  struct rb_node *____ptr___0 ;
  struct rb_node *tmp___2 ;
  struct rb_node const *__mptr___0 ;
  struct amdgpu_bo_va_mapping *tmp___3 ;
  struct list_head const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  unsigned int tmp___4 ;
  {
  if ((unsigned long )vm->va.rb_node != (unsigned long )((struct rb_node *)0)) {
    dev_err((struct device const *)adev->dev, "still active bo inside vm\n");
  } else {
  }
  tmp___0 = rb_first_postorder((struct rb_root const *)(& vm->va));
  ____ptr = tmp___0;
  if ((unsigned long )____ptr != (unsigned long )((struct rb_node *)0)) {
    __mptr = (struct rb_node const *)____ptr;
    tmp___1 = (struct amdgpu_bo_va_mapping *)__mptr + 0xfffffffffffffff0UL;
  } else {
    tmp___1 = (struct amdgpu_bo_va_mapping *)0;
  }
  mapping = tmp___1;
  goto ldv_44827;
  ldv_44826:
  list_del(& mapping->list);
  interval_tree_remove(& mapping->it, & vm->va);
  kfree((void const *)mapping);
  mapping = tmp;
  ldv_44827: ;
  if ((unsigned long )mapping != (unsigned long )((struct amdgpu_bo_va_mapping *)0)) {
    tmp___2 = rb_next_postorder((struct rb_node const *)(& mapping->it.rb));
    ____ptr___0 = tmp___2;
    if ((unsigned long )____ptr___0 != (unsigned long )((struct rb_node *)0)) {
      __mptr___0 = (struct rb_node const *)____ptr___0;
      tmp___3 = (struct amdgpu_bo_va_mapping *)__mptr___0 + 0xfffffffffffffff0UL;
    } else {
      tmp___3 = (struct amdgpu_bo_va_mapping *)0;
    }
    tmp = tmp___3;
    goto ldv_44826;
  } else {
  }
  __mptr___1 = (struct list_head const *)vm->freed.next;
  mapping = (struct amdgpu_bo_va_mapping *)__mptr___1;
  __mptr___2 = (struct list_head const *)mapping->list.next;
  tmp = (struct amdgpu_bo_va_mapping *)__mptr___2;
  goto ldv_44836;
  ldv_44835:
  list_del(& mapping->list);
  kfree((void const *)mapping);
  mapping = tmp;
  __mptr___3 = (struct list_head const *)tmp->list.next;
  tmp = (struct amdgpu_bo_va_mapping *)__mptr___3;
  ldv_44836: ;
  if ((unsigned long )(& mapping->list) != (unsigned long )(& vm->freed)) {
    goto ldv_44835;
  } else {
  }
  i = 0;
  goto ldv_44839;
  ldv_44838:
  amdgpu_bo_unref(& (vm->page_tables + (unsigned long )i)->bo);
  i = i + 1;
  ldv_44839:
  tmp___4 = amdgpu_vm_num_pdes(adev);
  if ((unsigned int )i < tmp___4) {
    goto ldv_44838;
  } else {
  }
  kfree((void const *)vm->page_tables);
  amdgpu_bo_unref(& vm->page_directory);
  i = 0;
  goto ldv_44842;
  ldv_44841:
  amdgpu_fence_unref(& vm->ids[i].flushed_updates);
  amdgpu_fence_unref(& vm->ids[i].last_id_use);
  i = i + 1;
  ldv_44842: ;
  if (i <= 15) {
    goto ldv_44841;
  } else {
  }
  mutex_destroy(& vm->mutex);
  return;
}
}
bool ldv_queue_work_on_425(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_426(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_427(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_428(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_429(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_439(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_441(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_440(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_443(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_442(struct workqueue_struct *ldv_func_arg1 ) ;
static int amdgpu_debugfs_sa_init(struct amdgpu_device *adev ) ;
int amdgpu_ib_get(struct amdgpu_ring *ring , struct amdgpu_vm *vm , unsigned int size ,
                  struct amdgpu_ib *ib )
{
  struct amdgpu_device *adev ;
  int r ;
  void *tmp ;
  {
  adev = ring->adev;
  if (size != 0U) {
    r = amdgpu_sa_bo_new(adev, & adev->ring_tmp_bo, & ib->sa_bo, size, 256U);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "failed to get a new IB (%d)\n",
              r);
      return (r);
    } else {
    }
    tmp = amdgpu_sa_bo_cpu_addr(ib->sa_bo);
    ib->ptr = (u32 *)tmp;
    if ((unsigned long )vm == (unsigned long )((struct amdgpu_vm *)0)) {
      ib->gpu_addr = amdgpu_sa_bo_gpu_addr(ib->sa_bo);
    } else {
      ib->gpu_addr = 0ULL;
    }
  } else {
    ib->sa_bo = (struct amdgpu_sa_bo *)0;
    ib->ptr = (u32 *)0U;
    ib->gpu_addr = 0ULL;
  }
  amdgpu_sync_create(& ib->sync);
  ib->ring = ring;
  ib->fence = (struct amdgpu_fence *)0;
  ib->user = (struct amdgpu_user_fence *)0;
  ib->vm = vm;
  ib->gds_base = 0U;
  ib->gds_size = 0U;
  ib->gws_base = 0U;
  ib->gws_size = 0U;
  ib->oa_base = 0U;
  ib->oa_size = 0U;
  ib->flags = 0U;
  return (0);
}
}
void amdgpu_ib_free(struct amdgpu_device *adev , struct amdgpu_ib *ib )
{
  {
  amdgpu_sync_free(adev, & ib->sync, ib->fence);
  amdgpu_sa_bo_free(adev, & ib->sa_bo, ib->fence);
  amdgpu_fence_unref(& ib->fence);
  return;
}
}
int amdgpu_ib_schedule(struct amdgpu_device *adev , unsigned int num_ibs , struct amdgpu_ib *ibs ,
                       void *owner )
{
  struct amdgpu_ib *ib ;
  struct amdgpu_ring *ring ;
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx *old_ctx ;
  struct amdgpu_vm *vm ;
  unsigned int i ;
  int r ;
  struct amdgpu_fence *vm_id_fence ;
  uint64_t addr ;
  u64 tmp ;
  {
  ib = ibs;
  r = 0;
  if (num_ibs == 0U) {
    return (-22);
  } else {
  }
  ring = ibs->ring;
  ctx = ibs->ctx;
  vm = ibs->vm;
  if (! ring->ready) {
    dev_err((struct device const *)adev->dev, "couldn\'t schedule ib\n");
    return (-22);
  } else {
  }
  r = amdgpu_ring_lock(ring, num_ibs * 288U);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "scheduling IB failed (%d).\n", r);
    return (r);
  } else {
  }
  if ((unsigned long )vm != (unsigned long )((struct amdgpu_vm *)0)) {
    vm_id_fence = (struct amdgpu_fence *)0;
    vm_id_fence = amdgpu_vm_grab_id(ibs->ring, ibs->vm);
    amdgpu_sync_fence(& ibs->sync, vm_id_fence);
  } else {
  }
  r = amdgpu_sync_rings(& ibs->sync, ring);
  if (r != 0) {
    amdgpu_ring_unlock_undo(ring);
    dev_err((struct device const *)adev->dev, "failed to sync rings (%d)\n", r);
    return (r);
  } else {
  }
  if ((unsigned long )vm != (unsigned long )((struct amdgpu_vm *)0)) {
    amdgpu_vm_flush(ring, vm, ib->sync.last_vm_update);
  } else {
  }
  if ((unsigned long )vm != (unsigned long )((struct amdgpu_vm *)0) && (unsigned long )(ring->funcs)->emit_gds_switch != (unsigned long )((void (* )(struct amdgpu_ring * ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ,
                                                                                                                                                                u32 ))0)) {
    (*((ring->funcs)->emit_gds_switch))(ring, (ib->vm)->ids[ring->idx].id, ib->gds_base,
                                        ib->gds_size, ib->gws_base, ib->gws_size,
                                        ib->oa_base, ib->oa_size);
  } else {
  }
  if ((unsigned long )(ring->funcs)->emit_hdp_flush != (unsigned long )((void (* )(struct amdgpu_ring * ))0)) {
    (*((ring->funcs)->emit_hdp_flush))(ring);
  } else {
  }
  old_ctx = ring->current_ctx;
  i = 0U;
  goto ldv_47781;
  ldv_47780:
  ib = ibs + (unsigned long )i;
  if (((unsigned long )ib->ring != (unsigned long )ring || (unsigned long )ib->ctx != (unsigned long )ctx) || (unsigned long )ib->vm != (unsigned long )vm) {
    ring->current_ctx = old_ctx;
    amdgpu_ring_unlock_undo(ring);
    return (-22);
  } else {
  }
  (*((ring->funcs)->emit_ib))(ring, ib);
  ring->current_ctx = ctx;
  i = i + 1U;
  ldv_47781: ;
  if (i < num_ibs) {
    goto ldv_47780;
  } else {
  }
  r = amdgpu_fence_emit(ring, owner, & ib->fence);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "failed to emit fence (%d)\n", r);
    ring->current_ctx = old_ctx;
    amdgpu_ring_unlock_undo(ring);
    return (r);
  } else {
  }
  if ((unsigned long )ib->user != (unsigned long )((struct amdgpu_user_fence *)0)) {
    tmp = amdgpu_bo_gpu_offset((ib->user)->bo);
    addr = tmp;
    addr = (uint64_t )(ib->user)->offset + addr;
    (*((ring->funcs)->emit_fence))(ring, addr, (ib->fence)->seq, 1U);
  } else {
  }
  if ((unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0)) {
    amdgpu_vm_fence(adev, ib->vm, ib->fence);
  } else {
  }
  amdgpu_ring_unlock_commit(ring);
  return (0);
}
}
int amdgpu_ib_pool_init(struct amdgpu_device *adev )
{
  int r ;
  int tmp ;
  {
  if ((int )adev->ib_pool_ready) {
    return (0);
  } else {
  }
  r = amdgpu_sa_bo_manager_init(adev, & adev->ring_tmp_bo, 1048576U, 4096U, 2U);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_sa_bo_manager_start(adev, & adev->ring_tmp_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->ib_pool_ready = 1;
  tmp = amdgpu_debugfs_sa_init(adev);
  if (tmp != 0) {
    dev_err((struct device const *)adev->dev, "failed to register debugfs file for SA\n");
  } else {
  }
  return (0);
}
}
void amdgpu_ib_pool_fini(struct amdgpu_device *adev )
{
  {
  if ((int )adev->ib_pool_ready) {
    amdgpu_sa_bo_manager_suspend(adev, & adev->ring_tmp_bo);
    amdgpu_sa_bo_manager_fini(adev, & adev->ring_tmp_bo);
    adev->ib_pool_ready = 0;
  } else {
  }
  return;
}
}
int amdgpu_ib_ring_tests(struct amdgpu_device *adev )
{
  unsigned int i ;
  int r ;
  struct amdgpu_ring *ring ;
  {
  i = 0U;
  goto ldv_47799;
  ldv_47798:
  ring = adev->rings[i];
  if ((unsigned long )ring == (unsigned long )((struct amdgpu_ring *)0) || ! ring->ready) {
    goto ldv_47797;
  } else {
  }
  r = (*((ring->funcs)->test_ib))(ring);
  if (r != 0) {
    ring->ready = 0;
    adev->needs_reset = 0;
    if ((unsigned long )((struct amdgpu_ring *)(& adev->gfx.gfx_ring)) == (unsigned long )ring) {
      drm_err("amdgpu: failed testing IB on GFX ring (%d).\n", r);
      adev->accel_working = 0;
      return (r);
    } else {
      drm_err("amdgpu: failed testing IB on ring %d (%d).\n", i, r);
    }
  } else {
  }
  ldv_47797:
  i = i + 1U;
  ldv_47799: ;
  if (i <= 15U) {
    goto ldv_47798;
  } else {
  }
  return (0);
}
}
static int amdgpu_debugfs_sa_info(struct seq_file *m , void *data )
{
  struct drm_info_node *node ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  node = (struct drm_info_node *)m->private;
  dev = (node->minor)->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_sa_bo_dump_debug_info(& adev->ring_tmp_bo, m);
  return (0);
}
}
static struct drm_info_list amdgpu_debugfs_sa_list[1U] = { {"amdgpu_sa_info", & amdgpu_debugfs_sa_info, 0U, (void *)0}};
static int amdgpu_debugfs_sa_init(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = amdgpu_debugfs_add_files(adev, (struct drm_info_list *)(& amdgpu_debugfs_sa_list),
                                 1U);
  return (tmp);
}
}
bool ldv_queue_work_on_439(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_440(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_441(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_442(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_443(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_453(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_455(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_454(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_457(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_456(struct workqueue_struct *ldv_func_arg1 ) ;
static void amdgpu_pll_reduce_ratio(unsigned int *nom , unsigned int *den , unsigned int nom_min ,
                                    unsigned int den_min )
{
  unsigned int tmp ;
  unsigned long tmp___0 ;
  {
  tmp___0 = gcd((unsigned long )*nom, (unsigned long )*den);
  tmp = (unsigned int )tmp___0;
  *nom = *nom / tmp;
  *den = *den / tmp;
  if (*nom < nom_min) {
    tmp = ((*nom + nom_min) - 1U) / *nom;
    *nom = *nom * tmp;
    *den = *den * tmp;
  } else {
  }
  if (*den < den_min) {
    tmp = ((*den + den_min) - 1U) / *den;
    *nom = *nom * tmp;
    *den = *den * tmp;
  } else {
  }
  return;
}
}
static void amdgpu_pll_get_fb_ref_div(unsigned int nom , unsigned int den , unsigned int post_div ,
                                      unsigned int fb_div_max , unsigned int ref_div_max ,
                                      unsigned int *fb_div , unsigned int *ref_div )
{
  unsigned int _min1 ;
  unsigned int _min2 ;
  unsigned int _min1___0 ;
  unsigned int _max1 ;
  unsigned int __x ;
  unsigned int __d ;
  unsigned int _max2 ;
  unsigned int _min2___0 ;
  unsigned int __x___0 ;
  unsigned int __d___0 ;
  unsigned int __x___1 ;
  unsigned int __d___1 ;
  {
  _min1 = 128U / post_div;
  _min2 = ref_div_max;
  ref_div_max = _min1 < _min2 ? _min1 : _min2;
  __x = den;
  __d = post_div;
  _max1 = (__d / 2U + __x) / __d;
  _max2 = 1U;
  _min1___0 = _max1 > _max2 ? _max1 : _max2;
  _min2___0 = ref_div_max;
  *ref_div = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  __x___0 = (*ref_div * nom) * post_div;
  __d___0 = den;
  *fb_div = (__d___0 / 2U + __x___0) / __d___0;
  if (*fb_div > fb_div_max) {
    __x___1 = *ref_div * fb_div_max;
    __d___1 = *fb_div;
    *ref_div = (__d___1 / 2U + __x___1) / __d___1;
    *fb_div = fb_div_max;
  } else {
  }
  return;
}
}
void amdgpu_pll_compute(struct amdgpu_pll *pll , u32 freq , u32 *dot_clock_p , u32 *fb_div_p ,
                        u32 *frac_fb_div_p , u32 *ref_div_p , u32 *post_div_p )
{
  unsigned int target_clock ;
  unsigned int fb_div_min ;
  unsigned int fb_div_max ;
  unsigned int fb_div ;
  unsigned int post_div_min ;
  unsigned int post_div_max ;
  unsigned int post_div ;
  unsigned int ref_div_min ;
  unsigned int ref_div_max ;
  unsigned int ref_div ;
  unsigned int post_div_best ;
  unsigned int diff_best ;
  unsigned int nom ;
  unsigned int den ;
  unsigned int vco_min ;
  unsigned int vco_max ;
  unsigned int diff ;
  long ret ;
  int __x___0 ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  unsigned int tmp ;
  long tmp___0 ;
  {
  target_clock = (pll->flags & 1024U) == 0U ? freq / 10U : freq;
  fb_div_min = pll->min_feedback_div;
  fb_div_max = pll->max_feedback_div;
  if ((pll->flags & 1024U) != 0U) {
    fb_div_min = fb_div_min * 10U;
    fb_div_max = fb_div_max * 10U;
  } else {
  }
  if ((pll->flags & 4U) != 0U) {
    ref_div_min = pll->reference_div;
  } else {
    ref_div_min = pll->min_ref_div;
  }
  if ((pll->flags & 1024U) != 0U && (pll->flags & 4U) != 0U) {
    ref_div_max = pll->reference_div;
  } else {
    ref_div_max = pll->max_ref_div;
  }
  if ((pll->flags & 4096U) != 0U) {
    post_div_min = pll->post_div;
    post_div_max = pll->post_div;
  } else {
    if ((pll->flags & 8192U) != 0U) {
      vco_min = pll->lcd_pll_out_min;
      vco_max = pll->lcd_pll_out_max;
    } else {
      vco_min = pll->pll_out_min;
      vco_max = pll->pll_out_max;
    }
    if ((pll->flags & 1024U) != 0U) {
      vco_min = vco_min * 10U;
      vco_max = vco_max * 10U;
    } else {
    }
    post_div_min = vco_min / target_clock;
    if (target_clock * post_div_min < vco_min) {
      post_div_min = post_div_min + 1U;
    } else {
    }
    if (pll->min_post_div > post_div_min) {
      post_div_min = pll->min_post_div;
    } else {
    }
    post_div_max = vco_max / target_clock;
    if (target_clock * post_div_max > vco_max) {
      post_div_max = post_div_max - 1U;
    } else {
    }
    if (pll->max_post_div < post_div_max) {
      post_div_max = pll->max_post_div;
    } else {
    }
  }
  nom = target_clock;
  den = pll->reference_freq;
  amdgpu_pll_reduce_ratio(& nom, & den, fb_div_min, post_div_min);
  if ((pll->flags & 16384U) != 0U) {
    post_div_best = post_div_min;
  } else {
    post_div_best = post_div_max;
  }
  diff_best = 4294967295U;
  post_div = post_div_min;
  goto ldv_47885;
  ldv_47884:
  amdgpu_pll_get_fb_ref_div(nom, den, post_div, fb_div_max, ref_div_max, & fb_div,
                            & ref_div);
  __x___0 = (int )(target_clock - (pll->reference_freq * fb_div) / (ref_div * post_div));
  ret = (long )(__x___0 < 0 ? - __x___0 : __x___0);
  diff = (unsigned int )ret;
  if (diff < diff_best || (diff == diff_best && (pll->flags & 16384U) == 0U)) {
    post_div_best = post_div;
    diff_best = diff;
  } else {
  }
  post_div = post_div + 1U;
  ldv_47885: ;
  if (post_div <= post_div_max) {
    goto ldv_47884;
  } else {
  }
  post_div = post_div_best;
  amdgpu_pll_get_fb_ref_div(nom, den, post_div, fb_div_max, ref_div_max, & fb_div,
                            & ref_div);
  amdgpu_pll_reduce_ratio(& fb_div, & ref_div, fb_div_min, ref_div_min);
  if ((pll->flags & 1024U) != 0U && fb_div % 10U != 0U) {
    _max1 = fb_div_min;
    _max2 = - (fb_div % 10U) * 20U + 240U;
    fb_div_min = _max1 > _max2 ? _max1 : _max2;
    if (fb_div < fb_div_min) {
      tmp = ((fb_div_min + fb_div) - 1U) / fb_div;
      fb_div = fb_div * tmp;
      ref_div = ref_div * tmp;
    } else {
    }
  } else {
  }
  if ((pll->flags & 1024U) != 0U) {
    *fb_div_p = fb_div / 10U;
    *frac_fb_div_p = fb_div % 10U;
  } else {
    *fb_div_p = fb_div;
    *frac_fb_div_p = 0U;
  }
  *dot_clock_p = ((pll->reference_freq * *fb_div_p) * 10U + pll->reference_freq * *frac_fb_div_p) / ((ref_div * post_div) * 10U);
  *ref_div_p = ref_div;
  *post_div_p = post_div;
  tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("amdgpu_pll_compute", "%d - %d, pll dividers - fb: %d.%d ref: %d, post %d\n",
                        freq, *dot_clock_p * 10U, *fb_div_p, *frac_fb_div_p, ref_div,
                        post_div);
  } else {
  }
  return;
}
}
u32 amdgpu_pll_get_use_mask(struct drm_crtc *crtc )
{
  struct drm_device *dev ;
  struct drm_crtc *test_crtc ;
  struct amdgpu_crtc *test_amdgpu_crtc ;
  u32 pll_in_use ;
  struct list_head const *__mptr ;
  struct drm_crtc const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = crtc->dev;
  pll_in_use = 0U;
  __mptr = (struct list_head const *)dev->mode_config.crtc_list.next;
  test_crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
  goto ldv_47907;
  ldv_47906: ;
  if ((unsigned long )crtc == (unsigned long )test_crtc) {
    goto ldv_47903;
  } else {
  }
  __mptr___0 = (struct drm_crtc const *)test_crtc;
  test_amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  if (test_amdgpu_crtc->pll_id != 255U) {
    pll_in_use = (u32 )(1 << (int )test_amdgpu_crtc->pll_id) | pll_in_use;
  } else {
  }
  ldv_47903:
  __mptr___1 = (struct list_head const *)test_crtc->head.next;
  test_crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
  ldv_47907: ;
  if ((unsigned long )(& test_crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
    goto ldv_47906;
  } else {
  }
  return (pll_in_use);
}
}
int amdgpu_pll_get_shared_dp_ppll(struct drm_crtc *crtc )
{
  struct drm_device *dev ;
  struct drm_crtc *test_crtc ;
  struct amdgpu_crtc *test_amdgpu_crtc ;
  struct list_head const *__mptr ;
  struct drm_crtc const *__mptr___0 ;
  int tmp ;
  int tmp___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = crtc->dev;
  __mptr = (struct list_head const *)dev->mode_config.crtc_list.next;
  test_crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
  goto ldv_47923;
  ldv_47922: ;
  if ((unsigned long )crtc == (unsigned long )test_crtc) {
    goto ldv_47919;
  } else {
  }
  __mptr___0 = (struct drm_crtc const *)test_crtc;
  test_amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  if ((unsigned long )test_amdgpu_crtc->encoder != (unsigned long )((struct drm_encoder *)0)) {
    tmp = amdgpu_atombios_encoder_get_encoder_mode(test_amdgpu_crtc->encoder);
    if (tmp == 0) {
      goto _L;
    } else {
      tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(test_amdgpu_crtc->encoder);
      if (tmp___0 == 5) {
        _L:
        if (test_amdgpu_crtc->pll_id != 255U) {
          return ((int )test_amdgpu_crtc->pll_id);
        } else {
        }
      } else {
      }
    }
  } else {
  }
  ldv_47919:
  __mptr___1 = (struct list_head const *)test_crtc->head.next;
  test_crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
  ldv_47923: ;
  if ((unsigned long )(& test_crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
    goto ldv_47922;
  } else {
  }
  return (255);
}
}
int amdgpu_pll_get_shared_nondp_ppll(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct drm_crtc *test_crtc ;
  struct amdgpu_crtc *test_amdgpu_crtc ;
  u32 adjusted_clock ;
  u32 test_adjusted_clock ;
  struct list_head const *__mptr___0 ;
  struct drm_crtc const *__mptr___1 ;
  int tmp ;
  int tmp___0 ;
  struct list_head const *__mptr___2 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adjusted_clock = amdgpu_crtc->adjusted_clock;
  if (adjusted_clock == 0U) {
    return (255);
  } else {
  }
  __mptr___0 = (struct list_head const *)dev->mode_config.crtc_list.next;
  test_crtc = (struct drm_crtc *)__mptr___0 + 0xfffffffffffffff0UL;
  goto ldv_47944;
  ldv_47943: ;
  if ((unsigned long )crtc == (unsigned long )test_crtc) {
    goto ldv_47940;
  } else {
  }
  __mptr___1 = (struct drm_crtc const *)test_crtc;
  test_amdgpu_crtc = (struct amdgpu_crtc *)__mptr___1;
  if ((unsigned long )test_amdgpu_crtc->encoder != (unsigned long )((struct drm_encoder *)0)) {
    tmp = amdgpu_atombios_encoder_get_encoder_mode(test_amdgpu_crtc->encoder);
    if (tmp != 0) {
      tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(test_amdgpu_crtc->encoder);
      if (tmp___0 != 5) {
        if ((unsigned long )test_amdgpu_crtc->connector == (unsigned long )amdgpu_crtc->connector) {
          if (test_amdgpu_crtc->pll_id != 255U) {
            return ((int )test_amdgpu_crtc->pll_id);
          } else {
          }
        } else {
        }
        test_adjusted_clock = test_amdgpu_crtc->adjusted_clock;
        if (((crtc->mode.clock == test_crtc->mode.clock && adjusted_clock == test_adjusted_clock) && (int )amdgpu_crtc->ss_enabled == (int )test_amdgpu_crtc->ss_enabled) && test_amdgpu_crtc->pll_id != 255U) {
          return ((int )test_amdgpu_crtc->pll_id);
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  ldv_47940:
  __mptr___2 = (struct list_head const *)test_crtc->head.next;
  test_crtc = (struct drm_crtc *)__mptr___2 + 0xfffffffffffffff0UL;
  ldv_47944: ;
  if ((unsigned long )(& test_crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
    goto ldv_47943;
  } else {
  }
  return (255);
}
}
bool ldv_queue_work_on_453(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_454(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_455(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_456(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_457(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_467(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_469(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_468(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_471(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_470(struct workqueue_struct *ldv_func_arg1 ) ;
void amdgpu_ucode_print_mc_hdr(struct common_firmware_header const *hdr ) ;
void amdgpu_ucode_print_smc_hdr(struct common_firmware_header const *hdr ) ;
void amdgpu_ucode_print_gfx_hdr(struct common_firmware_header const *hdr ) ;
void amdgpu_ucode_print_rlc_hdr(struct common_firmware_header const *hdr ) ;
void amdgpu_ucode_print_sdma_hdr(struct common_firmware_header const *hdr ) ;
int amdgpu_ucode_validate(struct firmware const *fw ) ;
bool amdgpu_ucode_hdr_version(union amdgpu_firmware_header *hdr , u16 hdr_major ,
                              u16 hdr_minor ) ;
int amdgpu_ucode_init_bo(struct amdgpu_device *adev ) ;
int amdgpu_ucode_fini_bo(struct amdgpu_device *adev ) ;
static void amdgpu_ucode_print_common_hdr(struct common_firmware_header const *hdr )
{
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  long tmp___5 ;
  long tmp___6 ;
  long tmp___7 ;
  long tmp___8 ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "size_bytes: %u\n", hdr->size_bytes);
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "header_size_bytes: %u\n",
                        hdr->header_size_bytes);
  } else {
  }
  tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___1 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "header_version_major: %u\n",
                        (int )hdr->header_version_major);
  } else {
  }
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "header_version_minor: %u\n",
                        (int )hdr->header_version_minor);
  } else {
  }
  tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___3 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "ip_version_major: %u\n",
                        (int )hdr->ip_version_major);
  } else {
  }
  tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___4 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "ip_version_minor: %u\n",
                        (int )hdr->ip_version_minor);
  } else {
  }
  tmp___5 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___5 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "ucode_version: 0x%08x\n",
                        hdr->ucode_version);
  } else {
  }
  tmp___6 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___6 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "ucode_size_bytes: %u\n",
                        hdr->ucode_size_bytes);
  } else {
  }
  tmp___7 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___7 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "ucode_array_offset_bytes: %u\n",
                        hdr->ucode_array_offset_bytes);
  } else {
  }
  tmp___8 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___8 != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_common_hdr", "crc32: 0x%08x\n", hdr->crc32);
  } else {
  }
  return;
}
}
void amdgpu_ucode_print_mc_hdr(struct common_firmware_header const *hdr )
{
  u16 version_major ;
  u16 version_minor ;
  long tmp ;
  struct mc_firmware_header_v1_0 const *mc_hdr ;
  struct common_firmware_header const *__mptr ;
  long tmp___0 ;
  long tmp___1 ;
  {
  version_major = hdr->header_version_major;
  version_minor = hdr->header_version_minor;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_mc_hdr", "MC\n");
  } else {
  }
  amdgpu_ucode_print_common_hdr(hdr);
  if ((unsigned int )version_major == 1U) {
    __mptr = hdr;
    mc_hdr = (struct mc_firmware_header_v1_0 const *)((struct mc_firmware_header_v1_0 *)__mptr);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_mc_hdr", "io_debug_size_bytes: %u\n",
                          mc_hdr->io_debug_size_bytes);
    } else {
    }
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_mc_hdr", "io_debug_array_offset_bytes: %u\n",
                          mc_hdr->io_debug_array_offset_bytes);
    } else {
    }
  } else {
    drm_err("Unknown MC ucode version: %u.%u\n", (int )version_major, (int )version_minor);
  }
  return;
}
}
void amdgpu_ucode_print_smc_hdr(struct common_firmware_header const *hdr )
{
  u16 version_major ;
  u16 version_minor ;
  long tmp ;
  struct smc_firmware_header_v1_0 const *smc_hdr ;
  struct common_firmware_header const *__mptr ;
  long tmp___0 ;
  {
  version_major = hdr->header_version_major;
  version_minor = hdr->header_version_minor;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_smc_hdr", "SMC\n");
  } else {
  }
  amdgpu_ucode_print_common_hdr(hdr);
  if ((unsigned int )version_major == 1U) {
    __mptr = hdr;
    smc_hdr = (struct smc_firmware_header_v1_0 const *)((struct smc_firmware_header_v1_0 *)__mptr);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_smc_hdr", "ucode_start_addr: %u\n",
                          smc_hdr->ucode_start_addr);
    } else {
    }
  } else {
    drm_err("Unknown SMC ucode version: %u.%u\n", (int )version_major, (int )version_minor);
  }
  return;
}
}
void amdgpu_ucode_print_gfx_hdr(struct common_firmware_header const *hdr )
{
  u16 version_major ;
  u16 version_minor ;
  long tmp ;
  struct gfx_firmware_header_v1_0 const *gfx_hdr ;
  struct common_firmware_header const *__mptr ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  version_major = hdr->header_version_major;
  version_minor = hdr->header_version_minor;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_gfx_hdr", "GFX\n");
  } else {
  }
  amdgpu_ucode_print_common_hdr(hdr);
  if ((unsigned int )version_major == 1U) {
    __mptr = hdr;
    gfx_hdr = (struct gfx_firmware_header_v1_0 const *)((struct gfx_firmware_header_v1_0 *)__mptr);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_gfx_hdr", "ucode_feature_version: %u\n",
                          gfx_hdr->ucode_feature_version);
    } else {
    }
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_gfx_hdr", "jt_offset: %u\n", gfx_hdr->jt_offset);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_gfx_hdr", "jt_size: %u\n", gfx_hdr->jt_size);
    } else {
    }
  } else {
    drm_err("Unknown GFX ucode version: %u.%u\n", (int )version_major, (int )version_minor);
  }
  return;
}
}
void amdgpu_ucode_print_rlc_hdr(struct common_firmware_header const *hdr )
{
  u16 version_major ;
  u16 version_minor ;
  long tmp ;
  struct rlc_firmware_header_v1_0 const *rlc_hdr ;
  struct common_firmware_header const *__mptr ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  struct rlc_firmware_header_v2_0 const *rlc_hdr___0 ;
  struct common_firmware_header const *__mptr___0 ;
  long tmp___5 ;
  long tmp___6 ;
  long tmp___7 ;
  long tmp___8 ;
  long tmp___9 ;
  long tmp___10 ;
  long tmp___11 ;
  long tmp___12 ;
  long tmp___13 ;
  long tmp___14 ;
  long tmp___15 ;
  long tmp___16 ;
  long tmp___17 ;
  long tmp___18 ;
  long tmp___19 ;
  long tmp___20 ;
  long tmp___21 ;
  long tmp___22 ;
  {
  version_major = hdr->header_version_major;
  version_minor = hdr->header_version_minor;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "RLC\n");
  } else {
  }
  amdgpu_ucode_print_common_hdr(hdr);
  if ((unsigned int )version_major == 1U) {
    __mptr = hdr;
    rlc_hdr = (struct rlc_firmware_header_v1_0 const *)((struct rlc_firmware_header_v1_0 *)__mptr);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "ucode_feature_version: %u\n",
                          rlc_hdr->ucode_feature_version);
    } else {
    }
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "save_and_restore_offset: %u\n",
                          rlc_hdr->save_and_restore_offset);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "clear_state_descriptor_offset: %u\n",
                          rlc_hdr->clear_state_descriptor_offset);
    } else {
    }
    tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___3 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "avail_scratch_ram_locations: %u\n",
                          rlc_hdr->avail_scratch_ram_locations);
    } else {
    }
    tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___4 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "master_pkt_description_offset: %u\n",
                          rlc_hdr->master_pkt_description_offset);
    } else {
    }
  } else
  if ((unsigned int )version_major == 2U) {
    __mptr___0 = hdr;
    rlc_hdr___0 = (struct rlc_firmware_header_v2_0 const *)((struct rlc_firmware_header_v2_0 *)__mptr___0);
    tmp___5 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___5 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "ucode_feature_version: %u\n",
                          rlc_hdr___0->ucode_feature_version);
    } else {
    }
    tmp___6 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___6 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "jt_offset: %u\n", rlc_hdr___0->jt_offset);
    } else {
    }
    tmp___7 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___7 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "jt_size: %u\n", rlc_hdr___0->jt_size);
    } else {
    }
    tmp___8 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___8 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "save_and_restore_offset: %u\n",
                          rlc_hdr___0->save_and_restore_offset);
    } else {
    }
    tmp___9 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___9 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "clear_state_descriptor_offset: %u\n",
                          rlc_hdr___0->clear_state_descriptor_offset);
    } else {
    }
    tmp___10 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___10 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "avail_scratch_ram_locations: %u\n",
                          rlc_hdr___0->avail_scratch_ram_locations);
    } else {
    }
    tmp___11 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___11 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_restore_list_size: %u\n",
                          rlc_hdr___0->reg_restore_list_size);
    } else {
    }
    tmp___12 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___12 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_start: %u\n",
                          rlc_hdr___0->reg_list_format_start);
    } else {
    }
    tmp___13 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___13 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_separate_start: %u\n",
                          rlc_hdr___0->reg_list_format_separate_start);
    } else {
    }
    tmp___14 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___14 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "starting_offsets_start: %u\n",
                          rlc_hdr___0->starting_offsets_start);
    } else {
    }
    tmp___15 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___15 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_size_bytes: %u\n",
                          rlc_hdr___0->reg_list_format_size_bytes);
    } else {
    }
    tmp___16 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___16 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_array_offset_bytes: %u\n",
                          rlc_hdr___0->reg_list_format_array_offset_bytes);
    } else {
    }
    tmp___17 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___17 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_size_bytes: %u\n",
                          rlc_hdr___0->reg_list_size_bytes);
    } else {
    }
    tmp___18 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___18 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_array_offset_bytes: %u\n",
                          rlc_hdr___0->reg_list_array_offset_bytes);
    } else {
    }
    tmp___19 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___19 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_separate_size_bytes: %u\n",
                          rlc_hdr___0->reg_list_format_separate_size_bytes);
    } else {
    }
    tmp___20 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___20 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_format_separate_array_offset_bytes: %u\n",
                          rlc_hdr___0->reg_list_format_separate_array_offset_bytes);
    } else {
    }
    tmp___21 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___21 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_separate_size_bytes: %u\n",
                          rlc_hdr___0->reg_list_separate_size_bytes);
    } else {
    }
    tmp___22 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___22 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_rlc_hdr", "reg_list_separate_size_bytes: %u\n",
                          rlc_hdr___0->reg_list_separate_size_bytes);
    } else {
    }
  } else {
    drm_err("Unknown RLC ucode version: %u.%u\n", (int )version_major, (int )version_minor);
  }
  return;
}
}
void amdgpu_ucode_print_sdma_hdr(struct common_firmware_header const *hdr )
{
  u16 version_major ;
  u16 version_minor ;
  long tmp ;
  struct sdma_firmware_header_v1_0 const *sdma_hdr ;
  struct common_firmware_header const *__mptr ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  struct sdma_firmware_header_v1_1 const *sdma_v1_1_hdr ;
  struct sdma_firmware_header_v1_0 const *__mptr___0 ;
  long tmp___4 ;
  {
  version_major = hdr->header_version_major;
  version_minor = hdr->header_version_minor;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "SDMA\n");
  } else {
  }
  amdgpu_ucode_print_common_hdr(hdr);
  if ((unsigned int )version_major == 1U) {
    __mptr = hdr;
    sdma_hdr = (struct sdma_firmware_header_v1_0 const *)((struct sdma_firmware_header_v1_0 *)__mptr);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "ucode_feature_version: %u\n",
                          sdma_hdr->ucode_feature_version);
    } else {
    }
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "ucode_change_version: %u\n",
                          sdma_hdr->ucode_change_version);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "jt_offset: %u\n", sdma_hdr->jt_offset);
    } else {
    }
    tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___3 != 0L) {
      drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "jt_size: %u\n", sdma_hdr->jt_size);
    } else {
    }
    if ((unsigned int )version_minor != 0U) {
      __mptr___0 = sdma_hdr;
      sdma_v1_1_hdr = (struct sdma_firmware_header_v1_1 const *)((struct sdma_firmware_header_v1_1 *)__mptr___0);
      tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
      if (tmp___4 != 0L) {
        drm_ut_debug_printk("amdgpu_ucode_print_sdma_hdr", "digest_size: %u\n", sdma_v1_1_hdr->digest_size);
      } else {
      }
    } else {
    }
  } else {
    drm_err("Unknown SDMA ucode version: %u.%u\n", (int )version_major, (int )version_minor);
  }
  return;
}
}
int amdgpu_ucode_validate(struct firmware const *fw )
{
  struct common_firmware_header const *hdr ;
  {
  hdr = (struct common_firmware_header const *)fw->data;
  if ((unsigned long )fw->size == (unsigned long )hdr->size_bytes) {
    return (0);
  } else {
  }
  return (-22);
}
}
bool amdgpu_ucode_hdr_version(union amdgpu_firmware_header *hdr , u16 hdr_major ,
                              u16 hdr_minor )
{
  {
  if ((int )hdr->common.header_version_major == (int )hdr_major && (int )hdr->common.header_version_minor == (int )hdr_minor) {
    return (0);
  } else {
  }
  return (1);
}
}
static int amdgpu_ucode_init_single_fw(struct amdgpu_firmware_info *ucode , uint64_t mc_addr ,
                                       void *kptr )
{
  struct common_firmware_header const *header ;
  {
  header = (struct common_firmware_header const *)0;
  if ((unsigned long )ucode->fw == (unsigned long )((struct firmware const *)0)) {
    return (0);
  } else {
  }
  ucode->mc_addr = mc_addr;
  ucode->kaddr = kptr;
  header = (struct common_firmware_header const *)(ucode->fw)->data;
  memcpy(ucode->kaddr, (void const *)(ucode->fw)->data + (unsigned long )header->ucode_array_offset_bytes,
           (size_t )header->ucode_size_bytes);
  return (0);
}
}
int amdgpu_ucode_init_bo(struct amdgpu_device *adev )
{
  struct amdgpu_bo **bo ;
  uint64_t fw_mc_addr ;
  void *fw_buf_ptr ;
  uint64_t fw_offset ;
  int i ;
  int err ;
  struct amdgpu_firmware_info *ucode ;
  struct common_firmware_header const *header ;
  {
  bo = & adev->firmware.fw_buf;
  fw_buf_ptr = (void *)0;
  fw_offset = 0ULL;
  ucode = (struct amdgpu_firmware_info *)0;
  header = (struct common_firmware_header const *)0;
  err = amdgpu_bo_create(adev, (unsigned long )adev->firmware.fw_size, 4096, 1, 2U,
                         0ULL, (struct sg_table *)0, bo);
  if (err != 0) {
    dev_err((struct device const *)adev->dev, "(%d) Firmware buffer allocate failed\n",
            err);
    err = -12;
    goto failed;
  } else {
  }
  err = amdgpu_bo_reserve(*bo, 0);
  if (err != 0) {
    amdgpu_bo_unref(bo);
    dev_err((struct device const *)adev->dev, "(%d) Firmware buffer reserve failed\n",
            err);
    goto failed;
  } else {
  }
  err = amdgpu_bo_pin(*bo, 2U, & fw_mc_addr);
  if (err != 0) {
    amdgpu_bo_unreserve(*bo);
    amdgpu_bo_unref(bo);
    dev_err((struct device const *)adev->dev, "(%d) Firmware buffer pin failed\n",
            err);
    goto failed;
  } else {
  }
  err = amdgpu_bo_kmap(*bo, & fw_buf_ptr);
  if (err != 0) {
    dev_err((struct device const *)adev->dev, "(%d) Firmware buffer kmap failed\n",
            err);
    amdgpu_bo_unpin(*bo);
    amdgpu_bo_unreserve(*bo);
    amdgpu_bo_unref(bo);
    goto failed;
  } else {
  }
  amdgpu_bo_unreserve(*bo);
  fw_offset = 0ULL;
  i = 0;
  goto ldv_43751;
  ldv_43750:
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )i;
  if ((unsigned long )ucode->fw != (unsigned long )((struct firmware const *)0)) {
    header = (struct common_firmware_header const *)(ucode->fw)->data;
    amdgpu_ucode_init_single_fw(ucode, fw_mc_addr + fw_offset, fw_buf_ptr + fw_offset);
    fw_offset = ((uint64_t )((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200ULL) + fw_offset;
  } else {
  }
  i = i + 1;
  ldv_43751: ;
  if (i <= 7) {
    goto ldv_43750;
  } else {
  }
  failed: ;
  if (err != 0) {
    adev->firmware.smu_load = 0;
  } else {
  }
  return (err);
}
}
int amdgpu_ucode_fini_bo(struct amdgpu_device *adev )
{
  int i ;
  struct amdgpu_firmware_info *ucode ;
  {
  ucode = (struct amdgpu_firmware_info *)0;
  i = 0;
  goto ldv_43759;
  ldv_43758:
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )i;
  if ((unsigned long )ucode->fw != (unsigned long )((struct firmware const *)0)) {
    ucode->mc_addr = 0ULL;
    ucode->kaddr = (void *)0;
  } else {
  }
  i = i + 1;
  ldv_43759: ;
  if (i <= 7) {
    goto ldv_43758;
  } else {
  }
  amdgpu_bo_unref(& adev->firmware.fw_buf);
  adev->firmware.fw_buf = (struct amdgpu_bo *)0;
  return (0);
}
}
bool ldv_queue_work_on_467(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_468(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_469(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_470(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_471(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static unsigned long arch_local_save_flags___5(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"./arch/x86/include/asm/paravirt.h"),
                         "i" (831), "i" (12UL));
    ldv_4860: ;
    goto ldv_4860;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (43UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static int __atomic_add_unless___3(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___3(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___3(v, a, u);
  return (tmp != u);
}
}
__inline static bool static_key_false___4(struct static_key *key )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = static_key_count(key);
  tmp___0 = ldv__builtin_expect(tmp > 0, 0L);
  if (tmp___0 != 0L) {
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int rcu_read_lock_sched_held___4(void)
{
  int lockdep_opinion ;
  int tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  unsigned long _flags ;
  int tmp___5 ;
  int tmp___6 ;
  {
  lockdep_opinion = 0;
  tmp = debug_lockdep_rcu_enabled();
  if (tmp == 0) {
    return (1);
  } else {
  }
  tmp___0 = rcu_is_watching();
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (0);
  } else {
  }
  tmp___2 = rcu_lockdep_current_cpu_online();
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    return (0);
  } else {
  }
  if (debug_locks != 0) {
    lockdep_opinion = lock_is_held(& rcu_sched_lock_map);
  } else {
  }
  if (lockdep_opinion != 0) {
    tmp___6 = 1;
  } else {
    tmp___4 = preempt_count();
    if (tmp___4 != 0) {
      tmp___6 = 1;
    } else {
      _flags = arch_local_save_flags___5();
      tmp___5 = arch_irqs_disabled_flags(_flags);
      if (tmp___5 != 0) {
        tmp___6 = 1;
      } else {
        tmp___6 = 0;
      }
    }
  }
  return (tmp___6);
}
}
bool ldv_queue_work_on_481(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_483(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_482(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_485(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_484(struct workqueue_struct *ldv_func_arg1 ) ;
extern void *idr_find_slowpath(struct idr * , int ) ;
extern int idr_alloc(struct idr * , void * , int , int , gfp_t ) ;
extern void idr_remove(struct idr * , int ) ;
__inline static void *idr_find(struct idr *idr , int id )
{
  struct idr_layer *hint ;
  struct idr_layer *________p1 ;
  struct idr_layer *_________p1 ;
  union __anonunion___u_168 __u ;
  int tmp ;
  struct idr_layer *________p1___0 ;
  struct idr_layer *_________p1___0 ;
  union __anonunion___u_170 __u___0 ;
  int tmp___0 ;
  void *tmp___1 ;
  {
  __read_once_size((void const volatile *)(& idr->hint), (void *)(& __u.__c), 8);
  _________p1 = __u.__val;
  ________p1 = _________p1;
  tmp = debug_lockdep_rcu_enabled();
  hint = ________p1;
  if ((unsigned long )hint != (unsigned long )((struct idr_layer *)0) && (id & -256) == hint->prefix) {
    __read_once_size((void const volatile *)(& hint->ary) + ((unsigned long )id & 255UL),
                     (void *)(& __u___0.__c), 8);
    _________p1___0 = __u___0.__val;
    ________p1___0 = _________p1___0;
    tmp___0 = debug_lockdep_rcu_enabled();
    return ((void *)________p1___0);
  } else {
  }
  tmp___1 = idr_find_slowpath(idr, id);
  return (tmp___1);
}
}
__inline static int kref_put_mutex___3(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___3(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static void drm_gem_object_unreference_unlocked___3(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___3(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
__inline static void trace_amdgpu_bo_list_set(struct amdgpu_bo_list *list , struct amdgpu_bo *bo )
{
  struct tracepoint_func *it_func_ptr ;
  void *it_func ;
  void *__data ;
  struct tracepoint_func *________p1 ;
  struct tracepoint_func *_________p1 ;
  union __anonunion___u_307 __u ;
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  struct tracepoint_func *________p1___0 ;
  struct tracepoint_func *_________p1___0 ;
  union __anonunion___u_309 __u___0 ;
  bool __warned___0 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp___1 = static_key_false___4(& __tracepoint_amdgpu_bo_list_set.key);
  if ((int )tmp___1) {
    rcu_read_lock_sched_notrace();
    __read_once_size((void const volatile *)(& __tracepoint_amdgpu_bo_list_set.funcs),
                     (void *)(& __u.__c), 8);
    _________p1 = __u.__val;
    ________p1 = _________p1;
    tmp = debug_lockdep_rcu_enabled();
    if (tmp != 0 && ! __warned) {
      tmp___0 = rcu_read_lock_sched_held___4();
      if (tmp___0 == 0) {
        __warned = 1;
        lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                               187, "suspicious rcu_dereference_check() usage");
      } else {
      }
    } else {
    }
    it_func_ptr = ________p1;
    if ((unsigned long )it_func_ptr != (unsigned long )((struct tracepoint_func *)0)) {
      ldv_44161:
      it_func = it_func_ptr->func;
      __data = it_func_ptr->data;
      (*((void (*)(void * , struct amdgpu_bo_list * , struct amdgpu_bo * ))it_func))(__data,
                                                                                     list,
                                                                                     bo);
      it_func_ptr = it_func_ptr + 1;
      if ((unsigned long )it_func_ptr->func != (unsigned long )((void *)0)) {
        goto ldv_44161;
      } else {
      }
    } else {
    }
    rcu_read_unlock_sched_notrace();
  } else {
  }
  rcu_read_lock_sched_notrace();
  __read_once_size((void const volatile *)(& __tracepoint_amdgpu_bo_list_set.funcs),
                   (void *)(& __u___0.__c), 8);
  _________p1___0 = __u___0.__val;
  ________p1___0 = _________p1___0;
  tmp___2 = debug_lockdep_rcu_enabled();
  if (tmp___2 != 0 && ! __warned___0) {
    tmp___3 = rcu_read_lock_sched_held___4();
    if (tmp___3 == 0) {
      __warned___0 = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu_trace.h",
                             187, "suspicious rcu_dereference_check() usage");
    } else {
    }
  } else {
  }
  rcu_read_unlock_sched_notrace();
  return;
}
}
static int amdgpu_bo_list_create(struct amdgpu_fpriv *fpriv , struct amdgpu_bo_list **result ,
                                 int *id )
{
  int r ;
  void *tmp ;
  struct lock_class_key __key ;
  {
  tmp = kzalloc(200UL, 208U);
  *result = (struct amdgpu_bo_list *)tmp;
  if ((unsigned long )*result == (unsigned long )((struct amdgpu_bo_list *)0)) {
    return (-12);
  } else {
  }
  mutex_lock_nested(& fpriv->bo_list_lock, 0U);
  r = idr_alloc(& fpriv->bo_list_handles, (void *)*result, 1, 0, 208U);
  if (r < 0) {
    mutex_unlock(& fpriv->bo_list_lock);
    kfree((void const *)*result);
    return (r);
  } else {
  }
  *id = r;
  __mutex_init(& (*result)->lock, "&(*result)->lock", & __key);
  (*result)->num_entries = 0U;
  (*result)->array = (struct amdgpu_bo_list_entry *)0;
  mutex_lock_nested(& (*result)->lock, 0U);
  mutex_unlock(& fpriv->bo_list_lock);
  return (0);
}
}
static void amdgpu_bo_list_destroy(struct amdgpu_fpriv *fpriv , int id )
{
  struct amdgpu_bo_list *list ;
  void *tmp ;
  {
  mutex_lock_nested(& fpriv->bo_list_lock, 0U);
  tmp = idr_find(& fpriv->bo_list_handles, id);
  list = (struct amdgpu_bo_list *)tmp;
  if ((unsigned long )list != (unsigned long )((struct amdgpu_bo_list *)0)) {
    mutex_lock_nested(& list->lock, 0U);
    idr_remove(& fpriv->bo_list_handles, id);
    mutex_unlock(& list->lock);
    amdgpu_bo_list_free(list);
  } else {
  }
  mutex_unlock(& fpriv->bo_list_lock);
  return;
}
}
static int amdgpu_bo_list_set(struct amdgpu_device *adev , struct drm_file *filp ,
                              struct amdgpu_bo_list *list , struct drm_amdgpu_bo_list_entry *info ,
                              unsigned int num_entries )
{
  struct amdgpu_bo_list_entry *array ;
  struct amdgpu_bo *gds_obj ;
  struct amdgpu_bo *gws_obj ;
  struct amdgpu_bo *oa_obj ;
  bool has_userptr ;
  unsigned int i ;
  void *tmp ;
  struct amdgpu_bo_list_entry *entry ;
  struct drm_gem_object *gobj ;
  struct drm_gem_object const *__mptr ;
  bool tmp___0 ;
  {
  gds_obj = adev->gds.gds_gfx_bo;
  gws_obj = adev->gds.gws_gfx_bo;
  oa_obj = adev->gds.oa_gfx_bo;
  has_userptr = 0;
  tmp = drm_malloc_ab((size_t )num_entries, 64UL);
  array = (struct amdgpu_bo_list_entry *)tmp;
  if ((unsigned long )array == (unsigned long )((struct amdgpu_bo_list_entry *)0)) {
    return (-12);
  } else {
  }
  memset((void *)array, 0, (unsigned long )num_entries * 64UL);
  i = 0U;
  goto ldv_44522;
  ldv_44521:
  entry = array + (unsigned long )i;
  gobj = drm_gem_object_lookup(adev->ddev, filp, (info + (unsigned long )i)->bo_handle);
  if ((unsigned long )gobj == (unsigned long )((struct drm_gem_object *)0)) {
    goto error_free;
  } else {
  }
  __mptr = (struct drm_gem_object const *)gobj;
  entry->robj = amdgpu_bo_ref((struct amdgpu_bo *)__mptr + 0xfffffffffffffbc0UL);
  drm_gem_object_unreference_unlocked___3(gobj);
  entry->priority = (info + (unsigned long )i)->bo_priority;
  entry->prefered_domains = (entry->robj)->initial_domain;
  entry->allowed_domains = entry->prefered_domains;
  if (entry->allowed_domains == 4U) {
    entry->allowed_domains = entry->allowed_domains | 2U;
  } else {
  }
  tmp___0 = amdgpu_ttm_tt_has_userptr((entry->robj)->tbo.ttm);
  if ((int )tmp___0) {
    has_userptr = 1;
    entry->prefered_domains = 2U;
    entry->allowed_domains = 2U;
  } else {
  }
  entry->tv.bo = & (entry->robj)->tbo;
  entry->tv.shared = 1;
  if (entry->prefered_domains == 8U) {
    gds_obj = entry->robj;
  } else {
  }
  if (entry->prefered_domains == 16U) {
    gws_obj = entry->robj;
  } else {
  }
  if (entry->prefered_domains == 32U) {
    oa_obj = entry->robj;
  } else {
  }
  trace_amdgpu_bo_list_set(list, entry->robj);
  i = i + 1U;
  ldv_44522: ;
  if (i < num_entries) {
    goto ldv_44521;
  } else {
  }
  i = 0U;
  goto ldv_44525;
  ldv_44524:
  amdgpu_bo_unref(& (list->array + (unsigned long )i)->robj);
  i = i + 1U;
  ldv_44525: ;
  if (list->num_entries > i) {
    goto ldv_44524;
  } else {
  }
  drm_free_large((void *)list->array);
  list->gds_obj = gds_obj;
  list->gws_obj = gws_obj;
  list->oa_obj = oa_obj;
  list->has_userptr = has_userptr;
  list->array = array;
  list->num_entries = num_entries;
  return (0);
  error_free:
  drm_free_large((void *)array);
  return (-2);
}
}
struct amdgpu_bo_list *amdgpu_bo_list_get(struct amdgpu_fpriv *fpriv , int id )
{
  struct amdgpu_bo_list *result ;
  void *tmp ;
  {
  mutex_lock_nested(& fpriv->bo_list_lock, 0U);
  tmp = idr_find(& fpriv->bo_list_handles, id);
  result = (struct amdgpu_bo_list *)tmp;
  if ((unsigned long )result != (unsigned long )((struct amdgpu_bo_list *)0)) {
    mutex_lock_nested(& result->lock, 0U);
  } else {
  }
  mutex_unlock(& fpriv->bo_list_lock);
  return (result);
}
}
void amdgpu_bo_list_put(struct amdgpu_bo_list *list )
{
  {
  mutex_unlock(& list->lock);
  return;
}
}
void amdgpu_bo_list_free(struct amdgpu_bo_list *list )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_44540;
  ldv_44539:
  amdgpu_bo_unref(& (list->array + (unsigned long )i)->robj);
  i = i + 1U;
  ldv_44540: ;
  if (list->num_entries > i) {
    goto ldv_44539;
  } else {
  }
  mutex_destroy(& list->lock);
  drm_free_large((void *)list->array);
  kfree((void const *)list);
  return;
}
}
int amdgpu_bo_list_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  u32 info_size ;
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  union drm_amdgpu_bo_list *args ;
  u32 handle ;
  void const *uptr ;
  struct drm_amdgpu_bo_list_entry *info ;
  struct amdgpu_bo_list *list ;
  int r ;
  void *tmp ;
  unsigned long bytes ;
  unsigned long tmp___0 ;
  unsigned long bytes___0 ;
  u32 _min1 ;
  unsigned int _min2 ;
  unsigned int i ;
  unsigned long tmp___1 ;
  long tmp___2 ;
  {
  info_size = 8U;
  adev = (struct amdgpu_device *)dev->dev_private;
  fpriv = (struct amdgpu_fpriv *)filp->driver_priv;
  args = (union drm_amdgpu_bo_list *)data;
  handle = args->in.list_handle;
  uptr = (void const *)args->in.bo_info_ptr;
  tmp = drm_malloc_ab((size_t )args->in.bo_number, 8UL);
  info = (struct drm_amdgpu_bo_list_entry *)tmp;
  if ((unsigned long )info == (unsigned long )((struct drm_amdgpu_bo_list_entry *)0)) {
    return (-12);
  } else {
  }
  r = -14;
  tmp___2 = ldv__builtin_expect(args->in.bo_info_size == info_size, 1L);
  if (tmp___2 != 0L) {
    bytes = (unsigned long )(args->in.bo_number * args->in.bo_info_size);
    tmp___0 = copy_from_user((void *)info, uptr, bytes);
    if (tmp___0 != 0UL) {
      goto error_free;
    } else {
    }
  } else {
    _min1 = args->in.bo_info_size;
    _min2 = info_size;
    bytes___0 = (unsigned long )((unsigned int const )_min1 < (unsigned int const )_min2 ? (unsigned int const )_min1 : _min2);
    memset((void *)info, 0, (size_t )(args->in.bo_number * info_size));
    i = 0U;
    goto ldv_44564;
    ldv_44563:
    tmp___1 = copy_from_user((void *)info + (unsigned long )i, uptr, bytes___0);
    if (tmp___1 != 0UL) {
      goto error_free;
    } else {
    }
    uptr = uptr + (unsigned long )args->in.bo_info_size;
    i = i + 1U;
    ldv_44564: ;
    if (args->in.bo_number > i) {
      goto ldv_44563;
    } else {
    }
  }
  switch (args->in.operation) {
  case 0U:
  r = amdgpu_bo_list_create(fpriv, & list, (int *)(& handle));
  if (r != 0) {
    goto error_free;
  } else {
  }
  r = amdgpu_bo_list_set(adev, filp, list, info, args->in.bo_number);
  amdgpu_bo_list_put(list);
  if (r != 0) {
    goto error_free;
  } else {
  }
  goto ldv_44567;
  case 1U:
  amdgpu_bo_list_destroy(fpriv, (int )handle);
  handle = 0U;
  goto ldv_44567;
  case 2U:
  r = -2;
  list = amdgpu_bo_list_get(fpriv, (int )handle);
  if ((unsigned long )list == (unsigned long )((struct amdgpu_bo_list *)0)) {
    goto error_free;
  } else {
  }
  r = amdgpu_bo_list_set(adev, filp, list, info, args->in.bo_number);
  amdgpu_bo_list_put(list);
  if (r != 0) {
    goto error_free;
  } else {
  }
  goto ldv_44567;
  default:
  r = -22;
  goto error_free;
  }
  ldv_44567:
  memset((void *)args, 0, 24UL);
  args->out.list_handle = handle;
  drm_free_large((void *)info);
  return (0);
  error_free:
  drm_free_large((void *)info);
  return (r);
}
}
bool ldv_queue_work_on_481(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_482(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_483(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_484(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_485(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_495(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_497(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_496(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_499(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_498(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static void *idr_find___0(struct idr *idr , int id )
{
  struct idr_layer *hint ;
  struct idr_layer *________p1 ;
  struct idr_layer *_________p1 ;
  union __anonunion___u_168___0 __u ;
  int tmp ;
  struct idr_layer *________p1___0 ;
  struct idr_layer *_________p1___0 ;
  union __anonunion___u_170___0 __u___0 ;
  int tmp___0 ;
  void *tmp___1 ;
  {
  __read_once_size((void const volatile *)(& idr->hint), (void *)(& __u.__c), 8);
  _________p1 = __u.__val;
  ________p1 = _________p1;
  tmp = debug_lockdep_rcu_enabled();
  hint = ________p1;
  if ((unsigned long )hint != (unsigned long )((struct idr_layer *)0) && (id & -256) == hint->prefix) {
    __read_once_size((void const volatile *)(& hint->ary) + ((unsigned long )id & 255UL),
                     (void *)(& __u___0.__c), 8);
    _________p1___0 = __u___0.__val;
    ________p1___0 = _________p1___0;
    tmp___0 = debug_lockdep_rcu_enabled();
    return ((void *)________p1___0);
  } else {
  }
  tmp___1 = idr_find_slowpath(idr, id);
  return (tmp___1);
}
}
__inline static void kref_init(struct kref *kref )
{
  {
  atomic_set(& kref->refcount, 1);
  return;
}
}
__inline static void kref_get___2(struct kref *kref )
{
  bool __warned ;
  int __ret_warn_once ;
  int tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = atomic_add_return(1, & kref->refcount);
  __ret_warn_once = tmp <= 1;
  tmp___2 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___2 != 0L) {
    __ret_warn_on = ! __warned;
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      warn_slowpath_null("include/linux/kref.h", 47);
    } else {
    }
    tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___1 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  ldv__builtin_expect(__ret_warn_once != 0, 0L);
  return;
}
}
__inline static int kref_sub___3(struct kref *kref , unsigned int count , void (*release)(struct kref * ) )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 71);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = atomic_sub_and_test((int )count, & kref->refcount);
  if (tmp___0 != 0) {
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static int kref_put___3(struct kref *kref , void (*release)(struct kref * ) )
{
  int tmp ;
  {
  tmp = kref_sub___3(kref, 1U, release);
  return (tmp);
}
}
int amdgpu_ctx_alloc(struct amdgpu_device *adev , struct amdgpu_fpriv *fpriv , u32 *id ,
                     u32 flags ) ;
int amdgpu_ctx_free(struct amdgpu_device *adev , struct amdgpu_fpriv *fpriv , u32 id ) ;
static void amdgpu_ctx_do_release(struct kref *ref )
{
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx_mgr *mgr ;
  struct kref const *__mptr ;
  {
  __mptr = (struct kref const *)ref;
  ctx = (struct amdgpu_ctx *)__mptr;
  mgr = & (ctx->fpriv)->ctx_mgr;
  idr_remove(& mgr->ctx_handles, (int )ctx->id);
  kfree((void const *)ctx);
  return;
}
}
int amdgpu_ctx_alloc(struct amdgpu_device *adev , struct amdgpu_fpriv *fpriv , u32 *id ,
                     u32 flags )
{
  int r ;
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx_mgr *mgr ;
  void *tmp ;
  {
  mgr = & fpriv->ctx_mgr;
  tmp = kmalloc(40UL, 208U);
  ctx = (struct amdgpu_ctx *)tmp;
  if ((unsigned long )ctx == (unsigned long )((struct amdgpu_ctx *)0)) {
    return (-12);
  } else {
  }
  mutex_lock_nested(& mgr->lock, 0U);
  r = idr_alloc(& mgr->ctx_handles, (void *)ctx, 0, 0, 208U);
  if (r < 0) {
    mutex_unlock(& mgr->lock);
    kfree((void const *)ctx);
    return (r);
  } else {
  }
  *id = (unsigned int )r;
  memset((void *)ctx, 0, 40UL);
  ctx->id = *id;
  ctx->fpriv = fpriv;
  kref_init(& ctx->refcount);
  mutex_unlock(& mgr->lock);
  return (0);
}
}
int amdgpu_ctx_free(struct amdgpu_device *adev , struct amdgpu_fpriv *fpriv , u32 id )
{
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx_mgr *mgr ;
  void *tmp ;
  {
  mgr = & fpriv->ctx_mgr;
  mutex_lock_nested(& mgr->lock, 0U);
  tmp = idr_find___0(& mgr->ctx_handles, (int )id);
  ctx = (struct amdgpu_ctx *)tmp;
  if ((unsigned long )ctx != (unsigned long )((struct amdgpu_ctx *)0)) {
    kref_put___3(& ctx->refcount, & amdgpu_ctx_do_release);
    mutex_unlock(& mgr->lock);
    return (0);
  } else {
  }
  mutex_unlock(& mgr->lock);
  return (-22);
}
}
static int amdgpu_ctx_query(struct amdgpu_device *adev , struct amdgpu_fpriv *fpriv ,
                            u32 id , union drm_amdgpu_ctx_out *out )
{
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx_mgr *mgr ;
  unsigned int reset_counter ;
  void *tmp ;
  int tmp___0 ;
  {
  mgr = & fpriv->ctx_mgr;
  mutex_lock_nested(& mgr->lock, 0U);
  tmp = idr_find___0(& mgr->ctx_handles, (int )id);
  ctx = (struct amdgpu_ctx *)tmp;
  if ((unsigned long )ctx == (unsigned long )((struct amdgpu_ctx *)0)) {
    mutex_unlock(& mgr->lock);
    return (-22);
  } else {
  }
  out->state.flags = ctx->state.flags;
  out->state.hangs = ctx->state.hangs;
  tmp___0 = atomic_read((atomic_t const *)(& adev->gpu_reset_counter));
  reset_counter = (unsigned int )tmp___0;
  if (ctx->reset_counter == reset_counter) {
    out->state.reset_status = 0U;
  } else {
    out->state.reset_status = 3U;
  }
  ctx->reset_counter = reset_counter;
  mutex_unlock(& mgr->lock);
  return (0);
}
}
void amdgpu_ctx_fini(struct amdgpu_fpriv *fpriv )
{
  struct idr *idp ;
  struct amdgpu_ctx *ctx ;
  u32 id ;
  struct amdgpu_ctx_mgr *mgr ;
  int tmp ;
  void *tmp___0 ;
  {
  mgr = & fpriv->ctx_mgr;
  idp = & mgr->ctx_handles;
  id = 0U;
  goto ldv_43680;
  ldv_43679:
  tmp = kref_put___3(& ctx->refcount, & amdgpu_ctx_do_release);
  if (tmp != 1) {
    drm_err("ctx (id=%ul) is still alive\n", ctx->id);
  } else {
  }
  id = id + 1U;
  ldv_43680:
  tmp___0 = idr_get_next(idp, (int *)(& id));
  ctx = (struct amdgpu_ctx *)tmp___0;
  if ((unsigned long )ctx != (unsigned long )((struct amdgpu_ctx *)0)) {
    goto ldv_43679;
  } else {
  }
  mutex_destroy(& mgr->lock);
  return;
}
}
int amdgpu_ctx_ioctl(struct drm_device *dev , void *data , struct drm_file *filp )
{
  int r ;
  u32 id ;
  u32 flags ;
  union drm_amdgpu_ctx *args ;
  struct amdgpu_device *adev ;
  struct amdgpu_fpriv *fpriv ;
  {
  args = (union drm_amdgpu_ctx *)data;
  adev = (struct amdgpu_device *)dev->dev_private;
  fpriv = (struct amdgpu_fpriv *)filp->driver_priv;
  r = 0;
  id = args->in.ctx_id;
  flags = args->in.flags;
  switch (args->in.op) {
  case 1U:
  r = amdgpu_ctx_alloc(adev, fpriv, & id, flags);
  args->out.alloc.ctx_id = id;
  goto ldv_43694;
  case 2U:
  r = amdgpu_ctx_free(adev, fpriv, id);
  goto ldv_43694;
  case 3U:
  r = amdgpu_ctx_query(adev, fpriv, id, & args->out);
  goto ldv_43694;
  default: ;
  return (-22);
  }
  ldv_43694: ;
  return (r);
}
}
struct amdgpu_ctx *amdgpu_ctx_get(struct amdgpu_fpriv *fpriv , u32 id )
{
  struct amdgpu_ctx *ctx ;
  struct amdgpu_ctx_mgr *mgr ;
  void *tmp ;
  {
  mgr = & fpriv->ctx_mgr;
  mutex_lock_nested(& mgr->lock, 0U);
  tmp = idr_find___0(& mgr->ctx_handles, (int )id);
  ctx = (struct amdgpu_ctx *)tmp;
  if ((unsigned long )ctx != (unsigned long )((struct amdgpu_ctx *)0)) {
    kref_get___2(& ctx->refcount);
  } else {
  }
  mutex_unlock(& mgr->lock);
  return (ctx);
}
}
int amdgpu_ctx_put(struct amdgpu_ctx *ctx )
{
  struct amdgpu_fpriv *fpriv ;
  struct amdgpu_ctx_mgr *mgr ;
  {
  if ((unsigned long )ctx == (unsigned long )((struct amdgpu_ctx *)0)) {
    return (-22);
  } else {
  }
  fpriv = ctx->fpriv;
  mgr = & fpriv->ctx_mgr;
  mutex_lock_nested(& mgr->lock, 0U);
  kref_put___3(& ctx->refcount, & amdgpu_ctx_do_release);
  mutex_unlock(& mgr->lock);
  return (0);
}
}
bool ldv_queue_work_on_495(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_496(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_497(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_498(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_499(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_509(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_511(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_510(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_513(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_512(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static struct reservation_object_list *reservation_object_get_list(struct reservation_object *obj )
{
  bool __warned ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = debug_lockdep_rcu_enabled();
  if (tmp != 0 && ! __warned) {
    tmp___0 = lock_is_held(& obj->lock.base.dep_map);
    if (tmp___0 == 0) {
      __warned = 1;
      lockdep_rcu_suspicious("include/linux/reservation.h", 113, "suspicious rcu_dereference_protected() usage");
    } else {
    }
  } else {
  }
  return (obj->fence);
}
}
__inline static struct amdgpu_fence *amdgpu_fence_later(struct amdgpu_fence *a , struct amdgpu_fence *b )
{
  long tmp ;
  {
  if ((unsigned long )a == (unsigned long )((struct amdgpu_fence *)0)) {
    return (b);
  } else {
  }
  if ((unsigned long )b == (unsigned long )((struct amdgpu_fence *)0)) {
    return (a);
  } else {
  }
  tmp = ldv__builtin_expect((unsigned long )a->ring != (unsigned long )b->ring, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/inst/current/envs/linux-4.2-rc1.tar.xz/linux-4.2-rc1/drivers/gpu/drm/amd/amdgpu/amdgpu.h"),
                         "i" (459), "i" (12UL));
    ldv_42045: ;
    goto ldv_42045;
  } else {
  }
  if (a->seq > b->seq) {
    return (a);
  } else {
    return (b);
  }
}
}
void amdgpu_sync_create(struct amdgpu_sync *sync )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_44496;
  ldv_44495:
  sync->semaphores[i] = (struct amdgpu_semaphore *)0;
  i = i + 1U;
  ldv_44496: ;
  if (i <= 3U) {
    goto ldv_44495;
  } else {
  }
  i = 0U;
  goto ldv_44499;
  ldv_44498:
  sync->sync_to[i] = (struct amdgpu_fence *)0;
  i = i + 1U;
  ldv_44499: ;
  if (i <= 15U) {
    goto ldv_44498;
  } else {
  }
  sync->last_vm_update = (struct amdgpu_fence *)0;
  return;
}
}
void amdgpu_sync_fence(struct amdgpu_sync *sync , struct amdgpu_fence *fence )
{
  struct amdgpu_fence *other ;
  struct amdgpu_fence *tmp ;
  struct amdgpu_fence *tmp___0 ;
  {
  if ((unsigned long )fence == (unsigned long )((struct amdgpu_fence *)0)) {
    return;
  } else {
  }
  other = sync->sync_to[(fence->ring)->idx];
  tmp = amdgpu_fence_later(fence, other);
  sync->sync_to[(fence->ring)->idx] = amdgpu_fence_ref(tmp);
  amdgpu_fence_unref(& other);
  if ((unsigned long )fence->owner == (unsigned long )((void *)1)) {
    other = sync->last_vm_update;
    tmp___0 = amdgpu_fence_later(fence, other);
    sync->last_vm_update = amdgpu_fence_ref(tmp___0);
    amdgpu_fence_unref(& other);
  } else {
  }
  return;
}
}
int amdgpu_sync_resv(struct amdgpu_device *adev , struct amdgpu_sync *sync , struct reservation_object *resv ,
                     void *owner )
{
  struct reservation_object_list *flist ;
  struct fence *f ;
  struct amdgpu_fence *fence ;
  unsigned int i ;
  int r ;
  struct amdgpu_fence *tmp ;
  long tmp___0 ;
  bool __warned ;
  int tmp___1 ;
  int tmp___2 ;
  struct amdgpu_fence *tmp___3 ;
  long tmp___4 ;
  {
  r = 0;
  if ((unsigned long )resv == (unsigned long )((struct reservation_object *)0)) {
    return (-22);
  } else {
  }
  f = reservation_object_get_excl(resv);
  if ((unsigned long )f != (unsigned long )((struct fence *)0)) {
    tmp = to_amdgpu_fence(f);
    fence = tmp;
  } else {
    fence = (struct amdgpu_fence *)0;
  }
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0) && (unsigned long )(fence->ring)->adev == (unsigned long )adev) {
    amdgpu_sync_fence(sync, fence);
  } else
  if ((unsigned long )f != (unsigned long )((struct fence *)0)) {
    tmp___0 = fence_wait(f, 1);
    r = (int )tmp___0;
  } else {
  }
  flist = reservation_object_get_list(resv);
  if ((unsigned long )flist == (unsigned long )((struct reservation_object_list *)0) || r != 0) {
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_44521;
  ldv_44520:
  tmp___1 = debug_lockdep_rcu_enabled();
  if (tmp___1 != 0 && ! __warned) {
    tmp___2 = lock_is_held(& resv->lock.base.dep_map);
    if (tmp___2 == 0) {
      __warned = 1;
      lockdep_rcu_suspicious("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_sync.c",
                             121, "suspicious rcu_dereference_protected() usage");
    } else {
    }
  } else {
  }
  f = flist->shared[i];
  if ((unsigned long )f != (unsigned long )((struct fence *)0)) {
    tmp___3 = to_amdgpu_fence(f);
    fence = tmp___3;
  } else {
    fence = (struct amdgpu_fence *)0;
  }
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence *)0) && (unsigned long )(fence->ring)->adev == (unsigned long )adev) {
    if ((unsigned long )fence->owner != (unsigned long )owner || (unsigned long )fence->owner == (unsigned long )((void *)0)) {
      amdgpu_sync_fence(sync, fence);
    } else {
    }
  } else
  if ((unsigned long )f != (unsigned long )((struct fence *)0)) {
    tmp___4 = fence_wait(f, 1);
    r = (int )tmp___4;
    if (r != 0) {
      goto ldv_44519;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_44521: ;
  if (flist->shared_count > i) {
    goto ldv_44520;
  } else {
  }
  ldv_44519: ;
  return (r);
}
}
int amdgpu_sync_rings(struct amdgpu_sync *sync , struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  unsigned int count ;
  int i ;
  int r ;
  struct amdgpu_fence *fence ;
  struct amdgpu_semaphore *semaphore ;
  struct amdgpu_ring *other ;
  bool tmp ;
  int tmp___0 ;
  unsigned int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  {
  adev = ring->adev;
  count = 0U;
  i = 0;
  goto ldv_44535;
  ldv_44534:
  fence = sync->sync_to[i];
  other = adev->rings[i];
  tmp = amdgpu_fence_need_sync(fence, ring);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    goto ldv_44533;
  } else {
  }
  if (! other->ready) {
    dev_err((struct device const *)adev->dev, "Syncing to a disabled ring!");
    return (-22);
  } else {
  }
  if (count > 3U) {
    r = amdgpu_fence_wait(fence, 0);
    if (r != 0) {
      return (r);
    } else {
    }
    goto ldv_44533;
  } else {
  }
  r = amdgpu_semaphore_create(adev, & semaphore);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp___1 = count;
  count = count + 1U;
  sync->semaphores[tmp___1] = semaphore;
  r = amdgpu_ring_alloc(other, 16U);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp___2 = amdgpu_semaphore_emit_signal(other, semaphore);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
    amdgpu_ring_undo(other);
    r = amdgpu_fence_wait(fence, 0);
    if (r != 0) {
      return (r);
    } else {
    }
    goto ldv_44533;
  } else {
  }
  tmp___4 = amdgpu_semaphore_emit_wait(ring, semaphore);
  if (tmp___4) {
    tmp___5 = 0;
  } else {
    tmp___5 = 1;
  }
  if (tmp___5) {
    amdgpu_ring_undo(other);
    r = amdgpu_fence_wait(fence, 0);
    if (r != 0) {
      return (r);
    } else {
    }
    goto ldv_44533;
  } else {
  }
  amdgpu_ring_commit(other);
  amdgpu_fence_note_sync(fence, ring);
  ldv_44533:
  i = i + 1;
  ldv_44535: ;
  if (i <= 15) {
    goto ldv_44534;
  } else {
  }
  return (0);
}
}
void amdgpu_sync_free(struct amdgpu_device *adev , struct amdgpu_sync *sync , struct amdgpu_fence *fence )
{
  unsigned int i ;
  {
  i = 0U;
  goto ldv_44544;
  ldv_44543:
  amdgpu_semaphore_free(adev, (struct amdgpu_semaphore **)(& sync->semaphores) + (unsigned long )i,
                        fence);
  i = i + 1U;
  ldv_44544: ;
  if (i <= 3U) {
    goto ldv_44543;
  } else {
  }
  i = 0U;
  goto ldv_44547;
  ldv_44546:
  amdgpu_fence_unref((struct amdgpu_fence **)(& sync->sync_to) + (unsigned long )i);
  i = i + 1U;
  ldv_44547: ;
  if (i <= 15U) {
    goto ldv_44546;
  } else {
  }
  amdgpu_fence_unref(& sync->last_vm_update);
  return;
}
}
bool ldv_queue_work_on_509(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_510(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_511(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_512(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_513(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_523(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_525(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_524(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_527(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_526(struct workqueue_struct *ldv_func_arg1 ) ;
extern int pci_bus_read_config_word(struct pci_bus * , unsigned int , int , u16 * ) ;
extern int pci_bus_write_config_word(struct pci_bus * , unsigned int , int , u16 ) ;
__inline static int pci_read_config_word(struct pci_dev const *dev , int where ,
                                         u16 *val )
{
  int tmp ;
  {
  tmp = pci_bus_read_config_word(dev->bus, dev->devfn, where, val);
  return (tmp);
}
}
__inline static int pci_write_config_word(struct pci_dev const *dev , int where ,
                                          u16 val )
{
  int tmp ;
  {
  tmp = pci_bus_write_config_word(dev->bus, dev->devfn, where, (int )val);
  return (tmp);
}
}
extern int pcie_capability_read_dword(struct pci_dev * , int , u32 * ) ;
extern void pci_clear_master(struct pci_dev * ) ;
__inline static int pci_pcie_cap(struct pci_dev *dev )
{
  {
  return ((int )dev->pcie_cap);
}
}
extern int drm_pcie_get_speed_cap_mask(struct drm_device * , u32 * ) ;
struct amd_ip_funcs const cik_common_ip_funcs ;
void cik_srbm_select(struct amdgpu_device *adev , u32 me , u32 pipe , u32 queue ,
                     u32 vmid ) ;
struct amd_ip_funcs const gmc_v7_0_ip_funcs ;
void gmc_v7_0_mc_stop(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save ) ;
void gmc_v7_0_mc_resume(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save ) ;
int gmc_v7_0_mc_wait_for_idle(struct amdgpu_device *adev ) ;
struct amd_ip_funcs const cik_ih_ip_funcs ;
struct amd_ip_funcs const dce_v8_0_ip_funcs ;
struct amd_ip_funcs const gfx_v7_0_ip_funcs ;
void gfx_v7_0_rlc_stop(struct amdgpu_device *adev ) ;
uint64_t gfx_v7_0_get_gpu_clock_counter(struct amdgpu_device *adev ) ;
void gfx_v7_0_select_se_sh(struct amdgpu_device *adev , u32 se_num , u32 sh_num ) ;
int gfx_v7_0_get_cu_info(struct amdgpu_device *adev , struct amdgpu_cu_info *cu_info ) ;
struct amd_ip_funcs const cik_sdma_ip_funcs ;
struct amd_ip_funcs const uvd_v4_2_ip_funcs ;
struct amd_ip_funcs const vce_v2_0_ip_funcs ;
struct amd_ip_funcs const ci_dpm_ip_funcs ;
struct amd_ip_funcs const kv_dpm_ip_funcs ;
static u32 cik_pcie_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->pcie_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 14U, reg, 0);
  amdgpu_mm_rreg(adev, 14U, 0);
  r = amdgpu_mm_rreg(adev, 15U, 0);
  spin_unlock_irqrestore(& adev->pcie_idx_lock, flags);
  return (r);
}
}
static void cik_pcie_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->pcie_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 14U, reg, 0);
  amdgpu_mm_rreg(adev, 14U, 0);
  amdgpu_mm_wreg(adev, 15U, v, 0);
  amdgpu_mm_rreg(adev, 15U, 0);
  spin_unlock_irqrestore(& adev->pcie_idx_lock, flags);
  return;
}
}
static u32 cik_smc_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, reg, 0);
  r = amdgpu_mm_rreg(adev, 129U, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (r);
}
}
static void cik_smc_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, reg, 0);
  amdgpu_mm_wreg(adev, 129U, v, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return;
}
}
static u32 cik_uvd_ctx_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->uvd_ctx_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 15656U, reg & 511U, 0);
  r = amdgpu_mm_rreg(adev, 15657U, 0);
  spin_unlock_irqrestore(& adev->uvd_ctx_idx_lock, flags);
  return (r);
}
}
static void cik_uvd_ctx_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->uvd_ctx_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 15656U, reg & 511U, 0);
  amdgpu_mm_wreg(adev, 15657U, v, 0);
  spin_unlock_irqrestore(& adev->uvd_ctx_idx_lock, flags);
  return;
}
}
static u32 cik_didt_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->didt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 12928U, reg, 0);
  r = amdgpu_mm_rreg(adev, 12929U, 0);
  spin_unlock_irqrestore(& adev->didt_idx_lock, flags);
  return (r);
}
}
static void cik_didt_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->didt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 12928U, reg, 0);
  amdgpu_mm_wreg(adev, 12929U, v, 0);
  spin_unlock_irqrestore(& adev->didt_idx_lock, flags);
  return;
}
}
static u32 const bonaire_golden_spm_registers[3U] = { 49664U, 3774873599U, 3758096384U};
static u32 const bonaire_golden_common_registers[12U] =
  { 12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const bonaire_golden_registers[123U] =
  { 3285U, 819U, 819U, 3284U,
        790464U, 262656U, 9860U, 65536U,
        360968U, 61440U, 4294909951U, 1310720U,
        61568U, 4261154815U, 256U, 61581U,
        1073741824U, 1073742336U, 9740U, 4294967295U,
        0U, 9741U, 4027580415U, 1024U,
        9742U, 131612U, 131584U, 798U,
        128U, 0U, 5868U, 240U,
        112U, 5872U, 4029751295U, 2150629376U,
        9790U, 1937192823U, 302055425U, 3395U,
        8454144U, 1082847232U, 7180U, 822083857U,
        17U, 3026U, 1937192823U, 302055425U,
        2179U, 32694U, 2204081U, 2180U,
        32694U, 2105777U, 2144U, 32694U,
        8593U, 2182U, 32694U, 2171313U,
        2183U, 32694U, 2105777U, 2167U,
        32694U, 8593U, 2168U, 32694U,
        8593U, 3466U, 63U, 10U,
        3467U, 63U, 10U, 2745U,
        475134U, 8866U, 2307U, 2047U,
        0U, 8837U, 4026531903U, 7U,
        8956U, 8193U, 1U, 8905U,
        4294967295U, 16777215U, 49793U, 65295U,
        0U, 41619U, 134217727U, 100663296U,
        310U, 4095U, 256U, 3998U,
        1U, 2U, 9280U, 50331648U,
        56805000U, 8960U, 255U, 1U,
        912U, 8191U, 8191U, 9240U,
        127U, 32U, 9538U, 65536U,
        65536U, 11013U, 1023U, 243U,
        11011U, 4294967295U, 4146U};
static u32 const bonaire_mgcg_cgcg_init[246U] =
  { 12552U, 4294967295U, 4294967292U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 3221225728U, 61618U,
        4294967295U, 3221225728U, 61617U, 4294967295U,
        3221225728U, 5497U, 4294967295U, 6291712U,
        61600U, 4294967295U, 256U, 61573U,
        4294967295U, 100663552U, 61576U, 4294967295U,
        256U, 61574U, 4294967295U, 100663552U,
        61569U, 4294967295U, 256U, 61624U,
        4294967295U, 256U, 61577U, 4294967295U,
        256U, 61568U, 4294967295U, 256U,
        61580U, 4294967295U, 256U, 61581U,
        4294967295U, 256U, 61588U, 4294967295U,
        256U, 61589U, 4294967295U, 256U,
        61590U, 4294967295U, 256U, 61591U,
        4294967295U, 256U, 61592U, 4294967295U,
        256U, 61599U, 4294967295U, 256U,
        61598U, 4294967295U, 256U, 61572U,
        4294967295U, 100663552U, 61604U, 4294967295U,
        256U, 61597U, 4294967295U, 256U,
        61613U, 4294967295U, 256U, 61612U,
        4294967295U, 256U, 61596U, 4294967295U,
        256U, 49664U, 4294967295U, 3758096384U,
        61448U, 4294967295U, 65536U, 61449U,
        4294967295U, 196610U, 61450U, 4294967295U,
        262151U, 61451U, 4294967295U, 393221U,
        61452U, 4294967295U, 589832U, 61453U,
        4294967295U, 65536U, 61454U, 4294967295U,
        196610U, 61455U, 4294967295U, 262151U,
        61456U, 4294967295U, 393221U, 61457U,
        4294967295U, 589832U, 61458U, 4294967295U,
        65536U, 61459U, 4294967295U, 196610U,
        61460U, 4294967295U, 262151U, 61461U,
        4294967295U, 393221U, 61462U, 4294967295U,
        589832U, 61463U, 4294967295U, 65536U,
        61464U, 4294967295U, 196610U, 61465U,
        4294967295U, 262151U, 61466U, 4294967295U,
        393221U, 61467U, 4294967295U, 589832U,
        61468U, 4294967295U, 65536U, 61469U,
        4294967295U, 196610U, 61470U, 4294967295U,
        262151U, 61471U, 4294967295U, 393221U,
        61472U, 4294967295U, 589832U, 61473U,
        4294967295U, 65536U, 61474U, 4294967295U,
        196610U, 61475U, 4294967295U, 262151U,
        61476U, 4294967295U, 393221U, 61477U,
        4294967295U, 589832U, 61478U, 4294967295U,
        65536U, 61479U, 4294967295U, 196610U,
        61480U, 4294967295U, 262151U, 61481U,
        4294967295U, 393221U, 61482U, 4294967295U,
        589832U, 61440U, 4294967295U, 2531262976U,
        8642U, 4294967295U, 9437440U, 12553U,
        4294967295U, 2097215U, 14U, 4294967295U,
        20971548U, 15U, 983040U, 983040U,
        136U, 4294967295U, 3227516940U, 137U,
        3221229567U, 256U, 996U, 4294967295U,
        256U, 998U, 257U, 0U,
        2090U, 4294967295U, 260U, 5497U,
        4278194175U, 256U, 3123U, 3221229567U,
        260U, 12409U, 1U, 1U,
        13315U, 4278194160U, 256U, 13827U,
        4278194160U, 256U};
static u32 const spectre_golden_spm_registers[3U] = { 49664U, 3774873599U, 3758096384U};
static u32 const spectre_golden_common_registers[12U] =
  { 12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const spectre_golden_registers[75U] =
  { 61440U, 4294909951U, 2526282240U, 61443U,
        4294901761U, 4278190080U, 61568U, 4294709247U,
        256U, 7094U, 65793U, 65536U,
        9741U, 4027580415U, 1024U, 9742U,
        4294967292U, 131584U, 5868U, 240U,
        112U, 5872U, 4029751295U, 2150629376U,
        9790U, 1937192823U, 302055425U, 9951U,
        16711680U, 16515072U, 3026U, 1937192823U,
        302055425U, 8837U, 4026531903U, 7U,
        8905U, 4294967295U, 16777215U, 41172U,
        1061109759U, 130U, 41173U, 63U,
        0U, 3998U, 1U, 2U,
        9295U, 4294902751U, 4U, 12762U,
        8U, 8U, 8960U, 2303U,
        2048U, 9538U, 65536U, 65536U,
        11011U, 4294967295U, 1417032208U, 34110U,
        33489407U, 2U, 34086U, 8386560U,
        2097152U, 32855U, 4294967295U, 3904U,
        49741U, 4294967295U, 1U};
static u32 const spectre_mgcg_cgcg_init[261U] =
  { 12552U, 4294967295U, 4294967292U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 256U, 61618U,
        4294967295U, 256U, 61617U, 4294967295U,
        256U, 5497U, 4294967295U, 6291712U,
        61600U, 4294967295U, 256U, 61573U,
        4294967295U, 100663552U, 61576U, 4294967295U,
        256U, 61574U, 4294967295U, 100663552U,
        61569U, 4294967295U, 256U, 61624U,
        4294967295U, 256U, 61577U, 4294967295U,
        256U, 61568U, 4294967295U, 256U,
        61580U, 4294967295U, 256U, 61581U,
        4294967295U, 256U, 61588U, 4294967295U,
        256U, 61589U, 4294967295U, 256U,
        61590U, 4294967295U, 256U, 61591U,
        4294967295U, 256U, 61592U, 4294967295U,
        256U, 61599U, 4294967295U, 256U,
        61598U, 4294967295U, 256U, 61572U,
        4294967295U, 100663552U, 61604U, 4294967295U,
        256U, 61597U, 4294967295U, 256U,
        61613U, 4294967295U, 256U, 61612U,
        4294967295U, 256U, 61596U, 4294967295U,
        256U, 49664U, 4294967295U, 3758096384U,
        61448U, 4294967295U, 65536U, 61449U,
        4294967295U, 196610U, 61450U, 4294967295U,
        262151U, 61451U, 4294967295U, 393221U,
        61452U, 4294967295U, 589832U, 61453U,
        4294967295U, 65536U, 61454U, 4294967295U,
        196610U, 61455U, 4294967295U, 262151U,
        61456U, 4294967295U, 393221U, 61457U,
        4294967295U, 589832U, 61458U, 4294967295U,
        65536U, 61459U, 4294967295U, 196610U,
        61460U, 4294967295U, 262151U, 61461U,
        4294967295U, 393221U, 61462U, 4294967295U,
        589832U, 61463U, 4294967295U, 65536U,
        61464U, 4294967295U, 196610U, 61465U,
        4294967295U, 262151U, 61466U, 4294967295U,
        393221U, 61467U, 4294967295U, 589832U,
        61468U, 4294967295U, 65536U, 61469U,
        4294967295U, 196610U, 61470U, 4294967295U,
        262151U, 61471U, 4294967295U, 393221U,
        61472U, 4294967295U, 589832U, 61473U,
        4294967295U, 65536U, 61474U, 4294967295U,
        196610U, 61475U, 4294967295U, 262151U,
        61476U, 4294967295U, 393221U, 61477U,
        4294967295U, 589832U, 61478U, 4294967295U,
        65536U, 61479U, 4294967295U, 196610U,
        61480U, 4294967295U, 262151U, 61481U,
        4294967295U, 393221U, 61482U, 4294967295U,
        589832U, 61483U, 4294967295U, 65536U,
        61484U, 4294967295U, 196610U, 61485U,
        4294967295U, 262151U, 61486U, 4294967295U,
        393221U, 61487U, 4294967295U, 589832U,
        61440U, 4294967295U, 2531262976U, 8642U,
        4294967295U, 9437440U, 12553U, 4294967295U,
        2097215U, 14U, 4294967295U, 20971548U,
        15U, 983040U, 983040U, 136U,
        4294967295U, 3227516940U, 137U, 3221229567U,
        256U, 996U, 4294967295U, 256U,
        998U, 257U, 0U, 2090U,
        4294967295U, 260U, 5497U, 4278194175U,
        256U, 3123U, 3221229567U, 260U,
        12409U, 1U, 1U, 13315U,
        4278194160U, 256U, 13827U, 4278194160U,
        256U};
static u32 const kalindi_golden_spm_registers[3U] = { 49664U, 3774873599U, 3758096384U};
static u32 const kalindi_golden_common_registers[12U] =
  { 12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const kalindi_golden_registers[90U] =
  { 61440U, 4294959103U, 1855209536U, 5497U,
        4284514303U, 4227858688U, 61576U, 4278194175U,
        256U, 61577U, 4278194175U, 256U,
        61568U, 4294709247U, 256U, 7094U,
        65793U, 65536U, 9740U, 4294967295U,
        0U, 9741U, 4027580415U, 1024U,
        5868U, 240U, 112U, 5872U,
        4029751295U, 2150629376U, 9790U, 1937192823U,
        302055425U, 9791U, 4294967295U, 16U,
        9951U, 16711680U, 16515072U, 8204U,
        7951U, 4106U, 3026U, 1937192823U,
        302055425U, 2306U, 1048575U, 786559U,
        8837U, 4026531903U, 7U, 8905U,
        1073692671U, 16764927U, 49793U, 65295U,
        0U, 41619U, 134217727U, 100663296U,
        310U, 4095U, 256U, 3998U,
        1U, 2U, 12762U, 8U,
        8U, 8960U, 255U, 3U,
        34110U, 33489407U, 2U, 34086U,
        8386560U, 2097152U, 32855U, 4294967295U,
        3904U, 8753U, 2046691U, 130U,
        8757U, 31U, 16U, 49741U,
        4294967295U, 0U};
static u32 const kalindi_mgcg_cgcg_init[165U] =
  { 12552U, 4294967295U, 4294967292U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 256U, 61618U,
        4294967295U, 256U, 61617U, 4294967295U,
        256U, 5497U, 4294967295U, 6291712U,
        61600U, 4294967295U, 256U, 61573U,
        4294967295U, 100663552U, 61576U, 4294967295U,
        256U, 61574U, 4294967295U, 100663552U,
        61569U, 4294967295U, 256U, 61624U,
        4294967295U, 256U, 61577U, 4294967295U,
        256U, 61568U, 4294967295U, 256U,
        61580U, 4294967295U, 256U, 61581U,
        4294967295U, 256U, 61588U, 4294967295U,
        256U, 61589U, 4294967295U, 256U,
        61590U, 4294967295U, 256U, 61591U,
        4294967295U, 256U, 61592U, 4294967295U,
        256U, 61599U, 4294967295U, 256U,
        61598U, 4294967295U, 256U, 61572U,
        4294967295U, 100663552U, 61604U, 4294967295U,
        256U, 61597U, 4294967295U, 256U,
        61613U, 4294967295U, 256U, 61612U,
        4294967295U, 256U, 61596U, 4294967295U,
        256U, 49664U, 4294967295U, 3758096384U,
        61448U, 4294967295U, 65536U, 61449U,
        4294967295U, 196610U, 61450U, 4294967295U,
        262151U, 61451U, 4294967295U, 393221U,
        61452U, 4294967295U, 589832U, 61453U,
        4294967295U, 65536U, 61454U, 4294967295U,
        196610U, 61455U, 4294967295U, 262151U,
        61456U, 4294967295U, 393221U, 61457U,
        4294967295U, 589832U, 61440U, 4294967295U,
        2531262976U, 8642U, 4294967295U, 9437440U,
        12553U, 4294967295U, 2097215U, 14U,
        4294967295U, 20971548U, 15U, 983040U,
        983040U, 136U, 4294967295U, 3227516940U,
        137U, 3221229567U, 256U, 2090U,
        4294967295U, 260U, 5497U, 4278194175U,
        256U, 3123U, 3221229567U, 260U,
        12409U, 1U, 1U, 13315U,
        4278194160U, 256U, 13827U, 4278194160U,
        256U};
static u32 const hawaii_golden_spm_registers[3U] = { 49664U, 3774873599U, 3758096384U};
static u32 const hawaii_golden_common_registers[15U] =
  { 49664U, 4294967295U, 3758096384U, 41172U,
        4294967295U, 973084186U, 41173U, 4294967295U,
        46U, 9860U, 4294967295U, 98824U,
        9790U, 4294967295U, 302059523U};
static u32 const hawaii_golden_registers[108U] =
  { 3285U, 819U, 819U, 9860U,
        65536U, 360968U, 9740U, 4294967295U,
        0U, 9741U, 4027580415U, 1024U,
        9742U, 131612U, 131584U, 798U,
        128U, 0U, 5868U, 240U,
        112U, 5872U, 4029751295U, 2150629376U,
        3395U, 8454144U, 1082847232U, 7180U,
        822083857U, 17U, 3026U, 1937192823U,
        302055425U, 2120U, 127U, 27U,
        2167U, 32694U, 8593U, 3466U,
        63U, 10U, 3467U, 63U,
        10U, 2745U, 475134U, 8866U,
        2307U, 2047U, 0U, 8956U,
        8193U, 1U, 8905U, 4294967295U,
        16777215U, 49793U, 65295U, 0U,
        41619U, 134217727U, 100663296U, 3998U,
        1U, 2U, 12762U, 8U,
        8U, 12764U, 3840U, 2048U,
        12765U, 3840U, 2048U, 12774U,
        16777215U, 16744383U, 12775U, 16777215U,
        16744367U, 8960U, 255U, 2048U,
        912U, 8191U, 8191U, 9240U,
        127U, 32U, 9538U, 65536U,
        65536U, 11136U, 1048576U, 1044604U,
        11013U, 1023U, 15U, 11012U,
        4294967295U, 1969552876U, 11011U, 4294967295U,
        824228264U, 11010U, 536870912U, 261881856U};
static u32 const hawaii_mgcg_cgcg_init[321U] =
  { 12552U, 4294967295U, 4294967293U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 256U, 61618U,
        4294967295U, 256U, 61617U, 4294967295U,
        256U, 5497U, 4294967295U, 2097408U,
        61600U, 4294967295U, 256U, 61573U,
        4294967295U, 100663552U, 61576U, 4294967295U,
        256U, 61574U, 4294967295U, 100663552U,
        61569U, 4294967295U, 256U, 61624U,
        4294967295U, 256U, 61577U, 4294967295U,
        256U, 61568U, 4294967295U, 256U,
        61580U, 4294967295U, 256U, 61581U,
        4294967295U, 256U, 61588U, 4294967295U,
        256U, 61589U, 4294967295U, 256U,
        61590U, 4294967295U, 256U, 61591U,
        4294967295U, 256U, 61592U, 4294967295U,
        256U, 61599U, 4294967295U, 256U,
        61598U, 4294967295U, 256U, 61572U,
        4294967295U, 100663552U, 61604U, 4294967295U,
        256U, 61597U, 4294967295U, 256U,
        61613U, 4294967295U, 256U, 61612U,
        4294967295U, 256U, 61596U, 4294967295U,
        256U, 49664U, 4294967295U, 3758096384U,
        61448U, 4294967295U, 65536U, 61449U,
        4294967295U, 196610U, 61450U, 4294967295U,
        262151U, 61451U, 4294967295U, 393221U,
        61452U, 4294967295U, 589832U, 61453U,
        4294967295U, 65536U, 61454U, 4294967295U,
        196610U, 61455U, 4294967295U, 262151U,
        61456U, 4294967295U, 393221U, 61457U,
        4294967295U, 589832U, 61458U, 4294967295U,
        65536U, 61459U, 4294967295U, 196610U,
        61460U, 4294967295U, 262151U, 61461U,
        4294967295U, 393221U, 61462U, 4294967295U,
        589832U, 61463U, 4294967295U, 65536U,
        61464U, 4294967295U, 196610U, 61465U,
        4294967295U, 262151U, 61466U, 4294967295U,
        393221U, 61467U, 4294967295U, 589832U,
        61468U, 4294967295U, 65536U, 61469U,
        4294967295U, 196610U, 61470U, 4294967295U,
        262151U, 61471U, 4294967295U, 393221U,
        61472U, 4294967295U, 589832U, 61473U,
        4294967295U, 65536U, 61474U, 4294967295U,
        196610U, 61475U, 4294967295U, 262151U,
        61476U, 4294967295U, 393221U, 61477U,
        4294967295U, 589832U, 61478U, 4294967295U,
        65536U, 61479U, 4294967295U, 196610U,
        61480U, 4294967295U, 262151U, 61481U,
        4294967295U, 393221U, 61482U, 4294967295U,
        589832U, 61483U, 4294967295U, 65536U,
        61484U, 4294967295U, 196610U, 61485U,
        4294967295U, 262151U, 61486U, 4294967295U,
        393221U, 61487U, 4294967295U, 589832U,
        61488U, 4294967295U, 65536U, 61489U,
        4294967295U, 196610U, 61490U, 4294967295U,
        262151U, 61491U, 4294967295U, 393221U,
        61492U, 4294967295U, 589832U, 61493U,
        4294967295U, 65536U, 61494U, 4294967295U,
        196610U, 61495U, 4294967295U, 262151U,
        61496U, 4294967295U, 393221U, 61497U,
        4294967295U, 589832U, 61498U, 4294967295U,
        65536U, 61499U, 4294967295U, 196610U,
        61500U, 4294967295U, 262151U, 61501U,
        4294967295U, 393221U, 61502U, 4294967295U,
        589832U, 12486U, 4294967295U, 131584U,
        3284U, 4294967295U, 512U, 1392U,
        4294967295U, 1024U, 5498U, 4294967295U,
        0U, 3028U, 4294967295U, 2306U,
        61440U, 4294967295U, 2526282240U, 8642U,
        4294967295U, 9437440U, 12553U, 4294967295U,
        2097215U, 14U, 4294967295U, 20971548U,
        15U, 983040U, 983040U, 136U,
        4294967295U, 3227516940U, 137U, 3221229567U,
        256U, 996U, 4294967295U, 256U,
        998U, 257U, 0U, 2090U,
        4294967295U, 260U, 5497U, 4278194175U,
        256U, 3123U, 3221229567U, 260U,
        12409U, 1U, 1U, 13315U,
        4278194160U, 256U, 13827U, 4278194160U,
        256U};
static u32 const godavari_golden_registers[96U] =
  { 5497U, 4284514303U, 4227858688U, 7094U,
        65793U, 65536U, 9740U, 4294967295U,
        0U, 155840U, 4027580415U, 1024U,
        6220U, 4294967295U, 65536U, 5868U,
        240U, 112U, 5872U, 4029751295U,
        2150629376U, 9790U, 1937192823U, 302055425U,
        9791U, 4294967295U, 16U, 8204U,
        7951U, 4106U, 3026U, 1937192823U,
        302055425U, 2306U, 1048575U, 786559U,
        8837U, 4026531903U, 7U, 8905U,
        4294967295U, 16715775U, 49793U, 65295U,
        0U, 41619U, 134217727U, 100663296U,
        310U, 4095U, 256U, 13317U,
        65536U, 8454145U, 13829U, 65536U,
        8454145U, 3998U, 1U, 2U,
        12762U, 8U, 8U, 12764U,
        3840U, 2048U, 12765U, 3840U,
        2048U, 12774U, 16777215U, 16744383U,
        12775U, 16777215U, 16744367U, 8960U,
        255U, 1U, 34110U, 33489407U,
        2U, 34086U, 8386560U, 2097152U,
        32855U, 4294967295U, 3904U, 8753U,
        2046691U, 130U, 8757U, 31U,
        16U, 49741U, 4294967295U, 0U};
static void cik_init_golden_registers(struct amdgpu_device *adev )
{
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& bonaire_mgcg_cgcg_init),
                                   246U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& bonaire_golden_registers),
                                   123U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& bonaire_golden_common_registers),
                                   12U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& bonaire_golden_spm_registers),
                                   3U);
  goto ldv_53109;
  case 2U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_mgcg_cgcg_init),
                                   165U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_golden_registers),
                                   90U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_golden_common_registers),
                                   12U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_golden_spm_registers),
                                   3U);
  goto ldv_53109;
  case 4U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_mgcg_cgcg_init),
                                   165U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& godavari_golden_registers),
                                   96U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_golden_common_registers),
                                   12U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& kalindi_golden_spm_registers),
                                   3U);
  goto ldv_53109;
  case 1U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& spectre_mgcg_cgcg_init),
                                   261U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& spectre_golden_registers),
                                   75U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& spectre_golden_common_registers),
                                   12U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& spectre_golden_spm_registers),
                                   3U);
  goto ldv_53109;
  case 3U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& hawaii_mgcg_cgcg_init),
                                   321U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& hawaii_golden_registers),
                                   108U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& hawaii_golden_common_registers),
                                   15U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& hawaii_golden_spm_registers),
                                   3U);
  goto ldv_53109;
  default: ;
  goto ldv_53109;
  }
  ldv_53109:
  mutex_unlock(& adev->grbm_idx_mutex);
  return;
}
}
static u32 cik_get_xclk(struct amdgpu_device *adev )
{
  u32 reference_clock ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  reference_clock = adev->clock.spll.reference_freq;
  if ((adev->flags & 131072UL) != 0UL) {
    tmp = (*(adev->smc_rreg))(adev, 3223322624U);
    if ((tmp & 32768U) != 0U) {
      return (reference_clock / 2U);
    } else {
    }
  } else {
    tmp___0 = (*(adev->smc_rreg))(adev, 3226468768U);
    if ((tmp___0 & 2U) != 0U) {
      return (reference_clock / 4U);
    } else {
    }
  }
  return (reference_clock);
}
}
void cik_srbm_select(struct amdgpu_device *adev , u32 me , u32 pipe , u32 queue ,
                     u32 vmid )
{
  u32 srbm_gfx_cntl ;
  {
  srbm_gfx_cntl = (((pipe & 3U) | ((me << 2) & 12U)) | ((vmid << 4) & 255U)) | ((queue << 8) & 1792U);
  amdgpu_mm_wreg(adev, 913U, srbm_gfx_cntl, 0);
  return;
}
}
static void cik_vga_set_state(struct amdgpu_device *adev , bool state )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 5385U, 0);
  if (! state) {
    tmp = tmp | 2U;
  } else {
    tmp = tmp & 4294967293U;
  }
  amdgpu_mm_wreg(adev, 5385U, tmp, 0);
  return;
}
}
static bool cik_read_disabled_bios(struct amdgpu_device *adev )
{
  u32 bus_cntl ;
  u32 d1vga_control ;
  u32 d2vga_control ;
  u32 vga_render_control ;
  u32 rom_cntl ;
  bool r ;
  {
  d1vga_control = 0U;
  d2vga_control = 0U;
  vga_render_control = 0U;
  bus_cntl = amdgpu_mm_rreg(adev, 5384U, 0);
  if (adev->mode_info.num_crtc != 0) {
    d1vga_control = amdgpu_mm_rreg(adev, 204U, 0);
    d2vga_control = amdgpu_mm_rreg(adev, 206U, 0);
    vga_render_control = amdgpu_mm_rreg(adev, 192U, 0);
  } else {
  }
  rom_cntl = (*(adev->smc_rreg))(adev, 3227516928U);
  amdgpu_mm_wreg(adev, 5384U, bus_cntl & 4294967293U, 0);
  if (adev->mode_info.num_crtc != 0) {
    amdgpu_mm_wreg(adev, 204U, d1vga_control & 4294967038U, 0);
    amdgpu_mm_wreg(adev, 206U, d2vga_control & 4294967038U, 0);
    amdgpu_mm_wreg(adev, 192U, vga_render_control & 4294770687U, 0);
  } else {
  }
  (*(adev->smc_wreg))(adev, 3227516928U, rom_cntl | 2U);
  r = amdgpu_read_bios(adev);
  amdgpu_mm_wreg(adev, 5384U, bus_cntl, 0);
  if (adev->mode_info.num_crtc != 0) {
    amdgpu_mm_wreg(adev, 204U, d1vga_control, 0);
    amdgpu_mm_wreg(adev, 206U, d2vga_control, 0);
    amdgpu_mm_wreg(adev, 192U, vga_render_control, 0);
  } else {
  }
  (*(adev->smc_wreg))(adev, 3227516928U, rom_cntl);
  return (r);
}
}
static struct amdgpu_allowed_register_entry cik_allowed_read_registers[56U] =
  { {8196U, 0, (_Bool)0},
        {9790U, 0, (_Bool)0},
        {2520U, 0, (_Bool)0},
        {9796U, 0, (_Bool)0},
        {9797U, 0, (_Bool)0},
        {9798U, 0, (_Bool)0},
        {9799U, 0, (_Bool)0},
        {9800U, 0, (_Bool)0},
        {9801U, 0, (_Bool)0},
        {9802U, 0, (_Bool)0},
        {9803U, 0, (_Bool)0},
        {9804U, 0, (_Bool)0},
        {9805U, 0, (_Bool)0},
        {9806U, 0, (_Bool)0},
        {9807U, 0, (_Bool)0},
        {9808U, 0, (_Bool)0},
        {9809U, 0, (_Bool)0},
        {9810U, 0, (_Bool)0},
        {9811U, 0, (_Bool)0},
        {9812U, 0, (_Bool)0},
        {9813U, 0, (_Bool)0},
        {9814U, 0, (_Bool)0},
        {9815U, 0, (_Bool)0},
        {9816U, 0, (_Bool)0},
        {9817U, 0, (_Bool)0},
        {9818U, 0, (_Bool)0},
        {9819U, 0, (_Bool)0},
        {9820U, 0, (_Bool)0},
        {9821U, 0, (_Bool)0},
        {9822U, 0, (_Bool)0},
        {9823U, 0, (_Bool)0},
        {9824U, 0, (_Bool)0},
        {9825U, 0, (_Bool)0},
        {9826U, 0, (_Bool)0},
        {9827U, 0, (_Bool)0},
        {9828U, 0, (_Bool)0},
        {9829U, 0, (_Bool)0},
        {9830U, 0, (_Bool)0},
        {9831U, 0, (_Bool)0},
        {9832U, 0, (_Bool)0},
        {9833U, 0, (_Bool)0},
        {9834U, 0, (_Bool)0},
        {9835U, 0, (_Bool)0},
        {9836U, 0, (_Bool)0},
        {9837U, 0, (_Bool)0},
        {9838U, 0, (_Bool)0},
        {9839U, 0, (_Bool)0},
        {9840U, 0, (_Bool)0},
        {9841U, 0, (_Bool)0},
        {9842U, 0, (_Bool)0},
        {9843U, 0, (_Bool)0},
        {9789U, 0, 1},
        {9951U, 0, 1},
        {9791U, 0, 0},
        {41172U, 0, 1},
        {41173U, 0, 1}};
static u32 cik_read_indexed_register(struct amdgpu_device *adev , u32 se_num , u32 sh_num ,
                                     u32 reg_offset )
{
  u32 val ;
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  if (se_num != 4294967295U || sh_num != 4294967295U) {
    gfx_v7_0_select_se_sh(adev, se_num, sh_num);
  } else {
  }
  val = amdgpu_mm_rreg(adev, reg_offset, 0);
  if (se_num != 4294967295U || sh_num != 4294967295U) {
    gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  } else {
  }
  mutex_unlock(& adev->grbm_idx_mutex);
  return (val);
}
}
static int cik_read_register(struct amdgpu_device *adev , u32 se_num , u32 sh_num ,
                             u32 reg_offset , u32 *value )
{
  u32 i ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  *value = 0U;
  i = 0U;
  goto ldv_53193;
  ldv_53192: ;
  if (cik_allowed_read_registers[i].reg_offset != reg_offset) {
    goto ldv_53191;
  } else {
  }
  if (! cik_allowed_read_registers[i].untouched) {
    if ((int )cik_allowed_read_registers[i].grbm_indexed) {
      tmp = cik_read_indexed_register(adev, se_num, sh_num, reg_offset);
      *value = tmp;
    } else {
      tmp___0 = amdgpu_mm_rreg(adev, reg_offset, 0);
      *value = tmp___0;
    }
  } else {
  }
  return (0);
  ldv_53191:
  i = i + 1U;
  ldv_53193: ;
  if (i <= 55U) {
    goto ldv_53192;
  } else {
  }
  return (-22);
}
}
static void cik_print_gpu_status_regs(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  {
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 8194U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 8197U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE0=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 8198U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE1=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 8206U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE2=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 8207U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE3=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 13325U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_STATUS_REG   = 0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 13837U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA1_STATUS_REG   = 0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 8608U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STAT = 0x%08x\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 8605U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT1 = 0x%08x\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 8606U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT2 = 0x%08x\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 8604U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT3 = 0x%08x\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 8328U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_BUSY_STAT = 0x%08x\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 8329U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STALLED_STAT1 = 0x%08x\n",
            tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 8327U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STATUS = 0x%08x\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 8325U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_BUSY_STAT = 0x%08x\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 8326U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STALLED_STAT1 = 0x%08x\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 8324U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STATUS = 0x%08x\n", tmp___18);
  return;
}
}
u32 amdgpu_cik_gpu_check_soft_reset(struct amdgpu_device *adev )
{
  u32 reset_mask ;
  u32 tmp ;
  bool tmp___0 ;
  long tmp___1 ;
  {
  reset_mask = 0U;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((tmp & 1205780480U) != 0U) {
    reset_mask = reset_mask | 1U;
  } else {
  }
  if ((tmp & 805306368U) != 0U) {
    reset_mask = reset_mask | 8U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 8194U, 0);
  if ((tmp & 16777216U) != 0U) {
    reset_mask = reset_mask | 64U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 13325U, 0);
  if ((tmp & 1U) == 0U) {
    reset_mask = reset_mask | 4U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 13837U, 0);
  if ((tmp & 1U) == 0U) {
    reset_mask = reset_mask | 32U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  if ((tmp & 32U) != 0U) {
    reset_mask = reset_mask | 4U;
  } else {
  }
  if ((tmp & 64U) != 0U) {
    reset_mask = reset_mask | 32U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 131072U) != 0U) {
    reset_mask = reset_mask | 256U;
  } else {
  }
  if ((tmp & 16384U) != 0U) {
    reset_mask = reset_mask | 128U;
  } else {
  }
  if ((tmp & 32U) != 0U) {
    reset_mask = reset_mask | 16U;
  } else {
  }
  if ((tmp & 256U) != 0U) {
    reset_mask = reset_mask | 512U;
  } else {
  }
  if ((tmp & 7680U) != 0U) {
    reset_mask = reset_mask | 1024U;
  } else {
  }
  tmp___0 = (*((adev->mode_info.funcs)->is_display_hung))(adev);
  if ((int )tmp___0) {
    reset_mask = reset_mask | 2048U;
  } else {
  }
  if ((reset_mask & 1024U) != 0U) {
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_cik_gpu_check_soft_reset", "MC busy: 0x%08X, clearing.\n",
                          reset_mask);
    } else {
    }
    reset_mask = reset_mask & 4294966271U;
  } else {
  }
  return (reset_mask);
}
}
static void cik_gpu_soft_reset(struct amdgpu_device *adev , u32 reset_mask )
{
  struct amdgpu_mode_mc_save save ;
  u32 grbm_soft_reset ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  int tmp___2 ;
  {
  grbm_soft_reset = 0U;
  srbm_soft_reset = 0U;
  if (reset_mask == 0U) {
    return;
  } else {
  }
  _dev_info((struct device const *)adev->dev, "GPU softreset: 0x%08X\n", reset_mask);
  cik_print_gpu_status_regs(adev);
  tmp___0 = amdgpu_mm_rreg(adev, 1343U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
            tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 1335U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
            tmp___1);
  gfx_v7_0_rlc_stop(adev);
  amdgpu_mm_wreg(adev, 8630U, 352321536U, 0);
  amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
  if ((reset_mask & 4U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13330U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13330U, tmp, 0);
  } else {
  }
  if ((reset_mask & 32U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13842U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13842U, tmp, 0);
  } else {
  }
  gmc_v7_0_mc_stop(adev, & save);
  tmp___2 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___2 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  if ((reset_mask & 11U) != 0U) {
    grbm_soft_reset = 65537U;
  } else {
  }
  if ((reset_mask & 8U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if ((reset_mask & 4U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1048576U;
  } else {
  }
  if ((reset_mask & 32U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 64U;
  } else {
  }
  if ((reset_mask & 2048U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 32U;
  } else {
  }
  if ((reset_mask & 64U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 4U;
  } else {
  }
  if ((reset_mask & 128U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 32768U;
  } else {
  }
  if ((reset_mask & 256U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if ((reset_mask & 16U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if ((reset_mask & 512U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 131072U;
  } else {
  }
  if ((adev->flags & 131072UL) == 0UL) {
    if ((reset_mask & 1024U) != 0U) {
      srbm_soft_reset = srbm_soft_reset | 2048U;
    } else {
    }
  } else {
  }
  if (grbm_soft_reset != 0U) {
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    tmp = tmp | grbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 8200U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    __const_udelay(214750UL);
    tmp = ~ grbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 8200U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
  } else {
  }
  if (srbm_soft_reset != 0U) {
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
  } else {
  }
  __const_udelay(214750UL);
  gmc_v7_0_mc_resume(adev, & save);
  __const_udelay(214750UL);
  cik_print_gpu_status_regs(adev);
  return;
}
}
static void kv_save_regs_for_reset(struct amdgpu_device *adev , struct kv_reset_save_regs *save )
{
  {
  save->gmcon_reng_execute = amdgpu_mm_rreg(adev, 3394U, 0);
  save->gmcon_misc = amdgpu_mm_rreg(adev, 3395U, 0);
  save->gmcon_misc3 = amdgpu_mm_rreg(adev, 3409U, 0);
  amdgpu_mm_wreg(adev, 3394U, save->gmcon_reng_execute & 4294967294U, 0);
  amdgpu_mm_wreg(adev, 3395U, save->gmcon_misc & 4294899711U, 0);
  return;
}
}
static void kv_restore_regs_for_reset(struct amdgpu_device *adev , struct kv_reset_save_regs *save )
{
  int i ;
  {
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  amdgpu_mm_wreg(adev, 3406U, 536875263U, 0);
  i = 0;
  goto ldv_53226;
  ldv_53225:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53226: ;
  if (i <= 4) {
    goto ldv_53225;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  amdgpu_mm_wreg(adev, 3406U, 805310719U, 0);
  i = 0;
  goto ldv_53229;
  ldv_53228:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53229: ;
  if (i <= 4) {
    goto ldv_53228;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 2162688U, 0);
  amdgpu_mm_wreg(adev, 3406U, 2684358911U, 0);
  i = 0;
  goto ldv_53232;
  ldv_53231:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53232: ;
  if (i <= 4) {
    goto ldv_53231;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 135171U, 0);
  amdgpu_mm_wreg(adev, 3406U, 2952794367U, 0);
  i = 0;
  goto ldv_53235;
  ldv_53234:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53235: ;
  if (i <= 4) {
    goto ldv_53234;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 11008U, 0);
  amdgpu_mm_wreg(adev, 3406U, 3221229823U, 0);
  i = 0;
  goto ldv_53238;
  ldv_53237:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53238: ;
  if (i <= 4) {
    goto ldv_53237;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  amdgpu_mm_wreg(adev, 3406U, 3489665279U, 0);
  i = 0;
  goto ldv_53241;
  ldv_53240:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53241: ;
  if (i <= 4) {
    goto ldv_53240;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 4325376U, 0);
  amdgpu_mm_wreg(adev, 3406U, 268439807U, 0);
  i = 0;
  goto ldv_53244;
  ldv_53243:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53244: ;
  if (i <= 4) {
    goto ldv_53243;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 1180162U, 0);
  amdgpu_mm_wreg(adev, 3406U, 1342181631U, 0);
  i = 0;
  goto ldv_53247;
  ldv_53246:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53247: ;
  if (i <= 4) {
    goto ldv_53246;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 4079158U, 0);
  amdgpu_mm_wreg(adev, 3406U, 1610617087U, 0);
  i = 0;
  goto ldv_53250;
  ldv_53249:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53250: ;
  if (i <= 4) {
    goto ldv_53249;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 3620670U, 0);
  amdgpu_mm_wreg(adev, 3406U, 1879052543U, 0);
  i = 0;
  goto ldv_53253;
  ldv_53252:
  amdgpu_mm_wreg(adev, 3407U, 0U, 0);
  i = i + 1;
  ldv_53253: ;
  if (i <= 4) {
    goto ldv_53252;
  } else {
  }
  amdgpu_mm_wreg(adev, 3407U, 4068146U, 0);
  amdgpu_mm_wreg(adev, 3406U, 3758100735U, 0);
  amdgpu_mm_wreg(adev, 3409U, save->gmcon_misc3, 0);
  amdgpu_mm_wreg(adev, 3395U, save->gmcon_misc, 0);
  amdgpu_mm_wreg(adev, 3394U, save->gmcon_reng_execute, 0);
  return;
}
}
static void cik_gpu_pci_config_reset(struct amdgpu_device *adev )
{
  struct amdgpu_mode_mc_save save ;
  struct kv_reset_save_regs kv_save ;
  u32 tmp ;
  u32 i ;
  int tmp___0 ;
  u32 tmp___1 ;
  {
  kv_save.gmcon_reng_execute = 0U;
  kv_save.gmcon_misc = 0U;
  kv_save.gmcon_misc3 = 0U;
  _dev_info((struct device const *)adev->dev, "GPU pci config reset\n");
  amdgpu_mm_wreg(adev, 8630U, 352321536U, 0);
  amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
  tmp = amdgpu_mm_rreg(adev, 13330U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 13330U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 13842U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 13842U, tmp, 0);
  gfx_v7_0_rlc_stop(adev);
  __const_udelay(214750UL);
  gmc_v7_0_mc_stop(adev, & save);
  tmp___0 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___0 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timed out !\n");
  } else {
  }
  if ((adev->flags & 131072UL) != 0UL) {
    kv_save_regs_for_reset(adev, & kv_save);
  } else {
  }
  pci_clear_master(adev->pdev);
  amdgpu_pci_config_reset(adev);
  __const_udelay(429500UL);
  i = 0U;
  goto ldv_53264;
  ldv_53263:
  tmp___1 = amdgpu_mm_rreg(adev, 5386U, 0);
  if (tmp___1 != 4294967295U) {
    goto ldv_53262;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53264: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_53263;
  } else {
  }
  ldv_53262: ;
  if ((adev->flags & 131072UL) != 0UL) {
    kv_restore_regs_for_reset(adev, & kv_save);
  } else {
  }
  return;
}
}
static void cik_set_bios_scratch_engine_hung(struct amdgpu_device *adev , bool hung )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 1484U, 0);
  tmp = tmp___0;
  if ((int )hung) {
    tmp = tmp | 536870912U;
  } else {
    tmp = tmp & 3758096383U;
  }
  amdgpu_mm_wreg(adev, 1484U, tmp, 0);
  return;
}
}
static int cik_asic_reset(struct amdgpu_device *adev )
{
  u32 reset_mask ;
  {
  reset_mask = amdgpu_cik_gpu_check_soft_reset(adev);
  if (reset_mask != 0U) {
    cik_set_bios_scratch_engine_hung(adev, 1);
  } else {
  }
  cik_gpu_soft_reset(adev, reset_mask);
  reset_mask = amdgpu_cik_gpu_check_soft_reset(adev);
  if (reset_mask != 0U && amdgpu_hard_reset != 0) {
    cik_gpu_pci_config_reset(adev);
  } else {
  }
  reset_mask = amdgpu_cik_gpu_check_soft_reset(adev);
  if (reset_mask == 0U) {
    cik_set_bios_scratch_engine_hung(adev, 0);
  } else {
  }
  return (0);
}
}
static int cik_set_uvd_clock(struct amdgpu_device *adev , u32 clock , u32 cntl_reg ,
                             u32 status_reg )
{
  int r ;
  int i ;
  struct atom_clock_dividers dividers ;
  u32 tmp ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  {
  r = amdgpu_atombios_get_clock_dividers(adev, 0, clock, 0, & dividers);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, cntl_reg);
  tmp = tmp & 4294966912U;
  tmp = dividers.post_divider | tmp;
  (*(adev->smc_wreg))(adev, cntl_reg, tmp);
  i = 0;
  goto ldv_53290;
  ldv_53289:
  tmp___0 = (*(adev->smc_rreg))(adev, status_reg);
  if ((int )tmp___0 & 1) {
    goto ldv_53284;
  } else {
  }
  __ms = 10UL;
  goto ldv_53287;
  ldv_53286:
  __const_udelay(4295000UL);
  ldv_53287:
  tmp___1 = __ms;
  __ms = __ms - 1UL;
  if (tmp___1 != 0UL) {
    goto ldv_53286;
  } else {
  }
  i = i + 1;
  ldv_53290: ;
  if (i <= 99) {
    goto ldv_53289;
  } else {
  }
  ldv_53284: ;
  if (i == 100) {
    return (-110);
  } else {
  }
  return (0);
}
}
static int cik_set_uvd_clocks(struct amdgpu_device *adev , u32 vclk , u32 dclk )
{
  int r ;
  {
  r = 0;
  r = cik_set_uvd_clock(adev, vclk, 3226468516U, 3226468520U);
  if (r != 0) {
    return (r);
  } else {
  }
  r = cik_set_uvd_clock(adev, dclk, 3226468508U, 3226468512U);
  return (r);
}
}
static int cik_set_vce_clocks(struct amdgpu_device *adev , u32 evclk , u32 ecclk )
{
  int r ;
  int i ;
  struct atom_clock_dividers dividers ;
  u32 tmp ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  u32 tmp___2 ;
  unsigned long __ms___0 ;
  unsigned long tmp___3 ;
  {
  r = amdgpu_atombios_get_clock_dividers(adev, 0, ecclk, 0, & dividers);
  if (r != 0) {
    return (r);
  } else {
  }
  i = 0;
  goto ldv_53312;
  ldv_53311:
  tmp___0 = (*(adev->smc_rreg))(adev, 3226468528U);
  if ((int )tmp___0 & 1) {
    goto ldv_53306;
  } else {
  }
  __ms = 10UL;
  goto ldv_53309;
  ldv_53308:
  __const_udelay(4295000UL);
  ldv_53309:
  tmp___1 = __ms;
  __ms = __ms - 1UL;
  if (tmp___1 != 0UL) {
    goto ldv_53308;
  } else {
  }
  i = i + 1;
  ldv_53312: ;
  if (i <= 99) {
    goto ldv_53311;
  } else {
  }
  ldv_53306: ;
  if (i == 100) {
    return (-110);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3226468524U);
  tmp = tmp & 4294966912U;
  tmp = dividers.post_divider | tmp;
  (*(adev->smc_wreg))(adev, 3226468524U, tmp);
  i = 0;
  goto ldv_53319;
  ldv_53318:
  tmp___2 = (*(adev->smc_rreg))(adev, 3226468528U);
  if ((int )tmp___2 & 1) {
    goto ldv_53313;
  } else {
  }
  __ms___0 = 10UL;
  goto ldv_53316;
  ldv_53315:
  __const_udelay(4295000UL);
  ldv_53316:
  tmp___3 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___3 != 0UL) {
    goto ldv_53315;
  } else {
  }
  i = i + 1;
  ldv_53319: ;
  if (i <= 99) {
    goto ldv_53318;
  } else {
  }
  ldv_53313: ;
  if (i == 100) {
    return (-110);
  } else {
  }
  return (0);
}
}
static void cik_pcie_gen3_enable(struct amdgpu_device *adev )
{
  struct pci_dev *root ;
  int bridge_pos ;
  int gpu_pos ;
  u32 speed_cntl ;
  u32 mask ;
  u32 current_data_rate ;
  int ret ;
  int i ;
  u16 tmp16 ;
  u16 bridge_cfg ;
  u16 gpu_cfg ;
  u16 bridge_cfg2 ;
  u16 gpu_cfg2 ;
  u32 max_lw ;
  u32 current_lw ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  {
  root = ((adev->pdev)->bus)->self;
  if (amdgpu_pcie_gen2 == 0) {
    return;
  } else {
  }
  if ((adev->flags & 131072UL) != 0UL) {
    return;
  } else {
  }
  ret = drm_pcie_get_speed_cap_mask(adev->ddev, & mask);
  if (ret != 0) {
    return;
  } else {
  }
  if ((mask & 6U) == 0U) {
    return;
  } else {
  }
  speed_cntl = (*(adev->pcie_rreg))(adev, 268501156U);
  current_data_rate = (speed_cntl & 24576U) >> 13;
  if ((mask & 4U) != 0U) {
    if (current_data_rate == 2U) {
      printk("\016[drm] PCIE gen 3 link speeds already enabled\n");
      return;
    } else {
    }
    printk("\016[drm] enabling PCIE gen 3 link speeds, disable with amdgpu.pcie_gen2=0\n");
  } else
  if ((mask & 2U) != 0U) {
    if (current_data_rate == 1U) {
      printk("\016[drm] PCIE gen 2 link speeds already enabled\n");
      return;
    } else {
    }
    printk("\016[drm] enabling PCIE gen 2 link speeds, disable with amdgpu.pcie_gen2=0\n");
  } else {
  }
  bridge_pos = pci_pcie_cap(root);
  if (bridge_pos == 0) {
    return;
  } else {
  }
  gpu_pos = pci_pcie_cap(adev->pdev);
  if (gpu_pos == 0) {
    return;
  } else {
  }
  if ((mask & 4U) != 0U) {
    if (current_data_rate != 2U) {
      pci_read_config_word((struct pci_dev const *)root, bridge_pos + 16, & bridge_cfg);
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 16, & gpu_cfg);
      tmp16 = (u16 )((unsigned int )bridge_cfg | 512U);
      pci_write_config_word((struct pci_dev const *)root, bridge_pos + 16, (int )tmp16);
      tmp16 = (u16 )((unsigned int )gpu_cfg | 512U);
      pci_write_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 16, (int )tmp16);
      tmp = (*(adev->pcie_rreg))(adev, 20971560U);
      max_lw = (tmp & 224U) >> 5;
      current_lw = (tmp & 28U) >> 2;
      if (current_lw < max_lw) {
        tmp = (*(adev->pcie_rreg))(adev, 268501154U);
        if ((tmp & 512U) != 0U) {
          tmp = tmp & 4294959096U;
          tmp = tmp | max_lw;
          tmp = tmp | 5376U;
          (*(adev->pcie_wreg))(adev, 268501154U, tmp);
        } else {
        }
      } else {
      }
      i = 0;
      goto ldv_53345;
      ldv_53344:
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 10, & tmp16);
      if (((int )tmp16 & 32) != 0) {
        goto ldv_53339;
      } else {
      }
      pci_read_config_word((struct pci_dev const *)root, bridge_pos + 16, & bridge_cfg);
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 16, & gpu_cfg);
      pci_read_config_word((struct pci_dev const *)root, bridge_pos + 48, & bridge_cfg2);
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 48, & gpu_cfg2);
      tmp = (*(adev->pcie_rreg))(adev, 268501174U);
      tmp = tmp | 8192U;
      (*(adev->pcie_wreg))(adev, 268501174U, tmp);
      tmp = (*(adev->pcie_rreg))(adev, 268501174U);
      tmp = tmp | 32U;
      (*(adev->pcie_wreg))(adev, 268501174U, tmp);
      __ms = 100UL;
      goto ldv_53342;
      ldv_53341:
      __const_udelay(4295000UL);
      ldv_53342:
      tmp___0 = __ms;
      __ms = __ms - 1UL;
      if (tmp___0 != 0UL) {
        goto ldv_53341;
      } else {
      }
      pci_read_config_word((struct pci_dev const *)root, bridge_pos + 16, & tmp16);
      tmp16 = (unsigned int )tmp16 & 65023U;
      tmp16 = (u16 )(((int )((short )bridge_cfg) & 512) | (int )((short )tmp16));
      pci_write_config_word((struct pci_dev const *)root, bridge_pos + 16, (int )tmp16);
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 16, & tmp16);
      tmp16 = (unsigned int )tmp16 & 65023U;
      tmp16 = (u16 )(((int )((short )gpu_cfg) & 512) | (int )((short )tmp16));
      pci_write_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 16, (int )tmp16);
      pci_read_config_word((struct pci_dev const *)root, bridge_pos + 48, & tmp16);
      tmp16 = (unsigned int )tmp16 & 61935U;
      tmp16 = (u16 )(((int )((short )bridge_cfg2) & 3600) | (int )((short )tmp16));
      pci_write_config_word((struct pci_dev const *)root, bridge_pos + 48, (int )tmp16);
      pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 48, & tmp16);
      tmp16 = (unsigned int )tmp16 & 61935U;
      tmp16 = (u16 )(((int )((short )gpu_cfg2) & 3600) | (int )((short )tmp16));
      pci_write_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 48, (int )tmp16);
      tmp = (*(adev->pcie_rreg))(adev, 268501174U);
      tmp = tmp & 4294959103U;
      (*(adev->pcie_wreg))(adev, 268501174U, tmp);
      i = i + 1;
      ldv_53345: ;
      if (i <= 9) {
        goto ldv_53344;
      } else {
      }
      ldv_53339: ;
    } else {
    }
  } else {
  }
  speed_cntl = speed_cntl | 288U;
  speed_cntl = speed_cntl & 4294967231U;
  (*(adev->pcie_wreg))(adev, 268501156U, speed_cntl);
  pci_read_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 48, & tmp16);
  tmp16 = (unsigned int )tmp16 & 65520U;
  if ((mask & 4U) != 0U) {
    tmp16 = (u16 )((unsigned int )tmp16 | 3U);
  } else
  if ((mask & 2U) != 0U) {
    tmp16 = (u16 )((unsigned int )tmp16 | 2U);
  } else {
    tmp16 = (u16 )((unsigned int )tmp16 | 1U);
  }
  pci_write_config_word((struct pci_dev const *)adev->pdev, gpu_pos + 48, (int )tmp16);
  speed_cntl = (*(adev->pcie_rreg))(adev, 268501156U);
  speed_cntl = speed_cntl | 512U;
  (*(adev->pcie_wreg))(adev, 268501156U, speed_cntl);
  i = 0;
  goto ldv_53348;
  ldv_53347:
  speed_cntl = (*(adev->pcie_rreg))(adev, 268501156U);
  if ((speed_cntl & 512U) == 0U) {
    goto ldv_53346;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_53348: ;
  if (adev->usec_timeout > i) {
    goto ldv_53347;
  } else {
  }
  ldv_53346: ;
  return;
}
}
static void cik_program_aspm(struct amdgpu_device *adev )
{
  u32 data ;
  u32 orig ;
  bool disable_l0s ;
  bool disable_l1 ;
  bool disable_plloff_in_l1 ;
  bool disable_clkreq ;
  bool clk_req_support ;
  struct pci_dev *root ;
  u32 lnkcap ;
  {
  disable_l0s = 0;
  disable_l1 = 0;
  disable_plloff_in_l1 = 0;
  disable_clkreq = 0;
  if (amdgpu_aspm == 0) {
    return;
  } else {
  }
  if ((adev->flags & 131072UL) != 0UL) {
    return;
  } else {
  }
  data = (*(adev->pcie_rreg))(adev, 268501155U);
  orig = data;
  data = data & 4294967040U;
  data = data | 292U;
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 268501155U, data);
  } else {
  }
  data = (*(adev->pcie_rreg))(adev, 268501173U);
  orig = data;
  data = data | 1073741824U;
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 268501173U, data);
  } else {
  }
  data = (*(adev->pcie_rreg))(adev, 20971584U);
  orig = data;
  data = data | 64U;
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 20971584U, data);
  } else {
  }
  data = (*(adev->pcie_rreg))(adev, 268501152U);
  orig = data;
  data = data & 4294902015U;
  data = data | 65536U;
  if (! disable_l0s) {
    data = data | 1792U;
  } else {
  }
  if (! disable_l1) {
    data = data | 28672U;
    data = data & 4294901759U;
    if (orig != data) {
      (*(adev->pcie_wreg))(adev, 268501152U, data);
    } else {
    }
    if (! disable_plloff_in_l1) {
      data = (*(adev->pcie_rreg))(adev, 17825810U);
      orig = data;
      data = data & 4294959231U;
      data = data | 8064U;
      if (orig != data) {
        (*(adev->pcie_wreg))(adev, 17825810U, data);
      } else {
      }
      data = (*(adev->pcie_rreg))(adev, 17825811U);
      orig = data;
      data = data & 4294959231U;
      data = data | 8064U;
      if (orig != data) {
        (*(adev->pcie_wreg))(adev, 17825811U, data);
      } else {
      }
      data = (*(adev->pcie_rreg))(adev, 34603026U);
      orig = data;
      data = data & 4294959231U;
      data = data | 8064U;
      if (orig != data) {
        (*(adev->pcie_wreg))(adev, 34603026U, data);
      } else {
      }
      data = (*(adev->pcie_rreg))(adev, 34603027U);
      orig = data;
      data = data & 4294959231U;
      data = data | 8064U;
      if (orig != data) {
        (*(adev->pcie_wreg))(adev, 34603027U, data);
      } else {
      }
      data = (*(adev->pcie_rreg))(adev, 268501154U);
      orig = data;
      data = data & 4288675839U;
      data = data | 4288675839U;
      if (orig != data) {
        (*(adev->pcie_wreg))(adev, 268501154U, data);
      } else {
      }
      if (! disable_clkreq) {
        root = ((adev->pdev)->bus)->self;
        clk_req_support = 0;
        pcie_capability_read_dword(root, 12, & lnkcap);
        if ((lnkcap & 262144U) != 0U) {
          clk_req_support = 1;
        } else {
        }
      } else {
        clk_req_support = 0;
      }
      if ((int )clk_req_support) {
        data = (*(adev->pcie_rreg))(adev, 268501169U);
        orig = data;
        data = data | 393216U;
        if (orig != data) {
          (*(adev->pcie_wreg))(adev, 268501169U, data);
        } else {
        }
        data = (*(adev->smc_rreg))(adev, 3226468776U);
        orig = data;
        data = data & 4294901760U;
        data = data | 257U;
        if (orig != data) {
          (*(adev->smc_wreg))(adev, 3226468776U, data);
        } else {
        }
        data = (*(adev->smc_rreg))(adev, 3226468780U);
        orig = data;
        data = data & 4294901760U;
        data = data | 257U;
        if (orig != data) {
          (*(adev->smc_wreg))(adev, 3226468780U, data);
        } else {
        }
        data = (*(adev->smc_rreg))(adev, 3226468768U);
        orig = data;
        data = data & 4294967291U;
        if (orig != data) {
          (*(adev->smc_wreg))(adev, 3226468768U, data);
        } else {
        }
        data = (*(adev->smc_rreg))(adev, 3226468772U);
        orig = data;
        data = data & 4294967287U;
        if (orig != data) {
          (*(adev->smc_wreg))(adev, 3226468772U, data);
        } else {
        }
        data = (*(adev->smc_rreg))(adev, 3226468764U);
        orig = data;
        data = data & 4294902015U;
        data = data | 1024U;
        if (orig != data) {
          (*(adev->smc_wreg))(adev, 3226468764U, data);
        } else {
        }
      } else {
      }
    } else {
    }
  } else
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 268501152U, data);
  } else {
  }
  data = (*(adev->pcie_rreg))(adev, 20971548U);
  orig = data;
  data = data | 851968U;
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 20971548U, data);
  } else {
  }
  if (! disable_l0s) {
    data = (*(adev->pcie_rreg))(adev, 268501155U);
    if ((data & 4278190080U) == 4278190080U) {
      data = (*(adev->pcie_rreg))(adev, 20971560U);
      if ((data & 2U) != 0U && (int )data & 1) {
        data = (*(adev->pcie_rreg))(adev, 268501152U);
        orig = data;
        data = data & 4294963455U;
        if (orig != data) {
          (*(adev->pcie_wreg))(adev, 268501152U, data);
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  return;
}
}
static u32 cik_get_rev_id(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 5465U, 0);
  return (tmp >> 28);
}
}
static struct amdgpu_ip_block_version const bonaire_ip_blocks[9U] =
  { {0, 1U, 0U, 0U, & cik_common_ip_funcs},
        {1, 7U, 0U, 0U, & gmc_v7_0_ip_funcs},
        {2, 2U, 0U, 0U, & cik_ih_ip_funcs},
        {3, 7U, 0U, 0U, & ci_dpm_ip_funcs},
        {4, 8U, 2U, 0U, & dce_v8_0_ip_funcs},
        {5, 7U, 2U, 0U, & gfx_v7_0_ip_funcs},
        {6, 2U, 0U, 0U, & cik_sdma_ip_funcs},
        {7, 4U, 2U, 0U, & uvd_v4_2_ip_funcs},
        {8, 2U, 0U, 0U, & vce_v2_0_ip_funcs}};
static struct amdgpu_ip_block_version const hawaii_ip_blocks[9U] =
  { {0, 1U, 0U, 0U, & cik_common_ip_funcs},
        {1, 7U, 0U, 0U, & gmc_v7_0_ip_funcs},
        {2, 2U, 0U, 0U, & cik_ih_ip_funcs},
        {3, 7U, 0U, 0U, & ci_dpm_ip_funcs},
        {4, 8U, 5U, 0U, & dce_v8_0_ip_funcs},
        {5, 7U, 3U, 0U, & gfx_v7_0_ip_funcs},
        {6, 2U, 0U, 0U, & cik_sdma_ip_funcs},
        {7, 4U, 2U, 0U, & uvd_v4_2_ip_funcs},
        {8, 2U, 0U, 0U, & vce_v2_0_ip_funcs}};
static struct amdgpu_ip_block_version const kabini_ip_blocks[9U] =
  { {0, 1U, 0U, 0U, & cik_common_ip_funcs},
        {1, 7U, 0U, 0U, & gmc_v7_0_ip_funcs},
        {2, 2U, 0U, 0U, & cik_ih_ip_funcs},
        {3, 7U, 0U, 0U, & kv_dpm_ip_funcs},
        {4, 8U, 3U, 0U, & dce_v8_0_ip_funcs},
        {5, 7U, 2U, 0U, & gfx_v7_0_ip_funcs},
        {6, 2U, 0U, 0U, & cik_sdma_ip_funcs},
        {7, 4U, 2U, 0U, & uvd_v4_2_ip_funcs},
        {8, 2U, 0U, 0U, & vce_v2_0_ip_funcs}};
static struct amdgpu_ip_block_version const mullins_ip_blocks[9U] =
  { {0, 1U, 0U, 0U, & cik_common_ip_funcs},
        {1, 7U, 0U, 0U, & gmc_v7_0_ip_funcs},
        {2, 2U, 0U, 0U, & cik_ih_ip_funcs},
        {3, 7U, 0U, 0U, & kv_dpm_ip_funcs},
        {4, 8U, 3U, 0U, & dce_v8_0_ip_funcs},
        {5, 7U, 2U, 0U, & gfx_v7_0_ip_funcs},
        {6, 2U, 0U, 0U, & cik_sdma_ip_funcs},
        {7, 4U, 2U, 0U, & uvd_v4_2_ip_funcs},
        {8, 2U, 0U, 0U, & vce_v2_0_ip_funcs}};
static struct amdgpu_ip_block_version const kaveri_ip_blocks[9U] =
  { {0, 1U, 0U, 0U, & cik_common_ip_funcs},
        {1, 7U, 0U, 0U, & gmc_v7_0_ip_funcs},
        {2, 2U, 0U, 0U, & cik_ih_ip_funcs},
        {3, 7U, 0U, 0U, & kv_dpm_ip_funcs},
        {4, 8U, 1U, 0U, & dce_v8_0_ip_funcs},
        {5, 7U, 1U, 0U, & gfx_v7_0_ip_funcs},
        {6, 2U, 0U, 0U, & cik_sdma_ip_funcs},
        {7, 4U, 2U, 0U, & uvd_v4_2_ip_funcs},
        {8, 2U, 0U, 0U, & vce_v2_0_ip_funcs}};
int cik_set_ip_blocks(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& bonaire_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_53375;
  case 3U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& hawaii_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_53375;
  case 1U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& kaveri_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_53375;
  case 2U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& kabini_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_53375;
  case 4U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& mullins_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_53375;
  default: ;
  return (-22);
  }
  ldv_53375: ;
  return (0);
}
}
static struct amdgpu_asic_funcs const cik_asic_funcs =
     {& cik_read_disabled_bios, & cik_read_register, & cik_vga_set_state, & cik_asic_reset,
    & gmc_v7_0_mc_wait_for_idle, & cik_get_xclk, & gfx_v7_0_get_gpu_clock_counter,
    & gfx_v7_0_get_cu_info, & cik_set_uvd_clocks, & cik_set_vce_clocks};
static int cik_common_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->smc_rreg = & cik_smc_rreg;
  adev->smc_wreg = & cik_smc_wreg;
  adev->pcie_rreg = & cik_pcie_rreg;
  adev->pcie_wreg = & cik_pcie_wreg;
  adev->uvd_ctx_rreg = & cik_uvd_ctx_rreg;
  adev->uvd_ctx_wreg = & cik_uvd_ctx_wreg;
  adev->didt_rreg = & cik_didt_rreg;
  adev->didt_wreg = & cik_didt_wreg;
  adev->asic_funcs = & cik_asic_funcs;
  adev->has_uvd = 1;
  adev->rev_id = cik_get_rev_id(adev);
  adev->external_rev_id = 255U;
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  adev->cg_flags = 130939U;
  adev->pg_flags = 0U;
  adev->external_rev_id = adev->rev_id + 20U;
  goto ldv_53395;
  case 3U:
  adev->cg_flags = 130907U;
  adev->pg_flags = 0U;
  adev->external_rev_id = 40U;
  goto ldv_53395;
  case 1U:
  adev->cg_flags = 130171U;
  adev->pg_flags = 8U;
  if (((unsigned int )(adev->pdev)->device == 4882U || (unsigned int )(adev->pdev)->device == 4886U) || (unsigned int )(adev->pdev)->device == 4887U) {
    adev->external_rev_id = 65U;
  } else {
    adev->external_rev_id = 1U;
  }
  goto ldv_53395;
  case 2U: ;
  case 4U:
  adev->cg_flags = 130171U;
  adev->pg_flags = 8U;
  if ((unsigned int )adev->asic_type == 2U) {
    if (adev->rev_id == 0U) {
      adev->external_rev_id = 129U;
    } else
    if (adev->rev_id == 1U) {
      adev->external_rev_id = 130U;
    } else
    if (adev->rev_id == 2U) {
      adev->external_rev_id = 133U;
    } else {
    }
  } else {
    adev->external_rev_id = adev->rev_id + 161U;
  }
  goto ldv_53395;
  default: ;
  return (-22);
  }
  ldv_53395: ;
  return (0);
}
}
static int cik_common_sw_init(void *handle )
{
  {
  return (0);
}
}
static int cik_common_sw_fini(void *handle )
{
  {
  return (0);
}
}
static int cik_common_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cik_init_golden_registers(adev);
  cik_pcie_gen3_enable(adev);
  cik_program_aspm(adev);
  return (0);
}
}
static int cik_common_hw_fini(void *handle )
{
  {
  return (0);
}
}
static int cik_common_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_common_hw_fini((void *)adev);
  return (tmp);
}
}
static int cik_common_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_common_hw_init((void *)adev);
  return (tmp);
}
}
static bool cik_common_is_idle(void *handle )
{
  {
  return (1);
}
}
static int cik_common_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void cik_common_print_status(void *handle )
{
  {
  return;
}
}
static int cik_common_soft_reset(void *handle )
{
  {
  return (0);
}
}
static int cik_common_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int cik_common_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const cik_common_ip_funcs =
     {& cik_common_early_init, (int (*)(void * ))0, & cik_common_sw_init, & cik_common_sw_fini,
    & cik_common_hw_init, & cik_common_hw_fini, & cik_common_suspend, & cik_common_resume,
    & cik_common_is_idle, & cik_common_wait_for_idle, & cik_common_soft_reset, & cik_common_print_status,
    & cik_common_set_clockgating_state, & cik_common_set_powergating_state};
int ldv_retval_73 ;
extern int ldv_probe_121(void) ;
extern int ldv_release_120(void) ;
extern int ldv_release_121(void) ;
extern int ldv_probe_120(void) ;
int ldv_retval_72 ;
void ldv_initialize_amdgpu_asic_funcs_121(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  cik_asic_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_120(void)
{
  void *ldvarg918 ;
  void *tmp ;
  enum amd_powergating_state ldvarg923 ;
  void *ldvarg920 ;
  void *tmp___0 ;
  enum amd_clockgating_state ldvarg919 ;
  void *ldvarg916 ;
  void *tmp___1 ;
  void *ldvarg926 ;
  void *tmp___2 ;
  void *ldvarg928 ;
  void *tmp___3 ;
  void *ldvarg921 ;
  void *tmp___4 ;
  void *ldvarg922 ;
  void *tmp___5 ;
  void *ldvarg927 ;
  void *tmp___6 ;
  void *ldvarg925 ;
  void *tmp___7 ;
  void *ldvarg915 ;
  void *tmp___8 ;
  void *ldvarg917 ;
  void *tmp___9 ;
  void *ldvarg924 ;
  void *tmp___10 ;
  void *ldvarg929 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg918 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg920 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg916 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg926 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg928 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg921 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg922 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg927 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg925 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg915 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg917 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg924 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg929 = tmp___11;
  ldv_memset((void *)(& ldvarg923), 0, 4UL);
  ldv_memset((void *)(& ldvarg919), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_hw_fini(ldvarg929);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_hw_fini(ldvarg929);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_hw_fini(ldvarg929);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 1: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_print_status(ldvarg928);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_print_status(ldvarg928);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_print_status(ldvarg928);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 2: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_early_init(ldvarg927);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_early_init(ldvarg927);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_early_init(ldvarg927);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 3: ;
  if (ldv_state_variable_120 == 2) {
    ldv_retval_73 = cik_common_suspend(ldvarg926);
    if (ldv_retval_73 == 0) {
      ldv_state_variable_120 = 3;
    } else {
    }
  } else {
  }
  goto ldv_53475;
  case 4: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_sw_init(ldvarg925);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_sw_init(ldvarg925);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_sw_init(ldvarg925);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 5: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_set_powergating_state(ldvarg924, ldvarg923);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_set_powergating_state(ldvarg924, ldvarg923);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_set_powergating_state(ldvarg924, ldvarg923);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 6: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_wait_for_idle(ldvarg922);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_wait_for_idle(ldvarg922);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_wait_for_idle(ldvarg922);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 7: ;
  if (ldv_state_variable_120 == 3) {
    ldv_retval_72 = cik_common_resume(ldvarg921);
    if (ldv_retval_72 == 0) {
      ldv_state_variable_120 = 2;
    } else {
    }
  } else {
  }
  goto ldv_53475;
  case 8: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_set_clockgating_state(ldvarg920, ldvarg919);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_set_clockgating_state(ldvarg920, ldvarg919);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_set_clockgating_state(ldvarg920, ldvarg919);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 9: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_hw_init(ldvarg918);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_hw_init(ldvarg918);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_hw_init(ldvarg918);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 10: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_soft_reset(ldvarg917);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_soft_reset(ldvarg917);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_soft_reset(ldvarg917);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 11: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_sw_fini(ldvarg916);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_sw_fini(ldvarg916);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_sw_fini(ldvarg916);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 12: ;
  if (ldv_state_variable_120 == 2) {
    cik_common_is_idle(ldvarg915);
    ldv_state_variable_120 = 2;
  } else {
  }
  if (ldv_state_variable_120 == 1) {
    cik_common_is_idle(ldvarg915);
    ldv_state_variable_120 = 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    cik_common_is_idle(ldvarg915);
    ldv_state_variable_120 = 3;
  } else {
  }
  goto ldv_53475;
  case 13: ;
  if (ldv_state_variable_120 == 2) {
    ldv_release_120();
    ldv_state_variable_120 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_120 == 3) {
    ldv_release_120();
    ldv_state_variable_120 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_53475;
  case 14: ;
  if (ldv_state_variable_120 == 1) {
    ldv_probe_120();
    ldv_state_variable_120 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_53475;
  default:
  ldv_stop();
  }
  ldv_53475: ;
  return;
}
}
void ldv_main_exported_121(void)
{
  u32 ldvarg402 ;
  u32 ldvarg406 ;
  u32 ldvarg403 ;
  u32 ldvarg408 ;
  u32 ldvarg409 ;
  u32 ldvarg405 ;
  struct amdgpu_cu_info *ldvarg400 ;
  void *tmp ;
  u32 ldvarg404 ;
  u32 *ldvarg407 ;
  void *tmp___0 ;
  bool ldvarg401 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(72UL);
  ldvarg400 = (struct amdgpu_cu_info *)tmp;
  tmp___0 = ldv_init_zalloc(4UL);
  ldvarg407 = (u32 *)tmp___0;
  ldv_memset((void *)(& ldvarg402), 0, 4UL);
  ldv_memset((void *)(& ldvarg406), 0, 4UL);
  ldv_memset((void *)(& ldvarg403), 0, 4UL);
  ldv_memset((void *)(& ldvarg408), 0, 4UL);
  ldv_memset((void *)(& ldvarg409), 0, 4UL);
  ldv_memset((void *)(& ldvarg405), 0, 4UL);
  ldv_memset((void *)(& ldvarg404), 0, 4UL);
  ldv_memset((void *)(& ldvarg401), 0, 1UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_121 == 1) {
    gfx_v7_0_get_gpu_clock_counter(cik_asic_funcs_group0);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    gfx_v7_0_get_gpu_clock_counter(cik_asic_funcs_group0);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 1: ;
  if (ldv_state_variable_121 == 1) {
    cik_asic_reset(cik_asic_funcs_group0);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_asic_reset(cik_asic_funcs_group0);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 2: ;
  if (ldv_state_variable_121 == 2) {
    cik_read_disabled_bios(cik_asic_funcs_group0);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 3: ;
  if (ldv_state_variable_121 == 1) {
    gmc_v7_0_mc_wait_for_idle(cik_asic_funcs_group0);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    gmc_v7_0_mc_wait_for_idle(cik_asic_funcs_group0);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 4: ;
  if (ldv_state_variable_121 == 1) {
    cik_set_uvd_clocks(cik_asic_funcs_group0, ldvarg409, ldvarg408);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_set_uvd_clocks(cik_asic_funcs_group0, ldvarg409, ldvarg408);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 5: ;
  if (ldv_state_variable_121 == 1) {
    cik_read_register(cik_asic_funcs_group0, ldvarg405, ldvarg404, ldvarg406, ldvarg407);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_read_register(cik_asic_funcs_group0, ldvarg405, ldvarg404, ldvarg406, ldvarg407);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 6: ;
  if (ldv_state_variable_121 == 1) {
    cik_set_vce_clocks(cik_asic_funcs_group0, ldvarg403, ldvarg402);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_set_vce_clocks(cik_asic_funcs_group0, ldvarg403, ldvarg402);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 7: ;
  if (ldv_state_variable_121 == 1) {
    cik_vga_set_state(cik_asic_funcs_group0, (int )ldvarg401);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_vga_set_state(cik_asic_funcs_group0, (int )ldvarg401);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 8: ;
  if (ldv_state_variable_121 == 1) {
    gfx_v7_0_get_cu_info(cik_asic_funcs_group0, ldvarg400);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    gfx_v7_0_get_cu_info(cik_asic_funcs_group0, ldvarg400);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 9: ;
  if (ldv_state_variable_121 == 1) {
    cik_get_xclk(cik_asic_funcs_group0);
    ldv_state_variable_121 = 1;
  } else {
  }
  if (ldv_state_variable_121 == 2) {
    cik_get_xclk(cik_asic_funcs_group0);
    ldv_state_variable_121 = 2;
  } else {
  }
  goto ldv_53505;
  case 10: ;
  if (ldv_state_variable_121 == 2) {
    ldv_release_121();
    ldv_state_variable_121 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_53505;
  case 11: ;
  if (ldv_state_variable_121 == 1) {
    ldv_probe_121();
    ldv_state_variable_121 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_53505;
  default:
  ldv_stop();
  }
  ldv_53505: ;
  return;
}
}
bool ldv_queue_work_on_523(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_524(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_525(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_526(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_527(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_537(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_539(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_538(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_541(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_540(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static void writeq(unsigned long val , void volatile *addr )
{
  {
  __asm__ volatile ("movq %0,%1": : "r" (val), "m" (*((unsigned long volatile *)addr)): "memory");
  return;
}
}
extern int request_firmware(struct firmware const ** , char const * , struct device * ) ;
extern void release_firmware(struct firmware const * ) ;
extern int dma_supported(struct device * , u64 ) ;
extern int dma_set_mask(struct device * , u64 ) ;
__inline static int dma_set_coherent_mask(struct device *dev , u64 mask )
{
  int tmp ;
  {
  tmp = dma_supported(dev, mask);
  if (tmp == 0) {
    return (-5);
  } else {
  }
  dev->coherent_dma_mask = mask;
  return (0);
}
}
__inline static int pci_set_dma_mask(struct pci_dev *dev , u64 mask )
{
  int tmp ;
  {
  tmp = dma_set_mask(& dev->dev, mask);
  return (tmp);
}
}
__inline static int pci_set_consistent_dma_mask(struct pci_dev *dev , u64 mask )
{
  int tmp ;
  {
  tmp = dma_set_coherent_mask(& dev->dev, mask);
  return (tmp);
}
}
int amdgpu_irq_add_id(struct amdgpu_device *adev , unsigned int src_id , struct amdgpu_irq_src *source ) ;
static void gmc_v7_0_set_gart_funcs(struct amdgpu_device *adev ) ;
static void gmc_v7_0_set_irq_funcs(struct amdgpu_device *adev ) ;
int gmc_v7_0_mc_wait_for_idle(struct amdgpu_device *adev )
{
  unsigned int i ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  i = 0U;
  goto ldv_43701;
  ldv_43700:
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0 & 7936U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43701: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43700;
  } else {
  }
  return (-1);
}
}
void gmc_v7_0_mc_stop(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 blackout ;
  {
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->stop_mc_access))(adev, save);
  } else {
  }
  (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  blackout = amdgpu_mm_rreg(adev, 2091U, 0);
  if ((blackout & 7U) != 1U) {
    amdgpu_mm_wreg(adev, 5412U, 0U, 0);
    blackout = blackout & 4294967288U;
    amdgpu_mm_wreg(adev, 2091U, blackout | 1U, 0);
  } else {
  }
  __const_udelay(429500UL);
  return;
}
}
void gmc_v7_0_mc_resume(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 2091U, 0);
  tmp = tmp & 4294967288U;
  amdgpu_mm_wreg(adev, 2091U, tmp, 0);
  tmp = 1U;
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, 5412U, tmp, 0);
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->resume_mc_access))(adev, save);
  } else {
  }
  return;
}
}
static int gmc_v7_0_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gmc_v7_0_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  chip_name = "bonaire";
  goto ldv_43721;
  case 3U:
  chip_name = "hawaii";
  goto ldv_43721;
  case 1U: ;
  case 2U: ;
  return (0);
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c"),
                       "i" (138), "i" (12UL));
  ldv_43726: ;
  goto ldv_43726;
  }
  ldv_43721:
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_mc.bin", chip_name);
  err = request_firmware(& adev->mc.fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->mc.fw);
  out: ;
  if (err != 0) {
    printk("\vcik_mc: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    release_firmware(adev->mc.fw);
    adev->mc.fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static int gmc_v7_0_mc_load_microcode(struct amdgpu_device *adev )
{
  struct mc_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  __le32 const *io_mc_regs ;
  u32 running ;
  u32 blackout ;
  int i ;
  int ucode_size ;
  int regs_size ;
  u32 tmp ;
  __le32 const *tmp___0 ;
  __u32 tmp___1 ;
  __le32 const *tmp___2 ;
  __u32 tmp___3 ;
  __le32 const *tmp___4 ;
  __u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  {
  fw_data = (__le32 const *)0U;
  io_mc_regs = (__le32 const *)0U;
  blackout = 0U;
  if ((unsigned long )adev->mc.fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct mc_firmware_header_v1_0 const *)(adev->mc.fw)->data;
  amdgpu_ucode_print_mc_hdr(& hdr->header);
  adev->mc.fw_version = hdr->header.ucode_version;
  regs_size = (int )((unsigned int )hdr->io_debug_size_bytes / 8U);
  io_mc_regs = (__le32 const *)(adev->mc.fw)->data + (unsigned long )hdr->io_debug_array_offset_bytes;
  ucode_size = (int )((unsigned int )hdr->header.ucode_size_bytes / 4U);
  fw_data = (__le32 const *)(adev->mc.fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  tmp = amdgpu_mm_rreg(adev, 2610U, 0);
  running = tmp & 1U;
  if (running == 0U) {
    if (running != 0U) {
      blackout = amdgpu_mm_rreg(adev, 2091U, 0);
      amdgpu_mm_wreg(adev, 2091U, blackout | 1U, 0);
    } else {
    }
    amdgpu_mm_wreg(adev, 2610U, 8U, 0);
    amdgpu_mm_wreg(adev, 2610U, 16U, 0);
    i = 0;
    goto ldv_43740;
    ldv_43739:
    tmp___0 = io_mc_regs;
    io_mc_regs = io_mc_regs + 1;
    tmp___1 = __le32_to_cpup(tmp___0);
    amdgpu_mm_wreg(adev, 2705U, tmp___1, 0);
    tmp___2 = io_mc_regs;
    io_mc_regs = io_mc_regs + 1;
    tmp___3 = __le32_to_cpup(tmp___2);
    amdgpu_mm_wreg(adev, 2706U, tmp___3, 0);
    i = i + 1;
    ldv_43740: ;
    if (i < regs_size) {
      goto ldv_43739;
    } else {
    }
    i = 0;
    goto ldv_43743;
    ldv_43742:
    tmp___4 = fw_data;
    fw_data = fw_data + 1;
    tmp___5 = __le32_to_cpup(tmp___4);
    amdgpu_mm_wreg(adev, 2611U, tmp___5, 0);
    i = i + 1;
    ldv_43743: ;
    if (i < ucode_size) {
      goto ldv_43742;
    } else {
    }
    amdgpu_mm_wreg(adev, 2610U, 8U, 0);
    amdgpu_mm_wreg(adev, 2610U, 4U, 0);
    amdgpu_mm_wreg(adev, 2610U, 1U, 0);
    i = 0;
    goto ldv_43747;
    ldv_43746:
    tmp___6 = amdgpu_mm_rreg(adev, 2618U, 0);
    if ((tmp___6 & 1073741824U) >> 30 != 0U) {
      goto ldv_43745;
    } else {
    }
    __const_udelay(4295UL);
    i = i + 1;
    ldv_43747: ;
    if (adev->usec_timeout > i) {
      goto ldv_43746;
    } else {
    }
    ldv_43745:
    i = 0;
    goto ldv_43750;
    ldv_43749:
    tmp___7 = amdgpu_mm_rreg(adev, 2618U, 0);
    if ((int )tmp___7 < 0) {
      goto ldv_43748;
    } else {
    }
    __const_udelay(4295UL);
    i = i + 1;
    ldv_43750: ;
    if (adev->usec_timeout > i) {
      goto ldv_43749;
    } else {
    }
    ldv_43748: ;
    if (running != 0U) {
      amdgpu_mm_wreg(adev, 2091U, blackout, 0);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void gmc_v7_0_vram_gtt_location(struct amdgpu_device *adev , struct amdgpu_mc *mc )
{
  {
  if (mc->mc_vram_size > 1098437885952ULL) {
    dev_warn((struct device const *)adev->dev, "limiting VRAM\n");
    mc->real_vram_size = 1098437885952ULL;
    mc->mc_vram_size = 1098437885952ULL;
  } else {
  }
  amdgpu_vram_location(adev, & adev->mc, 0ULL);
  adev->mc.gtt_base_align = 0ULL;
  amdgpu_gtt_location(adev, mc);
  return;
}
}
static void gmc_v7_0_mc_program(struct amdgpu_device *adev )
{
  struct amdgpu_mode_mc_save save ;
  u32 tmp ;
  int i ;
  int j ;
  int tmp___0 ;
  int tmp___1 ;
  {
  i = 0;
  j = 0;
  goto ldv_43763;
  ldv_43762:
  amdgpu_mm_wreg(adev, (u32 )(j + 2821), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2822), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2823), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2824), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2825), 0U, 0);
  i = i + 1;
  j = j + 6;
  ldv_43763: ;
  if (i <= 31) {
    goto ldv_43762;
  } else {
  }
  amdgpu_mm_wreg(adev, 5416U, 0U, 0);
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->set_vga_render_state))(adev, 0);
  } else {
  }
  gmc_v7_0_mc_stop(adev, & save);
  tmp___0 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___0 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  amdgpu_mm_wreg(adev, 2061U, (u32 )(adev->mc.vram_start >> 12), 0);
  amdgpu_mm_wreg(adev, 2062U, (u32 )(adev->mc.vram_end >> 12), 0);
  amdgpu_mm_wreg(adev, 2063U, (u32 )(adev->vram_scratch.gpu_addr >> 12), 0);
  tmp = (u32 )(adev->mc.vram_end >> 24) << 16U;
  tmp = ((u32 )(adev->mc.vram_start >> 24) & 65535U) | tmp;
  amdgpu_mm_wreg(adev, 2057U, tmp, 0);
  amdgpu_mm_wreg(adev, 2817U, (u32 )(adev->mc.vram_start >> 8), 0);
  amdgpu_mm_wreg(adev, 2818U, 1073742080U, 0);
  amdgpu_mm_wreg(adev, 2819U, 1073741823U, 0);
  amdgpu_mm_wreg(adev, 2060U, 0U, 0);
  amdgpu_mm_wreg(adev, 2058U, 268435455U, 0);
  amdgpu_mm_wreg(adev, 2059U, 268435455U, 0);
  tmp___1 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___1 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  gmc_v7_0_mc_resume(adev, & save);
  amdgpu_mm_wreg(adev, 5412U, 3U, 0);
  tmp = amdgpu_mm_rreg(adev, 3027U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 3027U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 2816U, 0);
  amdgpu_mm_wreg(adev, 2816U, tmp, 0);
  return;
}
}
static int gmc_v7_0_mc_init(struct amdgpu_device *adev )
{
  u32 tmp ;
  int chansize ;
  int numchan ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  unsigned long long _max1 ;
  u64 _max2 ;
  {
  tmp = amdgpu_mm_rreg(adev, 2520U, 0);
  if ((tmp & 256U) >> 8 != 0U) {
    chansize = 64;
  } else {
    chansize = 32;
  }
  tmp = amdgpu_mm_rreg(adev, 2049U, 0);
  switch ((tmp & 61440U) >> 12) {
  case 0U: ;
  default:
  numchan = 1;
  goto ldv_43773;
  case 1U:
  numchan = 2;
  goto ldv_43773;
  case 2U:
  numchan = 4;
  goto ldv_43773;
  case 3U:
  numchan = 8;
  goto ldv_43773;
  case 4U:
  numchan = 3;
  goto ldv_43773;
  case 5U:
  numchan = 6;
  goto ldv_43773;
  case 6U:
  numchan = 10;
  goto ldv_43773;
  case 7U:
  numchan = 12;
  goto ldv_43773;
  case 8U:
  numchan = 16;
  goto ldv_43773;
  }
  ldv_43773:
  adev->mc.vram_width = (unsigned int )(numchan * chansize);
  adev->mc.aper_base = (adev->pdev)->resource[0].start;
  adev->mc.aper_size = (adev->pdev)->resource[0].start != 0ULL || (adev->pdev)->resource[0].end != (adev->pdev)->resource[0].start ? ((adev->pdev)->resource[0].end - (adev->pdev)->resource[0].start) + 1ULL : 0ULL;
  tmp___0 = amdgpu_mm_rreg(adev, 5386U, 0);
  adev->mc.mc_vram_size = (unsigned long long )tmp___0 * 1048576ULL;
  tmp___1 = amdgpu_mm_rreg(adev, 5386U, 0);
  adev->mc.real_vram_size = (unsigned long long )tmp___1 * 1048576ULL;
  adev->mc.visible_vram_size = adev->mc.aper_size;
  if (amdgpu_gart_size == -1) {
    _max1 = 1073741824ULL;
    _max2 = adev->mc.mc_vram_size;
    adev->mc.gtt_size = _max1 > _max2 ? _max1 : _max2;
  } else {
    adev->mc.gtt_size = (unsigned long long )amdgpu_gart_size << 20;
  }
  gmc_v7_0_vram_gtt_location(adev, & adev->mc);
  return (0);
}
}
static void gmc_v7_0_gart_flush_gpu_tlb(struct amdgpu_device *adev , u32 vmid )
{
  {
  amdgpu_mm_wreg(adev, 5408U, 0U, 0);
  amdgpu_mm_wreg(adev, 1310U, (u32 )(1 << (int )vmid), 0);
  return;
}
}
static int gmc_v7_0_gart_set_pte_pde(struct amdgpu_device *adev , void *cpu_pt_addr ,
                                     u32 gpu_page_idx , uint64_t addr , u32 flags )
{
  void *ptr ;
  uint64_t value ;
  {
  ptr = cpu_pt_addr;
  value = addr & 0xfffffffffffff000ULL;
  value = (uint64_t )flags | value;
  writeq((unsigned long )value, (void volatile *)ptr + (unsigned long )(gpu_page_idx * 8U));
  return (0);
}
}
static int gmc_v7_0_gart_enable(struct amdgpu_device *adev )
{
  int r ;
  int i ;
  u32 tmp ;
  {
  if ((unsigned long )adev->gart.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    dev_err((struct device const *)adev->dev, "No VRAM object for PCIE GART.\n");
    return (-22);
  } else {
  }
  r = amdgpu_gart_table_vram_pin(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 2073U, 0);
  tmp = tmp | 1U;
  tmp = tmp | 2U;
  tmp = tmp | 24U;
  tmp = tmp | 64U;
  tmp = tmp & 4294967263U;
  amdgpu_mm_wreg(adev, 2073U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1280U, 0);
  tmp = tmp | 1U;
  tmp = tmp | 2U;
  tmp = tmp | 512U;
  tmp = tmp | 1024U;
  tmp = tmp | 229376U;
  tmp = (tmp & 4293394431U) | 524288U;
  amdgpu_mm_wreg(adev, 1280U, tmp, 0);
  tmp = 1U;
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, 1281U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1282U, 0);
  tmp = tmp | 1048576U;
  tmp = (tmp & 4294967232U) | 4U;
  tmp = (tmp & 4293951487U) | 131072U;
  amdgpu_mm_wreg(adev, 1282U, tmp, 0);
  amdgpu_mm_wreg(adev, 1367U, (u32 )(adev->mc.gtt_start >> 12), 0);
  amdgpu_mm_wreg(adev, 1375U, (u32 )(adev->mc.gtt_end >> 12) - 1U, 0);
  amdgpu_mm_wreg(adev, 1359U, (u32 )(adev->gart.table_addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1350U, (unsigned int )(adev->dummy_page.addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1292U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = tmp | 1U;
  tmp = tmp & 4294967289U;
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  amdgpu_mm_wreg(adev, 1397U, 0U, 0);
  amdgpu_mm_wreg(adev, 1398U, 0U, 0);
  amdgpu_mm_wreg(adev, 1399U, 0U, 0);
  amdgpu_mm_wreg(adev, 1368U, 0U, 0);
  amdgpu_mm_wreg(adev, 1376U, adev->vm_manager.max_pfn - 1U, 0);
  i = 1;
  goto ldv_43805;
  ldv_43804: ;
  if (i <= 7) {
    amdgpu_mm_wreg(adev, (u32 )(i + 1359), (u32 )(adev->gart.table_addr >> 12), 0);
  } else {
    amdgpu_mm_wreg(adev, (u32 )(i + 1286), (u32 )(adev->gart.table_addr >> 12), 0);
  }
  i = i + 1;
  ldv_43805: ;
  if (i <= 15) {
    goto ldv_43804;
  } else {
  }
  amdgpu_mm_wreg(adev, 1351U, (unsigned int )(adev->dummy_page.addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1293U, 4U, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = tmp | 1U;
  tmp = (tmp & 4294967289U) | 2U;
  tmp = tmp | 8U;
  tmp = tmp | 16U;
  tmp = tmp | 64U;
  tmp = tmp | 128U;
  tmp = tmp | 512U;
  tmp = tmp | 1024U;
  tmp = tmp | 4096U;
  tmp = tmp | 8192U;
  tmp = tmp | 32768U;
  tmp = tmp | 65536U;
  tmp = tmp | 262144U;
  tmp = tmp | 524288U;
  tmp = (tmp & 4043309055U) | ((u32 )((amdgpu_vm_block_size + -9) << 24) & 251658240U);
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  if ((unsigned int )adev->asic_type == 1U) {
    tmp = amdgpu_mm_rreg(adev, 1561U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 1561U, tmp, 0);
  } else {
  }
  gmc_v7_0_gart_flush_gpu_tlb(adev, 0U);
  printk("\016[drm] PCIE GART of %uM enabled (table at 0x%016llX).\n", (unsigned int )(adev->mc.gtt_size >> 20),
         adev->gart.table_addr);
  adev->gart.ready = 1;
  return (0);
}
}
static int gmc_v7_0_gart_init(struct amdgpu_device *adev )
{
  int r ;
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )adev->gart.robj != (unsigned long )((struct amdgpu_bo *)0)) {
    __ret_warn_on = 1;
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gmc_v7_0.c",
                        561, "R600 PCIE GART already initialized\n");
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
    return (0);
  } else {
  }
  r = amdgpu_gart_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gart.table_size = adev->gart.num_gpu_pages * 8U;
  tmp___0 = amdgpu_gart_table_vram_alloc(adev);
  return (tmp___0);
}
}
static void gmc_v7_0_gart_disable(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  amdgpu_mm_wreg(adev, 1284U, 0U, 0);
  amdgpu_mm_wreg(adev, 1285U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 2073U, 0);
  tmp = tmp & 4294967294U;
  tmp = tmp & 4294967293U;
  tmp = tmp & 4294967231U;
  amdgpu_mm_wreg(adev, 2073U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1280U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, 1280U, tmp, 0);
  amdgpu_mm_wreg(adev, 1281U, 0U, 0);
  amdgpu_gart_table_vram_unpin(adev);
  return;
}
}
static void gmc_v7_0_gart_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_gart_table_vram_free(adev);
  amdgpu_gart_fini(adev);
  return;
}
}
static int gmc_v7_0_vm_init(struct amdgpu_device *adev )
{
  u64 tmp ;
  u32 tmp___0 ;
  {
  adev->vm_manager.nvm = 8U;
  if ((adev->flags & 131072UL) != 0UL) {
    tmp___0 = amdgpu_mm_rreg(adev, 2074U, 0);
    tmp = (u64 )tmp___0;
    tmp = tmp << 22;
    adev->vm_manager.vram_base_offset = tmp;
  } else {
    adev->vm_manager.vram_base_offset = 0ULL;
  }
  return (0);
}
}
static void gmc_v7_0_vm_fini(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void gmc_v7_0_vm_decode_fault(struct amdgpu_device *adev , u32 status , u32 addr ,
                                     u32 mc_client )
{
  u32 mc_id ;
  u32 vmid ;
  u32 protections ;
  char block[5U] ;
  {
  vmid = (status & 503316480U) >> 25;
  protections = status & 255U;
  block[0] = (char )(mc_client >> 24);
  block[1] = (char )(mc_client >> 16);
  block[2] = (char )(mc_client >> 8);
  block[3] = (char )mc_client;
  block[4] = 0;
  mc_id = (status & 2093056U) >> 12;
  printk("VM fault (0x%02x, vmid %d) at page %u, %s from \'%s\' (0x%08x) (%d)\n",
         protections, vmid, addr, (status & 16777216U) >> 24 != 0U ? (char *)"write" : (char *)"read",
         (char *)(& block), mc_client, mc_id);
  return;
}
}
static u32 const mc_cg_registers[9U] =
  { 2094U, 2096U, 2095U, 2334U,
        3284U, 2451U, 2450U, 2452U,
        1392U};
static u32 const mc_cg_ls_en[9U] =
  { 524288U, 524288U, 524288U, 524288U,
        524288U, 524288U, 524288U, 524288U,
        524288U};
static u32 const mc_cg_en[9U] =
  { 262144U, 262144U, 262144U, 262144U,
        262144U, 262144U, 262144U, 262144U,
        262144U};
static void gmc_v7_0_enable_mc_ls(struct amdgpu_device *adev , bool enable )
{
  int i ;
  u32 orig ;
  u32 data ;
  {
  i = 0;
  goto ldv_43850;
  ldv_43849:
  data = amdgpu_mm_rreg(adev, mc_cg_registers[i], 0);
  orig = data;
  if ((int )enable && (adev->cg_flags & 256U) != 0U) {
    data = (u32 )mc_cg_ls_en[i] | data;
  } else {
    data = (u32 )(~ mc_cg_ls_en[i]) & data;
  }
  if (data != orig) {
    amdgpu_mm_wreg(adev, mc_cg_registers[i], data, 0);
  } else {
  }
  i = i + 1;
  ldv_43850: ;
  if ((unsigned int )i <= 8U) {
    goto ldv_43849;
  } else {
  }
  return;
}
}
static void gmc_v7_0_enable_mc_mgcg(struct amdgpu_device *adev , bool enable )
{
  int i ;
  u32 orig ;
  u32 data ;
  {
  i = 0;
  goto ldv_43862;
  ldv_43861:
  data = amdgpu_mm_rreg(adev, mc_cg_registers[i], 0);
  orig = data;
  if ((int )enable && (adev->cg_flags & 512U) != 0U) {
    data = (u32 )mc_cg_en[i] | data;
  } else {
    data = (u32 )(~ mc_cg_en[i]) & data;
  }
  if (data != orig) {
    amdgpu_mm_wreg(adev, mc_cg_registers[i], data, 0);
  } else {
  }
  i = i + 1;
  ldv_43862: ;
  if ((unsigned int )i <= 8U) {
    goto ldv_43861;
  } else {
  }
  return;
}
}
static void gmc_v7_0_enable_bif_mgls(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  data = (*(adev->pcie_rreg))(adev, 20971548U);
  orig = data;
  if ((int )enable && (adev->cg_flags & 4096U) != 0U) {
    data = data | 65536U;
    data = data | 262144U;
    data = data | 524288U;
    data = data | 131072U;
  } else {
    data = data & 4294901759U;
    data = data & 4294705151U;
    data = data & 4294443007U;
    data = data & 4294836223U;
  }
  if (orig != data) {
    (*(adev->pcie_wreg))(adev, 20971548U, data);
  } else {
  }
  return;
}
}
static void gmc_v7_0_enable_hdp_mgcg(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  data = amdgpu_mm_rreg(adev, 2816U, 0);
  orig = data;
  if ((int )enable && (adev->cg_flags & 65536U) != 0U) {
    data = data & 4286578687U;
  } else {
    data = data | 8388608U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 2816U, data, 0);
  } else {
  }
  return;
}
}
static void gmc_v7_0_enable_hdp_ls(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  data = amdgpu_mm_rreg(adev, 3028U, 0);
  orig = data;
  if ((int )enable && (adev->cg_flags & 32768U) != 0U) {
    data = data | 1U;
  } else {
    data = data & 4294967294U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 3028U, data, 0);
  } else {
  }
  return;
}
}
static int gmc_v7_0_convert_vram_type(int mc_seq_vram_type )
{
  {
  switch (mc_seq_vram_type) {
  case 268435456: ;
  return (1);
  case 536870912: ;
  return (2);
  case 805306368: ;
  return (3);
  case 1073741824: ;
  return (4);
  case 1342177280: ;
  return (5);
  case 1610612736: ;
  return (6);
  case -1342177280: ;
  return (7);
  default: ;
  return (0);
  }
}
}
static int gmc_v7_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v7_0_set_gart_funcs(adev);
  gmc_v7_0_set_irq_funcs(adev);
  if ((adev->flags & 131072UL) != 0UL) {
    adev->mc.vram_type = 0U;
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 2688U, 0);
    tmp = tmp___0;
    tmp = tmp & 4026531840U;
    tmp___1 = gmc_v7_0_convert_vram_type((int )tmp);
    adev->mc.vram_type = (u32 )tmp___1;
  }
  return (0);
}
}
static int gmc_v7_0_sw_init(void *handle )
{
  int r ;
  int dma_bits ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_gem_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 146U, & adev->mc.vm_fault);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 147U, & adev->mc.vm_fault);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->vm_manager.max_pfn = (u32 )(amdgpu_vm_size << 18);
  adev->mc.mc_mask = 1099511627775ULL;
  adev->need_dma32 = 0;
  dma_bits = (int )adev->need_dma32 ? 32 : 40;
  r = pci_set_dma_mask(adev->pdev, dma_bits != 64 ? (1ULL << dma_bits) - 1ULL : 0xffffffffffffffffULL);
  if (r != 0) {
    adev->need_dma32 = 1;
    dma_bits = 32;
    printk("\famdgpu: No suitable DMA available.\n");
  } else {
  }
  r = pci_set_consistent_dma_mask(adev->pdev, dma_bits != 64 ? (1ULL << dma_bits) - 1ULL : 0xffffffffffffffffULL);
  if (r != 0) {
    pci_set_consistent_dma_mask(adev->pdev, 4294967295ULL);
    printk("\famdgpu: No coherent DMA available.\n");
  } else {
  }
  r = gmc_v7_0_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load mc firmware!\n");
    return (r);
  } else {
  }
  r = gmc_v7_0_mc_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gmc_v7_0_gart_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! adev->vm_manager.enabled) {
    r = gmc_v7_0_vm_init(adev);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "vm manager initialization failed (%d).\n",
              r);
      return (r);
    } else {
    }
    adev->vm_manager.enabled = 1;
  } else {
  }
  return (r);
}
}
static int gmc_v7_0_sw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->vm_manager.enabled) {
    i = 0;
    goto ldv_43910;
    ldv_43909:
    amdgpu_fence_unref((struct amdgpu_fence **)(& adev->vm_manager.active) + (unsigned long )i);
    i = i + 1;
    ldv_43910: ;
    if (i <= 15) {
      goto ldv_43909;
    } else {
    }
    gmc_v7_0_vm_fini(adev);
    adev->vm_manager.enabled = 0;
  } else {
  }
  gmc_v7_0_gart_fini(adev);
  amdgpu_gem_fini(adev);
  amdgpu_bo_fini(adev);
  return (0);
}
}
static int gmc_v7_0_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v7_0_mc_program(adev);
  if ((adev->flags & 131072UL) == 0UL) {
    r = gmc_v7_0_mc_load_microcode(adev);
    if (r != 0) {
      drm_err("Failed to load MC firmware!\n");
      return (r);
    } else {
    }
  } else {
  }
  r = gmc_v7_0_gart_enable(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int gmc_v7_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v7_0_gart_disable(adev);
  return (0);
}
}
static int gmc_v7_0_suspend(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->vm_manager.enabled) {
    i = 0;
    goto ldv_43927;
    ldv_43926:
    amdgpu_fence_unref((struct amdgpu_fence **)(& adev->vm_manager.active) + (unsigned long )i);
    i = i + 1;
    ldv_43927: ;
    if (i <= 15) {
      goto ldv_43926;
    } else {
    }
    gmc_v7_0_vm_fini(adev);
    adev->vm_manager.enabled = 0;
  } else {
  }
  gmc_v7_0_hw_fini((void *)adev);
  return (0);
}
}
static int gmc_v7_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = gmc_v7_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! adev->vm_manager.enabled) {
    r = gmc_v7_0_vm_init(adev);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "vm manager initialization failed (%d).\n",
              r);
      return (r);
    } else {
    }
    adev->vm_manager.enabled = 1;
  } else {
  }
  return (r);
}
}
static bool gmc_v7_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 7936U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int gmc_v7_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43946;
  ldv_43945:
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0 & 7936U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43946: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43945;
  } else {
  }
  return (-110);
}
}
static void gmc_v7_0_print_status(void *handle )
{
  int i ;
  int j ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "GMC 8.x registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 1343U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
            tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 1335U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
            tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 2073U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_MX_L1_TLB_CNTL=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 1280U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 1281U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL2=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 1282U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL3=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 1367U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PAGE_TABLE_START_ADDR=0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 1375U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PAGE_TABLE_END_ADDR=0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 1350U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR=0x%08X\n",
            tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 1292U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_CNTL2=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 1284U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_CNTL=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 1397U, 0);
  _dev_info((struct device const *)adev->dev, "  0x15D4=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 1398U, 0);
  _dev_info((struct device const *)adev->dev, "  0x15D8=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 1399U, 0);
  _dev_info((struct device const *)adev->dev, "  0x15DC=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 1368U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PAGE_TABLE_START_ADDR=0x%08X\n",
            tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 1376U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PAGE_TABLE_END_ADDR=0x%08X\n",
            tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 1351U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR=0x%08X\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 1293U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_CNTL2=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 1285U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_CNTL=0x%08X\n", tmp___19);
  i = 0;
  goto ldv_43955;
  ldv_43954: ;
  if (i <= 7) {
    tmp___20 = amdgpu_mm_rreg(adev, (u32 )(i + 1359), 0);
    _dev_info((struct device const *)adev->dev, "  VM_CONTEXT%d_PAGE_TABLE_BASE_ADDR=0x%08X\n",
              i, tmp___20);
  } else {
    tmp___21 = amdgpu_mm_rreg(adev, (u32 )(i + 1286), 0);
    _dev_info((struct device const *)adev->dev, "  VM_CONTEXT%d_PAGE_TABLE_BASE_ADDR=0x%08X\n",
              i, tmp___21);
  }
  i = i + 1;
  ldv_43955: ;
  if (i <= 15) {
    goto ldv_43954;
  } else {
  }
  tmp___22 = amdgpu_mm_rreg(adev, 2061U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_LOW_ADDR=0x%08X\n",
            tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 2062U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_HIGH_ADDR=0x%08X\n",
            tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 2063U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_DEFAULT_ADDR=0x%08X\n",
            tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 2057U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_FB_LOCATION=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 2060U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_BASE=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 2058U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_TOP=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 2059U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_BOT=0x%08X\n", tmp___28);
  if ((unsigned int )adev->asic_type == 1U) {
    tmp___29 = amdgpu_mm_rreg(adev, 1561U, 0);
    _dev_info((struct device const *)adev->dev, "  CHUB_CONTROL=0x%08X\n", tmp___29);
  } else {
  }
  tmp___30 = amdgpu_mm_rreg(adev, 5416U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_REG_COHERENCY_FLUSH_CNTL=0x%08X\n",
            tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 2817U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_BASE=0x%08X\n",
            tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 2818U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_INFO=0x%08X\n",
            tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 2819U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_SIZE=0x%08X\n",
            tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 3027U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_MISC_CNTL=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 2816U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_HOST_PATH_CNTL=0x%08X\n", tmp___35);
  i = 0;
  j = 0;
  goto ldv_43958;
  ldv_43957:
  _dev_info((struct device const *)adev->dev, "  %d:\n", i);
  tmp___36 = amdgpu_mm_rreg(adev, (u32 )(j + 2821), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2821, tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, (u32 )(j + 2822), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2822, tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, (u32 )(j + 2823), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2823, tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, (u32 )(j + 2824), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2824, tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, (u32 )(j + 2825), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2825, tmp___40);
  i = i + 1;
  j = j + 6;
  ldv_43958: ;
  if (i <= 31) {
    goto ldv_43957;
  } else {
  }
  tmp___41 = amdgpu_mm_rreg(adev, 5412U, 0);
  _dev_info((struct device const *)adev->dev, "  BIF_FB_EN=0x%08X\n", tmp___41);
  return;
}
}
static int gmc_v7_0_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_mode_mc_save save ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  srbm_soft_reset = 0U;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 256U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 131072U;
  } else {
  }
  if ((tmp & 7680U) != 0U) {
    if ((adev->flags & 131072UL) == 0UL) {
      srbm_soft_reset = srbm_soft_reset | 2048U;
    } else {
    }
  } else {
  }
  if (srbm_soft_reset != 0U) {
    gmc_v7_0_print_status((void *)adev);
    gmc_v7_0_mc_stop(adev, & save);
    tmp___1 = gmc_v7_0_wait_for_idle((void *)adev);
    if (tmp___1 != 0) {
      dev_warn((struct device const *)adev->dev, "Wait for GMC idle timed out !\n");
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    gmc_v7_0_mc_resume(adev, & save);
    __const_udelay(214750UL);
    gmc_v7_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int gmc_v7_0_vm_fault_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                             unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 tmp ;
  u32 bits ;
  {
  bits = 299592U;
  switch ((unsigned int )state) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = ~ bits & tmp;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = ~ bits & tmp;
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  goto ldv_43976;
  case 1U:
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = tmp | bits;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = tmp | bits;
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  goto ldv_43976;
  default: ;
  goto ldv_43976;
  }
  ldv_43976: ;
  return (0);
}
}
static int gmc_v7_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  u32 addr ;
  u32 status ;
  u32 mc_client ;
  u32 tmp_ ;
  u32 tmp ;
  {
  addr = amdgpu_mm_rreg(adev, 1343U, 0);
  status = amdgpu_mm_rreg(adev, 1335U, 0);
  mc_client = amdgpu_mm_rreg(adev, 1337U, 0);
  dev_err((struct device const *)adev->dev, "GPU fault detected: %d 0x%08x\n", entry->src_id,
          entry->src_data);
  dev_err((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
          addr);
  dev_err((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
          status);
  gmc_v7_0_vm_decode_fault(adev, status, addr, mc_client);
  tmp = amdgpu_mm_rreg(adev, 1293U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_ | 1U;
  amdgpu_mm_wreg(adev, 1293U, tmp_, 0);
  return (0);
}
}
static int gmc_v7_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  if ((adev->flags & 131072UL) == 0UL) {
    gmc_v7_0_enable_mc_mgcg(adev, (int )gate);
    gmc_v7_0_enable_mc_ls(adev, (int )gate);
  } else {
  }
  gmc_v7_0_enable_bif_mgls(adev, (int )gate);
  gmc_v7_0_enable_hdp_mgcg(adev, (int )gate);
  gmc_v7_0_enable_hdp_ls(adev, (int )gate);
  return (0);
}
}
static int gmc_v7_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const gmc_v7_0_ip_funcs =
     {& gmc_v7_0_early_init, (int (*)(void * ))0, & gmc_v7_0_sw_init, & gmc_v7_0_sw_fini,
    & gmc_v7_0_hw_init, & gmc_v7_0_hw_fini, & gmc_v7_0_suspend, & gmc_v7_0_resume,
    & gmc_v7_0_is_idle, & gmc_v7_0_wait_for_idle, & gmc_v7_0_soft_reset, & gmc_v7_0_print_status,
    & gmc_v7_0_set_clockgating_state, & gmc_v7_0_set_powergating_state};
static struct amdgpu_gart_funcs const gmc_v7_0_gart_funcs = {& gmc_v7_0_gart_flush_gpu_tlb, & gmc_v7_0_gart_set_pte_pde};
static struct amdgpu_irq_src_funcs const gmc_v7_0_irq_funcs = {& gmc_v7_0_vm_fault_interrupt_state, & gmc_v7_0_process_interrupt};
static void gmc_v7_0_set_gart_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->gart.gart_funcs == (unsigned long )((struct amdgpu_gart_funcs const *)0)) {
    adev->gart.gart_funcs = & gmc_v7_0_gart_funcs;
  } else {
  }
  return;
}
}
static void gmc_v7_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->mc.vm_fault.num_types = 1U;
  adev->mc.vm_fault.funcs = & gmc_v7_0_irq_funcs;
  return;
}
}
extern int ldv_release_119(void) ;
int ldv_retval_28 ;
extern int ldv_probe_119(void) ;
int ldv_retval_27 ;
void ldv_initialize_amdgpu_gart_funcs_118(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gmc_v7_0_gart_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_117(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gmc_v7_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gmc_v7_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_119(void)
{
  void *ldvarg558 ;
  void *tmp ;
  void *ldvarg562 ;
  void *tmp___0 ;
  void *ldvarg554 ;
  void *tmp___1 ;
  void *ldvarg561 ;
  void *tmp___2 ;
  void *ldvarg553 ;
  void *tmp___3 ;
  void *ldvarg557 ;
  void *tmp___4 ;
  void *ldvarg551 ;
  void *tmp___5 ;
  void *ldvarg560 ;
  void *tmp___6 ;
  void *ldvarg549 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg552 ;
  enum amd_powergating_state ldvarg556 ;
  void *ldvarg559 ;
  void *tmp___8 ;
  void *ldvarg550 ;
  void *tmp___9 ;
  void *ldvarg555 ;
  void *tmp___10 ;
  void *ldvarg548 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg558 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg562 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg554 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg561 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg553 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg557 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg551 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg560 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg549 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg559 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg550 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg555 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg548 = tmp___11;
  ldv_memset((void *)(& ldvarg552), 0, 4UL);
  ldv_memset((void *)(& ldvarg556), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_hw_fini(ldvarg562);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_hw_fini(ldvarg562);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_hw_fini(ldvarg562);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 1: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_print_status(ldvarg561);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_print_status(ldvarg561);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_print_status(ldvarg561);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 2: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_early_init(ldvarg560);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_early_init(ldvarg560);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_early_init(ldvarg560);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 3: ;
  if (ldv_state_variable_119 == 2) {
    ldv_retval_28 = gmc_v7_0_suspend(ldvarg559);
    if (ldv_retval_28 == 0) {
      ldv_state_variable_119 = 3;
    } else {
    }
  } else {
  }
  goto ldv_44038;
  case 4: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_sw_init(ldvarg558);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_sw_init(ldvarg558);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_sw_init(ldvarg558);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 5: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_set_powergating_state(ldvarg557, ldvarg556);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_set_powergating_state(ldvarg557, ldvarg556);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_set_powergating_state(ldvarg557, ldvarg556);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 6: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_wait_for_idle(ldvarg555);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_wait_for_idle(ldvarg555);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_wait_for_idle(ldvarg555);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 7: ;
  if (ldv_state_variable_119 == 3) {
    ldv_retval_27 = gmc_v7_0_resume(ldvarg554);
    if (ldv_retval_27 == 0) {
      ldv_state_variable_119 = 2;
    } else {
    }
  } else {
  }
  goto ldv_44038;
  case 8: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_set_clockgating_state(ldvarg553, ldvarg552);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_set_clockgating_state(ldvarg553, ldvarg552);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_set_clockgating_state(ldvarg553, ldvarg552);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 9: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_hw_init(ldvarg551);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_hw_init(ldvarg551);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_hw_init(ldvarg551);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 10: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_soft_reset(ldvarg550);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_soft_reset(ldvarg550);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_soft_reset(ldvarg550);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 11: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_sw_fini(ldvarg549);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_sw_fini(ldvarg549);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_sw_fini(ldvarg549);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 12: ;
  if (ldv_state_variable_119 == 1) {
    gmc_v7_0_is_idle(ldvarg548);
    ldv_state_variable_119 = 1;
  } else {
  }
  if (ldv_state_variable_119 == 3) {
    gmc_v7_0_is_idle(ldvarg548);
    ldv_state_variable_119 = 3;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    gmc_v7_0_is_idle(ldvarg548);
    ldv_state_variable_119 = 2;
  } else {
  }
  goto ldv_44038;
  case 13: ;
  if (ldv_state_variable_119 == 3) {
    ldv_release_119();
    ldv_state_variable_119 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_119 == 2) {
    ldv_release_119();
    ldv_state_variable_119 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_44038;
  case 14: ;
  if (ldv_state_variable_119 == 1) {
    ldv_probe_119();
    ldv_state_variable_119 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_44038;
  default:
  ldv_stop();
  }
  ldv_44038: ;
  return;
}
}
void ldv_main_exported_118(void)
{
  u32 ldvarg19 ;
  u32 ldvarg22 ;
  void *ldvarg20 ;
  void *tmp ;
  uint64_t ldvarg18 ;
  u32 ldvarg21 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg20 = tmp;
  ldv_memset((void *)(& ldvarg19), 0, 4UL);
  ldv_memset((void *)(& ldvarg22), 0, 4UL);
  ldv_memset((void *)(& ldvarg18), 0, 8UL);
  ldv_memset((void *)(& ldvarg21), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_118 == 1) {
    gmc_v7_0_gart_flush_gpu_tlb(gmc_v7_0_gart_funcs_group0, ldvarg22);
    ldv_state_variable_118 = 1;
  } else {
  }
  goto ldv_44063;
  case 1: ;
  if (ldv_state_variable_118 == 1) {
    gmc_v7_0_gart_set_pte_pde(gmc_v7_0_gart_funcs_group0, ldvarg20, ldvarg19, ldvarg18,
                              ldvarg21);
    ldv_state_variable_118 = 1;
  } else {
  }
  goto ldv_44063;
  default:
  ldv_stop();
  }
  ldv_44063: ;
  return;
}
}
void ldv_main_exported_117(void)
{
  unsigned int ldvarg476 ;
  struct amdgpu_iv_entry *ldvarg475 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg477 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg475 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg476), 0, 4UL);
  ldv_memset((void *)(& ldvarg477), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_117 == 1) {
    gmc_v7_0_vm_fault_interrupt_state(gmc_v7_0_irq_funcs_group0, gmc_v7_0_irq_funcs_group1,
                                      ldvarg476, ldvarg477);
    ldv_state_variable_117 = 1;
  } else {
  }
  goto ldv_44073;
  case 1: ;
  if (ldv_state_variable_117 == 1) {
    gmc_v7_0_process_interrupt(gmc_v7_0_irq_funcs_group0, gmc_v7_0_irq_funcs_group1,
                               ldvarg475);
    ldv_state_variable_117 = 1;
  } else {
  }
  goto ldv_44073;
  default:
  ldv_stop();
  }
  ldv_44073: ;
  return;
}
}
bool ldv_queue_work_on_537(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_538(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_539(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_540(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_541(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_551(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_553(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_552(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_555(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_554(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_ih_ring_init(struct amdgpu_device *adev , unsigned int ring_size , bool use_bus_addr ) ;
void amdgpu_ih_ring_fini(struct amdgpu_device *adev ) ;
int amdgpu_irq_init(struct amdgpu_device *adev ) ;
void amdgpu_irq_fini(struct amdgpu_device *adev ) ;
static void cik_ih_set_interrupt_funcs(struct amdgpu_device *adev ) ;
static void cik_ih_enable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_cntl ;
  u32 tmp ;
  u32 ih_rb_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3974U, 0);
  ih_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3968U, 0);
  ih_rb_cntl = tmp___0;
  ih_cntl = ih_cntl | 1U;
  ih_rb_cntl = ih_rb_cntl | 1U;
  amdgpu_mm_wreg(adev, 3974U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3968U, ih_rb_cntl, 0);
  adev->irq.ih.enabled = 1;
  return;
}
}
static void cik_ih_disable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_rb_cntl ;
  u32 tmp ;
  u32 ih_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3968U, 0);
  ih_rb_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3974U, 0);
  ih_cntl = tmp___0;
  ih_rb_cntl = ih_rb_cntl & 4294967294U;
  ih_cntl = ih_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 3968U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3974U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3970U, 0U, 0);
  amdgpu_mm_wreg(adev, 3971U, 0U, 0);
  adev->irq.ih.enabled = 0;
  adev->irq.ih.rptr = 0U;
  return;
}
}
static int cik_ih_irq_init(struct amdgpu_device *adev )
{
  int ret ;
  int rb_bufsz ;
  u32 interrupt_cntl ;
  u32 ih_cntl ;
  u32 ih_rb_cntl ;
  u64 wptr_off ;
  unsigned long tmp ;
  {
  ret = 0;
  cik_ih_disable_interrupts(adev);
  amdgpu_mm_wreg(adev, 5403U, (u32 )(adev->dummy_page.addr >> 8), 0);
  interrupt_cntl = amdgpu_mm_rreg(adev, 5402U, 0);
  interrupt_cntl = interrupt_cntl & 4294967294U;
  interrupt_cntl = interrupt_cntl & 4294967287U;
  amdgpu_mm_wreg(adev, 5402U, interrupt_cntl, 0);
  amdgpu_mm_wreg(adev, 3969U, (u32 )(adev->irq.ih.gpu_addr >> 8), 0);
  tmp = __roundup_pow_of_two((unsigned long )(adev->irq.ih.ring_size / 4U));
  rb_bufsz = __ilog2_u64((u64 )tmp);
  ih_rb_cntl = (unsigned int )(rb_bufsz << 1) | 2147549184U;
  ih_rb_cntl = ih_rb_cntl | 256U;
  wptr_off = adev->wb.gpu_addr + (uint64_t )(adev->irq.ih.wptr_offs * 4U);
  amdgpu_mm_wreg(adev, 3973U, (unsigned int )wptr_off, 0);
  amdgpu_mm_wreg(adev, 3972U, (unsigned int )(wptr_off >> 32ULL) & 255U, 0);
  amdgpu_mm_wreg(adev, 3968U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3970U, 0U, 0);
  amdgpu_mm_wreg(adev, 3971U, 0U, 0);
  ih_cntl = 17301504U;
  if ((int )adev->irq.msi_enabled) {
    ih_cntl = ih_cntl | 16U;
  } else {
  }
  amdgpu_mm_wreg(adev, 3974U, ih_cntl, 0);
  pci_set_master(adev->pdev);
  cik_ih_enable_interrupts(adev);
  return (ret);
}
}
static void cik_ih_irq_disable(struct amdgpu_device *adev )
{
  unsigned long __ms ;
  unsigned long tmp ;
  {
  cik_ih_disable_interrupts(adev);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43669;
    ldv_43668:
    __const_udelay(4295000UL);
    ldv_43669:
    tmp = __ms;
    __ms = __ms - 1UL;
    if (tmp != 0UL) {
      goto ldv_43668;
    } else {
    }
  }
  return;
}
}
static u32 cik_ih_get_wptr(struct amdgpu_device *adev )
{
  u32 wptr ;
  u32 tmp ;
  {
  wptr = *(adev->wb.wb + (unsigned long )adev->irq.ih.wptr_offs);
  if ((int )wptr & 1) {
    wptr = wptr & 4294967294U;
    dev_warn((struct device const *)adev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",
             wptr, adev->irq.ih.rptr, (wptr + 16U) & adev->irq.ih.ptr_mask);
    adev->irq.ih.rptr = (wptr + 16U) & adev->irq.ih.ptr_mask;
    tmp = amdgpu_mm_rreg(adev, 3968U, 0);
    tmp = tmp | 2147483648U;
    amdgpu_mm_wreg(adev, 3968U, tmp, 0);
  } else {
  }
  return (adev->irq.ih.ptr_mask & wptr);
}
}
static void cik_ih_decode_iv(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry )
{
  u32 ring_index ;
  u32 dw[4U] ;
  {
  ring_index = adev->irq.ih.rptr >> 2;
  dw[0] = *(adev->irq.ih.ring + (unsigned long )ring_index);
  dw[1] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 1U));
  dw[2] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 2U));
  dw[3] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 3U));
  entry->src_id = dw[0] & 255U;
  entry->src_data = dw[1] & 268435455U;
  entry->ring_id = dw[2] & 255U;
  entry->vm_id = (dw[2] >> 8) & 255U;
  entry->pas_id = dw[2] >> 16;
  adev->irq.ih.rptr = adev->irq.ih.rptr + 16U;
  return;
}
}
static void cik_ih_set_rptr(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 3970U, adev->irq.ih.rptr, 0);
  return;
}
}
static int cik_ih_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cik_ih_set_interrupt_funcs(adev);
  return (0);
}
}
static int cik_ih_sw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_ih_ring_init(adev, 65536U, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_init(adev);
  return (r);
}
}
static int cik_ih_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_irq_fini(adev);
  amdgpu_ih_ring_fini(adev);
  return (0);
}
}
static int cik_ih_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = cik_ih_irq_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int cik_ih_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cik_ih_irq_disable(adev);
  return (0);
}
}
static int cik_ih_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_ih_hw_fini((void *)adev);
  return (tmp);
}
}
static int cik_ih_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_ih_hw_init((void *)adev);
  return (tmp);
}
}
static bool cik_ih_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int cik_ih_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43727;
  ldv_43726:
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0 & 131072U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43727: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43726;
  } else {
  }
  return (-110);
}
}
static void cik_ih_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "CIK IH registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 5402U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 5403U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL2=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 3974U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_CNTL=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 3968U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 3969U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_BASE=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 3973U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_LO=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 3972U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_HI=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 3970U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_RPTR=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 3971U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR=0x%08X\n", tmp___9);
  return;
}
}
static int cik_ih_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  srbm_soft_reset = 0U;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    cik_ih_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    cik_ih_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int cik_ih_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int cik_ih_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const cik_ih_ip_funcs =
     {& cik_ih_early_init, (int (*)(void * ))0, & cik_ih_sw_init, & cik_ih_sw_fini,
    & cik_ih_hw_init, & cik_ih_hw_fini, & cik_ih_suspend, & cik_ih_resume, & cik_ih_is_idle,
    & cik_ih_wait_for_idle, & cik_ih_soft_reset, & cik_ih_print_status, & cik_ih_set_clockgating_state,
    & cik_ih_set_powergating_state};
static struct amdgpu_ih_funcs const cik_ih_funcs = {& cik_ih_get_wptr, & cik_ih_decode_iv, & cik_ih_set_rptr};
static void cik_ih_set_interrupt_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->irq.ih_funcs == (unsigned long )((struct amdgpu_ih_funcs const *)0)) {
    adev->irq.ih_funcs = & cik_ih_funcs;
  } else {
  }
  return;
}
}
extern int ldv_release_116(void) ;
int ldv_retval_71 ;
extern int ldv_probe_116(void) ;
int ldv_retval_70 ;
void ldv_initialize_amdgpu_ih_funcs_115(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  cik_ih_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_116(void)
{
  void *ldvarg883 ;
  void *tmp ;
  void *ldvarg877 ;
  void *tmp___0 ;
  void *ldvarg872 ;
  void *tmp___1 ;
  enum amd_powergating_state ldvarg878 ;
  void *ldvarg876 ;
  void *tmp___2 ;
  void *ldvarg873 ;
  void *tmp___3 ;
  void *ldvarg870 ;
  void *tmp___4 ;
  void *ldvarg879 ;
  void *tmp___5 ;
  void *ldvarg875 ;
  void *tmp___6 ;
  void *ldvarg881 ;
  void *tmp___7 ;
  void *ldvarg871 ;
  void *tmp___8 ;
  void *ldvarg882 ;
  void *tmp___9 ;
  void *ldvarg884 ;
  void *tmp___10 ;
  enum amd_clockgating_state ldvarg874 ;
  void *ldvarg880 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg883 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg877 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg872 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg876 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg873 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg870 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg879 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg875 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg881 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg871 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg882 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg884 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg880 = tmp___11;
  ldv_memset((void *)(& ldvarg878), 0, 4UL);
  ldv_memset((void *)(& ldvarg874), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_hw_fini(ldvarg884);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_hw_fini(ldvarg884);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_hw_fini(ldvarg884);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 1: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_print_status(ldvarg883);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_print_status(ldvarg883);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_print_status(ldvarg883);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 2: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_early_init(ldvarg882);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_early_init(ldvarg882);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_early_init(ldvarg882);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 3: ;
  if (ldv_state_variable_116 == 2) {
    ldv_retval_71 = cik_ih_suspend(ldvarg881);
    if (ldv_retval_71 == 0) {
      ldv_state_variable_116 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43780;
  case 4: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_sw_init(ldvarg880);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_sw_init(ldvarg880);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_sw_init(ldvarg880);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 5: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_set_powergating_state(ldvarg879, ldvarg878);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_set_powergating_state(ldvarg879, ldvarg878);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_set_powergating_state(ldvarg879, ldvarg878);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 6: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_wait_for_idle(ldvarg877);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_wait_for_idle(ldvarg877);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_wait_for_idle(ldvarg877);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 7: ;
  if (ldv_state_variable_116 == 3) {
    ldv_retval_70 = cik_ih_resume(ldvarg876);
    if (ldv_retval_70 == 0) {
      ldv_state_variable_116 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43780;
  case 8: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_set_clockgating_state(ldvarg875, ldvarg874);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_set_clockgating_state(ldvarg875, ldvarg874);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_set_clockgating_state(ldvarg875, ldvarg874);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 9: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_hw_init(ldvarg873);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_hw_init(ldvarg873);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_hw_init(ldvarg873);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 10: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_soft_reset(ldvarg872);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_soft_reset(ldvarg872);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_soft_reset(ldvarg872);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 11: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_sw_fini(ldvarg871);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_sw_fini(ldvarg871);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_sw_fini(ldvarg871);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 12: ;
  if (ldv_state_variable_116 == 2) {
    cik_ih_is_idle(ldvarg870);
    ldv_state_variable_116 = 2;
  } else {
  }
  if (ldv_state_variable_116 == 1) {
    cik_ih_is_idle(ldvarg870);
    ldv_state_variable_116 = 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    cik_ih_is_idle(ldvarg870);
    ldv_state_variable_116 = 3;
  } else {
  }
  goto ldv_43780;
  case 13: ;
  if (ldv_state_variable_116 == 2) {
    ldv_release_116();
    ldv_state_variable_116 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_116 == 3) {
    ldv_release_116();
    ldv_state_variable_116 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43780;
  case 14: ;
  if (ldv_state_variable_116 == 1) {
    ldv_probe_116();
    ldv_state_variable_116 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43780;
  default:
  ldv_stop();
  }
  ldv_43780: ;
  return;
}
}
void ldv_main_exported_115(void)
{
  struct amdgpu_iv_entry *ldvarg637 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg637 = (struct amdgpu_iv_entry *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_115 == 1) {
    cik_ih_decode_iv(cik_ih_funcs_group0, ldvarg637);
    ldv_state_variable_115 = 1;
  } else {
  }
  goto ldv_43801;
  case 1: ;
  if (ldv_state_variable_115 == 1) {
    cik_ih_get_wptr(cik_ih_funcs_group0);
    ldv_state_variable_115 = 1;
  } else {
  }
  goto ldv_43801;
  case 2: ;
  if (ldv_state_variable_115 == 1) {
    cik_ih_set_rptr(cik_ih_funcs_group0);
    ldv_state_variable_115 = 1;
  } else {
  }
  goto ldv_43801;
  default:
  ldv_stop();
  }
  ldv_43801: ;
  return;
}
}
bool ldv_queue_work_on_551(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_552(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_553(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_554(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_555(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_565(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_567(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_566(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_569(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_568(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_kv_notify_message_to_smu(struct amdgpu_device *adev , u32 id ) ;
int amdgpu_kv_dpm_get_enable_mask(struct amdgpu_device *adev , u32 *enable_mask ) ;
int amdgpu_kv_send_msg_to_smc_with_parameter(struct amdgpu_device *adev , PPSMC_Msg msg ,
                                             u32 parameter ) ;
int amdgpu_kv_read_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 *value ,
                                  u32 limit ) ;
int amdgpu_kv_smc_dpm_enable(struct amdgpu_device *adev , bool enable ) ;
int amdgpu_kv_smc_bapm_enable(struct amdgpu_device *adev , bool enable ) ;
int amdgpu_kv_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                u8 const *src , u32 byte_count , u32 limit ) ;
int amdgpu_kv_notify_message_to_smu(struct amdgpu_device *adev , u32 id )
{
  u32 i ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  {
  tmp = 0U;
  amdgpu_mm_wreg(adev, 148U, id & 65535U, 0);
  i = 0U;
  goto ldv_44051;
  ldv_44050:
  tmp___0 = amdgpu_mm_rreg(adev, 149U, 0);
  if ((tmp___0 & 65535U) != 0U) {
    goto ldv_44049;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_44051: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_44050;
  } else {
  }
  ldv_44049:
  tmp___1 = amdgpu_mm_rreg(adev, 149U, 0);
  tmp = tmp___1 & 65535U;
  if (tmp != 1U) {
    if (tmp == 255U) {
      return (-22);
    } else
    if (tmp == 254U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
int amdgpu_kv_dpm_get_enable_mask(struct amdgpu_device *adev , u32 *enable_mask )
{
  int ret ;
  {
  ret = amdgpu_kv_notify_message_to_smu(adev, 354U);
  if (ret == 0) {
    *enable_mask = (*(adev->smc_rreg))(adev, 2147483752U);
  } else {
  }
  return (ret);
}
}
int amdgpu_kv_send_msg_to_smc_with_parameter(struct amdgpu_device *adev , PPSMC_Msg msg ,
                                             u32 parameter )
{
  int tmp ;
  {
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp = amdgpu_kv_notify_message_to_smu(adev, (u32 )msg);
  return (tmp);
}
}
static int kv_set_smc_sram_address(struct amdgpu_device *adev , u32 smc_address ,
                                   u32 limit )
{
  u32 tmp_ ;
  u32 tmp ;
  {
  if ((smc_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_address + 3U > limit) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 128U, smc_address, 0);
  tmp = amdgpu_mm_rreg(adev, 144U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 144U, tmp_, 0);
  return (0);
}
}
int amdgpu_kv_read_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 *value ,
                                  u32 limit )
{
  int ret ;
  {
  ret = kv_set_smc_sram_address(adev, smc_address, limit);
  if (ret != 0) {
    return (ret);
  } else {
  }
  *value = amdgpu_mm_rreg(adev, 129U, 0);
  return (0);
}
}
int amdgpu_kv_smc_dpm_enable(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  int tmp___0 ;
  {
  if ((int )enable) {
    tmp = amdgpu_kv_notify_message_to_smu(adev, 334U);
    return (tmp);
  } else {
    tmp___0 = amdgpu_kv_notify_message_to_smu(adev, 335U);
    return (tmp___0);
  }
}
}
int amdgpu_kv_smc_bapm_enable(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  int tmp___0 ;
  {
  if ((int )enable) {
    tmp = amdgpu_kv_notify_message_to_smu(adev, 288U);
    return (tmp);
  } else {
    tmp___0 = amdgpu_kv_notify_message_to_smu(adev, 289U);
    return (tmp___0);
  }
}
}
int amdgpu_kv_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                u8 const *src , u32 byte_count , u32 limit )
{
  int ret ;
  u32 data ;
  u32 original_data ;
  u32 addr ;
  u32 extra_shift ;
  u32 t_byte ;
  u32 count ;
  u32 mask ;
  u8 const *tmp ;
  u8 const *tmp___0 ;
  {
  if (smc_start_address + byte_count > limit) {
    return (-22);
  } else {
  }
  addr = smc_start_address;
  t_byte = addr & 3U;
  if (t_byte != 0U) {
    addr = addr - t_byte;
    ret = kv_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      return (ret);
    } else {
    }
    original_data = amdgpu_mm_rreg(adev, 129U, 0);
    data = 0U;
    mask = 0U;
    count = 4U;
    goto ldv_44099;
    ldv_44098: ;
    if (t_byte != 0U) {
      mask = (mask << 8) | 255U;
      t_byte = t_byte - 1U;
    } else
    if (byte_count != 0U) {
      tmp = src;
      src = src + 1;
      data = (data << 8) + (u32 )*tmp;
      byte_count = byte_count - 1U;
      mask = mask << 8;
    } else {
      data = data << 8;
      mask = (mask << 8) | 255U;
    }
    count = count - 1U;
    ldv_44099: ;
    if (count != 0U) {
      goto ldv_44098;
    } else {
    }
    data = (original_data & mask) | data;
    ret = kv_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      return (ret);
    } else {
    }
    amdgpu_mm_wreg(adev, 129U, data, 0);
    addr = addr + 4U;
  } else {
  }
  goto ldv_44102;
  ldv_44101:
  data = (u32 )(((((int )*src << 24) + ((int )*(src + 1UL) << 16)) + ((int )*(src + 2UL) << 8)) + (int )*(src + 3UL));
  ret = kv_set_smc_sram_address(adev, addr, limit);
  if (ret != 0) {
    return (ret);
  } else {
  }
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  byte_count = byte_count - 4U;
  addr = addr + 4U;
  ldv_44102: ;
  if (byte_count > 3U) {
    goto ldv_44101;
  } else {
  }
  if (byte_count != 0U) {
    data = 0U;
    ret = kv_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      return (ret);
    } else {
    }
    original_data = amdgpu_mm_rreg(adev, 129U, 0);
    extra_shift = (4U - byte_count) * 8U;
    goto ldv_44105;
    ldv_44104:
    tmp___0 = src;
    src = src + 1;
    data = (data << 8) + (u32 )*tmp___0;
    byte_count = byte_count - 1U;
    ldv_44105: ;
    if (byte_count != 0U) {
      goto ldv_44104;
    } else {
    }
    data = data << (int )extra_shift;
    data = (~ ((u32 )(0xffffffffffffffffUL << (int )extra_shift)) & original_data) | data;
    ret = kv_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      return (ret);
    } else {
    }
    amdgpu_mm_wreg(adev, 129U, data, 0);
  } else {
  }
  return (0);
}
}
bool ldv_queue_work_on_565(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_566(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_567(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_568(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_569(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static __u32 __arch_swab32(__u32 val )
{
  {
  __asm__ ("bswapl %0": "=r" (val): "0" (val));
  return (val);
}
}
__inline static __u16 __fswab16(__u16 val )
{
  {
  return ((__u16 )((int )((short )((int )val << 8)) | (int )((short )((int )val >> 8))));
}
}
__inline static __u32 __fswab32(__u32 val )
{
  __u32 tmp ;
  {
  tmp = __arch_swab32(val);
  return (tmp);
}
}
extern struct workqueue_struct *system_wq ;
bool ldv_queue_work_on_579(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_581(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_580(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_583(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_582(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___0(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_579(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___0(system_wq, work);
  return (tmp);
}
}
void invoke_work_4(void) ;
void call_and_disable_work_4(struct work_struct *work ) ;
void activate_work_4(struct work_struct *work , int state ) ;
void call_and_disable_all_4(int state ) ;
void disable_work_4(struct work_struct *work ) ;
void amdgpu_dpm_print_class_info(u32 class , u32 class2 ) ;
void amdgpu_dpm_print_cap_info(u32 caps ) ;
void amdgpu_dpm_print_ps_status(struct amdgpu_device *adev , struct amdgpu_ps *rps ) ;
bool amdgpu_is_internal_thermal_sensor(enum amdgpu_int_thermal_type sensor ) ;
int amdgpu_get_platform_caps(struct amdgpu_device *adev ) ;
int amdgpu_parse_extended_power_table(struct amdgpu_device *adev ) ;
void amdgpu_free_extended_power_table(struct amdgpu_device *adev ) ;
void amdgpu_add_thermal_controller(struct amdgpu_device *adev ) ;
void gfx_v7_0_enter_rlc_safe_mode(struct amdgpu_device *adev ) ;
void gfx_v7_0_exit_rlc_safe_mode(struct amdgpu_device *adev ) ;
static void kv_dpm_set_dpm_funcs(struct amdgpu_device *adev ) ;
static void kv_dpm_set_irq_funcs(struct amdgpu_device *adev ) ;
static int kv_enable_nb_dpm(struct amdgpu_device *adev , bool enable ) ;
static void kv_init_graphics_levels(struct amdgpu_device *adev ) ;
static int kv_calculate_ds_divider(struct amdgpu_device *adev ) ;
static int kv_calculate_nbps_level_settings(struct amdgpu_device *adev ) ;
static int kv_calculate_dpm_settings(struct amdgpu_device *adev ) ;
static void kv_enable_new_levels(struct amdgpu_device *adev ) ;
static void kv_program_nbps_index_settings(struct amdgpu_device *adev , struct amdgpu_ps *new_rps ) ;
static int kv_set_enabled_level(struct amdgpu_device *adev , u32 level ) ;
static int kv_set_enabled_levels(struct amdgpu_device *adev ) ;
static int kv_force_dpm_highest(struct amdgpu_device *adev ) ;
static int kv_force_dpm_lowest(struct amdgpu_device *adev ) ;
static void kv_apply_state_adjust_rules(struct amdgpu_device *adev , struct amdgpu_ps *new_rps ,
                                        struct amdgpu_ps *old_rps ) ;
static int kv_set_thermal_temperature_range(struct amdgpu_device *adev , int min_temp ,
                                            int max_temp ) ;
static int kv_init_fps_limits(struct amdgpu_device *adev ) ;
static void kv_dpm_powergate_uvd(struct amdgpu_device *adev , bool gate ) ;
static void kv_dpm_powergate_vce(struct amdgpu_device *adev , bool gate ) ;
static void kv_dpm_powergate_samu(struct amdgpu_device *adev , bool gate ) ;
static void kv_dpm_powergate_acp(struct amdgpu_device *adev , bool gate ) ;
static u32 kv_convert_vid2_to_vid7(struct amdgpu_device *adev , struct sumo_vid_mapping_table *vid_mapping_table ,
                                   u32 vid_2bit )
{
  struct amdgpu_clock_voltage_dependency_table *vddc_sclk_table ;
  u32 i ;
  {
  vddc_sclk_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )vddc_sclk_table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && vddc_sclk_table->count != 0U) {
    if (vddc_sclk_table->count > vid_2bit) {
      return ((u32 )(vddc_sclk_table->entries + (unsigned long )vid_2bit)->v);
    } else {
      return ((u32 )(vddc_sclk_table->entries + (unsigned long )(vddc_sclk_table->count - 1U))->v);
    }
  } else {
    i = 0U;
    goto ldv_48452;
    ldv_48451: ;
    if ((u32 )vid_mapping_table->entries[i].vid_2bit == vid_2bit) {
      return ((u32 )vid_mapping_table->entries[i].vid_7bit);
    } else {
    }
    i = i + 1U;
    ldv_48452: ;
    if (vid_mapping_table->num_entries > i) {
      goto ldv_48451;
    } else {
    }
    return ((u32 )vid_mapping_table->entries[vid_mapping_table->num_entries - 1U].vid_7bit);
  }
}
}
static u32 kv_convert_vid7_to_vid2(struct amdgpu_device *adev , struct sumo_vid_mapping_table *vid_mapping_table ,
                                   u32 vid_7bit )
{
  struct amdgpu_clock_voltage_dependency_table *vddc_sclk_table ;
  u32 i ;
  {
  vddc_sclk_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )vddc_sclk_table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && vddc_sclk_table->count != 0U) {
    i = 0U;
    goto ldv_48462;
    ldv_48461: ;
    if ((u32 )(vddc_sclk_table->entries + (unsigned long )i)->v == vid_7bit) {
      return (i);
    } else {
    }
    i = i + 1U;
    ldv_48462: ;
    if (vddc_sclk_table->count > i) {
      goto ldv_48461;
    } else {
    }
    return (vddc_sclk_table->count - 1U);
  } else {
    i = 0U;
    goto ldv_48465;
    ldv_48464: ;
    if ((u32 )vid_mapping_table->entries[i].vid_7bit == vid_7bit) {
      return ((u32 )vid_mapping_table->entries[i].vid_2bit);
    } else {
    }
    i = i + 1U;
    ldv_48465: ;
    if (vid_mapping_table->num_entries > i) {
      goto ldv_48464;
    } else {
    }
    return ((u32 )vid_mapping_table->entries[vid_mapping_table->num_entries - 1U].vid_2bit);
  }
}
}
static void sumo_take_smu_control(struct amdgpu_device *adev , bool enable )
{
  {
  return;
}
}
static u32 sumo_get_sleep_divider_from_id(u32 id )
{
  {
  return ((u32 )(1 << (int )id));
}
}
static void sumo_construct_sclk_voltage_mapping_table(struct amdgpu_device *adev ,
                                                      struct sumo_sclk_voltage_mapping_table *sclk_voltage_mapping_table ,
                                                      ATOM_AVAILABLE_SCLK_LIST *table )
{
  u32 i ;
  u32 n ;
  u32 prev_sclk ;
  {
  n = 0U;
  prev_sclk = 0U;
  i = 0U;
  goto ldv_48483;
  ldv_48482: ;
  if ((table + (unsigned long )i)->ulSupportedSCLK > prev_sclk) {
    sclk_voltage_mapping_table->entries[n].sclk_frequency = (table + (unsigned long )i)->ulSupportedSCLK;
    sclk_voltage_mapping_table->entries[n].vid_2bit = (table + (unsigned long )i)->usVoltageIndex;
    prev_sclk = (table + (unsigned long )i)->ulSupportedSCLK;
    n = n + 1U;
  } else {
  }
  i = i + 1U;
  ldv_48483: ;
  if (i <= 4U) {
    goto ldv_48482;
  } else {
  }
  sclk_voltage_mapping_table->num_max_dpm_entries = n;
  return;
}
}
static void sumo_construct_vid_mapping_table(struct amdgpu_device *adev , struct sumo_vid_mapping_table *vid_mapping_table ,
                                             ATOM_AVAILABLE_SCLK_LIST *table )
{
  u32 i ;
  u32 j ;
  {
  i = 0U;
  goto ldv_48493;
  ldv_48492: ;
  if ((table + (unsigned long )i)->ulSupportedSCLK != 0U) {
    vid_mapping_table->entries[(int )(table + (unsigned long )i)->usVoltageIndex].vid_7bit = (table + (unsigned long )i)->usVoltageID;
    vid_mapping_table->entries[(int )(table + (unsigned long )i)->usVoltageIndex].vid_2bit = (table + (unsigned long )i)->usVoltageIndex;
  } else {
  }
  i = i + 1U;
  ldv_48493: ;
  if (i <= 4U) {
    goto ldv_48492;
  } else {
  }
  i = 0U;
  goto ldv_48500;
  ldv_48499: ;
  if ((unsigned int )vid_mapping_table->entries[i].vid_7bit == 0U) {
    j = i + 1U;
    goto ldv_48497;
    ldv_48496: ;
    if ((unsigned int )vid_mapping_table->entries[j].vid_7bit != 0U) {
      vid_mapping_table->entries[i] = vid_mapping_table->entries[j];
      vid_mapping_table->entries[j].vid_7bit = 0U;
      goto ldv_48495;
    } else {
    }
    j = j + 1U;
    ldv_48497: ;
    if (j <= 3U) {
      goto ldv_48496;
    } else {
    }
    ldv_48495: ;
    if (j == 4U) {
      goto ldv_48498;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_48500: ;
  if (i <= 3U) {
    goto ldv_48499;
  } else {
  }
  ldv_48498:
  vid_mapping_table->num_entries = i;
  return;
}
}
static struct kv_pt_config_reg const didt_config_kv[73U] =
  { {16U, 255U, 0U, 0U, 2},
        {16U, 65280U, 8U, 0U, 2},
        {16U, 16711680U, 16U, 0U, 2},
        {16U, 4278190080U, 24U, 0U, 2},
        {17U, 255U, 0U, 0U, 2},
        {17U, 65280U, 8U, 0U, 2},
        {17U, 16711680U, 16U, 0U, 2},
        {17U, 4278190080U, 24U, 0U, 2},
        {18U, 255U, 0U, 0U, 2},
        {18U, 65280U, 8U, 0U, 2},
        {18U, 16711680U, 16U, 0U, 2},
        {18U, 4278190080U, 24U, 0U, 2},
        {2U, 16383U, 0U, 4U, 2},
        {2U, 67043328U, 16U, 128U, 2},
        {2U, 2013265920U, 27U, 3U, 2},
        {1U, 65535U, 0U, 16383U, 2},
        {1U, 4294901760U, 16U, 16383U, 2},
        {0U, 1U, 0U, 0U, 2},
        {48U, 255U, 0U, 0U, 2},
        {48U, 65280U, 8U, 0U, 2},
        {48U, 16711680U, 16U, 0U, 2},
        {48U, 4278190080U, 24U, 0U, 2},
        {49U, 255U, 0U, 0U, 2},
        {49U, 65280U, 8U, 0U, 2},
        {49U, 16711680U, 16U, 0U, 2},
        {49U, 4278190080U, 24U, 0U, 2},
        {50U, 255U, 0U, 0U, 2},
        {50U, 65280U, 8U, 0U, 2},
        {50U, 16711680U, 16U, 0U, 2},
        {50U, 4278190080U, 24U, 0U, 2},
        {34U, 16383U, 0U, 4U, 2},
        {34U, 67043328U, 16U, 128U, 2},
        {34U, 2013265920U, 27U, 3U, 2},
        {33U, 65535U, 0U, 16383U, 2},
        {33U, 4294901760U, 16U, 16383U, 2},
        {32U, 1U, 0U, 0U, 2},
        {80U, 255U, 0U, 0U, 2},
        {80U, 65280U, 8U, 0U, 2},
        {80U, 16711680U, 16U, 0U, 2},
        {80U, 4278190080U, 24U, 0U, 2},
        {81U, 255U, 0U, 0U, 2},
        {81U, 65280U, 8U, 0U, 2},
        {81U, 16711680U, 16U, 0U, 2},
        {81U, 4278190080U, 24U, 0U, 2},
        {82U, 255U, 0U, 0U, 2},
        {82U, 65280U, 8U, 0U, 2},
        {82U, 16711680U, 16U, 0U, 2},
        {82U, 4278190080U, 24U, 0U, 2},
        {66U, 16383U, 0U, 4U, 2},
        {66U, 67043328U, 16U, 128U, 2},
        {66U, 2013265920U, 27U, 3U, 2},
        {65U, 65535U, 0U, 16383U, 2},
        {65U, 4294901760U, 16U, 16383U, 2},
        {64U, 1U, 0U, 0U, 2},
        {112U, 255U, 0U, 0U, 2},
        {112U, 65280U, 8U, 0U, 2},
        {112U, 16711680U, 16U, 0U, 2},
        {112U, 4278190080U, 24U, 0U, 2},
        {113U, 255U, 0U, 0U, 2},
        {113U, 65280U, 8U, 0U, 2},
        {113U, 16711680U, 16U, 0U, 2},
        {113U, 4278190080U, 24U, 0U, 2},
        {114U, 255U, 0U, 0U, 2},
        {114U, 65280U, 8U, 0U, 2},
        {114U, 16711680U, 16U, 0U, 2},
        {114U, 4278190080U, 24U, 0U, 2},
        {98U, 16383U, 0U, 4U, 2},
        {98U, 67043328U, 16U, 128U, 2},
        {98U, 2013265920U, 27U, 3U, 2},
        {97U, 65535U, 0U, 16383U, 2},
        {97U, 4294901760U, 16U, 16383U, 2},
        {96U, 1U, 0U, 0U, 2},
        {4294967295U, 0U, 0U, 0U, 0}};
static struct kv_ps *kv_get_ps(struct amdgpu_ps *rps )
{
  struct kv_ps *ps ;
  {
  ps = (struct kv_ps *)rps->ps_priv;
  return (ps);
}
}
static struct kv_power_info *kv_get_pi(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  {
  pi = (struct kv_power_info *)adev->pm.dpm.priv;
  return (pi);
}
}
static int kv_program_pt_config_registers(struct amdgpu_device *adev , struct kv_pt_config_reg const *cac_config_regs )
{
  struct kv_pt_config_reg const *config_regs ;
  u32 data ;
  u32 cache ;
  {
  config_regs = cac_config_regs;
  cache = 0U;
  if ((unsigned long )config_regs == (unsigned long )((struct kv_pt_config_reg const *)0)) {
    return (-22);
  } else {
  }
  goto ldv_48538;
  ldv_48537: ;
  if ((unsigned int )config_regs->type == 3U) {
    cache = ((unsigned int )(config_regs->value << (int )config_regs->shift) & (unsigned int )config_regs->mask) | cache;
  } else {
    switch ((unsigned int )config_regs->type) {
    case 1U:
    data = (*(adev->smc_rreg))(adev, config_regs->offset);
    goto ldv_48530;
    case 2U:
    data = (*(adev->didt_rreg))(adev, config_regs->offset);
    goto ldv_48530;
    default:
    data = amdgpu_mm_rreg(adev, config_regs->offset, 0);
    goto ldv_48530;
    }
    ldv_48530:
    data = (u32 )(~ config_regs->mask) & data;
    data = ((unsigned int )(config_regs->value << (int )config_regs->shift) & (unsigned int )config_regs->mask) | data;
    data = data | cache;
    cache = 0U;
    switch ((unsigned int )config_regs->type) {
    case 1U:
    (*(adev->smc_wreg))(adev, config_regs->offset, data);
    goto ldv_48534;
    case 2U:
    (*(adev->didt_wreg))(adev, config_regs->offset, data);
    goto ldv_48534;
    default:
    amdgpu_mm_wreg(adev, config_regs->offset, data, 0);
    goto ldv_48534;
    }
    ldv_48534: ;
  }
  config_regs = config_regs + 1;
  ldv_48538: ;
  if ((unsigned int )config_regs->offset != 4294967295U) {
    goto ldv_48537;
  } else {
  }
  return (0);
}
}
static void kv_do_enable_didt(struct amdgpu_device *adev , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 data ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_sq_ramping) {
    data = (*(adev->didt_rreg))(adev, 0U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 0U, data);
  } else {
  }
  if ((int )pi->caps_db_ramping) {
    data = (*(adev->didt_rreg))(adev, 32U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 32U, data);
  } else {
  }
  if ((int )pi->caps_td_ramping) {
    data = (*(adev->didt_rreg))(adev, 64U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 64U, data);
  } else {
  }
  if ((int )pi->caps_tcp_ramping) {
    data = (*(adev->didt_rreg))(adev, 96U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 96U, data);
  } else {
  }
  return;
}
}
static int kv_enable_didt(struct amdgpu_device *adev , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((((int )pi->caps_sq_ramping || (int )pi->caps_db_ramping) || (int )pi->caps_td_ramping) || (int )pi->caps_tcp_ramping) {
    gfx_v7_0_enter_rlc_safe_mode(adev);
    if ((int )enable) {
      ret = kv_program_pt_config_registers(adev, (struct kv_pt_config_reg const *)(& didt_config_kv));
      if (ret != 0) {
        gfx_v7_0_exit_rlc_safe_mode(adev);
        return (ret);
      } else {
      }
    } else {
    }
    kv_do_enable_didt(adev, (int )enable);
    gfx_v7_0_exit_rlc_safe_mode(adev);
  } else {
  }
  return (0);
}
}
static int kv_enable_smc_cac(struct amdgpu_device *adev , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )pi->caps_cac) {
    if ((int )enable) {
      ret = amdgpu_kv_notify_message_to_smu(adev, 83U);
      if (ret != 0) {
        pi->cac_enabled = 0;
      } else {
        pi->cac_enabled = 1;
      }
    } else
    if ((int )pi->cac_enabled) {
      amdgpu_kv_notify_message_to_smu(adev, 84U);
      pi->cac_enabled = 0;
    } else {
    }
  } else {
  }
  return (ret);
}
}
static int kv_process_firmware_header(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 tmp___0 ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = amdgpu_kv_read_smc_sram_dword(adev, 131124U, & tmp___0, pi->sram_end);
  if (ret == 0) {
    pi->dpm_table_start = tmp___0;
  } else {
  }
  ret = amdgpu_kv_read_smc_sram_dword(adev, 131120U, & tmp___0, pi->sram_end);
  if (ret == 0) {
    pi->soft_regs_start = tmp___0;
  } else {
  }
  return (ret);
}
}
static int kv_enable_dpm_voltage_scaling(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_voltage_change_enable = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 755U, (u8 const *)(& pi->graphics_voltage_change_enable),
                                    1U, pi->sram_end);
  return (ret);
}
}
static int kv_set_dpm_interval(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_interval = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 753U, (u8 const *)(& pi->graphics_interval),
                                    1U, pi->sram_end);
  return (ret);
}
}
static int kv_set_dpm_boot_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 752U, (u8 const *)(& pi->graphics_boot_level),
                                    1U, pi->sram_end);
  return (ret);
}
}
static void kv_program_vc(struct amdgpu_device *adev )
{
  {
  (*(adev->smc_wreg))(adev, 3223323048U, 1073725696U);
  return;
}
}
static void kv_clear_vc(struct amdgpu_device *adev )
{
  {
  (*(adev->smc_wreg))(adev, 3223323048U, 0U);
  return;
}
}
static int kv_set_divider_value(struct amdgpu_device *adev , u32 index , u32 sclk )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct atom_clock_dividers dividers ;
  int ret ;
  __u32 tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, sclk, 0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->graphics_level[index].SclkDid = (unsigned char )dividers.post_div;
  tmp___0 = __fswab32(sclk);
  pi->graphics_level[index].SclkFrequency = tmp___0;
  return (0);
}
}
static u16 kv_convert_8bit_index_to_voltage(struct amdgpu_device *adev , u16 voltage )
{
  {
  return ((unsigned int )voltage * 65511U + 6200U);
}
}
static u16 kv_convert_2bit_index_to_voltage(struct amdgpu_device *adev , u32 vid_2bit )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 vid_8bit ;
  u32 tmp___0 ;
  u16 tmp___1 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  tmp___0 = kv_convert_vid2_to_vid7(adev, & pi->sys_info.vid_mapping_table, vid_2bit);
  vid_8bit = tmp___0;
  tmp___1 = kv_convert_8bit_index_to_voltage(adev, (int )((unsigned short )vid_8bit));
  return (tmp___1);
}
}
static int kv_set_vid(struct amdgpu_device *adev , u32 index , u32 vid )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u16 tmp___0 ;
  __u32 tmp___1 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_level[index].VoltageDownH = (unsigned char )pi->voltage_drop_t;
  tmp___0 = kv_convert_2bit_index_to_voltage(adev, vid);
  tmp___1 = __fswab32((__u32 )tmp___0);
  pi->graphics_level[index].MinVddNb = tmp___1;
  return (0);
}
}
static int kv_set_at(struct amdgpu_device *adev , u32 index , u32 at )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  __u16 tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  tmp___0 = __fswab16((int )((unsigned short )at));
  pi->graphics_level[index].AT = tmp___0;
  return (0);
}
}
static void kv_dpm_power_level_enable(struct amdgpu_device *adev , u32 index , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_level[index].EnabledForActivity = (uint8_t )enable;
  return;
}
}
static void kv_start_dpm(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp = tmp___0;
  tmp = tmp | 1U;
  (*(adev->smc_wreg))(adev, 3223322624U, tmp);
  amdgpu_kv_smc_dpm_enable(adev, 1);
  return;
}
}
static void kv_stop_dpm(struct amdgpu_device *adev )
{
  {
  amdgpu_kv_smc_dpm_enable(adev, 0);
  return;
}
}
static void kv_start_am(struct amdgpu_device *adev )
{
  u32 sclk_pwrmgt_cntl ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 3223322632U);
  sclk_pwrmgt_cntl = tmp;
  sclk_pwrmgt_cntl = sclk_pwrmgt_cntl & 4294967247U;
  sclk_pwrmgt_cntl = sclk_pwrmgt_cntl | 2097152U;
  (*(adev->smc_wreg))(adev, 3223322632U, sclk_pwrmgt_cntl);
  return;
}
}
static void kv_reset_am(struct amdgpu_device *adev )
{
  u32 sclk_pwrmgt_cntl ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 3223322632U);
  sclk_pwrmgt_cntl = tmp;
  sclk_pwrmgt_cntl = sclk_pwrmgt_cntl | 48U;
  (*(adev->smc_wreg))(adev, 3223322632U, sclk_pwrmgt_cntl);
  return;
}
}
static int kv_freeze_sclk_dpm(struct amdgpu_device *adev , bool freeze )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )freeze ? 393U : 394U);
  return (tmp);
}
}
static int kv_force_lowest_valid(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = kv_force_dpm_lowest(adev);
  return (tmp);
}
}
static int kv_unforce_levels(struct amdgpu_device *adev )
{
  int tmp ;
  int tmp___0 ;
  {
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    tmp = amdgpu_kv_notify_message_to_smu(adev, 65U);
    return (tmp);
  } else {
    tmp___0 = kv_set_enabled_levels(adev);
    return (tmp___0);
  }
}
}
static int kv_update_sclk_t(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 low_sclk_interrupt_t ;
  int ret ;
  __u32 tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  low_sclk_interrupt_t = 0U;
  ret = 0;
  if ((int )pi->caps_sclk_throttle_low_notification) {
    tmp___0 = __fswab32(pi->low_sclk_interrupt_t);
    low_sclk_interrupt_t = tmp___0;
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 764U, (u8 const *)(& low_sclk_interrupt_t),
                                      4U, pi->sram_end);
  } else {
  }
  return (ret);
}
}
static int kv_program_bootup_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    i = (u32 )((int )pi->graphics_dpm_level_count + -1);
    goto ldv_48660;
    ldv_48659: ;
    if ((table->entries + (unsigned long )i)->clk == pi->boot_pl.sclk) {
      goto ldv_48658;
    } else {
    }
    i = i - 1U;
    ldv_48660: ;
    if (i != 0U) {
      goto ldv_48659;
    } else {
    }
    ldv_48658:
    pi->graphics_boot_level = (unsigned char )i;
    kv_dpm_power_level_enable(adev, i, 1);
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    if (table___0->num_max_dpm_entries == 0U) {
      return (-22);
    } else {
    }
    i = (u32 )((int )pi->graphics_dpm_level_count + -1);
    goto ldv_48664;
    ldv_48663: ;
    if (table___0->entries[i].sclk_frequency == pi->boot_pl.sclk) {
      goto ldv_48662;
    } else {
    }
    i = i - 1U;
    ldv_48664: ;
    if (i != 0U) {
      goto ldv_48663;
    } else {
    }
    ldv_48662:
    pi->graphics_boot_level = (unsigned char )i;
    kv_dpm_power_level_enable(adev, i, 1);
  }
  return (0);
}
}
static int kv_enable_auto_thermal_throttling(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_therm_throttle_enable = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 754U, (u8 const *)(& pi->graphics_therm_throttle_enable),
                                    1U, pi->sram_end);
  return (ret);
}
}
static int kv_upload_dpm_settings(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 84U, (u8 const *)(& pi->graphics_level),
                                    224U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 76U, (u8 const *)(& pi->graphics_dpm_level_count),
                                    1U, pi->sram_end);
  return (ret);
}
}
static u32 kv_get_clock_difference(u32 a , u32 b )
{
  {
  return (a >= b ? a - b : b - a);
}
}
static u32 kv_get_clk_bypass(struct amdgpu_device *adev , u32 clk )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 value ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_enable_dfs_bypass) {
    tmp___4 = kv_get_clock_difference(clk, 40000U);
    if (tmp___4 <= 199U) {
      value = 3U;
    } else {
      tmp___3 = kv_get_clock_difference(clk, 30000U);
      if (tmp___3 <= 199U) {
        value = 2U;
      } else {
        tmp___2 = kv_get_clock_difference(clk, 20000U);
        if (tmp___2 <= 199U) {
          value = 7U;
        } else {
          tmp___1 = kv_get_clock_difference(clk, 15000U);
          if (tmp___1 <= 199U) {
            value = 6U;
          } else {
            tmp___0 = kv_get_clock_difference(clk, 10000U);
            if (tmp___0 <= 199U) {
              value = 8U;
            } else {
              value = 0U;
            }
          }
        }
      }
    }
  } else {
    value = 0U;
  }
  return (value);
}
}
static int kv_populate_uvd_table(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_uvd_clock_voltage_dependency_table *table ;
  struct atom_clock_dividers dividers ;
  int ret ;
  u32 i ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  __u16 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_uvd_clock_voltage_dependency_table *)0) || (unsigned int )table->count == 0U) {
    return (0);
  } else {
  }
  pi->uvd_level_count = 0U;
  i = 0U;
  goto ldv_48695;
  ldv_48694: ;
  if ((unsigned int )pi->high_voltage_t != 0U && (int )pi->high_voltage_t < (int )(table->entries + (unsigned long )i)->v) {
    goto ldv_48693;
  } else {
  }
  tmp___0 = __fswab32((table->entries + (unsigned long )i)->vclk);
  pi->uvd_level[i].VclkFrequency = tmp___0;
  tmp___1 = __fswab32((table->entries + (unsigned long )i)->dclk);
  pi->uvd_level[i].DclkFrequency = tmp___1;
  tmp___2 = __fswab16((int )(table->entries + (unsigned long )i)->v);
  pi->uvd_level[i].MinVddNb = tmp___2;
  tmp___3 = kv_get_clk_bypass(adev, (table->entries + (unsigned long )i)->vclk);
  pi->uvd_level[i].VClkBypassCntl = (unsigned char )tmp___3;
  tmp___4 = kv_get_clk_bypass(adev, (table->entries + (unsigned long )i)->dclk);
  pi->uvd_level[i].DClkBypassCntl = (unsigned char )tmp___4;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, (table->entries + (unsigned long )i)->vclk,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->uvd_level[i].VclkDivider = (unsigned char )dividers.post_div;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, (table->entries + (unsigned long )i)->dclk,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->uvd_level[i].DclkDivider = (unsigned char )dividers.post_div;
  pi->uvd_level_count = (u8 )((int )pi->uvd_level_count + 1);
  i = i + 1U;
  ldv_48695: ;
  if ((u32 )table->count > i) {
    goto ldv_48694;
  } else {
  }
  ldv_48693:
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 78U, (u8 const *)(& pi->uvd_level_count),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->uvd_interval = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 748U, (u8 const *)(& pi->uvd_interval),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 328U, (u8 const *)(& pi->uvd_level),
                                    128U, pi->sram_end);
  return (ret);
}
}
static int kv_populate_vce_table(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  u32 i ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  struct atom_clock_dividers dividers ;
  __u32 tmp___0 ;
  __u16 tmp___1 ;
  u32 tmp___2 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_vce_clock_voltage_dependency_table *)0) || (unsigned int )table->count == 0U) {
    return (0);
  } else {
  }
  pi->vce_level_count = 0U;
  i = 0U;
  goto ldv_48706;
  ldv_48705: ;
  if ((unsigned int )pi->high_voltage_t != 0U && (int )pi->high_voltage_t < (int )(table->entries + (unsigned long )i)->v) {
    goto ldv_48704;
  } else {
  }
  tmp___0 = __fswab32((table->entries + (unsigned long )i)->evclk);
  pi->vce_level[i].Frequency = tmp___0;
  tmp___1 = __fswab16((int )(table->entries + (unsigned long )i)->v);
  pi->vce_level[i].MinVoltage = tmp___1;
  tmp___2 = kv_get_clk_bypass(adev, (table->entries + (unsigned long )i)->evclk);
  pi->vce_level[i].ClkBypassCntl = (unsigned char )tmp___2;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, (table->entries + (unsigned long )i)->evclk,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->vce_level[i].Divider = (unsigned char )dividers.post_div;
  pi->vce_level_count = (u8 )((int )pi->vce_level_count + 1);
  i = i + 1U;
  ldv_48706: ;
  if ((u32 )table->count > i) {
    goto ldv_48705;
  } else {
  }
  ldv_48704:
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 79U, (u8 const *)(& pi->vce_level_count),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->vce_interval = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 749U, (u8 const *)(& pi->vce_interval),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 456U, (u8 const *)(& pi->vce_level),
                                    96U, pi->sram_end);
  return (ret);
}
}
static int kv_populate_samu_table(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  struct atom_clock_dividers dividers ;
  int ret ;
  u32 i ;
  __u32 tmp___0 ;
  __u16 tmp___1 ;
  u32 tmp___2 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) || table->count == 0U) {
    return (0);
  } else {
  }
  pi->samu_level_count = 0U;
  i = 0U;
  goto ldv_48717;
  ldv_48716: ;
  if ((unsigned int )pi->high_voltage_t != 0U && (int )pi->high_voltage_t < (int )(table->entries + (unsigned long )i)->v) {
    goto ldv_48715;
  } else {
  }
  tmp___0 = __fswab32((table->entries + (unsigned long )i)->clk);
  pi->samu_level[i].Frequency = tmp___0;
  tmp___1 = __fswab16((int )(table->entries + (unsigned long )i)->v);
  pi->samu_level[i].MinVoltage = tmp___1;
  tmp___2 = kv_get_clk_bypass(adev, (table->entries + (unsigned long )i)->clk);
  pi->samu_level[i].ClkBypassCntl = (unsigned char )tmp___2;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, (table->entries + (unsigned long )i)->clk,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->samu_level[i].Divider = (unsigned char )dividers.post_div;
  pi->samu_level_count = (u8 )((int )pi->samu_level_count + 1);
  i = i + 1U;
  ldv_48717: ;
  if (table->count > i) {
    goto ldv_48716;
  } else {
  }
  ldv_48715:
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 81U, (u8 const *)(& pi->samu_level_count),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->samu_interval = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 751U, (u8 const *)(& pi->samu_interval),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 648U, (u8 const *)(& pi->samu_level),
                                    96U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int kv_populate_acp_table(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  struct atom_clock_dividers dividers ;
  int ret ;
  u32 i ;
  __u32 tmp___0 ;
  __u16 tmp___1 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) || table->count == 0U) {
    return (0);
  } else {
  }
  pi->acp_level_count = 0U;
  i = 0U;
  goto ldv_48727;
  ldv_48726:
  tmp___0 = __fswab32((table->entries + (unsigned long )i)->clk);
  pi->acp_level[i].Frequency = tmp___0;
  tmp___1 = __fswab16((int )(table->entries + (unsigned long )i)->v);
  pi->acp_level[i].MinVoltage = tmp___1;
  ret = amdgpu_atombios_get_clock_dividers(adev, 2, (table->entries + (unsigned long )i)->clk,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->acp_level[i].Divider = (unsigned char )dividers.post_div;
  pi->acp_level_count = (u8 )((int )pi->acp_level_count + 1);
  i = i + 1U;
  ldv_48727: ;
  if (table->count > i) {
    goto ldv_48726;
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 80U, (u8 const *)(& pi->acp_level_count),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->acp_interval = 1U;
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 750U, (u8 const *)(& pi->acp_interval),
                                    1U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 552U, (u8 const *)(& pi->acp_level),
                                    96U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static void kv_calculate_dfs_bypass_settings(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    i = 0U;
    goto ldv_48736;
    ldv_48735: ;
    if ((int )pi->caps_enable_dfs_bypass) {
      tmp___4 = kv_get_clock_difference((table->entries + (unsigned long )i)->clk,
                                        40000U);
      if (tmp___4 <= 199U) {
        pi->graphics_level[i].ClkBypassCntl = 3U;
      } else {
        tmp___3 = kv_get_clock_difference((table->entries + (unsigned long )i)->clk,
                                          30000U);
        if (tmp___3 <= 199U) {
          pi->graphics_level[i].ClkBypassCntl = 2U;
        } else {
          tmp___2 = kv_get_clock_difference((table->entries + (unsigned long )i)->clk,
                                            26600U);
          if (tmp___2 <= 199U) {
            pi->graphics_level[i].ClkBypassCntl = 7U;
          } else {
            tmp___1 = kv_get_clock_difference((table->entries + (unsigned long )i)->clk,
                                              20000U);
            if (tmp___1 <= 199U) {
              pi->graphics_level[i].ClkBypassCntl = 6U;
            } else {
              tmp___0 = kv_get_clock_difference((table->entries + (unsigned long )i)->clk,
                                                10000U);
              if (tmp___0 <= 199U) {
                pi->graphics_level[i].ClkBypassCntl = 8U;
              } else {
                pi->graphics_level[i].ClkBypassCntl = 0U;
              }
            }
          }
        }
      }
    } else {
      pi->graphics_level[i].ClkBypassCntl = 0U;
    }
    i = i + 1U;
    ldv_48736: ;
    if ((u32 )pi->graphics_dpm_level_count > i) {
      goto ldv_48735;
    } else {
    }
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    i = 0U;
    goto ldv_48740;
    ldv_48739: ;
    if ((int )pi->caps_enable_dfs_bypass) {
      tmp___9 = kv_get_clock_difference(table___0->entries[i].sclk_frequency, 40000U);
      if (tmp___9 <= 199U) {
        pi->graphics_level[i].ClkBypassCntl = 3U;
      } else {
        tmp___8 = kv_get_clock_difference(table___0->entries[i].sclk_frequency, 30000U);
        if (tmp___8 <= 199U) {
          pi->graphics_level[i].ClkBypassCntl = 2U;
        } else {
          tmp___7 = kv_get_clock_difference(table___0->entries[i].sclk_frequency,
                                            26600U);
          if (tmp___7 <= 199U) {
            pi->graphics_level[i].ClkBypassCntl = 7U;
          } else {
            tmp___6 = kv_get_clock_difference(table___0->entries[i].sclk_frequency,
                                              20000U);
            if (tmp___6 <= 199U) {
              pi->graphics_level[i].ClkBypassCntl = 6U;
            } else {
              tmp___5 = kv_get_clock_difference(table___0->entries[i].sclk_frequency,
                                                10000U);
              if (tmp___5 <= 199U) {
                pi->graphics_level[i].ClkBypassCntl = 8U;
              } else {
                pi->graphics_level[i].ClkBypassCntl = 0U;
              }
            }
          }
        }
      }
    } else {
      pi->graphics_level[i].ClkBypassCntl = 0U;
    }
    i = i + 1U;
    ldv_48740: ;
    if ((u32 )pi->graphics_dpm_level_count > i) {
      goto ldv_48739;
    } else {
    }
  }
  return;
}
}
static int kv_enable_ulv(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )enable ? 98U : 99U);
  return (tmp);
}
}
static void kv_reset_acp_boot_level(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->acp_boot_level = 255U;
  return;
}
}
static void kv_update_current_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct kv_ps *new_ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  {
  tmp = kv_get_ps(rps);
  new_ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  pi->current_rps = *rps;
  pi->current_ps = *new_ps;
  pi->current_rps.ps_priv = (void *)(& pi->current_ps);
  return;
}
}
static void kv_update_requested_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct kv_ps *new_ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  {
  tmp = kv_get_ps(rps);
  new_ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  pi->requested_rps = *rps;
  pi->requested_ps = *new_ps;
  pi->requested_rps.ps_priv = (void *)(& pi->requested_ps);
  return;
}
}
static void kv_dpm_enable_bapm(struct amdgpu_device *adev , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->bapm_enable) {
    ret = amdgpu_kv_smc_bapm_enable(adev, (int )enable);
    if (ret != 0) {
      drm_err("amdgpu_kv_smc_bapm_enable failed\n");
    } else {
    }
  } else {
  }
  return;
}
}
static int kv_dpm_enable(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  bool tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = kv_process_firmware_header(adev);
  if (ret != 0) {
    drm_err("kv_process_firmware_header failed\n");
    return (ret);
  } else {
  }
  kv_init_fps_limits(adev);
  kv_init_graphics_levels(adev);
  ret = kv_program_bootup_state(adev);
  if (ret != 0) {
    drm_err("kv_program_bootup_state failed\n");
    return (ret);
  } else {
  }
  kv_calculate_dfs_bypass_settings(adev);
  ret = kv_upload_dpm_settings(adev);
  if (ret != 0) {
    drm_err("kv_upload_dpm_settings failed\n");
    return (ret);
  } else {
  }
  ret = kv_populate_uvd_table(adev);
  if (ret != 0) {
    drm_err("kv_populate_uvd_table failed\n");
    return (ret);
  } else {
  }
  ret = kv_populate_vce_table(adev);
  if (ret != 0) {
    drm_err("kv_populate_vce_table failed\n");
    return (ret);
  } else {
  }
  ret = kv_populate_samu_table(adev);
  if (ret != 0) {
    drm_err("kv_populate_samu_table failed\n");
    return (ret);
  } else {
  }
  ret = kv_populate_acp_table(adev);
  if (ret != 0) {
    drm_err("kv_populate_acp_table failed\n");
    return (ret);
  } else {
  }
  kv_program_vc(adev);
  kv_start_am(adev);
  if ((int )pi->enable_auto_thermal_throttling) {
    ret = kv_enable_auto_thermal_throttling(adev);
    if (ret != 0) {
      drm_err("kv_enable_auto_thermal_throttling failed\n");
      return (ret);
    } else {
    }
  } else {
  }
  ret = kv_enable_dpm_voltage_scaling(adev);
  if (ret != 0) {
    drm_err("kv_enable_dpm_voltage_scaling failed\n");
    return (ret);
  } else {
  }
  ret = kv_set_dpm_interval(adev);
  if (ret != 0) {
    drm_err("kv_set_dpm_interval failed\n");
    return (ret);
  } else {
  }
  ret = kv_set_dpm_boot_state(adev);
  if (ret != 0) {
    drm_err("kv_set_dpm_boot_state failed\n");
    return (ret);
  } else {
  }
  ret = kv_enable_ulv(adev, 1);
  if (ret != 0) {
    drm_err("kv_enable_ulv failed\n");
    return (ret);
  } else {
  }
  kv_start_dpm(adev);
  ret = kv_enable_didt(adev, 1);
  if (ret != 0) {
    drm_err("kv_enable_didt failed\n");
    return (ret);
  } else {
  }
  ret = kv_enable_smc_cac(adev, 1);
  if (ret != 0) {
    drm_err("kv_enable_smc_cac failed\n");
    return (ret);
  } else {
  }
  kv_reset_acp_boot_level(adev);
  ret = amdgpu_kv_smc_bapm_enable(adev, 0);
  if (ret != 0) {
    drm_err("amdgpu_kv_smc_bapm_enable failed\n");
    return (ret);
  } else {
  }
  kv_update_current_ps(adev, adev->pm.dpm.boot_ps);
  if ((int )adev->irq.installed) {
    tmp___0 = amdgpu_is_internal_thermal_sensor(adev->pm.int_thermal_type);
    if ((int )tmp___0) {
      ret = kv_set_thermal_temperature_range(adev, 90000, 120000);
      if (ret != 0) {
        drm_err("kv_set_thermal_temperature_range failed\n");
        return (ret);
      } else {
      }
      amdgpu_irq_get(adev, & adev->pm.dpm.thermal.irq, 0U);
      amdgpu_irq_get(adev, & adev->pm.dpm.thermal.irq, 1U);
    } else {
    }
  } else {
  }
  return (ret);
}
}
static void kv_dpm_disable(struct amdgpu_device *adev )
{
  {
  amdgpu_irq_put(adev, & adev->pm.dpm.thermal.irq, 0U);
  amdgpu_irq_put(adev, & adev->pm.dpm.thermal.irq, 1U);
  amdgpu_kv_smc_bapm_enable(adev, 0);
  if ((unsigned int )adev->asic_type == 4U) {
    kv_enable_nb_dpm(adev, 0);
  } else {
  }
  kv_dpm_powergate_acp(adev, 0);
  kv_dpm_powergate_samu(adev, 0);
  kv_dpm_powergate_vce(adev, 0);
  kv_dpm_powergate_uvd(adev, 0);
  kv_enable_smc_cac(adev, 0);
  kv_enable_didt(adev, 0);
  kv_clear_vc(adev);
  kv_stop_dpm(adev);
  kv_enable_ulv(adev, 0);
  kv_reset_am(adev);
  kv_update_current_ps(adev, adev->pm.dpm.boot_ps);
  return;
}
}
static void kv_init_sclk_t(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->low_sclk_interrupt_t = 0U;
  return;
}
}
static int kv_init_fps_limits(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  u16 tmp___0 ;
  __u16 tmp___1 ;
  __u16 tmp___2 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )pi->caps_fps) {
    tmp___0 = 45U;
    tmp___1 = __fswab16((int )tmp___0);
    pi->fps_high_t = tmp___1;
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 82U, (u8 const *)(& pi->fps_high_t),
                                      2U, pi->sram_end);
    tmp___0 = 30U;
    tmp___2 = __fswab16((int )tmp___0);
    pi->fps_low_t = (u8 )tmp___2;
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 758U, (u8 const *)(& pi->fps_low_t),
                                      2U, pi->sram_end);
  } else {
  }
  return (ret);
}
}
static void kv_init_powergate_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->uvd_power_gated = 0;
  pi->vce_power_gated = 0;
  pi->samu_power_gated = 0;
  pi->acp_power_gated = 0;
  return;
}
}
static int kv_enable_uvd_dpm(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )enable ? 340U : 341U);
  return (tmp);
}
}
static int kv_enable_vce_dpm(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )enable ? 346U : 347U);
  return (tmp);
}
}
static int kv_enable_samu_dpm(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )enable ? 342U : 343U);
  return (tmp);
}
}
static int kv_enable_acp_dpm(struct amdgpu_device *adev , bool enable )
{
  int tmp ;
  {
  tmp = amdgpu_kv_notify_message_to_smu(adev, (int )enable ? 344U : 345U);
  return (tmp);
}
}
static int kv_update_uvd_dpm(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_uvd_clock_voltage_dependency_table *table ;
  int ret ;
  u32 mask ;
  int tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  if (! gate) {
    if ((unsigned int )table->count != 0U) {
      pi->uvd_boot_level = (unsigned int )table->count + 255U;
    } else {
      pi->uvd_boot_level = 0U;
    }
    if (! pi->caps_uvd_dpm || (int )pi->caps_stable_p_state) {
      mask = (u32 )(1 << (int )pi->uvd_boot_level);
    } else {
      mask = 31U;
    }
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 744U, (u8 const *)(& pi->uvd_boot_level),
                                      1U, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
    amdgpu_kv_send_msg_to_smc_with_parameter(adev, 301, mask);
  } else {
  }
  tmp___0 = kv_enable_uvd_dpm(adev, (int )((bool )(! ((int )gate != 0))));
  return (tmp___0);
}
}
static u8 kv_get_vce_boot_level(struct amdgpu_device *adev , u32 evclk )
{
  u8 i ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  {
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  i = 0U;
  goto ldv_48822;
  ldv_48821: ;
  if ((table->entries + (unsigned long )i)->evclk >= evclk) {
    goto ldv_48820;
  } else {
  }
  i = (u8 )((int )i + 1);
  ldv_48822: ;
  if ((int )table->count > (int )i) {
    goto ldv_48821;
  } else {
  }
  ldv_48820: ;
  return (i);
}
}
static int kv_update_vce_dpm(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_new_state ,
                             struct amdgpu_ps *amdgpu_current_state )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  if (amdgpu_new_state->evclk != 0U && amdgpu_current_state->evclk == 0U) {
    kv_dpm_powergate_vce(adev, 0);
    ret = amdgpu_set_clockgating_state(adev, 8, 1);
    if (ret != 0) {
      return (ret);
    } else {
    }
    if ((int )pi->caps_stable_p_state) {
      pi->vce_boot_level = (unsigned int )table->count + 255U;
    } else {
      pi->vce_boot_level = kv_get_vce_boot_level(adev, amdgpu_new_state->evclk);
    }
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 745U, (u8 const *)(& pi->vce_boot_level),
                                      1U, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
    if ((int )pi->caps_stable_p_state) {
      amdgpu_kv_send_msg_to_smc_with_parameter(adev, 302, (u32 )(1 << (int )pi->vce_boot_level));
    } else {
    }
    kv_enable_vce_dpm(adev, 1);
  } else
  if (amdgpu_new_state->evclk == 0U && amdgpu_current_state->evclk != 0U) {
    kv_enable_vce_dpm(adev, 0);
    ret = amdgpu_set_clockgating_state(adev, 8, 0);
    if (ret != 0) {
      return (ret);
    } else {
    }
    kv_dpm_powergate_vce(adev, 1);
  } else {
  }
  return (0);
}
}
static int kv_update_samu_dpm(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  int ret ;
  int tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;
  if (! gate) {
    if ((int )pi->caps_stable_p_state) {
      pi->samu_boot_level = (unsigned int )((u8 )table->count) - 1U;
    } else {
      pi->samu_boot_level = 0U;
    }
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 747U, (u8 const *)(& pi->samu_boot_level),
                                      1U, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
    if ((int )pi->caps_stable_p_state) {
      amdgpu_kv_send_msg_to_smc_with_parameter(adev, 304, (u32 )(1 << (int )pi->samu_boot_level));
    } else {
    }
  } else {
  }
  tmp___0 = kv_enable_samu_dpm(adev, (int )((bool )(! ((int )gate != 0))));
  return (tmp___0);
}
}
static u8 kv_get_acp_boot_level(struct amdgpu_device *adev )
{
  u8 i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  {
  table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  i = 0U;
  goto ldv_48845;
  ldv_48844: ;
  goto ldv_48843;
  i = (u8 )((int )i + 1);
  ldv_48845: ;
  if ((u32 )i < table->count) {
    goto ldv_48844;
  } else {
  }
  ldv_48843: ;
  if ((u32 )i >= table->count) {
    i = (unsigned int )((u8 )table->count) - 1U;
  } else {
  }
  return (i);
}
}
static void kv_update_acp_boot_level(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u8 acp_boot_level ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if (! pi->caps_stable_p_state) {
    acp_boot_level = kv_get_acp_boot_level(adev);
    if ((int )pi->acp_boot_level != (int )acp_boot_level) {
      pi->acp_boot_level = acp_boot_level;
      amdgpu_kv_send_msg_to_smc_with_parameter(adev, 303, (u32 )(1 << (int )pi->acp_boot_level));
    } else {
    }
  } else {
  }
  return;
}
}
static int kv_update_acp_dpm(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  int ret ;
  int tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  if (! gate) {
    if ((int )pi->caps_stable_p_state) {
      pi->acp_boot_level = (unsigned int )((u8 )table->count) - 1U;
    } else {
      pi->acp_boot_level = kv_get_acp_boot_level(adev);
    }
    ret = amdgpu_kv_copy_bytes_to_smc(adev, pi->dpm_table_start + 746U, (u8 const *)(& pi->acp_boot_level),
                                      1U, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
    if ((int )pi->caps_stable_p_state) {
      amdgpu_kv_send_msg_to_smc_with_parameter(adev, 303, (u32 )(1 << (int )pi->acp_boot_level));
    } else {
    }
  } else {
  }
  tmp___0 = kv_enable_acp_dpm(adev, (int )((bool )(! ((int )gate != 0))));
  return (tmp___0);
}
}
static void kv_dpm_powergate_uvd(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->uvd_power_gated == (int )gate) {
    return;
  } else {
  }
  pi->uvd_power_gated = gate;
  if ((int )gate) {
    if ((int )pi->caps_uvd_pg) {
      ret = amdgpu_set_clockgating_state(adev, 7, 1);
      ret = amdgpu_set_powergating_state(adev, 7, 0);
    } else {
    }
    kv_update_uvd_dpm(adev, (int )gate);
    if ((int )pi->caps_uvd_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 96U);
    } else {
    }
  } else {
    if ((int )pi->caps_uvd_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 97U);
      ret = amdgpu_set_powergating_state(adev, 7, 1);
      ret = amdgpu_set_clockgating_state(adev, 7, 0);
    } else {
    }
    kv_update_uvd_dpm(adev, (int )gate);
  }
  return;
}
}
static void kv_dpm_powergate_vce(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->vce_power_gated == (int )gate) {
    return;
  } else {
  }
  pi->vce_power_gated = gate;
  if ((int )gate) {
    if ((int )pi->caps_vce_pg) {
      ret = amdgpu_set_powergating_state(adev, 8, 0);
      amdgpu_kv_notify_message_to_smu(adev, 270U);
    } else {
    }
  } else
  if ((int )pi->caps_vce_pg) {
    amdgpu_kv_notify_message_to_smu(adev, 271U);
    ret = amdgpu_set_powergating_state(adev, 8, 1);
  } else {
  }
  return;
}
}
static void kv_dpm_powergate_samu(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->samu_power_gated == (int )gate) {
    return;
  } else {
  }
  pi->samu_power_gated = gate;
  if ((int )gate) {
    kv_update_samu_dpm(adev, 1);
    if ((int )pi->caps_samu_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 313U);
    } else {
    }
  } else {
    if ((int )pi->caps_samu_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 314U);
    } else {
    }
    kv_update_samu_dpm(adev, 0);
  }
  return;
}
}
static void kv_dpm_powergate_acp(struct amdgpu_device *adev , bool gate )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if ((int )pi->acp_power_gated == (int )gate) {
    return;
  } else {
  }
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    return;
  } else {
  }
  pi->acp_power_gated = gate;
  if ((int )gate) {
    kv_update_acp_dpm(adev, 1);
    if ((int )pi->caps_acp_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 311U);
    } else {
    }
  } else {
    if ((int )pi->caps_acp_pg) {
      amdgpu_kv_notify_message_to_smu(adev, 312U);
    } else {
    }
    kv_update_acp_dpm(adev, 0);
  }
  return;
}
}
static void kv_set_valid_clock_range(struct amdgpu_device *adev , struct amdgpu_ps *new_rps )
{
  struct kv_ps *new_ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  u32 i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  {
  tmp = kv_get_ps(new_rps);
  new_ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    i = 0U;
    goto ldv_48890;
    ldv_48889: ;
    if ((table->entries + (unsigned long )i)->clk >= new_ps->levels[0].sclk || (u32 )((int )pi->graphics_dpm_level_count + -1) == i) {
      pi->lowest_valid = i;
      goto ldv_48888;
    } else {
    }
    i = i + 1U;
    ldv_48890: ;
    if ((u32 )pi->graphics_dpm_level_count > i) {
      goto ldv_48889;
    } else {
    }
    ldv_48888:
    i = (u32 )((int )pi->graphics_dpm_level_count + -1);
    goto ldv_48893;
    ldv_48892: ;
    if ((table->entries + (unsigned long )i)->clk <= new_ps->levels[new_ps->num_levels - 1U].sclk) {
      goto ldv_48891;
    } else {
    }
    i = i - 1U;
    ldv_48893: ;
    if (i != 0U) {
      goto ldv_48892;
    } else {
    }
    ldv_48891:
    pi->highest_valid = i;
    if (pi->lowest_valid > pi->highest_valid) {
      if (new_ps->levels[0].sclk - (table->entries + (unsigned long )pi->highest_valid)->clk > (table->entries + (unsigned long )pi->lowest_valid)->clk - new_ps->levels[new_ps->num_levels - 1U].sclk) {
        pi->highest_valid = pi->lowest_valid;
      } else {
        pi->lowest_valid = pi->highest_valid;
      }
    } else {
    }
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    i = 0U;
    goto ldv_48897;
    ldv_48896: ;
    if (table___0->entries[i].sclk_frequency >= new_ps->levels[0].sclk || (u32 )((int )pi->graphics_dpm_level_count + -1) == i) {
      pi->lowest_valid = i;
      goto ldv_48895;
    } else {
    }
    i = i + 1U;
    ldv_48897: ;
    if ((u32 )pi->graphics_dpm_level_count > i) {
      goto ldv_48896;
    } else {
    }
    ldv_48895:
    i = (u32 )((int )pi->graphics_dpm_level_count + -1);
    goto ldv_48900;
    ldv_48899: ;
    if (table___0->entries[i].sclk_frequency <= new_ps->levels[new_ps->num_levels - 1U].sclk) {
      goto ldv_48898;
    } else {
    }
    i = i - 1U;
    ldv_48900: ;
    if (i != 0U) {
      goto ldv_48899;
    } else {
    }
    ldv_48898:
    pi->highest_valid = i;
    if (pi->lowest_valid > pi->highest_valid) {
      if (new_ps->levels[0].sclk - table___0->entries[pi->highest_valid].sclk_frequency > table___0->entries[pi->lowest_valid].sclk_frequency - new_ps->levels[new_ps->num_levels - 1U].sclk) {
        pi->highest_valid = pi->lowest_valid;
      } else {
        pi->lowest_valid = pi->highest_valid;
      }
    } else {
    }
  }
  return;
}
}
static int kv_update_dfs_bypass_settings(struct amdgpu_device *adev , struct amdgpu_ps *new_rps )
{
  struct kv_ps *new_ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  int ret ;
  u8 clk_bypass_cntl ;
  {
  tmp = kv_get_ps(new_rps);
  new_ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  ret = 0;
  if ((int )pi->caps_enable_dfs_bypass) {
    clk_bypass_cntl = (int )new_ps->need_dfs_bypass ? pi->graphics_level[(int )pi->graphics_boot_level].ClkBypassCntl : 0U;
    ret = amdgpu_kv_copy_bytes_to_smc(adev, (pi->dpm_table_start + (u32 )pi->graphics_boot_level * 28U) + 107U,
                                      (u8 const *)(& clk_bypass_cntl), 1U, pi->sram_end);
  } else {
  }
  return (ret);
}
}
static int kv_enable_nb_dpm(struct amdgpu_device *adev , bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )enable) {
    if ((int )pi->enable_nb_dpm && ! pi->nb_dpm_enabled) {
      ret = amdgpu_kv_notify_message_to_smu(adev, 320U);
      if (ret == 0) {
        pi->nb_dpm_enabled = 1;
      } else {
      }
    } else {
    }
  } else
  if ((int )pi->enable_nb_dpm && (int )pi->nb_dpm_enabled) {
    ret = amdgpu_kv_notify_message_to_smu(adev, 321U);
    if (ret == 0) {
      pi->nb_dpm_enabled = 0;
    } else {
    }
  } else {
  }
  return (ret);
}
}
static int kv_dpm_force_performance_level(struct amdgpu_device *adev , enum amdgpu_dpm_forced_level level )
{
  int ret ;
  {
  if ((unsigned int )level == 2U) {
    ret = kv_force_dpm_highest(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else
  if ((unsigned int )level == 1U) {
    ret = kv_force_dpm_lowest(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else
  if ((unsigned int )level == 0U) {
    ret = kv_unforce_levels(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  adev->pm.dpm.forced_level = level;
  return (0);
}
}
static int kv_dpm_pre_set_power_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_ps requested_ps ;
  struct amdgpu_ps *new_ps ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  requested_ps = *(adev->pm.dpm.requested_ps);
  new_ps = & requested_ps;
  kv_update_requested_ps(adev, new_ps);
  kv_apply_state_adjust_rules(adev, & pi->requested_rps, & pi->current_rps);
  return (0);
}
}
static int kv_dpm_set_power_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_ps *new_ps ;
  struct amdgpu_ps *old_ps ;
  int ret ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  new_ps = & pi->requested_rps;
  old_ps = & pi->current_rps;
  if ((int )pi->bapm_enable) {
    ret = amdgpu_kv_smc_bapm_enable(adev, (int )adev->pm.dpm.ac_power);
    if (ret != 0) {
      drm_err("amdgpu_kv_smc_bapm_enable failed\n");
      return (ret);
    } else {
    }
  } else {
  }
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    if ((int )pi->enable_dpm) {
      kv_set_valid_clock_range(adev, new_ps);
      kv_update_dfs_bypass_settings(adev, new_ps);
      ret = kv_calculate_ds_divider(adev);
      if (ret != 0) {
        drm_err("kv_calculate_ds_divider failed\n");
        return (ret);
      } else {
      }
      kv_calculate_nbps_level_settings(adev);
      kv_calculate_dpm_settings(adev);
      kv_force_lowest_valid(adev);
      kv_enable_new_levels(adev);
      kv_upload_dpm_settings(adev);
      kv_program_nbps_index_settings(adev, new_ps);
      kv_unforce_levels(adev);
      kv_set_enabled_levels(adev);
      kv_force_lowest_valid(adev);
      kv_unforce_levels(adev);
      ret = kv_update_vce_dpm(adev, new_ps, old_ps);
      if (ret != 0) {
        drm_err("kv_update_vce_dpm failed\n");
        return (ret);
      } else {
      }
      kv_update_sclk_t(adev);
      if ((unsigned int )adev->asic_type == 4U) {
        kv_enable_nb_dpm(adev, 1);
      } else {
      }
    } else {
    }
  } else
  if ((int )pi->enable_dpm) {
    kv_set_valid_clock_range(adev, new_ps);
    kv_update_dfs_bypass_settings(adev, new_ps);
    ret = kv_calculate_ds_divider(adev);
    if (ret != 0) {
      drm_err("kv_calculate_ds_divider failed\n");
      return (ret);
    } else {
    }
    kv_calculate_nbps_level_settings(adev);
    kv_calculate_dpm_settings(adev);
    kv_freeze_sclk_dpm(adev, 1);
    kv_upload_dpm_settings(adev);
    kv_program_nbps_index_settings(adev, new_ps);
    kv_freeze_sclk_dpm(adev, 0);
    kv_set_enabled_levels(adev);
    ret = kv_update_vce_dpm(adev, new_ps, old_ps);
    if (ret != 0) {
      drm_err("kv_update_vce_dpm failed\n");
      return (ret);
    } else {
    }
    kv_update_acp_boot_level(adev);
    kv_update_sclk_t(adev);
    kv_enable_nb_dpm(adev, 1);
  } else {
  }
  return (0);
}
}
static void kv_dpm_post_set_power_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_ps *new_ps ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  new_ps = & pi->requested_rps;
  kv_update_current_ps(adev, new_ps);
  return;
}
}
static void kv_dpm_setup_asic(struct amdgpu_device *adev )
{
  {
  sumo_take_smu_control(adev, 1);
  kv_init_powergate_state(adev);
  kv_init_sclk_t(adev);
  return;
}
}
static void kv_construct_max_power_limits_table(struct amdgpu_device *adev , struct amdgpu_clock_and_voltage_limits *table )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  int idx ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if (pi->sys_info.sclk_voltage_mapping_table.num_max_dpm_entries != 0U) {
    idx = (int )(pi->sys_info.sclk_voltage_mapping_table.num_max_dpm_entries - 1U);
    table->sclk = pi->sys_info.sclk_voltage_mapping_table.entries[idx].sclk_frequency;
    table->vddc = kv_convert_2bit_index_to_voltage(adev, (u32 )pi->sys_info.sclk_voltage_mapping_table.entries[idx].vid_2bit);
  } else {
  }
  table->mclk = pi->sys_info.nbp_memory_clock[0];
  return;
}
}
static void kv_patch_voltage_values(struct amdgpu_device *adev )
{
  int i ;
  struct amdgpu_uvd_clock_voltage_dependency_table *uvd_table ;
  struct amdgpu_vce_clock_voltage_dependency_table *vce_table ;
  struct amdgpu_clock_voltage_dependency_table *samu_table ;
  struct amdgpu_clock_voltage_dependency_table *acp_table ;
  {
  uvd_table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  vce_table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  samu_table = & adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table;
  acp_table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  if ((unsigned int )uvd_table->count != 0U) {
    i = 0;
    goto ldv_48956;
    ldv_48955:
    (uvd_table->entries + (unsigned long )i)->v = kv_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(uvd_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48956: ;
    if ((int )uvd_table->count > i) {
      goto ldv_48955;
    } else {
    }
  } else {
  }
  if ((unsigned int )vce_table->count != 0U) {
    i = 0;
    goto ldv_48959;
    ldv_48958:
    (vce_table->entries + (unsigned long )i)->v = kv_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(vce_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48959: ;
    if ((int )vce_table->count > i) {
      goto ldv_48958;
    } else {
    }
  } else {
  }
  if (samu_table->count != 0U) {
    i = 0;
    goto ldv_48962;
    ldv_48961:
    (samu_table->entries + (unsigned long )i)->v = kv_convert_8bit_index_to_voltage(adev,
                                                                                    (int )(samu_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48962: ;
    if ((u32 )i < samu_table->count) {
      goto ldv_48961;
    } else {
    }
  } else {
  }
  if (acp_table->count != 0U) {
    i = 0;
    goto ldv_48965;
    ldv_48964:
    (acp_table->entries + (unsigned long )i)->v = kv_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(acp_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48965: ;
    if ((u32 )i < acp_table->count) {
      goto ldv_48964;
    } else {
    }
  } else {
  }
  return;
}
}
static void kv_construct_boot_state(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->boot_pl.sclk = pi->sys_info.bootup_sclk;
  pi->boot_pl.vddc_index = (u8 )pi->sys_info.bootup_nb_voltage_index;
  pi->boot_pl.ds_divider_index = 0U;
  pi->boot_pl.ss_divider_index = 0U;
  pi->boot_pl.allow_gnb_slow = 1U;
  pi->boot_pl.force_nbp_state = 0U;
  pi->boot_pl.display_wm = 0U;
  pi->boot_pl.vce_wm = 0U;
  return;
}
}
static int kv_force_dpm_highest(struct amdgpu_device *adev )
{
  int ret ;
  u32 enable_mask ;
  u32 i ;
  int tmp ;
  int tmp___0 ;
  {
  ret = amdgpu_kv_dpm_get_enable_mask(adev, & enable_mask);
  if (ret != 0) {
    return (ret);
  } else {
  }
  i = 7U;
  goto ldv_48979;
  ldv_48978: ;
  if (((u32 )(1 << (int )i) & enable_mask) != 0U) {
    goto ldv_48977;
  } else {
  }
  i = i - 1U;
  ldv_48979: ;
  if (i != 0U) {
    goto ldv_48978;
  } else {
  }
  ldv_48977: ;
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    tmp = amdgpu_kv_send_msg_to_smc_with_parameter(adev, 260, i);
    return (tmp);
  } else {
    tmp___0 = kv_set_enabled_level(adev, i);
    return (tmp___0);
  }
}
}
static int kv_force_dpm_lowest(struct amdgpu_device *adev )
{
  int ret ;
  u32 enable_mask ;
  u32 i ;
  int tmp ;
  int tmp___0 ;
  {
  ret = amdgpu_kv_dpm_get_enable_mask(adev, & enable_mask);
  if (ret != 0) {
    return (ret);
  } else {
  }
  i = 0U;
  goto ldv_48988;
  ldv_48987: ;
  if (((u32 )(1 << (int )i) & enable_mask) != 0U) {
    goto ldv_48986;
  } else {
  }
  i = i + 1U;
  ldv_48988: ;
  if (i <= 7U) {
    goto ldv_48987;
  } else {
  }
  ldv_48986: ;
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    tmp = amdgpu_kv_send_msg_to_smc_with_parameter(adev, 260, i);
    return (tmp);
  } else {
    tmp___0 = kv_set_enabled_level(adev, i);
    return (tmp___0);
  }
}
}
static u8 kv_get_sleep_divider_id_from_clock(struct amdgpu_device *adev , u32 sclk ,
                                             u32 min_sclk_in_sr )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  u32 temp ;
  u32 min ;
  u32 tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  min = 800U > min_sclk_in_sr ? 800U : min_sclk_in_sr;
  if (sclk < min) {
    return (0U);
  } else {
  }
  if (! pi->caps_sclk_ds) {
    return (0U);
  } else {
  }
  i = 5U;
  goto ldv_49000;
  ldv_48999:
  tmp___0 = sumo_get_sleep_divider_from_id(i);
  temp = sclk / tmp___0;
  if (temp >= min) {
    goto ldv_48998;
  } else {
  }
  i = i - 1U;
  ldv_49000: ;
  if (i != 0U) {
    goto ldv_48999;
  } else {
  }
  ldv_48998: ;
  return ((u8 )i);
}
}
static int kv_get_high_voltage_limit(struct amdgpu_device *adev , int *limit )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  int i ;
  u16 tmp___0 ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  u16 tmp___1 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    i = (int )(table->count - 1U);
    goto ldv_49009;
    ldv_49008: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___0 = kv_convert_8bit_index_to_voltage(adev, (int )(table->entries + (unsigned long )i)->v);
      if ((int )tmp___0 <= (int )pi->high_voltage_t) {
        *limit = i;
        return (0);
      } else {
      }
    } else {
    }
    i = i - 1;
    ldv_49009: ;
    if (i >= 0) {
      goto ldv_49008;
    } else {
    }
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    i = (int )(table___0->num_max_dpm_entries - 1U);
    goto ldv_49013;
    ldv_49012: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___1 = kv_convert_2bit_index_to_voltage(adev, (u32 )table___0->entries[i].vid_2bit);
      if ((int )tmp___1 <= (int )pi->high_voltage_t) {
        *limit = i;
        return (0);
      } else {
      }
    } else {
    }
    i = i - 1;
    ldv_49013: ;
    if (i >= 0) {
      goto ldv_49012;
    } else {
    }
  }
  *limit = 0;
  return (0);
}
}
static void kv_apply_state_adjust_rules(struct amdgpu_device *adev , struct amdgpu_ps *new_rps ,
                                        struct amdgpu_ps *old_rps )
{
  struct kv_ps *ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  u32 min_sclk ;
  u32 sclk ;
  u32 mclk ;
  int i ;
  int limit ;
  bool force_high ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 stable_p_state_sclk ;
  struct amdgpu_clock_and_voltage_limits *max_limits ;
  u16 tmp___1 ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  u16 tmp___2 ;
  {
  tmp = kv_get_ps(new_rps);
  ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  min_sclk = 10000U;
  mclk = 0U;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  stable_p_state_sclk = 0U;
  max_limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  if ((int )new_rps->vce_active) {
    new_rps->evclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].evclk;
    new_rps->ecclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].ecclk;
  } else {
    new_rps->evclk = 0U;
    new_rps->ecclk = 0U;
  }
  mclk = max_limits->mclk;
  sclk = min_sclk;
  if ((int )pi->caps_stable_p_state) {
    stable_p_state_sclk = (max_limits->sclk * 75U) / 100U;
    i = (int )(table->count - 1U);
    goto ldv_49033;
    ldv_49032: ;
    if ((table->entries + (unsigned long )i)->clk <= stable_p_state_sclk) {
      stable_p_state_sclk = (table->entries + (unsigned long )i)->clk;
      goto ldv_49031;
    } else {
    }
    i = i + 1;
    ldv_49033: ;
    if (i >= 0) {
      goto ldv_49032;
    } else {
    }
    ldv_49031: ;
    if (i > 0) {
      stable_p_state_sclk = (table->entries)->clk;
    } else {
    }
    sclk = stable_p_state_sclk;
  } else {
  }
  if ((int )new_rps->vce_active) {
    if (adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].sclk > sclk) {
      sclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].sclk;
    } else {
    }
  } else {
  }
  ps->need_dfs_bypass = 1;
  i = 0;
  goto ldv_49035;
  ldv_49034: ;
  if (ps->levels[i].sclk < sclk) {
    ps->levels[i].sclk = sclk;
  } else {
  }
  i = i + 1;
  ldv_49035: ;
  if ((u32 )i < ps->num_levels) {
    goto ldv_49034;
  } else {
  }
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    i = 0;
    goto ldv_49038;
    ldv_49037: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___1 = kv_convert_8bit_index_to_voltage(adev, (int )ps->levels[i].vddc_index);
      if ((int )pi->high_voltage_t < (int )tmp___1) {
        kv_get_high_voltage_limit(adev, & limit);
        ps->levels[i].sclk = (table->entries + (unsigned long )limit)->clk;
      } else {
      }
    } else {
    }
    i = i + 1;
    ldv_49038: ;
    if ((u32 )i < ps->num_levels) {
      goto ldv_49037;
    } else {
    }
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    i = 0;
    goto ldv_49042;
    ldv_49041: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___2 = kv_convert_8bit_index_to_voltage(adev, (int )ps->levels[i].vddc_index);
      if ((int )pi->high_voltage_t < (int )tmp___2) {
        kv_get_high_voltage_limit(adev, & limit);
        ps->levels[i].sclk = table___0->entries[limit].sclk_frequency;
      } else {
      }
    } else {
    }
    i = i + 1;
    ldv_49042: ;
    if ((u32 )i < ps->num_levels) {
      goto ldv_49041;
    } else {
    }
  }
  if ((int )pi->caps_stable_p_state) {
    i = 0;
    goto ldv_49045;
    ldv_49044:
    ps->levels[i].sclk = stable_p_state_sclk;
    i = i + 1;
    ldv_49045: ;
    if ((u32 )i < ps->num_levels) {
      goto ldv_49044;
    } else {
    }
  } else {
  }
  pi->video_start = (bool )(((new_rps->dclk != 0U || new_rps->vclk != 0U) || new_rps->evclk != 0U) || new_rps->ecclk != 0U);
  if ((new_rps->class & 7U) == 1U) {
    pi->battery_state = 1;
  } else {
    pi->battery_state = 0;
  }
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    ps->dpm0_pg_nb_ps_lo = 1U;
    ps->dpm0_pg_nb_ps_hi = 0U;
    ps->dpmx_nb_ps_lo = 1U;
    ps->dpmx_nb_ps_hi = 0U;
  } else {
    ps->dpm0_pg_nb_ps_lo = 3U;
    ps->dpm0_pg_nb_ps_hi = 0U;
    ps->dpmx_nb_ps_lo = 3U;
    ps->dpmx_nb_ps_hi = 0U;
    if (pi->sys_info.nb_dpm_enable != 0U) {
      force_high = (bool )(((pi->sys_info.nbp_memory_clock[3] <= mclk || (int )pi->video_start) || adev->pm.dpm.new_active_crtc_count > 2) || (int )pi->disable_nb_ps3_in_battery);
      ps->dpm0_pg_nb_ps_lo = (int )force_high ? 2U : 3U;
      ps->dpm0_pg_nb_ps_hi = 2U;
      ps->dpmx_nb_ps_lo = (int )force_high ? 2U : 3U;
      ps->dpmx_nb_ps_hi = 2U;
    } else {
    }
  }
  return;
}
}
static void kv_dpm_power_level_enabled_for_throttle(struct amdgpu_device *adev , u32 index ,
                                                    bool enable )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  pi->graphics_level[index].EnabledForThrottle = (uint8_t )enable;
  return;
}
}
static int kv_calculate_ds_divider(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 sclk_in_sr ;
  u32 i ;
  __u32 tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  sclk_in_sr = 10000U;
  if (pi->lowest_valid > pi->highest_valid) {
    return (-22);
  } else {
  }
  i = pi->lowest_valid;
  goto ldv_49060;
  ldv_49059:
  tmp___0 = __fswab32(pi->graphics_level[i].SclkFrequency);
  pi->graphics_level[i].DeepSleepDivId = kv_get_sleep_divider_id_from_clock(adev,
                                                                            tmp___0,
                                                                            sclk_in_sr);
  i = i + 1U;
  ldv_49060: ;
  if (pi->highest_valid >= i) {
    goto ldv_49059;
  } else {
  }
  return (0);
}
}
static int kv_calculate_nbps_level_settings(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  bool force_high ;
  struct amdgpu_clock_and_voltage_limits *max_limits ;
  u32 mclk ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  max_limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  mclk = max_limits->mclk;
  if (pi->lowest_valid > pi->highest_valid) {
    return (-22);
  } else {
  }
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    i = pi->lowest_valid;
    goto ldv_49071;
    ldv_49070:
    pi->graphics_level[i].GnbSlow = 1U;
    pi->graphics_level[i].ForceNbPs1 = 0U;
    pi->graphics_level[i].UpH = 0U;
    i = i + 1U;
    ldv_49071: ;
    if (pi->highest_valid >= i) {
      goto ldv_49070;
    } else {
    }
    if (pi->sys_info.nb_dpm_enable == 0U) {
      return (0);
    } else {
    }
    force_high = (bool )((pi->sys_info.nbp_memory_clock[3] <= mclk || adev->pm.dpm.new_active_crtc_count > 2) || (int )pi->video_start);
    if ((int )force_high) {
      i = pi->lowest_valid;
      goto ldv_49074;
      ldv_49073:
      pi->graphics_level[i].GnbSlow = 0U;
      i = i + 1U;
      ldv_49074: ;
      if (pi->highest_valid >= i) {
        goto ldv_49073;
      } else {
      }
    } else {
      if ((int )pi->battery_state) {
        pi->graphics_level[0].ForceNbPs1 = 1U;
      } else {
      }
      pi->graphics_level[1].GnbSlow = 0U;
      pi->graphics_level[2].GnbSlow = 0U;
      pi->graphics_level[3].GnbSlow = 0U;
      pi->graphics_level[4].GnbSlow = 0U;
    }
  } else {
    i = pi->lowest_valid;
    goto ldv_49077;
    ldv_49076:
    pi->graphics_level[i].GnbSlow = 1U;
    pi->graphics_level[i].ForceNbPs1 = 0U;
    pi->graphics_level[i].UpH = 0U;
    i = i + 1U;
    ldv_49077: ;
    if (pi->highest_valid >= i) {
      goto ldv_49076;
    } else {
    }
    if (pi->sys_info.nb_dpm_enable != 0U && (int )pi->battery_state) {
      pi->graphics_level[pi->lowest_valid].UpH = 40U;
      pi->graphics_level[pi->lowest_valid].GnbSlow = 0U;
      if (pi->lowest_valid != pi->highest_valid) {
        pi->graphics_level[pi->lowest_valid].ForceNbPs1 = 1U;
      } else {
      }
    } else {
    }
  }
  return (0);
}
}
static int kv_calculate_dpm_settings(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  if (pi->lowest_valid > pi->highest_valid) {
    return (-22);
  } else {
  }
  i = pi->lowest_valid;
  goto ldv_49085;
  ldv_49084:
  pi->graphics_level[i].DisplayWatermark = pi->highest_valid == i;
  i = i + 1U;
  ldv_49085: ;
  if (pi->highest_valid >= i) {
    goto ldv_49084;
  } else {
  }
  return (0);
}
}
static void kv_init_graphics_levels(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 vid_2bit ;
  u16 tmp___0 ;
  struct sumo_sclk_voltage_mapping_table *table___0 ;
  u16 tmp___1 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) && table->count != 0U) {
    pi->graphics_dpm_level_count = 0U;
    i = 0U;
    goto ldv_49096;
    ldv_49095: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___0 = kv_convert_8bit_index_to_voltage(adev, (int )(table->entries + (unsigned long )i)->v);
      if ((int )pi->high_voltage_t < (int )tmp___0) {
        goto ldv_49094;
      } else {
      }
    } else {
    }
    kv_set_divider_value(adev, i, (table->entries + (unsigned long )i)->clk);
    vid_2bit = kv_convert_vid7_to_vid2(adev, & pi->sys_info.vid_mapping_table, (u32 )(table->entries + (unsigned long )i)->v);
    kv_set_vid(adev, i, vid_2bit);
    kv_set_at(adev, i, pi->at[i]);
    kv_dpm_power_level_enabled_for_throttle(adev, i, 1);
    pi->graphics_dpm_level_count = (u8 )((int )pi->graphics_dpm_level_count + 1);
    i = i + 1U;
    ldv_49096: ;
    if (table->count > i) {
      goto ldv_49095;
    } else {
    }
    ldv_49094: ;
  } else {
    table___0 = & pi->sys_info.sclk_voltage_mapping_table;
    pi->graphics_dpm_level_count = 0U;
    i = 0U;
    goto ldv_49100;
    ldv_49099: ;
    if ((unsigned int )pi->high_voltage_t != 0U) {
      tmp___1 = kv_convert_2bit_index_to_voltage(adev, (u32 )table___0->entries[i].vid_2bit);
      if ((int )pi->high_voltage_t < (int )tmp___1) {
        goto ldv_49098;
      } else {
      }
    } else {
    }
    kv_set_divider_value(adev, i, table___0->entries[i].sclk_frequency);
    kv_set_vid(adev, i, (u32 )table___0->entries[i].vid_2bit);
    kv_set_at(adev, i, pi->at[i]);
    kv_dpm_power_level_enabled_for_throttle(adev, i, 1);
    pi->graphics_dpm_level_count = (u8 )((int )pi->graphics_dpm_level_count + 1);
    i = i + 1U;
    ldv_49100: ;
    if (table___0->num_max_dpm_entries > i) {
      goto ldv_49099;
    } else {
    }
    ldv_49098: ;
  }
  i = 0U;
  goto ldv_49102;
  ldv_49101:
  kv_dpm_power_level_enable(adev, i, 0);
  i = i + 1U;
  ldv_49102: ;
  if (i <= 7U) {
    goto ldv_49101;
  } else {
  }
  return;
}
}
static void kv_enable_new_levels(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  i = 0U;
  goto ldv_49110;
  ldv_49109: ;
  if (pi->lowest_valid <= i && pi->highest_valid >= i) {
    kv_dpm_power_level_enable(adev, i, 1);
  } else {
  }
  i = i + 1U;
  ldv_49110: ;
  if (i <= 7U) {
    goto ldv_49109;
  } else {
  }
  return;
}
}
static int kv_set_enabled_level(struct amdgpu_device *adev , u32 level )
{
  u32 new_mask ;
  int tmp ;
  {
  new_mask = (u32 )(1 << (int )level);
  tmp = amdgpu_kv_send_msg_to_smc_with_parameter(adev, 325, new_mask);
  return (tmp);
}
}
static int kv_set_enabled_levels(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 i ;
  u32 new_mask ;
  int tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  new_mask = 0U;
  i = pi->lowest_valid;
  goto ldv_49124;
  ldv_49123:
  new_mask = (u32 )(1 << (int )i) | new_mask;
  i = i + 1U;
  ldv_49124: ;
  if (pi->highest_valid >= i) {
    goto ldv_49123;
  } else {
  }
  tmp___0 = amdgpu_kv_send_msg_to_smc_with_parameter(adev, 325, new_mask);
  return (tmp___0);
}
}
static void kv_program_nbps_index_settings(struct amdgpu_device *adev , struct amdgpu_ps *new_rps )
{
  struct kv_ps *new_ps ;
  struct kv_ps *tmp ;
  struct kv_power_info *pi ;
  struct kv_power_info *tmp___0 ;
  u32 nbdpmconfig1 ;
  {
  tmp = kv_get_ps(new_rps);
  new_ps = tmp;
  tmp___0 = kv_get_pi(adev);
  pi = tmp___0;
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    return;
  } else {
  }
  if (pi->sys_info.nb_dpm_enable != 0U) {
    nbdpmconfig1 = (*(adev->smc_rreg))(adev, 260584U);
    nbdpmconfig1 = 0U;
    nbdpmconfig1 = (u32 )((((int )new_ps->dpm0_pg_nb_ps_lo | ((int )new_ps->dpm0_pg_nb_ps_hi << 8)) | ((int )new_ps->dpmx_nb_ps_lo << 16)) | ((int )new_ps->dpmx_nb_ps_hi << 24)) | nbdpmconfig1;
    (*(adev->smc_wreg))(adev, 260584U, nbdpmconfig1);
  } else {
  }
  return;
}
}
static int kv_set_thermal_temperature_range(struct amdgpu_device *adev , int min_temp ,
                                            int max_temp )
{
  int low_temp ;
  int high_temp ;
  u32 tmp ;
  {
  low_temp = 0;
  high_temp = 255000;
  if (low_temp < min_temp) {
    low_temp = min_temp;
  } else {
  }
  if (high_temp > max_temp) {
    high_temp = max_temp;
  } else {
  }
  if (high_temp < low_temp) {
    drm_err("invalid thermal range: %d - %d\n", low_temp, high_temp);
    return (-22);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3255828520U);
  tmp = tmp & 4294901760U;
  tmp = (u32 )((high_temp / 1000 + 49) | ((low_temp / 1000 + 49) << 8)) | tmp;
  (*(adev->smc_wreg))(adev, 3255828520U, tmp);
  adev->pm.dpm.thermal.min_temp = low_temp;
  adev->pm.dpm.thermal.max_temp = high_temp;
  return (0);
}
}
static int kv_parse_sys_info_table(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct amdgpu_mode_info *mode_info ;
  int index ;
  union igp_info___0 *igp_info ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  int i ;
  bool tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  mode_info = & adev->mode_info;
  index = 30;
  tmp___0 = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___0) {
    igp_info = (union igp_info___0 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    if ((unsigned int )crev != 8U) {
      drm_err("Unsupported IGP table: %d %d\n", (int )frev, (int )crev);
      return (-22);
    } else {
    }
    pi->sys_info.bootup_sclk = igp_info->info_8.ulBootUpEngineClock;
    pi->sys_info.bootup_uma_clk = igp_info->info_8.ulBootUpUMAClock;
    pi->sys_info.bootup_nb_voltage_index = igp_info->info_8.usBootUpNBVoltage;
    if ((unsigned int )igp_info->info_8.ucHtcTmpLmt == 0U) {
      pi->sys_info.htc_tmp_lmt = 203U;
    } else {
      pi->sys_info.htc_tmp_lmt = igp_info->info_8.ucHtcTmpLmt;
    }
    if ((unsigned int )igp_info->info_8.ucHtcHystLmt == 0U) {
      pi->sys_info.htc_hyst_lmt = 5U;
    } else {
      pi->sys_info.htc_hyst_lmt = igp_info->info_8.ucHtcHystLmt;
    }
    if ((int )pi->sys_info.htc_tmp_lmt <= (int )pi->sys_info.htc_hyst_lmt) {
      drm_err("The htcTmpLmt should be larger than htcHystLmt.\n");
    } else {
    }
    if ((igp_info->info_8.ulSystemConfig & 8U) != 0U) {
      pi->sys_info.nb_dpm_enable = 1U;
    } else {
      pi->sys_info.nb_dpm_enable = 0U;
    }
    i = 0;
    goto ldv_49160;
    ldv_49159:
    pi->sys_info.nbp_memory_clock[i] = igp_info->info_8.ulNbpStateMemclkFreq[i];
    pi->sys_info.nbp_n_clock[i] = igp_info->info_8.ulNbpStateNClkFreq[i];
    i = i + 1;
    ldv_49160: ;
    if (i <= 3) {
      goto ldv_49159;
    } else {
    }
    if ((igp_info->info_8.ulGPUCapInfo & 16U) != 0U) {
      pi->caps_enable_dfs_bypass = 1;
    } else {
    }
    sumo_construct_sclk_voltage_mapping_table(adev, & pi->sys_info.sclk_voltage_mapping_table,
                                              (ATOM_AVAILABLE_SCLK_LIST *)(& igp_info->info_8.sAvail_SCLK));
    sumo_construct_vid_mapping_table(adev, & pi->sys_info.vid_mapping_table, (ATOM_AVAILABLE_SCLK_LIST *)(& igp_info->info_8.sAvail_SCLK));
    kv_construct_max_power_limits_table(adev, & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);
  } else {
  }
  return (0);
}
}
static void kv_patch_boot_state(struct amdgpu_device *adev , struct kv_ps *ps )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  ps->num_levels = 1U;
  ps->levels[0] = pi->boot_pl;
  return;
}
}
static void kv_parse_pplib_non_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                          struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ,
                                          u8 table_rev )
{
  struct kv_ps *ps ;
  struct kv_ps *tmp ;
  {
  tmp = kv_get_ps(rps);
  ps = tmp;
  rps->caps = non_clock_info->ulCapsAndSettings;
  rps->class = (u32 )non_clock_info->usClassification;
  rps->class2 = (u32 )non_clock_info->usClassification2;
  if ((unsigned int )table_rev > 12U) {
    rps->vclk = non_clock_info->ulVCLK;
    rps->dclk = non_clock_info->ulDCLK;
  } else {
    rps->vclk = 0U;
    rps->dclk = 0U;
  }
  if ((rps->class & 8U) != 0U) {
    adev->pm.dpm.boot_ps = rps;
    kv_patch_boot_state(adev, ps);
  } else {
  }
  if ((rps->class & 1024U) != 0U) {
    adev->pm.dpm.uvd_ps = rps;
  } else {
  }
  return;
}
}
static void kv_parse_pplib_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                      int index , union pplib_clock_info *clock_info )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct kv_ps *ps ;
  struct kv_ps *tmp___0 ;
  struct kv_pl *pl ;
  u32 sclk ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  tmp___0 = kv_get_ps(rps);
  ps = tmp___0;
  pl = (struct kv_pl *)(& ps->levels) + (unsigned long )index;
  sclk = (u32 )clock_info->sumo.usEngineClockLow;
  sclk = (u32 )((int )clock_info->sumo.ucEngineClockHigh << 16) | sclk;
  pl->sclk = sclk;
  pl->vddc_index = clock_info->sumo.vddcIndex;
  ps->num_levels = (u32 )(index + 1);
  if ((int )pi->caps_sclk_ds) {
    pl->ds_divider_index = 5U;
    pl->ss_divider_index = 5U;
  } else {
  }
  return;
}
}
static int kv_parse_power_table(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ;
  union pplib_power_state *power_state ;
  int i ;
  int j ;
  int k ;
  int non_clock_array_index ;
  int clock_array_index ;
  union pplib_clock_info *clock_info ;
  struct _StateArray *state_array ;
  struct _ClockInfoArray *clock_info_array ;
  struct _NonClockInfoArray *non_clock_info_array ;
  union power_info *power_info ;
  int index ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  u8 *power_state_offset ;
  struct kv_ps *ps ;
  bool tmp ;
  int tmp___0 ;
  void *tmp___1 ;
  u8 *idx ;
  void *tmp___2 ;
  u32 sclk ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  power_info = (union power_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  amdgpu_add_thermal_controller(adev);
  state_array = (struct _StateArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usStateArrayOffset));
  clock_info_array = (struct _ClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usClockInfoArrayOffset));
  non_clock_info_array = (struct _NonClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usNonClockInfoArrayOffset));
  tmp___1 = kzalloc((unsigned long )state_array->ucNumEntries * 48UL, 208U);
  adev->pm.dpm.ps = (struct amdgpu_ps *)tmp___1;
  if ((unsigned long )adev->pm.dpm.ps == (unsigned long )((struct amdgpu_ps *)0)) {
    return (-12);
  } else {
  }
  power_state_offset = (u8 *)(& state_array->states);
  i = 0;
  goto ldv_49227;
  ldv_49226:
  power_state = (union pplib_power_state *)power_state_offset;
  non_clock_array_index = (int )power_state->v2.nonClockInfoIndex;
  non_clock_info = (struct _ATOM_PPLIB_NONCLOCK_INFO *)(& non_clock_info_array->nonClockInfo) + (unsigned long )non_clock_array_index;
  tmp___2 = kzalloc(72UL, 208U);
  ps = (struct kv_ps *)tmp___2;
  if ((unsigned long )ps == (unsigned long )((struct kv_ps *)0)) {
    kfree((void const *)adev->pm.dpm.ps);
    return (-12);
  } else {
  }
  (adev->pm.dpm.ps + (unsigned long )i)->ps_priv = (void *)ps;
  k = 0;
  idx = (u8 *)(& power_state->v2.clockInfoIndex);
  j = 0;
  goto ldv_49225;
  ldv_49224:
  clock_array_index = (int )*(idx + (unsigned long )j);
  if ((int )clock_info_array->ucNumEntries <= clock_array_index) {
    goto ldv_49222;
  } else {
  }
  if (k > 4) {
    goto ldv_49223;
  } else {
  }
  clock_info = (union pplib_clock_info *)(& clock_info_array->clockInfo) + (unsigned long )((int )clock_info_array->ucEntrySize * clock_array_index);
  kv_parse_pplib_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, k, clock_info);
  k = k + 1;
  ldv_49222:
  j = j + 1;
  ldv_49225: ;
  if ((int )power_state->v2.ucNumDPMLevels > j) {
    goto ldv_49224;
  } else {
  }
  ldv_49223:
  kv_parse_pplib_non_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, non_clock_info,
                                (int )non_clock_info_array->ucEntrySize);
  power_state_offset = power_state_offset + (unsigned long )((int )power_state->v2.ucNumDPMLevels + 2);
  i = i + 1;
  ldv_49227: ;
  if ((int )state_array->ucNumEntries > i) {
    goto ldv_49226;
  } else {
  }
  adev->pm.dpm.num_ps = (int )state_array->ucNumEntries;
  i = 0;
  goto ldv_49231;
  ldv_49230:
  clock_array_index = (int )adev->pm.dpm.vce_states[i].clk_idx;
  clock_info = (union pplib_clock_info *)(& clock_info_array->clockInfo) + (unsigned long )((int )clock_info_array->ucEntrySize * clock_array_index);
  sclk = (u32 )clock_info->sumo.usEngineClockLow;
  sclk = (u32 )((int )clock_info->sumo.ucEngineClockHigh << 16) | sclk;
  adev->pm.dpm.vce_states[i].sclk = sclk;
  adev->pm.dpm.vce_states[i].mclk = 0U;
  i = i + 1;
  ldv_49231: ;
  if (i <= 5) {
    goto ldv_49230;
  } else {
  }
  return (0);
}
}
static int kv_dpm_init(struct amdgpu_device *adev )
{
  struct kv_power_info *pi ;
  int ret ;
  int i ;
  void *tmp ;
  {
  tmp = kzalloc(1144UL, 208U);
  pi = (struct kv_power_info *)tmp;
  if ((unsigned long )pi == (unsigned long )((struct kv_power_info *)0)) {
    return (-12);
  } else {
  }
  adev->pm.dpm.priv = (void *)pi;
  ret = amdgpu_get_platform_caps(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_parse_extended_power_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  i = 0;
  goto ldv_49240;
  ldv_49239:
  pi->at[i] = 30U;
  i = i + 1;
  ldv_49240: ;
  if (i <= 4) {
    goto ldv_49239;
  } else {
  }
  pi->sram_end = 262144U;
  pi->enable_nb_dpm = 1;
  pi->caps_power_containment = 1;
  pi->caps_cac = 1;
  pi->enable_didt = 0;
  if ((int )pi->enable_didt) {
    pi->caps_sq_ramping = 1;
    pi->caps_db_ramping = 1;
    pi->caps_td_ramping = 1;
    pi->caps_tcp_ramping = 1;
  } else {
  }
  pi->caps_sclk_ds = 1;
  pi->enable_auto_thermal_throttling = 1;
  pi->disable_nb_ps3_in_battery = 0;
  if (amdgpu_bapm == 0) {
    pi->bapm_enable = 0;
  } else {
    pi->bapm_enable = 1;
  }
  pi->voltage_drop_t = 0U;
  pi->caps_sclk_throttle_low_notification = 0;
  pi->caps_fps = 0;
  pi->caps_uvd_pg = (adev->pg_flags & 8U) != 0U;
  pi->caps_uvd_dpm = 1;
  pi->caps_vce_pg = (adev->pg_flags & 16U) != 0U;
  pi->caps_samu_pg = (adev->pg_flags & 1024U) != 0U;
  pi->caps_acp_pg = (adev->pg_flags & 512U) != 0U;
  pi->caps_stable_p_state = 0;
  ret = kv_parse_sys_info_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  kv_patch_voltage_values(adev);
  kv_construct_boot_state(adev);
  ret = kv_parse_power_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->enable_dpm = 1;
  return (0);
}
}
static void kv_dpm_debugfs_print_current_performance_level(struct amdgpu_device *adev ,
                                                           struct seq_file *m )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  u32 current_index ;
  u32 tmp___0 ;
  u32 sclk ;
  u32 tmp___1 ;
  u16 vddc ;
  __u32 tmp___2 ;
  u32 tmp___3 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322644U);
  current_index = (tmp___0 & 2031616U) >> 16;
  if (current_index > 7U) {
    seq_printf(m, "invalid dpm profile %d\n", current_index);
  } else {
    tmp___2 = __fswab32(pi->graphics_level[current_index].SclkFrequency);
    sclk = tmp___2;
    tmp___3 = (*(adev->smc_rreg))(adev, 3223322772U);
    tmp___1 = (tmp___3 & 510U) >> 1;
    vddc = kv_convert_8bit_index_to_voltage(adev, (int )((unsigned short )tmp___1));
    seq_printf(m, "uvd    %sabled\n", (int )pi->uvd_power_gated ? (char *)"dis" : (char *)"en");
    seq_printf(m, "vce    %sabled\n", (int )pi->vce_power_gated ? (char *)"dis" : (char *)"en");
    seq_printf(m, "power level %d    sclk: %u vddc: %u\n", current_index, sclk, (int )vddc);
  }
  return;
}
}
static void kv_dpm_print_power_state(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  int i ;
  struct kv_ps *ps ;
  struct kv_ps *tmp ;
  struct kv_pl *pl ;
  u16 tmp___0 ;
  {
  tmp = kv_get_ps(rps);
  ps = tmp;
  amdgpu_dpm_print_class_info(rps->class, rps->class2);
  amdgpu_dpm_print_cap_info(rps->caps);
  printk("\tuvd    vclk: %d dclk: %d\n", rps->vclk, rps->dclk);
  i = 0;
  goto ldv_49259;
  ldv_49258:
  pl = (struct kv_pl *)(& ps->levels) + (unsigned long )i;
  tmp___0 = kv_convert_8bit_index_to_voltage(adev, (int )pl->vddc_index);
  printk("\t\tpower level %d    sclk: %u vddc: %u\n", i, pl->sclk, (int )tmp___0);
  i = i + 1;
  ldv_49259: ;
  if ((u32 )i < ps->num_levels) {
    goto ldv_49258;
  } else {
  }
  amdgpu_dpm_print_ps_status(adev, rps);
  return;
}
}
static void kv_dpm_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_49266;
  ldv_49265:
  kfree((void const *)(adev->pm.dpm.ps + (unsigned long )i)->ps_priv);
  i = i + 1;
  ldv_49266: ;
  if (adev->pm.dpm.num_ps > i) {
    goto ldv_49265;
  } else {
  }
  kfree((void const *)adev->pm.dpm.ps);
  kfree((void const *)adev->pm.dpm.priv);
  amdgpu_free_extended_power_table(adev);
  return;
}
}
static void kv_dpm_display_configuration_changed(struct amdgpu_device *adev )
{
  {
  return;
}
}
static u32 kv_dpm_get_sclk(struct amdgpu_device *adev , bool low )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  struct kv_ps *requested_state ;
  struct kv_ps *tmp___0 ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  tmp___0 = kv_get_ps(& pi->requested_rps);
  requested_state = tmp___0;
  if ((int )low) {
    return (requested_state->levels[0].sclk);
  } else {
    return (requested_state->levels[requested_state->num_levels - 1U].sclk);
  }
}
}
static u32 kv_dpm_get_mclk(struct amdgpu_device *adev , bool low )
{
  struct kv_power_info *pi ;
  struct kv_power_info *tmp ;
  {
  tmp = kv_get_pi(adev);
  pi = tmp;
  return (pi->sys_info.bootup_uma_clk);
}
}
static int kv_dpm_get_temp(struct amdgpu_device *adev )
{
  u32 temp ;
  int actual_temp ;
  {
  actual_temp = 0;
  temp = (*(adev->smc_rreg))(adev, 3224374796U);
  if (temp != 0U) {
    actual_temp = (int )(temp / 8U - 49U);
  } else {
    actual_temp = 0;
  }
  actual_temp = actual_temp * 1000;
  return (actual_temp);
}
}
static int kv_dpm_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  kv_dpm_set_dpm_funcs(adev);
  kv_dpm_set_irq_funcs(adev);
  return (0);
}
}
static int kv_dpm_late_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  kv_dpm_powergate_acp(adev, 1);
  kv_dpm_powergate_samu(adev, 1);
  kv_dpm_powergate_vce(adev, 1);
  kv_dpm_powergate_uvd(adev, 1);
  return (0);
}
}
static int kv_dpm_sw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct amdgpu_ps *tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = amdgpu_irq_add_id(adev, 230U, & adev->pm.dpm.thermal.irq);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_irq_add_id(adev, 231U, & adev->pm.dpm.thermal.irq);
  if (ret != 0) {
    return (ret);
  } else {
  }
  adev->pm.dpm.state = 3;
  adev->pm.dpm.user_state = 3;
  adev->pm.dpm.forced_level = 0;
  adev->pm.default_sclk = adev->clock.default_sclk;
  adev->pm.default_mclk = adev->clock.default_mclk;
  adev->pm.current_sclk = adev->clock.default_sclk;
  adev->pm.current_mclk = adev->clock.default_mclk;
  adev->pm.int_thermal_type = 0;
  if (amdgpu_dpm == 0) {
    return (0);
  } else {
  }
  __init_work(& adev->pm.dpm.thermal.work, 0);
  __constr_expr_0.counter = 137438953408L;
  adev->pm.dpm.thermal.work.data = __constr_expr_0;
  lockdep_init_map(& adev->pm.dpm.thermal.work.lockdep_map, "(&adev->pm.dpm.thermal.work)",
                   & __key, 0);
  INIT_LIST_HEAD(& adev->pm.dpm.thermal.work.entry);
  adev->pm.dpm.thermal.work.func = & amdgpu_dpm_thermal_work_handler;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = kv_dpm_init(adev);
  if (ret != 0) {
    goto dpm_failed;
  } else {
  }
  tmp = adev->pm.dpm.boot_ps;
  adev->pm.dpm.requested_ps = tmp;
  adev->pm.dpm.current_ps = tmp;
  if (amdgpu_dpm == 1) {
    amdgpu_pm_print_power_states(adev);
  } else {
  }
  ret = amdgpu_pm_sysfs_init(adev);
  if (ret != 0) {
    goto dpm_failed;
  } else {
  }
  mutex_unlock(& adev->pm.mutex);
  printk("\016[drm] amdgpu: dpm initialized\n");
  return (0);
  dpm_failed:
  kv_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  drm_err("amdgpu: dpm initialization failed\n");
  return (ret);
}
}
static int kv_dpm_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  amdgpu_pm_sysfs_fini(adev);
  kv_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static int kv_dpm_hw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  kv_dpm_setup_asic(adev);
  ret = kv_dpm_enable(adev);
  if (ret != 0) {
    adev->pm.dpm_enabled = 0;
  } else {
    adev->pm.dpm_enabled = 1;
  }
  mutex_unlock(& adev->pm.mutex);
  return (ret);
}
}
static int kv_dpm_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    kv_dpm_disable(adev);
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (0);
}
}
static int kv_dpm_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ps *tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    kv_dpm_disable(adev);
    tmp = adev->pm.dpm.boot_ps;
    adev->pm.dpm.requested_ps = tmp;
    adev->pm.dpm.current_ps = tmp;
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (0);
}
}
static int kv_dpm_resume(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    kv_dpm_setup_asic(adev);
    ret = kv_dpm_enable(adev);
    if (ret != 0) {
      adev->pm.dpm_enabled = 0;
    } else {
      adev->pm.dpm_enabled = 1;
    }
    mutex_unlock(& adev->pm.mutex);
    if ((int )adev->pm.dpm_enabled) {
      amdgpu_pm_compute_clocks(adev);
    } else {
    }
  } else {
  }
  return (0);
}
}
static bool kv_dpm_is_idle(void *handle )
{
  {
  return (1);
}
}
static int kv_dpm_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void kv_dpm_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "KV/KB DPM registers\n");
  tmp = (*(adev->didt_rreg))(adev, 0U);
  _dev_info((struct device const *)adev->dev, "  DIDT_SQ_CTRL0=0x%08X\n", tmp);
  tmp___0 = (*(adev->didt_rreg))(adev, 32U);
  _dev_info((struct device const *)adev->dev, "  DIDT_DB_CTRL0=0x%08X\n", tmp___0);
  tmp___1 = (*(adev->didt_rreg))(adev, 64U);
  _dev_info((struct device const *)adev->dev, "  DIDT_TD_CTRL0=0x%08X\n", tmp___1);
  tmp___2 = (*(adev->didt_rreg))(adev, 96U);
  _dev_info((struct device const *)adev->dev, "  DIDT_TCP_CTRL0=0x%08X\n", tmp___2);
  tmp___3 = (*(adev->smc_rreg))(adev, 3225423108U);
  _dev_info((struct device const *)adev->dev, "  LCAC_SX0_OVR_SEL=0x%08X\n", tmp___3);
  tmp___4 = (*(adev->smc_rreg))(adev, 3225423112U);
  _dev_info((struct device const *)adev->dev, "  LCAC_SX0_OVR_VAL=0x%08X\n", tmp___4);
  tmp___5 = (*(adev->smc_rreg))(adev, 3225423156U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC0_OVR_SEL=0x%08X\n", tmp___5);
  tmp___6 = (*(adev->smc_rreg))(adev, 3225423160U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC0_OVR_VAL=0x%08X\n", tmp___6);
  tmp___7 = (*(adev->smc_rreg))(adev, 3225423168U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC1_OVR_SEL=0x%08X\n", tmp___7);
  tmp___8 = (*(adev->smc_rreg))(adev, 3225423172U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC1_OVR_VAL=0x%08X\n", tmp___8);
  tmp___9 = (*(adev->smc_rreg))(adev, 3225423180U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC2_OVR_SEL=0x%08X\n", tmp___9);
  tmp___10 = (*(adev->smc_rreg))(adev, 3225423184U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC2_OVR_VAL=0x%08X\n", tmp___10);
  tmp___11 = (*(adev->smc_rreg))(adev, 3225423192U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC3_OVR_SEL=0x%08X\n", tmp___11);
  tmp___12 = (*(adev->smc_rreg))(adev, 3225423196U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC3_OVR_VAL=0x%08X\n", tmp___12);
  tmp___13 = (*(adev->smc_rreg))(adev, 3225423236U);
  _dev_info((struct device const *)adev->dev, "  LCAC_CPL_OVR_SEL=0x%08X\n", tmp___13);
  tmp___14 = (*(adev->smc_rreg))(adev, 3225423240U);
  _dev_info((struct device const *)adev->dev, "  LCAC_CPL_OVR_VAL=0x%08X\n", tmp___14);
  tmp___15 = (*(adev->smc_rreg))(adev, 3223323048U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_0=0x%08X\n",
            tmp___15);
  tmp___16 = (*(adev->smc_rreg))(adev, 3223322624U);
  _dev_info((struct device const *)adev->dev, "  GENERAL_PWRMGT=0x%08X\n", tmp___16);
  tmp___17 = (*(adev->smc_rreg))(adev, 3223322632U);
  _dev_info((struct device const *)adev->dev, "  SCLK_PWRMGT_CNTL=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 148U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_MESSAGE_0=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 149U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_RESP_0=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 164U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_MSG_ARG_0=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 128U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_INDEX_0=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 129U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_DATA_0=0x%08X\n", tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 144U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_ACCESS_CNTL=0x%08X\n",
            tmp___23);
  return;
}
}
static int kv_dpm_soft_reset(void *handle )
{
  {
  return (0);
}
}
static int kv_dpm_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                      unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cg_thermal_int ;
  {
  switch (type) {
  case 0U: ;
  switch ((unsigned int )state) {
  case 0U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3255828520U);
  cg_thermal_int = cg_thermal_int & 4278190079U;
  (*(adev->smc_wreg))(adev, 3255828520U, cg_thermal_int);
  goto ldv_49347;
  case 1U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3255828520U);
  cg_thermal_int = cg_thermal_int | 16777216U;
  (*(adev->smc_wreg))(adev, 3255828520U, cg_thermal_int);
  goto ldv_49347;
  default: ;
  goto ldv_49347;
  }
  ldv_49347: ;
  goto ldv_49350;
  case 1U: ;
  switch ((unsigned int )state) {
  case 0U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3255828520U);
  cg_thermal_int = cg_thermal_int & 4261412863U;
  (*(adev->smc_wreg))(adev, 3255828520U, cg_thermal_int);
  goto ldv_49353;
  case 1U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3255828520U);
  cg_thermal_int = cg_thermal_int | 33554432U;
  (*(adev->smc_wreg))(adev, 3255828520U, cg_thermal_int);
  goto ldv_49353;
  default: ;
  goto ldv_49353;
  }
  ldv_49353: ;
  goto ldv_49350;
  default: ;
  goto ldv_49350;
  }
  ldv_49350: ;
  return (0);
}
}
static int kv_dpm_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                    struct amdgpu_iv_entry *entry )
{
  bool queue_thermal ;
  long tmp ;
  long tmp___0 ;
  {
  queue_thermal = 0;
  if ((unsigned long )entry == (unsigned long )((struct amdgpu_iv_entry *)0)) {
    return (-22);
  } else {
  }
  switch (entry->src_id) {
  case 230U:
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("kv_dpm_process_interrupt", "IH: thermal low to high\n");
  } else {
  }
  adev->pm.dpm.thermal.high_to_low = 0;
  queue_thermal = 1;
  goto ldv_49365;
  case 231U:
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("kv_dpm_process_interrupt", "IH: thermal high to low\n");
  } else {
  }
  adev->pm.dpm.thermal.high_to_low = 1;
  queue_thermal = 1;
  goto ldv_49365;
  default: ;
  goto ldv_49365;
  }
  ldv_49365: ;
  if ((int )queue_thermal) {
    schedule_work(& adev->pm.dpm.thermal.work);
  } else {
  }
  return (0);
}
}
static int kv_dpm_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int kv_dpm_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const kv_dpm_ip_funcs =
     {& kv_dpm_early_init, & kv_dpm_late_init, & kv_dpm_sw_init, & kv_dpm_sw_fini, & kv_dpm_hw_init,
    & kv_dpm_hw_fini, & kv_dpm_suspend, & kv_dpm_resume, & kv_dpm_is_idle, & kv_dpm_wait_for_idle,
    & kv_dpm_soft_reset, & kv_dpm_print_status, & kv_dpm_set_clockgating_state, & kv_dpm_set_powergating_state};
static struct amdgpu_dpm_funcs const kv_dpm_funcs =
     {& kv_dpm_get_temp, & kv_dpm_pre_set_power_state, & kv_dpm_set_power_state, & kv_dpm_post_set_power_state,
    & kv_dpm_display_configuration_changed, & kv_dpm_get_sclk, & kv_dpm_get_mclk,
    & kv_dpm_print_power_state, & kv_dpm_debugfs_print_current_performance_level,
    & kv_dpm_force_performance_level, 0, & kv_dpm_powergate_uvd, 0, & kv_dpm_enable_bapm,
    0, 0, 0, 0};
static void kv_dpm_set_dpm_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.funcs == (unsigned long )((struct amdgpu_dpm_funcs const *)0)) {
    adev->pm.funcs = & kv_dpm_funcs;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const kv_dpm_irq_funcs = {& kv_dpm_set_interrupt_state, & kv_dpm_process_interrupt};
static void kv_dpm_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->pm.dpm.thermal.irq.num_types = 2U;
  adev->pm.dpm.thermal.irq.funcs = & kv_dpm_irq_funcs;
  return;
}
}
extern int ldv_probe_114(void) ;
extern int ldv_release_114(void) ;
int ldv_retval_10 ;
int ldv_retval_9 ;
void ldv_initialize_amdgpu_irq_src_funcs_112(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  kv_dpm_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  kv_dpm_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void invoke_work_4(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_4_0 == 2 || ldv_work_4_0 == 3) {
    ldv_work_4_0 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_4_0);
    ldv_work_4_0 = 1;
  } else {
  }
  goto ldv_49400;
  case 1: ;
  if (ldv_work_4_1 == 2 || ldv_work_4_1 == 3) {
    ldv_work_4_1 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_4_0);
    ldv_work_4_1 = 1;
  } else {
  }
  goto ldv_49400;
  case 2: ;
  if (ldv_work_4_2 == 2 || ldv_work_4_2 == 3) {
    ldv_work_4_2 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_4_0);
    ldv_work_4_2 = 1;
  } else {
  }
  goto ldv_49400;
  case 3: ;
  if (ldv_work_4_3 == 2 || ldv_work_4_3 == 3) {
    ldv_work_4_3 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_4_0);
    ldv_work_4_3 = 1;
  } else {
  }
  goto ldv_49400;
  default:
  ldv_stop();
  }
  ldv_49400: ;
  return;
}
}
void call_and_disable_work_4(struct work_struct *work )
{
  {
  if ((ldv_work_4_0 == 2 || ldv_work_4_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_4_0) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_4_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_4_1 == 2 || ldv_work_4_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_4_1) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_4_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_4_2 == 2 || ldv_work_4_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_4_2) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_4_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_4_3 == 2 || ldv_work_4_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_4_3) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_4_3 = 1;
    return;
  } else {
  }
  return;
}
}
void activate_work_4(struct work_struct *work , int state )
{
  {
  if (ldv_work_4_0 == 0) {
    ldv_work_struct_4_0 = work;
    ldv_work_4_0 = state;
    return;
  } else {
  }
  if (ldv_work_4_1 == 0) {
    ldv_work_struct_4_1 = work;
    ldv_work_4_1 = state;
    return;
  } else {
  }
  if (ldv_work_4_2 == 0) {
    ldv_work_struct_4_2 = work;
    ldv_work_4_2 = state;
    return;
  } else {
  }
  if (ldv_work_4_3 == 0) {
    ldv_work_struct_4_3 = work;
    ldv_work_4_3 = state;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_4(int state )
{
  {
  if (ldv_work_4_0 == state) {
    call_and_disable_work_4(ldv_work_struct_4_0);
  } else {
  }
  if (ldv_work_4_1 == state) {
    call_and_disable_work_4(ldv_work_struct_4_1);
  } else {
  }
  if (ldv_work_4_2 == state) {
    call_and_disable_work_4(ldv_work_struct_4_2);
  } else {
  }
  if (ldv_work_4_3 == state) {
    call_and_disable_work_4(ldv_work_struct_4_3);
  } else {
  }
  return;
}
}
void disable_work_4(struct work_struct *work )
{
  {
  if ((ldv_work_4_0 == 3 || ldv_work_4_0 == 2) && (unsigned long )ldv_work_struct_4_0 == (unsigned long )work) {
    ldv_work_4_0 = 1;
  } else {
  }
  if ((ldv_work_4_1 == 3 || ldv_work_4_1 == 2) && (unsigned long )ldv_work_struct_4_1 == (unsigned long )work) {
    ldv_work_4_1 = 1;
  } else {
  }
  if ((ldv_work_4_2 == 3 || ldv_work_4_2 == 2) && (unsigned long )ldv_work_struct_4_2 == (unsigned long )work) {
    ldv_work_4_2 = 1;
  } else {
  }
  if ((ldv_work_4_3 == 3 || ldv_work_4_3 == 2) && (unsigned long )ldv_work_struct_4_3 == (unsigned long )work) {
    ldv_work_4_3 = 1;
  } else {
  }
  return;
}
}
void work_init_4(void)
{
  {
  ldv_work_4_0 = 0;
  ldv_work_4_1 = 0;
  ldv_work_4_2 = 0;
  ldv_work_4_3 = 0;
  return;
}
}
void ldv_initialize_amdgpu_dpm_funcs_113(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  kv_dpm_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_112(void)
{
  enum amdgpu_interrupt_state ldvarg356 ;
  unsigned int ldvarg355 ;
  struct amdgpu_iv_entry *ldvarg354 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg354 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg356), 0, 4UL);
  ldv_memset((void *)(& ldvarg355), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_112 == 1) {
    kv_dpm_set_interrupt_state(kv_dpm_irq_funcs_group0, kv_dpm_irq_funcs_group1, ldvarg355,
                               ldvarg356);
    ldv_state_variable_112 = 1;
  } else {
  }
  goto ldv_49433;
  case 1: ;
  if (ldv_state_variable_112 == 1) {
    kv_dpm_process_interrupt(kv_dpm_irq_funcs_group0, kv_dpm_irq_funcs_group1, ldvarg354);
    ldv_state_variable_112 = 1;
  } else {
  }
  goto ldv_49433;
  default:
  ldv_stop();
  }
  ldv_49433: ;
  return;
}
}
void ldv_main_exported_114(void)
{
  void *ldvarg182 ;
  void *tmp ;
  void *ldvarg171 ;
  void *tmp___0 ;
  void *ldvarg180 ;
  void *tmp___1 ;
  void *ldvarg170 ;
  void *tmp___2 ;
  void *ldvarg179 ;
  void *tmp___3 ;
  void *ldvarg173 ;
  void *tmp___4 ;
  void *ldvarg176 ;
  void *tmp___5 ;
  void *ldvarg183 ;
  void *tmp___6 ;
  void *ldvarg185 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg174 ;
  void *ldvarg181 ;
  void *tmp___8 ;
  void *ldvarg177 ;
  void *tmp___9 ;
  void *ldvarg184 ;
  void *tmp___10 ;
  void *ldvarg172 ;
  void *tmp___11 ;
  enum amd_powergating_state ldvarg178 ;
  void *ldvarg175 ;
  void *tmp___12 ;
  int tmp___13 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg182 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg171 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg180 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg170 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg179 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg173 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg176 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg183 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg185 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg181 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg177 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg184 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg172 = tmp___11;
  tmp___12 = ldv_init_zalloc(1UL);
  ldvarg175 = tmp___12;
  ldv_memset((void *)(& ldvarg174), 0, 4UL);
  ldv_memset((void *)(& ldvarg178), 0, 4UL);
  tmp___13 = __VERIFIER_nondet_int();
  switch (tmp___13) {
  case 0: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_hw_fini(ldvarg185);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_hw_fini(ldvarg185);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_hw_fini(ldvarg185);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 1: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_print_status(ldvarg184);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_print_status(ldvarg184);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_print_status(ldvarg184);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 2: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_early_init(ldvarg183);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_early_init(ldvarg183);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_early_init(ldvarg183);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 3: ;
  if (ldv_state_variable_114 == 2) {
    ldv_retval_10 = kv_dpm_suspend(ldvarg182);
    if (ldv_retval_10 == 0) {
      ldv_state_variable_114 = 3;
    } else {
    }
  } else {
  }
  goto ldv_49456;
  case 4: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_late_init(ldvarg181);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_late_init(ldvarg181);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_late_init(ldvarg181);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 5: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_sw_init(ldvarg180);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_sw_init(ldvarg180);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_sw_init(ldvarg180);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 6: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_set_powergating_state(ldvarg179, ldvarg178);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_set_powergating_state(ldvarg179, ldvarg178);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_set_powergating_state(ldvarg179, ldvarg178);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 7: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_wait_for_idle(ldvarg177);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_wait_for_idle(ldvarg177);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_wait_for_idle(ldvarg177);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 8: ;
  if (ldv_state_variable_114 == 3) {
    ldv_retval_9 = kv_dpm_resume(ldvarg176);
    if (ldv_retval_9 == 0) {
      ldv_state_variable_114 = 2;
    } else {
    }
  } else {
  }
  goto ldv_49456;
  case 9: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_set_clockgating_state(ldvarg175, ldvarg174);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_set_clockgating_state(ldvarg175, ldvarg174);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_set_clockgating_state(ldvarg175, ldvarg174);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 10: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_hw_init(ldvarg173);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_hw_init(ldvarg173);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_hw_init(ldvarg173);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 11: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_soft_reset(ldvarg172);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_soft_reset(ldvarg172);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_soft_reset(ldvarg172);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 12: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_sw_fini(ldvarg171);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_sw_fini(ldvarg171);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_sw_fini(ldvarg171);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 13: ;
  if (ldv_state_variable_114 == 1) {
    kv_dpm_is_idle(ldvarg170);
    ldv_state_variable_114 = 1;
  } else {
  }
  if (ldv_state_variable_114 == 3) {
    kv_dpm_is_idle(ldvarg170);
    ldv_state_variable_114 = 3;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    kv_dpm_is_idle(ldvarg170);
    ldv_state_variable_114 = 2;
  } else {
  }
  goto ldv_49456;
  case 14: ;
  if (ldv_state_variable_114 == 3) {
    ldv_release_114();
    ldv_state_variable_114 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_114 == 2) {
    ldv_release_114();
    ldv_state_variable_114 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_49456;
  case 15: ;
  if (ldv_state_variable_114 == 1) {
    ldv_probe_114();
    ldv_state_variable_114 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_49456;
  default:
  ldv_stop();
  }
  ldv_49456: ;
  return;
}
}
void ldv_main_exported_113(void)
{
  struct amdgpu_ps *ldvarg647 ;
  void *tmp ;
  bool ldvarg645 ;
  struct seq_file *ldvarg651 ;
  void *tmp___0 ;
  bool ldvarg648 ;
  bool ldvarg646 ;
  enum amdgpu_dpm_forced_level ldvarg649 ;
  bool ldvarg650 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg647 = (struct amdgpu_ps *)tmp;
  tmp___0 = ldv_init_zalloc(256UL);
  ldvarg651 = (struct seq_file *)tmp___0;
  ldv_memset((void *)(& ldvarg645), 0, 1UL);
  ldv_memset((void *)(& ldvarg648), 0, 1UL);
  ldv_memset((void *)(& ldvarg646), 0, 1UL);
  ldv_memset((void *)(& ldvarg649), 0, 4UL);
  ldv_memset((void *)(& ldvarg650), 0, 1UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_debugfs_print_current_performance_level(kv_dpm_funcs_group0, ldvarg651);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 1: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_get_sclk(kv_dpm_funcs_group0, (int )ldvarg650);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 2: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_get_temp(kv_dpm_funcs_group0);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 3: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_force_performance_level(kv_dpm_funcs_group0, ldvarg649);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 4: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_post_set_power_state(kv_dpm_funcs_group0);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 5: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_display_configuration_changed(kv_dpm_funcs_group0);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 6: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_enable_bapm(kv_dpm_funcs_group0, (int )ldvarg648);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 7: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_print_power_state(kv_dpm_funcs_group0, ldvarg647);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 8: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_pre_set_power_state(kv_dpm_funcs_group0);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 9: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_set_power_state(kv_dpm_funcs_group0);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 10: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_get_mclk(kv_dpm_funcs_group0, (int )ldvarg646);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  case 11: ;
  if (ldv_state_variable_113 == 1) {
    kv_dpm_powergate_uvd(kv_dpm_funcs_group0, (int )ldvarg645);
    ldv_state_variable_113 = 1;
  } else {
  }
  goto ldv_49484;
  default:
  ldv_stop();
  }
  ldv_49484: ;
  return;
}
}
bool ldv_queue_work_on_579(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_580(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_581(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_582(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_583(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_593(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_595(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_594(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_597(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_596(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_ci_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                u8 const *src , u32 byte_count , u32 limit ) ;
void amdgpu_ci_start_smc(struct amdgpu_device *adev ) ;
void amdgpu_ci_reset_smc(struct amdgpu_device *adev ) ;
int amdgpu_ci_program_jump_on_start(struct amdgpu_device *adev ) ;
void amdgpu_ci_stop_smc_clock(struct amdgpu_device *adev ) ;
void amdgpu_ci_start_smc_clock(struct amdgpu_device *adev ) ;
bool amdgpu_ci_is_smc_running(struct amdgpu_device *adev ) ;
PPSMC_Result amdgpu_ci_send_msg_to_smc(struct amdgpu_device *adev , PPSMC_Msg msg ) ;
PPSMC_Result amdgpu_ci_wait_for_smc_inactive(struct amdgpu_device *adev ) ;
int amdgpu_ci_load_smc_ucode(struct amdgpu_device *adev , u32 limit ) ;
int amdgpu_ci_read_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 *value ,
                                  u32 limit ) ;
int amdgpu_ci_write_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address ,
                                   u32 value , u32 limit ) ;
static int ci_set_smc_sram_address(struct amdgpu_device *adev , u32 smc_address ,
                                   u32 limit )
{
  u32 tmp_ ;
  u32 tmp ;
  {
  if ((smc_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_address + 3U > limit) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 128U, smc_address, 0);
  tmp = amdgpu_mm_rreg(adev, 144U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 144U, tmp_, 0);
  return (0);
}
}
int amdgpu_ci_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                u8 const *src , u32 byte_count , u32 limit )
{
  unsigned long flags ;
  u32 data ;
  u32 original_data ;
  u32 addr ;
  u32 extra_shift ;
  int ret ;
  raw_spinlock_t *tmp ;
  u8 const *tmp___0 ;
  {
  ret = 0;
  if ((smc_start_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_start_address + byte_count > limit) {
    return (-22);
  } else {
  }
  addr = smc_start_address;
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  goto ldv_44455;
  ldv_44454:
  data = (u32 )(((((int )*src << 24) | ((int )*(src + 1UL) << 16)) | ((int )*(src + 2UL) << 8)) | (int )*(src + 3UL));
  ret = ci_set_smc_sram_address(adev, addr, limit);
  if (ret != 0) {
    goto done;
  } else {
  }
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  byte_count = byte_count - 4U;
  addr = addr + 4U;
  ldv_44455: ;
  if (byte_count > 3U) {
    goto ldv_44454;
  } else {
  }
  if (byte_count != 0U) {
    data = 0U;
    ret = ci_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      goto done;
    } else {
    }
    original_data = amdgpu_mm_rreg(adev, 129U, 0);
    extra_shift = (4U - byte_count) * 8U;
    goto ldv_44458;
    ldv_44457:
    tmp___0 = src;
    src = src + 1;
    data = (data << 8) + (u32 )*tmp___0;
    byte_count = byte_count - 1U;
    ldv_44458: ;
    if (byte_count != 0U) {
      goto ldv_44457;
    } else {
    }
    data = data << (int )extra_shift;
    data = (~ ((u32 )(0xffffffffffffffffUL << (int )extra_shift)) & original_data) | data;
    ret = ci_set_smc_sram_address(adev, addr, limit);
    if (ret != 0) {
      goto done;
    } else {
    }
    amdgpu_mm_wreg(adev, 129U, data, 0);
  } else {
  }
  done:
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (ret);
}
}
void amdgpu_ci_start_smc(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 2147483648U);
  tmp = tmp___0;
  tmp = tmp & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483648U, tmp);
  return;
}
}
void amdgpu_ci_reset_smc(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 2147483648U);
  tmp = tmp___0;
  tmp = tmp | 1U;
  (*(adev->smc_wreg))(adev, 2147483648U, tmp);
  return;
}
}
int amdgpu_ci_program_jump_on_start(struct amdgpu_device *adev )
{
  u8 data[4U] ;
  int tmp ;
  {
  data[0] = 224U;
  data[1] = 0U;
  data[2] = 128U;
  data[3] = 64U;
  tmp = amdgpu_ci_copy_bytes_to_smc(adev, 0U, (u8 const *)(& data), 4U, 5U);
  return (tmp);
}
}
void amdgpu_ci_stop_smc_clock(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 2147483652U);
  tmp = tmp___0;
  tmp = tmp | 1U;
  (*(adev->smc_wreg))(adev, 2147483652U, tmp);
  return;
}
}
void amdgpu_ci_start_smc_clock(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 2147483652U);
  tmp = tmp___0;
  tmp = tmp & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483652U, tmp);
  return;
}
}
bool amdgpu_ci_is_smc_running(struct amdgpu_device *adev )
{
  u32 clk ;
  u32 tmp ;
  u32 pc_c ;
  u32 tmp___0 ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  clk = tmp;
  tmp___0 = (*(adev->smc_rreg))(adev, 2147484528U);
  pc_c = tmp___0;
  if ((clk & 1U) == 0U && pc_c > 131327U) {
    return (1);
  } else {
  }
  return (0);
}
}
PPSMC_Result amdgpu_ci_send_msg_to_smc(struct amdgpu_device *adev , PPSMC_Msg msg )
{
  u32 tmp ;
  int i ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  tmp___0 = amdgpu_ci_is_smc_running(adev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (255U);
  } else {
  }
  amdgpu_mm_wreg(adev, 148U, (u32 )msg, 0);
  i = 0;
  goto ldv_44493;
  ldv_44492:
  tmp = amdgpu_mm_rreg(adev, 149U, 0);
  if (tmp != 0U) {
    goto ldv_44491;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_44493: ;
  if (adev->usec_timeout > i) {
    goto ldv_44492;
  } else {
  }
  ldv_44491:
  tmp = amdgpu_mm_rreg(adev, 149U, 0);
  return ((PPSMC_Result )tmp);
}
}
PPSMC_Result amdgpu_ci_wait_for_smc_inactive(struct amdgpu_device *adev )
{
  u32 tmp ;
  int i ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  tmp___0 = amdgpu_ci_is_smc_running(adev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return (1U);
  } else {
  }
  i = 0;
  goto ldv_44501;
  ldv_44500:
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  if ((tmp & 16777216U) == 0U) {
    goto ldv_44499;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_44501: ;
  if (adev->usec_timeout > i) {
    goto ldv_44500;
  } else {
  }
  ldv_44499: ;
  return (1U);
}
}
int amdgpu_ci_load_smc_ucode(struct amdgpu_device *adev , u32 limit )
{
  struct smc_firmware_header_v1_0 const *hdr ;
  unsigned long flags ;
  u32 ucode_start_address ;
  u32 ucode_size ;
  u8 const *src ;
  u32 data ;
  raw_spinlock_t *tmp ;
  u32 tmp_ ;
  u32 tmp___0 ;
  u32 tmp____0 ;
  u32 tmp___1 ;
  {
  if ((unsigned long )adev->pm.fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct smc_firmware_header_v1_0 const *)(adev->pm.fw)->data;
  amdgpu_ucode_print_smc_hdr(& hdr->header);
  adev->pm.fw_version = hdr->header.ucode_version;
  ucode_start_address = hdr->ucode_start_addr;
  ucode_size = hdr->header.ucode_size_bytes;
  src = (adev->pm.fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  if ((ucode_size & 3U) != 0U) {
    return (-22);
  } else {
  }
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, ucode_start_address, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 144U, 0);
  tmp_ = tmp___0;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_ | 1U;
  amdgpu_mm_wreg(adev, 144U, tmp_, 0);
  goto ldv_44517;
  ldv_44516:
  data = (u32 )(((((int )*src << 24) | ((int )*(src + 1UL) << 16)) | ((int )*(src + 2UL) << 8)) | (int )*(src + 3UL));
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  ucode_size = ucode_size - 4U;
  ldv_44517: ;
  if (ucode_size > 3U) {
    goto ldv_44516;
  } else {
  }
  tmp___1 = amdgpu_mm_rreg(adev, 144U, 0);
  tmp____0 = tmp___1;
  tmp____0 = tmp____0 & 4294967294U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 144U, tmp____0, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (0);
}
}
int amdgpu_ci_read_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 *value ,
                                  u32 limit )
{
  unsigned long flags ;
  int ret ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  ret = ci_set_smc_sram_address(adev, smc_address, limit);
  if (ret == 0) {
    *value = amdgpu_mm_rreg(adev, 129U, 0);
  } else {
  }
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (ret);
}
}
int amdgpu_ci_write_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address ,
                                   u32 value , u32 limit )
{
  unsigned long flags ;
  int ret ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  ret = ci_set_smc_sram_address(adev, smc_address, limit);
  if (ret == 0) {
    amdgpu_mm_wreg(adev, 129U, value, 0);
  } else {
  }
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (ret);
}
}
bool ldv_queue_work_on_593(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_594(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_595(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_596(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_597(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_607(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_609(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_608(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_611(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_610(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___1(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_607(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___0(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___1(system_wq, work);
  return (tmp);
}
}
void activate_work_5(struct work_struct *work , int state ) ;
void invoke_work_5(void) ;
void disable_work_5(struct work_struct *work ) ;
void call_and_disable_all_5(int state ) ;
void call_and_disable_work_5(struct work_struct *work ) ;
bool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev ) ;
int amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev , u8 perf_req ,
                                         bool advertise ) ;
u32 amdgpu_dpm_get_vblank_time(struct amdgpu_device *adev ) ;
u32 amdgpu_dpm_get_vrefresh(struct amdgpu_device *adev ) ;
enum amdgpu_pcie_gen amdgpu_get_pcie_gen_support(struct amdgpu_device *adev , u32 sys_mask ,
                                                 enum amdgpu_pcie_gen asic_gen , enum amdgpu_pcie_gen default_gen ) ;
u16 amdgpu_get_pcie_lane_support(struct amdgpu_device *adev , u16 asic_lanes , u16 default_lanes ) ;
u8 amdgpu_encode_pci_lane_width(u32 lanes ) ;
static struct ci_pt_defaults const defaults_hawaii_xt =
     {1U, 15U, 253U, 25U, 5U, 20U, 0U, 720896U, {46U, 0U, 0U, 136U, 0U, 0U, 114U, 96U,
                                               81U, 167U, 121U, 107U, 144U, 189U,
                                               121U}, {535U, 535U, 535U, 578U, 578U,
                                                       578U, 617U, 617U, 617U, 673U,
                                                       673U, 673U, 713U, 713U, 713U}};
static struct ci_pt_defaults const defaults_hawaii_pro =
     {1U, 15U, 253U, 25U, 5U, 20U, 0U, 413794U, {46U, 0U, 0U, 136U, 0U, 0U, 114U, 96U,
                                               81U, 167U, 121U, 107U, 144U, 189U,
                                               121U}, {535U, 535U, 535U, 578U, 578U,
                                                       578U, 617U, 617U, 617U, 673U,
                                                       673U, 673U, 713U, 713U, 713U}};
static struct ci_pt_defaults const defaults_bonaire_xt =
     {1U, 15U, 253U, 25U, 5U, 45U, 0U, 720896U, {121U, 595U, 605U, 174U, 114U, 128U,
                                               131U, 134U, 111U, 200U, 201U, 201U,
                                               47U, 77U, 97U}, {380U, 370U, 384U,
                                                                444U, 435U, 445U,
                                                                518U, 512U, 515U,
                                                                605U, 602U, 597U,
                                                                707U, 709U, 692U}};
static struct ci_pt_defaults const defaults_saturn_xt =
     {1U, 15U, 253U, 25U, 5U, 55U, 0U, 458752U, {140U, 583U, 585U, 166U, 128U, 129U,
                                               139U, 137U, 134U, 201U, 202U, 201U,
                                               77U, 77U, 77U}, {391U, 391U, 391U,
                                                                455U, 455U, 455U,
                                                                528U, 528U, 528U,
                                                                614U, 614U, 614U,
                                                                713U, 713U, 713U}};
static struct ci_pt_config_reg const didt_config_ci[73U] =
  { {16U, 255U, 0U, 0U, 2},
        {16U, 65280U, 8U, 0U, 2},
        {16U, 16711680U, 16U, 0U, 2},
        {16U, 4278190080U, 24U, 0U, 2},
        {17U, 255U, 0U, 0U, 2},
        {17U, 65280U, 8U, 0U, 2},
        {17U, 16711680U, 16U, 0U, 2},
        {17U, 4278190080U, 24U, 0U, 2},
        {18U, 255U, 0U, 0U, 2},
        {18U, 65280U, 8U, 0U, 2},
        {18U, 16711680U, 16U, 0U, 2},
        {18U, 4278190080U, 24U, 0U, 2},
        {2U, 16383U, 0U, 4U, 2},
        {2U, 67043328U, 16U, 128U, 2},
        {2U, 2013265920U, 27U, 3U, 2},
        {1U, 65535U, 0U, 16383U, 2},
        {1U, 4294901760U, 16U, 16383U, 2},
        {0U, 1U, 0U, 0U, 2},
        {48U, 255U, 0U, 0U, 2},
        {48U, 65280U, 8U, 0U, 2},
        {48U, 16711680U, 16U, 0U, 2},
        {48U, 4278190080U, 24U, 0U, 2},
        {49U, 255U, 0U, 0U, 2},
        {49U, 65280U, 8U, 0U, 2},
        {49U, 16711680U, 16U, 0U, 2},
        {49U, 4278190080U, 24U, 0U, 2},
        {50U, 255U, 0U, 0U, 2},
        {50U, 65280U, 8U, 0U, 2},
        {50U, 16711680U, 16U, 0U, 2},
        {50U, 4278190080U, 24U, 0U, 2},
        {34U, 16383U, 0U, 4U, 2},
        {34U, 67043328U, 16U, 128U, 2},
        {34U, 2013265920U, 27U, 3U, 2},
        {33U, 65535U, 0U, 16383U, 2},
        {33U, 4294901760U, 16U, 16383U, 2},
        {32U, 1U, 0U, 0U, 2},
        {80U, 255U, 0U, 0U, 2},
        {80U, 65280U, 8U, 0U, 2},
        {80U, 16711680U, 16U, 0U, 2},
        {80U, 4278190080U, 24U, 0U, 2},
        {81U, 255U, 0U, 0U, 2},
        {81U, 65280U, 8U, 0U, 2},
        {81U, 16711680U, 16U, 0U, 2},
        {81U, 4278190080U, 24U, 0U, 2},
        {82U, 255U, 0U, 0U, 2},
        {82U, 65280U, 8U, 0U, 2},
        {82U, 16711680U, 16U, 0U, 2},
        {82U, 4278190080U, 24U, 0U, 2},
        {66U, 16383U, 0U, 4U, 2},
        {66U, 67043328U, 16U, 128U, 2},
        {66U, 2013265920U, 27U, 3U, 2},
        {65U, 65535U, 0U, 16383U, 2},
        {65U, 4294901760U, 16U, 16383U, 2},
        {64U, 1U, 0U, 0U, 2},
        {112U, 255U, 0U, 0U, 2},
        {112U, 65280U, 8U, 0U, 2},
        {112U, 16711680U, 16U, 0U, 2},
        {112U, 4278190080U, 24U, 0U, 2},
        {113U, 255U, 0U, 0U, 2},
        {113U, 65280U, 8U, 0U, 2},
        {113U, 16711680U, 16U, 0U, 2},
        {113U, 4278190080U, 24U, 0U, 2},
        {114U, 255U, 0U, 0U, 2},
        {114U, 65280U, 8U, 0U, 2},
        {114U, 16711680U, 16U, 0U, 2},
        {114U, 4278190080U, 24U, 0U, 2},
        {98U, 16383U, 0U, 4U, 2},
        {98U, 67043328U, 16U, 128U, 2},
        {98U, 2013265920U, 27U, 3U, 2},
        {97U, 65535U, 0U, 16383U, 2},
        {97U, 4294901760U, 16U, 16383U, 2},
        {96U, 1U, 0U, 0U, 2},
        {4294967295U, 0U, 0U, 0U, 0}};
static u8 ci_get_memory_module_index(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 1485U, 0);
  return ((u8 )(tmp >> 16));
}
}
static int ci_copy_and_switch_arb_sets(struct amdgpu_device *adev , u32 arb_freq_src ,
                                       u32 arb_freq_dest )
{
  u32 mc_arb_dram_timing ;
  u32 mc_arb_dram_timing2 ;
  u32 burst_time ;
  u32 mc_cg_config ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp_ ;
  u32 tmp___1 ;
  u32 tmp____0 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp____1 ;
  u32 tmp___4 ;
  {
  switch (arb_freq_src) {
  case 10U:
  mc_arb_dram_timing = amdgpu_mm_rreg(adev, 2525U, 0);
  mc_arb_dram_timing2 = amdgpu_mm_rreg(adev, 2526U, 0);
  tmp = amdgpu_mm_rreg(adev, 2562U, 0);
  burst_time = tmp & 31U;
  goto ldv_48658;
  case 11U:
  mc_arb_dram_timing = amdgpu_mm_rreg(adev, 2556U, 0);
  mc_arb_dram_timing2 = amdgpu_mm_rreg(adev, 2559U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 2562U, 0);
  burst_time = (tmp___0 & 992U) >> 5;
  goto ldv_48658;
  default: ;
  return (-22);
  }
  ldv_48658: ;
  switch (arb_freq_dest) {
  case 10U:
  amdgpu_mm_wreg(adev, 2525U, mc_arb_dram_timing, 0);
  amdgpu_mm_wreg(adev, 2526U, mc_arb_dram_timing2, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 2562U, 0);
  tmp_ = tmp___1;
  tmp_ = tmp_ & 4294967264U;
  tmp_ = (burst_time & 31U) | tmp_;
  amdgpu_mm_wreg(adev, 2562U, tmp_, 0);
  goto ldv_48663;
  case 11U:
  amdgpu_mm_wreg(adev, 2556U, mc_arb_dram_timing, 0);
  amdgpu_mm_wreg(adev, 2559U, mc_arb_dram_timing2, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 2562U, 0);
  tmp____0 = tmp___2;
  tmp____0 = tmp____0 & 4294966303U;
  tmp____0 = ((burst_time << 5) & 992U) | tmp____0;
  amdgpu_mm_wreg(adev, 2562U, tmp____0, 0);
  goto ldv_48663;
  default: ;
  return (-22);
  }
  ldv_48663:
  tmp___3 = amdgpu_mm_rreg(adev, 2415U, 0);
  mc_cg_config = tmp___3 | 15U;
  amdgpu_mm_wreg(adev, 2415U, mc_cg_config, 0);
  tmp___4 = amdgpu_mm_rreg(adev, 2554U, 0);
  tmp____1 = tmp___4;
  tmp____1 = tmp____1 & 4294967040U;
  tmp____1 = (arb_freq_dest & 255U) | tmp____1;
  amdgpu_mm_wreg(adev, 2554U, tmp____1, 0);
  return (0);
}
}
static u8 ci_get_ddr3_mclk_frequency_ratio(u32 memory_clock )
{
  u8 mc_para_index ;
  {
  if (memory_clock <= 9999U) {
    mc_para_index = 0U;
  } else
  if (memory_clock > 79999U) {
    mc_para_index = 15U;
  } else {
    mc_para_index = (unsigned int )((unsigned char )((memory_clock - 10000U) / 5000U)) + 1U;
  }
  return (mc_para_index);
}
}
static u8 ci_get_mclk_frequency_ratio(u32 memory_clock , bool strobe_mode )
{
  u8 mc_para_index ;
  {
  if ((int )strobe_mode) {
    if (memory_clock <= 12499U) {
      mc_para_index = 0U;
    } else
    if (memory_clock > 47500U) {
      mc_para_index = 15U;
    } else {
      mc_para_index = (unsigned char )((memory_clock - 10000U) / 2500U);
    }
  } else
  if (memory_clock <= 64999U) {
    mc_para_index = 0U;
  } else
  if (memory_clock > 135000U) {
    mc_para_index = 15U;
  } else {
    mc_para_index = (unsigned char )((memory_clock - 60000U) / 5000U);
  }
  return (mc_para_index);
}
}
static void ci_trim_voltage_table_to_fit_state_table(struct amdgpu_device *adev ,
                                                     u32 max_voltage_steps , struct atom_voltage_table *voltage_table )
{
  unsigned int i ;
  unsigned int diff ;
  {
  if (voltage_table->count <= max_voltage_steps) {
    return;
  } else {
  }
  diff = voltage_table->count - max_voltage_steps;
  i = 0U;
  goto ldv_48685;
  ldv_48684:
  voltage_table->entries[i] = voltage_table->entries[i + diff];
  i = i + 1U;
  ldv_48685: ;
  if (i < max_voltage_steps) {
    goto ldv_48684;
  } else {
  }
  voltage_table->count = max_voltage_steps;
  return;
}
}
static int ci_get_std_voltage_value_sidd(struct amdgpu_device *adev , struct atom_voltage_table_entry *voltage_table ,
                                         u16 *std_voltage_hi_sidd , u16 *std_voltage_lo_sidd ) ;
static int ci_set_power_limit(struct amdgpu_device *adev , u32 n ) ;
static int ci_set_overdrive_target_tdp(struct amdgpu_device *adev , u32 target_tdp ) ;
static int ci_update_uvd_dpm(struct amdgpu_device *adev , bool gate ) ;
static void ci_dpm_set_dpm_funcs(struct amdgpu_device *adev ) ;
static void ci_dpm_set_irq_funcs(struct amdgpu_device *adev ) ;
static PPSMC_Result amdgpu_ci_send_msg_to_smc_with_parameter(struct amdgpu_device *adev ,
                                                             PPSMC_Msg msg , u32 parameter ) ;
static void ci_thermal_start_smc_fan_control(struct amdgpu_device *adev ) ;
static void ci_fan_ctrl_set_default_mode(struct amdgpu_device *adev ) ;
static struct ci_power_info *ci_get_pi(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  {
  pi = (struct ci_power_info *)adev->pm.dpm.priv;
  return (pi);
}
}
static struct ci_ps *ci_get_ps(struct amdgpu_ps *rps )
{
  struct ci_ps *ps ;
  {
  ps = (struct ci_ps *)rps->ps_priv;
  return (ps);
}
}
static void ci_initialize_powertune_defaults(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  switch ((int )(adev->pdev)->device) {
  case 26185: ;
  case 26192: ;
  case 26193: ;
  case 26200: ;
  case 26204: ;
  case 26205: ;
  default:
  pi->powertune_defaults = & defaults_bonaire_xt;
  goto ldv_48732;
  case 26176: ;
  case 26177: ;
  case 26182: ;
  case 26183:
  pi->powertune_defaults = & defaults_saturn_xt;
  goto ldv_48732;
  case 26552: ;
  case 26544:
  pi->powertune_defaults = & defaults_hawaii_xt;
  goto ldv_48732;
  case 26554: ;
  case 26545:
  pi->powertune_defaults = & defaults_hawaii_pro;
  goto ldv_48732;
  case 26528: ;
  case 26529: ;
  case 26530: ;
  case 26536: ;
  case 26537: ;
  case 26538: ;
  case 26553: ;
  case 26558:
  pi->powertune_defaults = & defaults_bonaire_xt;
  goto ldv_48732;
  }
  ldv_48732:
  pi->dte_tj_offset = 0U;
  pi->caps_power_containment = 1;
  pi->caps_cac = 0;
  pi->caps_sq_ramping = 0;
  pi->caps_db_ramping = 0;
  pi->caps_td_ramping = 0;
  pi->caps_tcp_ramping = 0;
  if ((int )pi->caps_power_containment) {
    pi->caps_cac = 1;
    if ((unsigned int )adev->asic_type == 3U) {
      pi->enable_bapm_feature = 0;
    } else {
      pi->enable_bapm_feature = 1;
    }
    pi->enable_tdc_limit_feature = 1;
    pi->enable_pkg_pwr_tracking_feature = 1;
  } else {
  }
  return;
}
}
static u8 ci_convert_to_vid(u16 vddc )
{
  {
  return ((u8 )(((1550 - (int )vddc) * 4) / 25));
}
}
static int ci_populate_bapm_vddc_vid_sidd(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u8 *hi_vid ;
  u8 *lo_vid ;
  u8 *hi2_vid ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  hi_vid = (u8 *)(& pi->smc_powertune_table.BapmVddCVidHiSidd);
  lo_vid = (u8 *)(& pi->smc_powertune_table.BapmVddCVidLoSidd);
  hi2_vid = (u8 *)(& pi->smc_powertune_table.BapmVddCVidHiSidd2);
  if ((unsigned long )adev->pm.dpm.dyn_state.cac_leakage_table.entries == (unsigned long )((union amdgpu_cac_leakage_entry *)0)) {
    return (-22);
  } else {
  }
  if (adev->pm.dpm.dyn_state.cac_leakage_table.count > 8U) {
    return (-22);
  } else {
  }
  if (adev->pm.dpm.dyn_state.cac_leakage_table.count != adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.count) {
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_48761;
  ldv_48760: ;
  if ((adev->pm.dpm.platform_caps & 8388608U) != 0U) {
    *(lo_vid + (unsigned long )i) = ci_convert_to_vid((int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc1);
    *(hi_vid + (unsigned long )i) = ci_convert_to_vid((int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc2);
    *(hi2_vid + (unsigned long )i) = ci_convert_to_vid((int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc3);
  } else {
    *(lo_vid + (unsigned long )i) = ci_convert_to_vid((int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField79.vddc);
    *(hi_vid + (unsigned long )i) = ci_convert_to_vid((int )((unsigned short )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField79.leakage));
  }
  i = i + 1U;
  ldv_48761: ;
  if (adev->pm.dpm.dyn_state.cac_leakage_table.count > i) {
    goto ldv_48760;
  } else {
  }
  return (0);
}
}
static int ci_populate_vddc_vid(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u8 *vid ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  vid = (u8 *)(& pi->smc_powertune_table.VddCVid);
  if (pi->vddc_voltage_table.count > 8U) {
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_48770;
  ldv_48769:
  *(vid + (unsigned long )i) = ci_convert_to_vid((int )pi->vddc_voltage_table.entries[i].value);
  i = i + 1U;
  ldv_48770: ;
  if (pi->vddc_voltage_table.count > i) {
    goto ldv_48769;
  } else {
  }
  return (0);
}
}
static int ci_populate_svi_load_line(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_pt_defaults const *pt_defaults ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pt_defaults = pi->powertune_defaults;
  pi->smc_powertune_table.SviLoadLineEn = pt_defaults->svi_load_line_en;
  pi->smc_powertune_table.SviLoadLineVddC = pt_defaults->svi_load_line_vddc;
  pi->smc_powertune_table.SviLoadLineTrimVddC = 3U;
  pi->smc_powertune_table.SviLoadLineOffsetVddC = 0U;
  return (0);
}
}
static int ci_populate_tdc_limit(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_pt_defaults const *pt_defaults ;
  u16 tdc_limit ;
  __u16 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pt_defaults = pi->powertune_defaults;
  tdc_limit = (unsigned int )(adev->pm.dpm.dyn_state.cac_tdp_table)->tdc * 256U;
  tmp___0 = __fswab16((int )tdc_limit);
  pi->smc_powertune_table.TDC_VDDC_PkgLimit = tmp___0;
  pi->smc_powertune_table.TDC_VDDC_ThrottleReleaseLimitPerc = pt_defaults->tdc_vddc_throttle_release_limit_perc;
  pi->smc_powertune_table.TDC_MAWt = pt_defaults->tdc_mawt;
  return (0);
}
}
static int ci_populate_dw8(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_pt_defaults const *pt_defaults ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pt_defaults = pi->powertune_defaults;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131180U, (u32 *)(& pi->smc_powertune_table.TdcWaterfallCtl),
                                      pi->sram_end);
  if (ret != 0) {
    return (-22);
  } else {
    pi->smc_powertune_table.TdcWaterfallCtl = pt_defaults->tdc_waterfall_ctl;
  }
  return (0);
}
}
static int ci_populate_fuzzy_fan(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  __u16 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )((short )adev->pm.dpm.fan.fan_output_sensitivity) < 0 || (unsigned int )adev->pm.dpm.fan.fan_output_sensitivity == 0U) {
    adev->pm.dpm.fan.fan_output_sensitivity = adev->pm.dpm.fan.default_fan_output_sensitivity;
  } else {
  }
  tmp___0 = __fswab16((int )adev->pm.dpm.fan.fan_output_sensitivity);
  pi->smc_powertune_table.FuzzyFan_PwmSetDelta = (int16_t )tmp___0;
  return (0);
}
}
static int ci_min_max_v_gnbl_pm_lid_from_bapm_vddc(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u8 *hi_vid ;
  u8 *lo_vid ;
  int i ;
  int min ;
  int max ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  hi_vid = (u8 *)(& pi->smc_powertune_table.BapmVddCVidHiSidd);
  lo_vid = (u8 *)(& pi->smc_powertune_table.BapmVddCVidLoSidd);
  max = (int )*hi_vid;
  min = max;
  i = 0;
  goto ldv_48803;
  ldv_48802: ;
  if ((unsigned int )*(hi_vid + (unsigned long )i) != 0U) {
    if ((int )*(hi_vid + (unsigned long )i) < min) {
      min = (int )*(hi_vid + (unsigned long )i);
    } else {
    }
    if ((int )*(hi_vid + (unsigned long )i) > max) {
      max = (int )*(hi_vid + (unsigned long )i);
    } else {
    }
  } else {
  }
  if ((unsigned int )*(lo_vid + (unsigned long )i) != 0U) {
    if ((int )*(lo_vid + (unsigned long )i) < min) {
      min = (int )*(lo_vid + (unsigned long )i);
    } else {
    }
    if ((int )*(lo_vid + (unsigned long )i) > max) {
      max = (int )*(lo_vid + (unsigned long )i);
    } else {
    }
  } else {
  }
  i = i + 1;
  ldv_48803: ;
  if (i <= 7) {
    goto ldv_48802;
  } else {
  }
  if (min == 0 || max == 0) {
    return (-22);
  } else {
  }
  pi->smc_powertune_table.GnbLPMLMaxVid = (unsigned char )max;
  pi->smc_powertune_table.GnbLPMLMinVid = (unsigned char )min;
  return (0);
}
}
static int ci_populate_bapm_vddc_base_leakage_sidd(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u16 hi_sidd ;
  u16 lo_sidd ;
  struct amdgpu_cac_tdp_table *cac_tdp_table ;
  __u16 tmp___0 ;
  __u16 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  hi_sidd = pi->smc_powertune_table.BapmVddCBaseLeakageHiSidd;
  lo_sidd = pi->smc_powertune_table.BapmVddCBaseLeakageLoSidd;
  cac_tdp_table = adev->pm.dpm.dyn_state.cac_tdp_table;
  hi_sidd = (unsigned int )((u16 )((unsigned int )cac_tdp_table->high_cac_leakage / 100U)) * 256U;
  lo_sidd = (unsigned int )((u16 )((unsigned int )cac_tdp_table->low_cac_leakage / 100U)) * 256U;
  tmp___0 = __fswab16((int )hi_sidd);
  pi->smc_powertune_table.BapmVddCBaseLeakageHiSidd = tmp___0;
  tmp___1 = __fswab16((int )lo_sidd);
  pi->smc_powertune_table.BapmVddCBaseLeakageLoSidd = tmp___1;
  return (0);
}
}
static int ci_populate_bapm_parameters_in_dpm_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_pt_defaults const *pt_defaults ;
  SMU7_Discrete_DpmTable *dpm_table ;
  struct amdgpu_cac_tdp_table *cac_tdp_table ;
  struct amdgpu_ppm_table *ppm ;
  int i ;
  int j ;
  int k ;
  u16 const *def1 ;
  u16 const *def2 ;
  __u16 tmp___0 ;
  __u16 tmp___1 ;
  __u32 tmp___2 ;
  __u16 tmp___3 ;
  __u16 tmp___4 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pt_defaults = pi->powertune_defaults;
  dpm_table = & pi->smc_state_table;
  cac_tdp_table = adev->pm.dpm.dyn_state.cac_tdp_table;
  ppm = adev->pm.dpm.dyn_state.ppm_table;
  dpm_table->DefaultTdp = (unsigned int )cac_tdp_table->tdp * 256U;
  dpm_table->TargetTdp = (unsigned int )cac_tdp_table->configurable_tdp * 256U;
  dpm_table->DTETjOffset = (unsigned char )pi->dte_tj_offset;
  dpm_table->GpuTjMax = (unsigned char )(pi->thermal_temp_setting.temperature_high / 1000);
  dpm_table->GpuTjHyst = 8U;
  dpm_table->DTEAmbientTempBase = pt_defaults->dte_ambient_temp_base;
  if ((unsigned long )ppm != (unsigned long )((struct amdgpu_ppm_table *)0)) {
    tmp___0 = __fswab16((int )((__u16 )(((int )((unsigned short )ppm->dgpu_tdp) * 256) / 1000)));
    dpm_table->PPM_PkgPwrLimit = tmp___0;
    tmp___1 = __fswab16((int )((unsigned int )((__u16 )ppm->tj_max) * 256U));
    dpm_table->PPM_TemperatureLimit = tmp___1;
  } else {
    dpm_table->PPM_PkgPwrLimit = 0U;
    dpm_table->PPM_TemperatureLimit = 0U;
  }
  tmp___2 = __fswab32(pt_defaults->bapm_temp_gradient);
  dpm_table->BAPM_TEMP_GRADIENT = tmp___2;
  def1 = (u16 const *)(& pt_defaults->bapmti_r);
  def2 = (u16 const *)(& pt_defaults->bapmti_rc);
  i = 0;
  goto ldv_48832;
  ldv_48831:
  j = 0;
  goto ldv_48829;
  ldv_48828:
  k = 0;
  goto ldv_48826;
  ldv_48825:
  tmp___3 = __fswab16((int )*def1);
  dpm_table->BAPMTI_R[i][j][k] = tmp___3;
  tmp___4 = __fswab16((int )*def2);
  dpm_table->BAPMTI_RC[i][j][k] = tmp___4;
  def1 = def1 + 1;
  def2 = def2 + 1;
  k = k + 1;
  ldv_48826: ;
  if (k <= 0) {
    goto ldv_48825;
  } else {
  }
  j = j + 1;
  ldv_48829: ;
  if (j <= 2) {
    goto ldv_48828;
  } else {
  }
  i = i + 1;
  ldv_48832: ;
  if (i <= 4) {
    goto ldv_48831;
  } else {
  }
  return (0);
}
}
static int ci_populate_pm_base(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 pm_fuse_table_offset ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_power_containment) {
    ret = amdgpu_ci_read_smc_sram_dword(adev, 131148U, & pm_fuse_table_offset, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_bapm_vddc_vid_sidd(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_vddc_vid(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_svi_load_line(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_tdc_limit(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_dw8(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_fuzzy_fan(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_min_max_v_gnbl_pm_lid_from_bapm_vddc(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = ci_populate_bapm_vddc_base_leakage_sidd(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ret = amdgpu_ci_copy_bytes_to_smc(adev, pm_fuse_table_offset, (u8 const *)(& pi->smc_powertune_table),
                                      76U, pi->sram_end);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void ci_do_enable_didt(struct amdgpu_device *adev , bool const enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 data ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_sq_ramping) {
    data = (*(adev->didt_rreg))(adev, 0U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 0U, data);
  } else {
  }
  if ((int )pi->caps_db_ramping) {
    data = (*(adev->didt_rreg))(adev, 32U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 32U, data);
  } else {
  }
  if ((int )pi->caps_td_ramping) {
    data = (*(adev->didt_rreg))(adev, 64U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 64U, data);
  } else {
  }
  if ((int )pi->caps_tcp_ramping) {
    data = (*(adev->didt_rreg))(adev, 96U);
    if ((int )enable) {
      data = data | 1U;
    } else {
      data = data & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 96U, data);
  } else {
  }
  return;
}
}
static int ci_program_pt_config_registers(struct amdgpu_device *adev , struct ci_pt_config_reg const *cac_config_regs )
{
  struct ci_pt_config_reg const *config_regs ;
  u32 data ;
  u32 cache ;
  {
  config_regs = cac_config_regs;
  cache = 0U;
  if ((unsigned long )config_regs == (unsigned long )((struct ci_pt_config_reg const *)0)) {
    return (-22);
  } else {
  }
  goto ldv_48862;
  ldv_48861: ;
  if ((unsigned int )config_regs->type == 3U) {
    cache = ((unsigned int )(config_regs->value << (int )config_regs->shift) & (unsigned int )config_regs->mask) | cache;
  } else {
    switch ((unsigned int )config_regs->type) {
    case 1U:
    data = (*(adev->smc_rreg))(adev, config_regs->offset);
    goto ldv_48854;
    case 2U:
    data = (*(adev->didt_rreg))(adev, config_regs->offset);
    goto ldv_48854;
    default:
    data = amdgpu_mm_rreg(adev, config_regs->offset, 0);
    goto ldv_48854;
    }
    ldv_48854:
    data = (u32 )(~ config_regs->mask) & data;
    data = ((unsigned int )(config_regs->value << (int )config_regs->shift) & (unsigned int )config_regs->mask) | data;
    data = data | cache;
    switch ((unsigned int )config_regs->type) {
    case 1U:
    (*(adev->smc_wreg))(adev, config_regs->offset, data);
    goto ldv_48858;
    case 2U:
    (*(adev->didt_wreg))(adev, config_regs->offset, data);
    goto ldv_48858;
    default:
    amdgpu_mm_wreg(adev, config_regs->offset, data, 0);
    goto ldv_48858;
    }
    ldv_48858:
    cache = 0U;
  }
  config_regs = config_regs + 1;
  ldv_48862: ;
  if ((unsigned int )config_regs->offset != 4294967295U) {
    goto ldv_48861;
  } else {
  }
  return (0);
}
}
static int ci_enable_didt(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((((int )pi->caps_sq_ramping || (int )pi->caps_db_ramping) || (int )pi->caps_td_ramping) || (int )pi->caps_tcp_ramping) {
    gfx_v7_0_enter_rlc_safe_mode(adev);
    if ((int )enable) {
      ret = ci_program_pt_config_registers(adev, (struct ci_pt_config_reg const *)(& didt_config_ci));
      if (ret != 0) {
        gfx_v7_0_exit_rlc_safe_mode(adev);
        return (ret);
      } else {
      }
    } else {
    }
    ci_do_enable_didt(adev, (int )enable);
    gfx_v7_0_exit_rlc_safe_mode(adev);
  } else {
  }
  return (0);
}
}
static int ci_enable_power_containment(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  int ret ;
  struct amdgpu_cac_tdp_table *cac_tdp_table ;
  u32 default_pwr_limit ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )enable) {
    pi->power_containment_features = 0U;
    if ((int )pi->caps_power_containment) {
      if ((int )pi->enable_bapm_feature) {
        smc_result = amdgpu_ci_send_msg_to_smc(adev, 135);
        if ((unsigned int )smc_result != 1U) {
          ret = -22;
        } else {
          pi->power_containment_features = pi->power_containment_features | 1U;
        }
      } else {
      }
      if ((int )pi->enable_tdc_limit_feature) {
        smc_result = amdgpu_ci_send_msg_to_smc(adev, 361);
        if ((unsigned int )smc_result != 1U) {
          ret = -22;
        } else {
          pi->power_containment_features = pi->power_containment_features | 2U;
        }
      } else {
      }
      if ((int )pi->enable_pkg_pwr_tracking_feature) {
        smc_result = amdgpu_ci_send_msg_to_smc(adev, 389);
        if ((unsigned int )smc_result != 1U) {
          ret = -22;
        } else {
          cac_tdp_table = adev->pm.dpm.dyn_state.cac_tdp_table;
          default_pwr_limit = (unsigned int )((int )cac_tdp_table->maximum_power_delivery_limit * 256);
          pi->power_containment_features = pi->power_containment_features | 4U;
          ci_set_power_limit(adev, default_pwr_limit);
        }
      } else {
      }
    } else {
    }
  } else
  if ((int )pi->caps_power_containment && pi->power_containment_features != 0U) {
    if ((pi->power_containment_features & 2U) != 0U) {
      amdgpu_ci_send_msg_to_smc(adev, 362);
    } else {
    }
    if ((int )pi->power_containment_features & 1) {
      amdgpu_ci_send_msg_to_smc(adev, 136);
    } else {
    }
    if ((pi->power_containment_features & 4U) != 0U) {
      amdgpu_ci_send_msg_to_smc(adev, 390);
    } else {
    }
    pi->power_containment_features = 0U;
  } else {
  }
  return (ret);
}
}
static int ci_enable_smc_cac(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )pi->caps_cac) {
    if ((int )enable) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 83);
      if ((unsigned int )smc_result != 1U) {
        ret = -22;
        pi->cac_enabled = 0;
      } else {
        pi->cac_enabled = 1;
      }
    } else
    if ((int )pi->cac_enabled) {
      amdgpu_ci_send_msg_to_smc(adev, 84);
      pi->cac_enabled = 0;
    } else {
    }
  } else {
  }
  return (ret);
}
}
static int ci_enable_thermal_based_sclk_dpm(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  smc_result = 1U;
  if (pi->thermal_sclk_dpm_enabled != 0U) {
    if ((int )enable) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 412);
    } else {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 413);
    }
  } else {
  }
  if ((unsigned int )smc_result == 1U) {
    return (0);
  } else {
    return (-22);
  }
}
}
static int ci_power_control_set_level(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_cac_tdp_table *cac_tdp_table ;
  s32 adjust_percent ;
  s32 target_tdp ;
  int ret ;
  bool adjust_polarity ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  cac_tdp_table = adev->pm.dpm.dyn_state.cac_tdp_table;
  ret = 0;
  adjust_polarity = 0;
  if ((int )pi->caps_power_containment) {
    adjust_percent = (s32 )((int )adjust_polarity ? adev->pm.dpm.tdp_adjustment : - adev->pm.dpm.tdp_adjustment);
    target_tdp = ((adjust_percent + 100) * (int )cac_tdp_table->configurable_tdp) / 100;
    ret = ci_set_overdrive_target_tdp(adev, (unsigned int )target_tdp);
  } else {
  }
  return (ret);
}
}
static void ci_dpm_powergate_uvd(struct amdgpu_device *adev , bool gate )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->uvd_power_gated == (int )gate) {
    return;
  } else {
  }
  pi->uvd_power_gated = gate;
  ci_update_uvd_dpm(adev, (int )gate);
  return;
}
}
static bool ci_dpm_vblank_too_short(struct amdgpu_device *adev )
{
  u32 vblank_time ;
  u32 tmp ;
  u32 switch_limit ;
  {
  tmp = amdgpu_dpm_get_vblank_time(adev);
  vblank_time = tmp;
  switch_limit = adev->mc.vram_type == 5U ? 450U : 300U;
  if (vblank_time < switch_limit) {
    return (1);
  } else {
    return (0);
  }
}
}
static void ci_apply_state_adjust_rules(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct ci_ps *ps ;
  struct ci_ps *tmp ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  struct amdgpu_clock_and_voltage_limits *max_limits ;
  bool disable_mclk_switching ;
  u32 sclk ;
  u32 mclk ;
  int i ;
  bool tmp___1 ;
  {
  tmp = ci_get_ps(rps);
  ps = tmp;
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  if ((int )rps->vce_active) {
    rps->evclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].evclk;
    rps->ecclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].ecclk;
  } else {
    rps->evclk = 0U;
    rps->ecclk = 0U;
  }
  if (adev->pm.dpm.new_active_crtc_count > 1) {
    disable_mclk_switching = 1;
  } else {
    tmp___1 = ci_dpm_vblank_too_short(adev);
    if ((int )tmp___1) {
      disable_mclk_switching = 1;
    } else {
      disable_mclk_switching = 0;
    }
  }
  if ((rps->class & 7U) == 1U) {
    pi->battery_state = 1;
  } else {
    pi->battery_state = 0;
  }
  if ((int )adev->pm.dpm.ac_power) {
    max_limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  } else {
    max_limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_dc;
  }
  if (! adev->pm.dpm.ac_power) {
    i = 0;
    goto ldv_48923;
    ldv_48922: ;
    if (ps->performance_levels[i].mclk > max_limits->mclk) {
      ps->performance_levels[i].mclk = max_limits->mclk;
    } else {
    }
    if (ps->performance_levels[i].sclk > max_limits->sclk) {
      ps->performance_levels[i].sclk = max_limits->sclk;
    } else {
    }
    i = i + 1;
    ldv_48923: ;
    if ((int )ps->performance_level_count > i) {
      goto ldv_48922;
    } else {
    }
  } else {
  }
  if ((int )disable_mclk_switching) {
    mclk = ps->performance_levels[(int )ps->performance_level_count + -1].mclk;
    sclk = ps->performance_levels[0].sclk;
  } else {
    mclk = ps->performance_levels[0].mclk;
    sclk = ps->performance_levels[0].sclk;
  }
  if ((int )rps->vce_active) {
    if (adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].sclk > sclk) {
      sclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].sclk;
    } else {
    }
    if (adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].mclk > mclk) {
      mclk = adev->pm.dpm.vce_states[(unsigned int )adev->pm.dpm.vce_level].mclk;
    } else {
    }
  } else {
  }
  ps->performance_levels[0].sclk = sclk;
  ps->performance_levels[0].mclk = mclk;
  if (ps->performance_levels[1].sclk < ps->performance_levels[0].sclk) {
    ps->performance_levels[1].sclk = ps->performance_levels[0].sclk;
  } else {
  }
  if ((int )disable_mclk_switching) {
    if (ps->performance_levels[0].mclk < ps->performance_levels[1].mclk) {
      ps->performance_levels[0].mclk = ps->performance_levels[1].mclk;
    } else {
    }
  } else
  if (ps->performance_levels[1].mclk < ps->performance_levels[0].mclk) {
    ps->performance_levels[1].mclk = ps->performance_levels[0].mclk;
  } else {
  }
  return;
}
}
static int ci_thermal_set_temperature_range(struct amdgpu_device *adev , int min_temp ,
                                            int max_temp )
{
  int low_temp ;
  int high_temp ;
  u32 tmp ;
  {
  low_temp = 0;
  high_temp = 255000;
  if (low_temp < min_temp) {
    low_temp = min_temp;
  } else {
  }
  if (high_temp > max_temp) {
    high_temp = max_temp;
  } else {
  }
  if (high_temp < low_temp) {
    drm_err("invalid thermal range: %d - %d\n", low_temp, high_temp);
    return (-22);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3224371212U);
  tmp = tmp & 4278190335U;
  tmp = (u32 )((high_temp / 1000 << 8) | (low_temp / 1000 << 16)) | tmp;
  (*(adev->smc_wreg))(adev, 3224371212U, tmp);
  adev->pm.dpm.thermal.min_temp = low_temp;
  adev->pm.dpm.thermal.max_temp = high_temp;
  return (0);
}
}
static int ci_thermal_enable_alert(struct amdgpu_device *adev , bool enable )
{
  u32 thermal_int ;
  u32 tmp ;
  PPSMC_Result result ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp = (*(adev->smc_rreg))(adev, 3224371212U);
  thermal_int = tmp;
  if ((int )enable) {
    thermal_int = thermal_int & 4244635647U;
    (*(adev->smc_wreg))(adev, 3224371212U, thermal_int);
    result = amdgpu_ci_send_msg_to_smc(adev, 266);
    if ((unsigned int )result != 1U) {
      tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("ci_thermal_enable_alert", "Could not enable thermal interrupts.\n");
      } else {
      }
      return (-22);
    } else {
    }
  } else {
    thermal_int = thermal_int | 50331648U;
    (*(adev->smc_wreg))(adev, 3224371212U, thermal_int);
    result = amdgpu_ci_send_msg_to_smc(adev, 307);
    if ((unsigned int )result != 1U) {
      tmp___1 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___1 != 0L) {
        drm_ut_debug_printk("ci_thermal_enable_alert", "Could not disable thermal interrupts.\n");
      } else {
      }
      return (-22);
    } else {
    }
  }
  return (0);
}
}
static void ci_fan_ctrl_set_static_mode(struct amdgpu_device *adev , u32 mode )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->fan_ctrl_is_in_default_mode) {
    tmp___1 = (*(adev->smc_rreg))(adev, 3224371308U);
    tmp___0 = (tmp___1 & 14336U) >> 11;
    pi->fan_ctrl_default_mode = tmp___0;
    tmp___2 = (*(adev->smc_rreg))(adev, 3224371308U);
    tmp___0 = tmp___2 & 255U;
    pi->t_min = tmp___0;
    pi->fan_ctrl_is_in_default_mode = 0;
  } else {
  }
  tmp___3 = (*(adev->smc_rreg))(adev, 3224371308U);
  tmp___0 = tmp___3 & 4294967040U;
  tmp___0 = tmp___0;
  (*(adev->smc_wreg))(adev, 3224371308U, tmp___0);
  tmp___4 = (*(adev->smc_rreg))(adev, 3224371308U);
  tmp___0 = tmp___4 & 4294952959U;
  tmp___0 = (mode << 11) | tmp___0;
  (*(adev->smc_wreg))(adev, 3224371308U, tmp___0);
  return;
}
}
static int ci_thermal_setup_fan_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  SMU7_Discrete_FanTable fan_table ;
  u32 duty100 ;
  u32 t_diff1 ;
  u32 t_diff2 ;
  u32 pwm_diff1 ;
  u32 pwm_diff2 ;
  u16 fdo_min ;
  u16 slope1 ;
  u16 slope2 ;
  u32 reference_clock ;
  u32 tmp___0 ;
  int ret ;
  u64 tmp64 ;
  u32 tmp___1 ;
  u32 __base ;
  u32 __rem ;
  __u16 tmp___2 ;
  __u16 tmp___3 ;
  __u16 tmp___4 ;
  __u16 tmp___5 ;
  __u16 tmp___6 ;
  __u16 tmp___7 ;
  __u16 tmp___8 ;
  __u32 tmp___9 ;
  __u16 tmp___10 ;
  u32 tmp___11 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  fan_table.FdoMode = 0U;
  fan_table.TempMin = (short)0;
  fan_table.TempMed = (short)0;
  fan_table.TempMax = (short)0;
  fan_table.Slope1 = (short)0;
  fan_table.Slope2 = (short)0;
  fan_table.FdoMin = (short)0;
  fan_table.HystUp = (short)0;
  fan_table.HystDown = (short)0;
  fan_table.HystSlope = (short)0;
  fan_table.TempRespLim = (short)0;
  fan_table.TempCurr = (short)0;
  fan_table.SlopeCurr = (short)0;
  fan_table.PwmCurr = (short)0;
  fan_table.RefreshPeriod = 0U;
  fan_table.FdoMax = (short)0;
  fan_table.TempSrc = (unsigned char)0;
  fan_table.Padding = (signed char)0;
  if (pi->fan_table_start == 0U) {
    adev->pm.dpm.fan.ucode_fan_control = 0;
    return (0);
  } else {
  }
  tmp___1 = (*(adev->smc_rreg))(adev, 3224371304U);
  duty100 = tmp___1 & 255U;
  if (duty100 == 0U) {
    adev->pm.dpm.fan.ucode_fan_control = 0;
    return (0);
  } else {
  }
  tmp64 = (unsigned long long )adev->pm.dpm.fan.pwm_min * (unsigned long long )duty100;
  __base = 10000U;
  __rem = (u32 )(tmp64 % (u64 )__base);
  tmp64 = tmp64 / (u64 )__base;
  fdo_min = (unsigned short )tmp64;
  t_diff1 = (u32 )((int )adev->pm.dpm.fan.t_med - (int )adev->pm.dpm.fan.t_min);
  t_diff2 = (u32 )((int )adev->pm.dpm.fan.t_high - (int )adev->pm.dpm.fan.t_med);
  pwm_diff1 = (u32 )((int )adev->pm.dpm.fan.pwm_med - (int )adev->pm.dpm.fan.pwm_min);
  pwm_diff2 = (u32 )((int )adev->pm.dpm.fan.pwm_high - (int )adev->pm.dpm.fan.pwm_med);
  slope1 = (unsigned short )((((duty100 * pwm_diff1) * 16U) / t_diff1 + 50U) / 100U);
  slope2 = (unsigned short )((((duty100 * pwm_diff2) * 16U) / t_diff2 + 50U) / 100U);
  tmp___2 = __fswab16((int )((__u16 )(((int )adev->pm.dpm.fan.t_min + 50) / 100)));
  fan_table.TempMin = (int16_t )tmp___2;
  tmp___3 = __fswab16((int )((__u16 )(((int )adev->pm.dpm.fan.t_med + 50) / 100)));
  fan_table.TempMed = (int16_t )tmp___3;
  tmp___4 = __fswab16((int )((__u16 )(((int )adev->pm.dpm.fan.t_max + 50) / 100)));
  fan_table.TempMax = (int16_t )tmp___4;
  tmp___5 = __fswab16((int )slope1);
  fan_table.Slope1 = (int16_t )tmp___5;
  tmp___6 = __fswab16((int )slope2);
  fan_table.Slope2 = (int16_t )tmp___6;
  tmp___7 = __fswab16((int )fdo_min);
  fan_table.FdoMin = (int16_t )tmp___7;
  tmp___8 = __fswab16((int )adev->pm.dpm.fan.t_hyst);
  fan_table.HystDown = (int16_t )tmp___8;
  fan_table.HystUp = 256;
  fan_table.HystSlope = 256;
  fan_table.TempRespLim = 1280;
  reference_clock = (*((adev->asic_funcs)->get_xclk))(adev);
  tmp___9 = __fswab32((adev->pm.dpm.fan.cycle_delay * reference_clock) / 1600U);
  fan_table.RefreshPeriod = tmp___9;
  tmp___10 = __fswab16((int )((unsigned short )duty100));
  fan_table.FdoMax = (int16_t )tmp___10;
  tmp___11 = (*(adev->smc_rreg))(adev, 3224371216U);
  tmp___0 = (tmp___11 & 267386880U) >> 20;
  fan_table.TempSrc = (unsigned char )tmp___0;
  ret = amdgpu_ci_copy_bytes_to_smc(adev, pi->fan_table_start, (u8 const *)(& fan_table),
                                    36U, pi->sram_end);
  if (ret != 0) {
    drm_err("Failed to load fan table to the SMC.");
    adev->pm.dpm.fan.ucode_fan_control = 0;
  } else {
  }
  return (0);
}
}
static int ci_fan_ctrl_start_smc_fan_control(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_od_fuzzy_fan_control_support) {
    ret = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 91, 0U);
    if ((unsigned int )ret != 1U) {
      return (-22);
    } else {
    }
    ret = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 410, (u32 )adev->pm.dpm.fan.default_max_fan_pwm);
    if ((unsigned int )ret != 1U) {
      return (-22);
    } else {
    }
  } else {
    ret = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 91, 1U);
    if ((unsigned int )ret != 1U) {
      return (-22);
    } else {
    }
  }
  pi->fan_is_controlled_by_smc = 1;
  return (0);
}
}
static int ci_fan_ctrl_stop_smc_fan_control(struct amdgpu_device *adev )
{
  PPSMC_Result ret ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = amdgpu_ci_send_msg_to_smc(adev, 92);
  if ((unsigned int )ret == 1U) {
    pi->fan_is_controlled_by_smc = 0;
    return (0);
  } else {
    return (-22);
  }
}
}
static int ci_dpm_get_fan_speed_percent(struct amdgpu_device *adev , u32 *speed )
{
  u32 duty ;
  u32 duty100 ;
  u64 tmp64 ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 __base ;
  u32 __rem ;
  {
  if ((int )adev->pm.no_fan) {
    return (-2);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3224371304U);
  duty100 = tmp & 255U;
  tmp___0 = (*(adev->smc_rreg))(adev, 3224371208U);
  duty = (tmp___0 & 130560U) >> 9;
  if (duty100 == 0U) {
    return (-22);
  } else {
  }
  tmp64 = (unsigned long long )duty * 100ULL;
  __base = duty100;
  __rem = (u32 )(tmp64 % (u64 )__base);
  tmp64 = tmp64 / (u64 )__base;
  *speed = (unsigned int )tmp64;
  if (*speed > 100U) {
    *speed = 100U;
  } else {
  }
  return (0);
}
}
static int ci_dpm_set_fan_speed_percent(struct amdgpu_device *adev , u32 speed )
{
  u32 tmp ;
  u32 duty ;
  u32 duty100 ;
  u64 tmp64 ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  u32 tmp___1 ;
  u32 __base ;
  u32 __rem ;
  u32 tmp___2 ;
  {
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  if ((int )adev->pm.no_fan) {
    return (-2);
  } else {
  }
  if ((int )pi->fan_is_controlled_by_smc) {
    return (-22);
  } else {
  }
  if (speed > 100U) {
    return (-22);
  } else {
  }
  tmp___1 = (*(adev->smc_rreg))(adev, 3224371304U);
  duty100 = tmp___1 & 255U;
  if (duty100 == 0U) {
    return (-22);
  } else {
  }
  tmp64 = (unsigned long long )speed * (unsigned long long )duty100;
  __base = 100U;
  __rem = (u32 )(tmp64 % (u64 )__base);
  tmp64 = tmp64 / (u64 )__base;
  duty = (unsigned int )tmp64;
  tmp___2 = (*(adev->smc_rreg))(adev, 3224371300U);
  tmp = tmp___2 & 4294967040U;
  tmp = tmp | duty;
  (*(adev->smc_wreg))(adev, 3224371300U, tmp);
  return (0);
}
}
static void ci_dpm_set_fan_control_mode(struct amdgpu_device *adev , u32 mode )
{
  {
  if (mode != 0U) {
    if ((int )adev->pm.dpm.fan.ucode_fan_control) {
      ci_fan_ctrl_stop_smc_fan_control(adev);
    } else {
    }
    ci_fan_ctrl_set_static_mode(adev, mode);
  } else
  if ((int )adev->pm.dpm.fan.ucode_fan_control) {
    ci_thermal_start_smc_fan_control(adev);
  } else {
    ci_fan_ctrl_set_default_mode(adev);
  }
  return;
}
}
static u32 ci_dpm_get_fan_control_mode(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )pi->fan_is_controlled_by_smc) {
    return (0U);
  } else {
  }
  tmp___1 = (*(adev->smc_rreg))(adev, 3224371308U);
  tmp___0 = tmp___1 & 14336U;
  return (tmp___0 >> 11);
}
}
static void ci_fan_ctrl_set_default_mode(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (! pi->fan_ctrl_is_in_default_mode) {
    tmp___1 = (*(adev->smc_rreg))(adev, 3224371308U);
    tmp___0 = tmp___1 & 4294952959U;
    tmp___0 = (pi->fan_ctrl_default_mode << 11) | tmp___0;
    (*(adev->smc_wreg))(adev, 3224371308U, tmp___0);
    tmp___2 = (*(adev->smc_rreg))(adev, 3224371308U);
    tmp___0 = tmp___2 & 4294967040U;
    tmp___0 = pi->t_min | tmp___0;
    (*(adev->smc_wreg))(adev, 3224371308U, tmp___0);
    pi->fan_ctrl_is_in_default_mode = 1;
  } else {
  }
  return;
}
}
static void ci_thermal_start_smc_fan_control(struct amdgpu_device *adev )
{
  {
  if ((int )adev->pm.dpm.fan.ucode_fan_control) {
    ci_fan_ctrl_start_smc_fan_control(adev);
    ci_fan_ctrl_set_static_mode(adev, 1U);
  } else {
  }
  return;
}
}
static void ci_thermal_initialize(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  {
  if ((unsigned int )adev->pm.fan_pulses_per_revolution != 0U) {
    tmp___0 = (*(adev->smc_rreg))(adev, 3224371312U);
    tmp = tmp___0 & 4294967288U;
    tmp = (u32 )((int )adev->pm.fan_pulses_per_revolution + -1) | tmp;
    (*(adev->smc_wreg))(adev, 3224371312U, tmp);
  } else {
  }
  tmp___1 = (*(adev->smc_rreg))(adev, 3224371308U);
  tmp = tmp___1 & 33554431U;
  tmp = tmp | 1342177280U;
  (*(adev->smc_wreg))(adev, 3224371308U, tmp);
  return;
}
}
static int ci_thermal_start_thermal_controller(struct amdgpu_device *adev )
{
  int ret ;
  {
  ci_thermal_initialize(adev);
  ret = ci_thermal_set_temperature_range(adev, 90000, 120000);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_thermal_enable_alert(adev, 1);
  if (ret != 0) {
    return (ret);
  } else {
  }
  if ((int )adev->pm.dpm.fan.ucode_fan_control) {
    ret = ci_thermal_setup_fan_table(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
    ci_thermal_start_smc_fan_control(adev);
  } else {
  }
  return (0);
}
}
static void ci_thermal_stop_thermal_controller(struct amdgpu_device *adev )
{
  {
  if (! adev->pm.no_fan) {
    ci_fan_ctrl_set_default_mode(adev);
  } else {
  }
  return;
}
}
static int ci_write_smc_soft_register(struct amdgpu_device *adev , u16 reg_offset ,
                                      u32 value )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = amdgpu_ci_write_smc_sram_dword(adev, pi->soft_regs_start + (u32 )reg_offset,
                                           value, pi->sram_end);
  return (tmp___0);
}
}
static void ci_init_fps_limits(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  SMU7_Discrete_DpmTable *table ;
  u16 tmp___0 ;
  __u16 tmp___1 ;
  __u16 tmp___2 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  table = & pi->smc_state_table;
  if ((int )pi->caps_fps) {
    tmp___0 = 45U;
    tmp___1 = __fswab16((int )tmp___0);
    table->FpsHighT = tmp___1;
    tmp___0 = 30U;
    tmp___2 = __fswab16((int )tmp___0);
    table->FpsLowT = tmp___2;
  } else {
  }
  return;
}
}
static int ci_update_sclk_t(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  u32 low_sclk_interrupt_t ;
  __u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = 0;
  low_sclk_interrupt_t = 0U;
  if ((int )pi->caps_sclk_throttle_low_notification) {
    tmp___0 = __fswab32(pi->low_sclk_interrupt_t);
    low_sclk_interrupt_t = tmp___0;
    ret = amdgpu_ci_copy_bytes_to_smc(adev, pi->dpm_table_start + 2020U, (u8 const *)(& low_sclk_interrupt_t),
                                      4U, pi->sram_end);
  } else {
  }
  return (ret);
}
}
static void ci_get_leakage_voltages(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u16 leakage_id ;
  u16 virtual_voltage_id ;
  u16 vddc ;
  u16 vddci ;
  int i ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pi->vddc_leakage.count = 0U;
  pi->vddci_leakage.count = 0U;
  if ((adev->pm.dpm.platform_caps & 8388608U) != 0U) {
    i = 0;
    goto ldv_49055;
    ldv_49054:
    virtual_voltage_id = (unsigned int )((u16 )i) + 65281U;
    tmp___0 = amdgpu_atombios_get_voltage_evv(adev, (int )virtual_voltage_id, & vddc);
    if (tmp___0 != 0) {
      goto ldv_49053;
    } else {
    }
    if ((unsigned int )vddc != 0U && (int )vddc != (int )virtual_voltage_id) {
      pi->vddc_leakage.actual_voltage[(int )pi->vddc_leakage.count] = vddc;
      pi->vddc_leakage.leakage_id[(int )pi->vddc_leakage.count] = virtual_voltage_id;
      pi->vddc_leakage.count = (u16 )((int )pi->vddc_leakage.count + 1);
    } else {
    }
    ldv_49053:
    i = i + 1;
    ldv_49055: ;
    if (i <= 7) {
      goto ldv_49054;
    } else {
    }
  } else {
    tmp___2 = amdgpu_atombios_get_leakage_id_from_vbios(adev, & leakage_id);
    if (tmp___2 == 0) {
      i = 0;
      goto ldv_49058;
      ldv_49057:
      virtual_voltage_id = (unsigned int )((u16 )i) + 65281U;
      tmp___1 = amdgpu_atombios_get_leakage_vddc_based_on_leakage_params(adev, & vddc,
                                                                         & vddci,
                                                                         (int )virtual_voltage_id,
                                                                         (int )leakage_id);
      if (tmp___1 == 0) {
        if ((unsigned int )vddc != 0U && (int )vddc != (int )virtual_voltage_id) {
          pi->vddc_leakage.actual_voltage[(int )pi->vddc_leakage.count] = vddc;
          pi->vddc_leakage.leakage_id[(int )pi->vddc_leakage.count] = virtual_voltage_id;
          pi->vddc_leakage.count = (u16 )((int )pi->vddc_leakage.count + 1);
        } else {
        }
        if ((unsigned int )vddci != 0U && (int )vddci != (int )virtual_voltage_id) {
          pi->vddci_leakage.actual_voltage[(int )pi->vddci_leakage.count] = vddci;
          pi->vddci_leakage.leakage_id[(int )pi->vddci_leakage.count] = virtual_voltage_id;
          pi->vddci_leakage.count = (u16 )((int )pi->vddci_leakage.count + 1);
        } else {
        }
      } else {
      }
      i = i + 1;
      ldv_49058: ;
      if (i <= 7) {
        goto ldv_49057;
      } else {
      }
    } else {
    }
  }
  return;
}
}
static void ci_set_dpm_event_sources(struct amdgpu_device *adev , u32 sources )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  bool want_thermal_protection ;
  enum amdgpu_dpm_event_src dpm_event_src ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  switch (sources) {
  case 0U: ;
  default:
  want_thermal_protection = 0;
  goto ldv_49070;
  case 1U:
  want_thermal_protection = 1;
  dpm_event_src = 2;
  goto ldv_49070;
  case 2U:
  want_thermal_protection = 1;
  dpm_event_src = 1;
  goto ldv_49070;
  case 3U:
  want_thermal_protection = 1;
  dpm_event_src = 4;
  goto ldv_49070;
  }
  ldv_49070: ;
  if ((int )want_thermal_protection) {
    tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
    if ((int )pi->thermal_protection) {
      tmp___0 = tmp___0 & 4294967291U;
    } else {
      tmp___0 = tmp___0 | 4U;
    }
    (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
  } else {
    tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
    tmp___0 = tmp___0 | 4U;
    (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
  }
  return;
}
}
static void ci_enable_auto_throttle_source(struct amdgpu_device *adev , enum amdgpu_dpm_auto_throttle_src source ,
                                           bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )enable) {
    if ((pi->active_auto_throttle_sources & (u32 )(1 << (int )source)) == 0U) {
      pi->active_auto_throttle_sources = pi->active_auto_throttle_sources | (u32 )(1 << (int )source);
      ci_set_dpm_event_sources(adev, pi->active_auto_throttle_sources);
    } else {
    }
  } else
  if ((pi->active_auto_throttle_sources & (u32 )(1 << (int )source)) != 0U) {
    pi->active_auto_throttle_sources = pi->active_auto_throttle_sources & (u32 )(~ (1 << (int )source));
    ci_set_dpm_event_sources(adev, pi->active_auto_throttle_sources);
  } else {
  }
  return;
}
}
static void ci_enable_vr_hot_gpio_interrupt(struct amdgpu_device *adev )
{
  {
  if ((adev->pm.dpm.platform_caps & 65536U) != 0U) {
    amdgpu_ci_send_msg_to_smc(adev, 330);
  } else {
  }
  return;
}
}
static int ci_unfreeze_sclk_mclk_dpm(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->need_update_smu7_dpm_table == 0U) {
    return (0);
  } else {
  }
  if (pi->sclk_dpm_key_disabled == 0U && (pi->need_update_smu7_dpm_table & 5U) != 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 394);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  if (pi->mclk_dpm_key_disabled == 0U && (pi->need_update_smu7_dpm_table & 2U) != 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 396);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  pi->need_update_smu7_dpm_table = 0U;
  return (0);
}
}
static int ci_enable_sclk_mclk_dpm(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  u32 tmp_ ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )enable) {
    if (pi->sclk_dpm_key_disabled == 0U) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 334);
      if ((unsigned int )smc_result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
    if (pi->mclk_dpm_key_disabled == 0U) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 336);
      if ((unsigned int )smc_result != 1U) {
        return (-22);
      } else {
      }
      tmp___0 = amdgpu_mm_rreg(adev, 3456U, 0);
      tmp_ = tmp___0;
      tmp_ = tmp_ & 2147483647U;
      tmp_ = tmp_ | 2147483648U;
      amdgpu_mm_wreg(adev, 3456U, tmp_, 0);
      (*(adev->smc_wreg))(adev, 3225423152U, 5U);
      (*(adev->smc_wreg))(adev, 3225423164U, 5U);
      (*(adev->smc_wreg))(adev, 3225423232U, 1048581U);
      __const_udelay(42950UL);
      (*(adev->smc_wreg))(adev, 3225423152U, 4194309U);
      (*(adev->smc_wreg))(adev, 3225423164U, 4194309U);
      (*(adev->smc_wreg))(adev, 3225423232U, 5242885U);
    } else {
    }
  } else {
    if (pi->sclk_dpm_key_disabled == 0U) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 335);
      if ((unsigned int )smc_result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
    if (pi->mclk_dpm_key_disabled == 0U) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 337);
      if ((unsigned int )smc_result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
  }
  return (0);
}
}
static int ci_start_dpm(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  int ret ;
  u32 tmp___0 ;
  u32 tmp_ ;
  u32 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp___0 = tmp___0 | 1U;
  (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322632U);
  tmp___0 = tmp___0 | 2097152U;
  (*(adev->smc_wreg))(adev, 3223322632U, tmp___0);
  ci_write_smc_soft_register(adev, 44, 4096U);
  tmp___1 = amdgpu_mm_rreg(adev, 5256U, 0);
  tmp_ = tmp___1;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 5256U, tmp_, 0);
  smc_result = amdgpu_ci_send_msg_to_smc(adev, 265);
  if ((unsigned int )smc_result != 1U) {
    return (-22);
  } else {
  }
  ret = ci_enable_sclk_mclk_dpm(adev, 1);
  if (ret != 0) {
    return (ret);
  } else {
  }
  if (pi->pcie_dpm_key_disabled == 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 310);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_freeze_sclk_mclk_dpm(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->need_update_smu7_dpm_table == 0U) {
    return (0);
  } else {
  }
  if (pi->sclk_dpm_key_disabled == 0U && (pi->need_update_smu7_dpm_table & 5U) != 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 393);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  if (pi->mclk_dpm_key_disabled == 0U && (pi->need_update_smu7_dpm_table & 2U) != 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 395);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_stop_dpm(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  int ret ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp___0 = tmp___0 & 4294967294U;
  (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322632U);
  tmp___0 = tmp___0 & 4292870143U;
  (*(adev->smc_wreg))(adev, 3223322632U, tmp___0);
  if (pi->pcie_dpm_key_disabled == 0U) {
    smc_result = amdgpu_ci_send_msg_to_smc(adev, 317);
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  ret = ci_enable_sclk_mclk_dpm(adev, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  smc_result = amdgpu_ci_send_msg_to_smc(adev, 309);
  if ((unsigned int )smc_result != 1U) {
    return (-22);
  } else {
  }
  return (0);
}
}
static void ci_enable_sclk_control(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322632U);
  tmp = tmp___0;
  if ((int )enable) {
    tmp = tmp & 4294967294U;
  } else {
    tmp = tmp | 1U;
  }
  (*(adev->smc_wreg))(adev, 3223322632U, tmp);
  return;
}
}
static PPSMC_Result amdgpu_ci_send_msg_to_smc_with_parameter(struct amdgpu_device *adev ,
                                                             PPSMC_Msg msg , u32 parameter )
{
  PPSMC_Result tmp ;
  {
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp = amdgpu_ci_send_msg_to_smc(adev, (int )msg);
  return (tmp);
}
}
static PPSMC_Result amdgpu_ci_send_msg_to_smc_return_parameter(struct amdgpu_device *adev ,
                                                               PPSMC_Msg msg , u32 *parameter )
{
  PPSMC_Result smc_result ;
  {
  smc_result = amdgpu_ci_send_msg_to_smc(adev, (int )msg);
  if ((unsigned int )smc_result == 1U && (unsigned long )parameter != (unsigned long )((u32 *)0U)) {
    *parameter = amdgpu_mm_rreg(adev, 164U, 0);
  } else {
  }
  return (smc_result);
}
}
static int ci_dpm_force_state_sclk(struct amdgpu_device *adev , u32 n )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->sclk_dpm_key_disabled == 0U) {
    tmp___0 = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 325, (u32 )(1 << (int )n));
    smc_result = tmp___0;
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_dpm_force_state_mclk(struct amdgpu_device *adev , u32 n )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->mclk_dpm_key_disabled == 0U) {
    tmp___0 = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 326, (u32 )(1 << (int )n));
    smc_result = tmp___0;
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_dpm_force_state_pcie(struct amdgpu_device *adev , u32 n )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->pcie_dpm_key_disabled == 0U) {
    tmp___0 = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 327, n);
    smc_result = tmp___0;
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_set_power_limit(struct amdgpu_device *adev , u32 n )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((pi->power_containment_features & 4U) != 0U) {
    tmp___0 = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 391, n);
    smc_result = tmp___0;
    if ((unsigned int )smc_result != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_set_overdrive_target_tdp(struct amdgpu_device *adev , u32 target_tdp )
{
  PPSMC_Result smc_result ;
  PPSMC_Result tmp ;
  {
  tmp = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 392, target_tdp);
  smc_result = tmp;
  if ((unsigned int )smc_result != 1U) {
    return (-22);
  } else {
  }
  return (0);
}
}
static u32 ci_get_average_sclk_freq(struct amdgpu_device *adev )
{
  u32 sclk_freq ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp ;
  {
  tmp = amdgpu_ci_send_msg_to_smc_return_parameter(adev, 512, & sclk_freq);
  smc_result = tmp;
  if ((unsigned int )smc_result != 1U) {
    sclk_freq = 0U;
  } else {
  }
  return (sclk_freq);
}
}
static u32 ci_get_average_mclk_freq(struct amdgpu_device *adev )
{
  u32 mclk_freq ;
  PPSMC_Result smc_result ;
  PPSMC_Result tmp ;
  {
  tmp = amdgpu_ci_send_msg_to_smc_return_parameter(adev, 513, & mclk_freq);
  smc_result = tmp;
  if ((unsigned int )smc_result != 1U) {
    mclk_freq = 0U;
  } else {
  }
  return (mclk_freq);
}
}
static void ci_dpm_start_smc(struct amdgpu_device *adev )
{
  int i ;
  u32 tmp ;
  {
  amdgpu_ci_program_jump_on_start(adev);
  amdgpu_ci_start_smc_clock(adev);
  amdgpu_ci_start_smc(adev);
  i = 0;
  goto ldv_49176;
  ldv_49175:
  tmp = (*(adev->smc_rreg))(adev, 260096U);
  if ((int )tmp & 1) {
    goto ldv_49174;
  } else {
  }
  i = i + 1;
  ldv_49176: ;
  if (adev->usec_timeout > i) {
    goto ldv_49175;
  } else {
  }
  ldv_49174: ;
  return;
}
}
static void ci_dpm_stop_smc(struct amdgpu_device *adev )
{
  {
  amdgpu_ci_reset_smc(adev);
  amdgpu_ci_stop_smc_clock(adev);
  return;
}
}
static int ci_process_firmware_header(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131124U, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->dpm_table_start = tmp___0;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131120U, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->soft_regs_start = tmp___0;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131140U, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->mc_reg_table_start = tmp___0;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131128U, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->fan_table_start = tmp___0;
  ret = amdgpu_ci_read_smc_sram_dword(adev, 131144U, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->arb_table_start = tmp___0;
  return (0);
}
}
static void ci_read_clock_registers(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pi->clock_registers.cg_spll_func_cntl = (*(adev->smc_rreg))(adev, 3226468672U);
  pi->clock_registers.cg_spll_func_cntl_2 = (*(adev->smc_rreg))(adev, 3226468676U);
  pi->clock_registers.cg_spll_func_cntl_3 = (*(adev->smc_rreg))(adev, 3226468680U);
  pi->clock_registers.cg_spll_func_cntl_4 = (*(adev->smc_rreg))(adev, 3226468684U);
  pi->clock_registers.cg_spll_spread_spectrum = (*(adev->smc_rreg))(adev, 3226468708U);
  pi->clock_registers.cg_spll_spread_spectrum_2 = (*(adev->smc_rreg))(adev, 3226468712U);
  pi->clock_registers.dll_cntl = amdgpu_mm_rreg(adev, 2793U, 0);
  pi->clock_registers.mclk_pwrmgt_cntl = amdgpu_mm_rreg(adev, 2792U, 0);
  pi->clock_registers.mpll_ad_func_cntl = amdgpu_mm_rreg(adev, 2800U, 0);
  pi->clock_registers.mpll_dq_func_cntl = amdgpu_mm_rreg(adev, 2801U, 0);
  pi->clock_registers.mpll_func_cntl = amdgpu_mm_rreg(adev, 2797U, 0);
  pi->clock_registers.mpll_func_cntl_1 = amdgpu_mm_rreg(adev, 2798U, 0);
  pi->clock_registers.mpll_func_cntl_2 = amdgpu_mm_rreg(adev, 2799U, 0);
  pi->clock_registers.mpll_ss1 = amdgpu_mm_rreg(adev, 2803U, 0);
  pi->clock_registers.mpll_ss2 = amdgpu_mm_rreg(adev, 2804U, 0);
  return;
}
}
static void ci_init_sclk_t(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pi->low_sclk_interrupt_t = 0U;
  return;
}
}
static void ci_enable_thermal_protection(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp = tmp___0;
  if ((int )enable) {
    tmp = tmp & 4294967291U;
  } else {
    tmp = tmp | 4U;
  }
  (*(adev->smc_wreg))(adev, 3223322624U, tmp);
  return;
}
}
static void ci_enable_acpi_power_management(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp = tmp___0;
  tmp = tmp | 2U;
  (*(adev->smc_wreg))(adev, 3223322624U, tmp);
  return;
}
}
static int ci_notify_smc_display_change(struct amdgpu_device *adev , bool has_display )
{
  PPSMC_Msg msg ;
  PPSMC_Result tmp ;
  {
  msg = (int )has_display ? 94U : 93U;
  tmp = amdgpu_ci_send_msg_to_smc(adev, (int )msg);
  return ((unsigned int )tmp == 1U ? 0 : -22);
}
}
static int ci_enable_ds_master_switch(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result tmp___0 ;
  PPSMC_Result tmp___1 ;
  PPSMC_Result tmp___2 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )enable) {
    if ((int )pi->caps_sclk_ds) {
      tmp___0 = amdgpu_ci_send_msg_to_smc(adev, 399);
      if ((unsigned int )tmp___0 != 1U) {
        return (-22);
      } else {
      }
    } else {
      tmp___1 = amdgpu_ci_send_msg_to_smc(adev, 400);
      if ((unsigned int )tmp___1 != 1U) {
        return (-22);
      } else {
      }
    }
  } else
  if ((int )pi->caps_sclk_ds) {
    tmp___2 = amdgpu_ci_send_msg_to_smc(adev, 400);
    if ((unsigned int )tmp___2 != 1U) {
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void ci_program_display_gap(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  u32 pre_vbi_time_in_us ;
  u32 frame_time_in_us ;
  u32 ref_clock ;
  u32 refresh_rate ;
  u32 tmp___1 ;
  u32 vblank_time ;
  u32 tmp___2 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322720U);
  tmp = tmp___0;
  ref_clock = adev->clock.spll.reference_freq;
  tmp___1 = amdgpu_dpm_get_vrefresh(adev);
  refresh_rate = tmp___1;
  tmp___2 = amdgpu_dpm_get_vblank_time(adev);
  vblank_time = tmp___2;
  tmp = tmp & 4294967292U;
  if (adev->pm.dpm.new_active_crtc_count > 0) {
    tmp = tmp;
  } else {
    tmp = tmp | 3U;
  }
  (*(adev->smc_wreg))(adev, 3223322720U, tmp);
  if (refresh_rate == 0U) {
    refresh_rate = 60U;
  } else {
  }
  if (vblank_time == 4294967295U) {
    vblank_time = 500U;
  } else {
  }
  frame_time_in_us = 1000000U / refresh_rate;
  pre_vbi_time_in_us = (frame_time_in_us - vblank_time) - 200U;
  tmp = (ref_clock / 100U) * pre_vbi_time_in_us;
  (*(adev->smc_wreg))(adev, 3223323184U, tmp);
  ci_write_smc_soft_register(adev, 12, 100U);
  ci_write_smc_soft_register(adev, 16, frame_time_in_us - pre_vbi_time_in_us);
  ci_notify_smc_display_change(adev, adev->pm.dpm.new_active_crtc_count == 1);
  return;
}
}
static void ci_enable_spread_spectrum(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )enable) {
    if ((int )pi->caps_sclk_ss_support) {
      tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
      tmp___0 = tmp___0 | 8388608U;
      (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
    } else {
    }
  } else {
    tmp___0 = (*(adev->smc_rreg))(adev, 3226468708U);
    tmp___0 = tmp___0 & 4294967294U;
    (*(adev->smc_wreg))(adev, 3226468708U, tmp___0);
    tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
    tmp___0 = tmp___0 & 4286578687U;
    (*(adev->smc_wreg))(adev, 3223322624U, tmp___0);
  }
  return;
}
}
static void ci_program_sstp(struct amdgpu_device *adev )
{
  {
  (*(adev->smc_wreg))(adev, 3223322692U, 200U);
  return;
}
}
static void ci_enable_display_gap(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322720U);
  tmp = tmp___0;
  tmp = tmp & 4244635644U;
  tmp = tmp | 16777219U;
  (*(adev->smc_wreg))(adev, 3223322720U, tmp);
  return;
}
}
static void ci_program_vc(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 3223322632U);
  tmp = tmp & 4294967247U;
  (*(adev->smc_wreg))(adev, 3223322632U, tmp);
  (*(adev->smc_wreg))(adev, 3223323048U, 1073725440U);
  (*(adev->smc_wreg))(adev, 3223323052U, 1024U);
  (*(adev->smc_wreg))(adev, 3223323056U, 12583040U);
  (*(adev->smc_wreg))(adev, 3223323060U, 12583424U);
  (*(adev->smc_wreg))(adev, 3223323064U, 12588672U);
  (*(adev->smc_wreg))(adev, 3223323068U, 12582963U);
  (*(adev->smc_wreg))(adev, 3223323072U, 12582963U);
  (*(adev->smc_wreg))(adev, 3223323076U, 1073725440U);
  return;
}
}
static void ci_clear_vc(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 3223322632U);
  tmp = tmp | 48U;
  (*(adev->smc_wreg))(adev, 3223322632U, tmp);
  (*(adev->smc_wreg))(adev, 3223323048U, 0U);
  (*(adev->smc_wreg))(adev, 3223323052U, 0U);
  (*(adev->smc_wreg))(adev, 3223323056U, 0U);
  (*(adev->smc_wreg))(adev, 3223323060U, 0U);
  (*(adev->smc_wreg))(adev, 3223323064U, 0U);
  (*(adev->smc_wreg))(adev, 3223323068U, 0U);
  (*(adev->smc_wreg))(adev, 3223323072U, 0U);
  (*(adev->smc_wreg))(adev, 3223323076U, 0U);
  return;
}
}
static int ci_upload_firmware(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int i ;
  int ret ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  i = 0;
  goto ldv_49251;
  ldv_49250:
  tmp___0 = (*(adev->smc_rreg))(adev, 3221225476U);
  if ((tmp___0 & 128U) != 0U) {
    goto ldv_49249;
  } else {
  }
  i = i + 1;
  ldv_49251: ;
  if (adev->usec_timeout > i) {
    goto ldv_49250;
  } else {
  }
  ldv_49249:
  (*(adev->smc_wreg))(adev, 2147483664U, 1U);
  amdgpu_ci_stop_smc_clock(adev);
  amdgpu_ci_reset_smc(adev);
  ret = amdgpu_ci_load_smc_ucode(adev, pi->sram_end);
  return (ret);
}
}
static int ci_get_svi2_voltage_table(struct amdgpu_device *adev , struct amdgpu_clock_voltage_dependency_table *voltage_dependency_table ,
                                     struct atom_voltage_table *voltage_table )
{
  u32 i ;
  {
  if ((unsigned long )voltage_dependency_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  voltage_table->mask_low = 0U;
  voltage_table->phase_delay = 0U;
  voltage_table->count = voltage_dependency_table->count;
  i = 0U;
  goto ldv_49259;
  ldv_49258:
  voltage_table->entries[i].value = (voltage_dependency_table->entries + (unsigned long )i)->v;
  voltage_table->entries[i].smio_low = 0U;
  i = i + 1U;
  ldv_49259: ;
  if (voltage_table->count > i) {
    goto ldv_49258;
  } else {
  }
  return (0);
}
}
static int ci_construct_voltage_tables(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->voltage_control == 1U) {
    ret = amdgpu_atombios_get_voltage_table(adev, 1, 0, & pi->vddc_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else
  if (pi->voltage_control == 2U) {
    ret = ci_get_svi2_voltage_table(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_mclk,
                                    & pi->vddc_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if (pi->vddc_voltage_table.count > 8U) {
    ci_trim_voltage_table_to_fit_state_table(adev, 8U, & pi->vddc_voltage_table);
  } else {
  }
  if (pi->vddci_control == 1U) {
    ret = amdgpu_atombios_get_voltage_table(adev, 4, 0, & pi->vddci_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else
  if (pi->vddci_control == 2U) {
    ret = ci_get_svi2_voltage_table(adev, & adev->pm.dpm.dyn_state.vddci_dependency_on_mclk,
                                    & pi->vddci_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if (pi->vddci_voltage_table.count > 4U) {
    ci_trim_voltage_table_to_fit_state_table(adev, 4U, & pi->vddci_voltage_table);
  } else {
  }
  if (pi->mvdd_control == 1U) {
    ret = amdgpu_atombios_get_voltage_table(adev, 2, 0, & pi->mvdd_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else
  if (pi->mvdd_control == 2U) {
    ret = ci_get_svi2_voltage_table(adev, & adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk,
                                    & pi->mvdd_voltage_table);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if (pi->mvdd_voltage_table.count > 4U) {
    ci_trim_voltage_table_to_fit_state_table(adev, 4U, & pi->mvdd_voltage_table);
  } else {
  }
  return (0);
}
}
static void ci_populate_smc_voltage_table(struct amdgpu_device *adev , struct atom_voltage_table_entry *voltage_table ,
                                          SMU7_Discrete_VoltageLevel *smc_voltage_table )
{
  int ret ;
  __u16 tmp ;
  __u16 tmp___0 ;
  __u16 tmp___1 ;
  {
  ret = ci_get_std_voltage_value_sidd(adev, voltage_table, & smc_voltage_table->StdVoltageHiSidd,
                                      & smc_voltage_table->StdVoltageLoSidd);
  if (ret != 0) {
    smc_voltage_table->StdVoltageHiSidd = (unsigned int )voltage_table->value * 4U;
    smc_voltage_table->StdVoltageLoSidd = (unsigned int )voltage_table->value * 4U;
  } else {
  }
  tmp = __fswab16((int )((unsigned int )voltage_table->value * 4U));
  smc_voltage_table->Voltage = tmp;
  tmp___0 = __fswab16((int )smc_voltage_table->StdVoltageHiSidd);
  smc_voltage_table->StdVoltageHiSidd = tmp___0;
  tmp___1 = __fswab16((int )smc_voltage_table->StdVoltageLoSidd);
  smc_voltage_table->StdVoltageLoSidd = tmp___1;
  return;
}
}
static int ci_populate_smc_vddc_table(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  unsigned int count ;
  __u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  table->VddcLevelCount = pi->vddc_voltage_table.count;
  count = 0U;
  goto ldv_49279;
  ldv_49278:
  ci_populate_smc_voltage_table(adev, (struct atom_voltage_table_entry *)(& pi->vddc_voltage_table.entries) + (unsigned long )count,
                                (SMU7_Discrete_VoltageLevel *)(& table->VddcLevel) + (unsigned long )count);
  if (pi->voltage_control == 1U) {
    table->VddcLevel[count].Smio = (int )table->VddcLevel[count].Smio | (int )((uint8_t )pi->vddc_voltage_table.entries[count].smio_low);
  } else {
    table->VddcLevel[count].Smio = 0U;
  }
  count = count + 1U;
  ldv_49279: ;
  if (table->VddcLevelCount > count) {
    goto ldv_49278;
  } else {
  }
  tmp___0 = __fswab32(table->VddcLevelCount);
  table->VddcLevelCount = tmp___0;
  return (0);
}
}
static int ci_populate_smc_vddci_table(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  unsigned int count ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  __u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  table->VddciLevelCount = pi->vddci_voltage_table.count;
  count = 0U;
  goto ldv_49288;
  ldv_49287:
  ci_populate_smc_voltage_table(adev, (struct atom_voltage_table_entry *)(& pi->vddci_voltage_table.entries) + (unsigned long )count,
                                (SMU7_Discrete_VoltageLevel *)(& table->VddciLevel) + (unsigned long )count);
  if (pi->vddci_control == 1U) {
    table->VddciLevel[count].Smio = (int )table->VddciLevel[count].Smio | (int )((uint8_t )pi->vddci_voltage_table.entries[count].smio_low);
  } else {
    table->VddciLevel[count].Smio = 0U;
  }
  count = count + 1U;
  ldv_49288: ;
  if (table->VddciLevelCount > count) {
    goto ldv_49287;
  } else {
  }
  tmp___0 = __fswab32(table->VddciLevelCount);
  table->VddciLevelCount = tmp___0;
  return (0);
}
}
static int ci_populate_smc_mvdd_table(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  unsigned int count ;
  __u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  table->MvddLevelCount = pi->mvdd_voltage_table.count;
  count = 0U;
  goto ldv_49297;
  ldv_49296:
  ci_populate_smc_voltage_table(adev, (struct atom_voltage_table_entry *)(& pi->mvdd_voltage_table.entries) + (unsigned long )count,
                                (SMU7_Discrete_VoltageLevel *)(& table->MvddLevel) + (unsigned long )count);
  if (pi->mvdd_control == 1U) {
    table->MvddLevel[count].Smio = (int )table->MvddLevel[count].Smio | (int )((uint8_t )pi->mvdd_voltage_table.entries[count].smio_low);
  } else {
    table->MvddLevel[count].Smio = 0U;
  }
  count = count + 1U;
  ldv_49297: ;
  if (table->MvddLevelCount > count) {
    goto ldv_49296;
  } else {
  }
  tmp___0 = __fswab32(table->MvddLevelCount);
  table->MvddLevelCount = tmp___0;
  return (0);
}
}
static int ci_populate_smc_voltage_tables(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  int ret ;
  {
  ret = ci_populate_smc_vddc_table(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_vddci_table(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_mvdd_table(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (0);
}
}
static int ci_populate_mvdd_value(struct amdgpu_device *adev , u32 mclk , SMU7_Discrete_VoltageLevel *voltage )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  i = 0U;
  if (pi->mvdd_control != 0U) {
    i = 0U;
    goto ldv_49313;
    ldv_49312: ;
    if ((adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk.entries + (unsigned long )i)->clk >= mclk) {
      voltage->Voltage = pi->mvdd_voltage_table.entries[i].value;
      goto ldv_49311;
    } else {
    }
    i = i + 1U;
    ldv_49313: ;
    if (adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk.count > i) {
      goto ldv_49312;
    } else {
    }
    ldv_49311: ;
    if (adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk.count <= i) {
      return (-22);
    } else {
    }
  } else {
  }
  return (-22);
}
}
static int ci_get_std_voltage_value_sidd(struct amdgpu_device *adev , struct atom_voltage_table_entry *voltage_table ,
                                         u16 *std_voltage_hi_sidd , u16 *std_voltage_lo_sidd )
{
  u16 v_index ;
  u16 idx ;
  bool voltage_found ;
  {
  voltage_found = 0;
  *std_voltage_hi_sidd = (unsigned int )voltage_table->value * 4U;
  *std_voltage_lo_sidd = (unsigned int )voltage_table->value * 4U;
  if ((unsigned long )adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries == (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    return (-22);
  } else {
  }
  if ((unsigned long )adev->pm.dpm.dyn_state.cac_leakage_table.entries != (unsigned long )((union amdgpu_cac_leakage_entry *)0)) {
    v_index = 0U;
    goto ldv_49325;
    ldv_49324: ;
    if ((int )voltage_table->value == (int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries + (unsigned long )v_index)->v) {
      voltage_found = 1;
      if ((unsigned int )v_index < adev->pm.dpm.dyn_state.cac_leakage_table.count) {
        idx = v_index;
      } else {
        idx = (unsigned int )((u16 )adev->pm.dpm.dyn_state.cac_leakage_table.count) - 1U;
      }
      *std_voltage_lo_sidd = (unsigned int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )idx)->__annonCompField79.vddc * 4U;
      *std_voltage_hi_sidd = (unsigned int )((u16 )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )idx)->__annonCompField79.leakage) * 4U;
      goto ldv_49323;
    } else {
    }
    v_index = (u16 )((int )v_index + 1);
    ldv_49325: ;
    if ((unsigned int )v_index < adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.count) {
      goto ldv_49324;
    } else {
    }
    ldv_49323: ;
    if (! voltage_found) {
      v_index = 0U;
      goto ldv_49328;
      ldv_49327: ;
      if ((int )voltage_table->value <= (int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries + (unsigned long )v_index)->v) {
        voltage_found = 1;
        if ((unsigned int )v_index < adev->pm.dpm.dyn_state.cac_leakage_table.count) {
          idx = v_index;
        } else {
          idx = (unsigned int )((u16 )adev->pm.dpm.dyn_state.cac_leakage_table.count) - 1U;
        }
        *std_voltage_lo_sidd = (unsigned int )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )idx)->__annonCompField79.vddc * 4U;
        *std_voltage_hi_sidd = (unsigned int )((u16 )(adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )idx)->__annonCompField79.leakage) * 4U;
        goto ldv_49326;
      } else {
      }
      v_index = (u16 )((int )v_index + 1);
      ldv_49328: ;
      if ((unsigned int )v_index < adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.count) {
        goto ldv_49327;
      } else {
      }
      ldv_49326: ;
    } else {
    }
  } else {
  }
  return (0);
}
}
static void ci_populate_phase_value_based_on_sclk(struct amdgpu_device *adev , struct amdgpu_phase_shedding_limits_table const *limits ,
                                                  u32 sclk , u32 *phase_shedding )
{
  unsigned int i ;
  {
  *phase_shedding = 1U;
  i = 0U;
  goto ldv_49338;
  ldv_49337: ;
  if ((limits->entries + (unsigned long )i)->sclk > sclk) {
    *phase_shedding = i;
    goto ldv_49336;
  } else {
  }
  i = i + 1U;
  ldv_49338: ;
  if ((unsigned int )limits->count > i) {
    goto ldv_49337;
  } else {
  }
  ldv_49336: ;
  return;
}
}
static void ci_populate_phase_value_based_on_mclk(struct amdgpu_device *adev , struct amdgpu_phase_shedding_limits_table const *limits ,
                                                  u32 mclk , u32 *phase_shedding )
{
  unsigned int i ;
  {
  *phase_shedding = 1U;
  i = 0U;
  goto ldv_49348;
  ldv_49347: ;
  if ((limits->entries + (unsigned long )i)->mclk > mclk) {
    *phase_shedding = i;
    goto ldv_49346;
  } else {
  }
  i = i + 1U;
  ldv_49348: ;
  if ((unsigned int )limits->count > i) {
    goto ldv_49347;
  } else {
  }
  ldv_49346: ;
  return;
}
}
static int ci_init_arb_table_index(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  int ret ;
  int tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = amdgpu_ci_read_smc_sram_dword(adev, pi->arb_table_start, & tmp___0, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  tmp___0 = tmp___0 & 16777215U;
  tmp___0 = tmp___0 | 184549376U;
  tmp___1 = amdgpu_ci_write_smc_sram_dword(adev, pi->arb_table_start, tmp___0, pi->sram_end);
  return (tmp___1);
}
}
static int ci_get_dependency_volt_by_clk(struct amdgpu_device *adev , struct amdgpu_clock_voltage_dependency_table *allowed_clock_voltage_table ,
                                         u32 clock , u32 *voltage )
{
  u32 i ;
  {
  i = 0U;
  if (allowed_clock_voltage_table->count == 0U) {
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_49363;
  ldv_49362: ;
  if ((allowed_clock_voltage_table->entries + (unsigned long )i)->clk >= clock) {
    *voltage = (u32 )(allowed_clock_voltage_table->entries + (unsigned long )i)->v;
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_49363: ;
  if (allowed_clock_voltage_table->count > i) {
    goto ldv_49362;
  } else {
  }
  *voltage = (u32 )(allowed_clock_voltage_table->entries + (unsigned long )(i - 1U))->v;
  return (0);
}
}
static u8 ci_get_sleep_divider_id_from_clock(struct amdgpu_device *adev , u32 sclk ,
                                             u32 min_sclk_in_sr )
{
  u32 i ;
  u32 tmp ;
  u32 min ;
  {
  min = 800U > min_sclk_in_sr ? 800U : min_sclk_in_sr;
  if (sclk < min) {
    return (0U);
  } else {
  }
  i = 5U;
  ldv_49374:
  tmp = sclk >> (int )i;
  if (tmp >= min || i == 0U) {
    goto ldv_49373;
  } else {
  }
  i = i - 1U;
  goto ldv_49374;
  ldv_49373: ;
  return ((u8 )i);
}
}
static int ci_initial_switch_from_arb_f0_to_f1(struct amdgpu_device *adev )
{
  int tmp ;
  {
  tmp = ci_copy_and_switch_arb_sets(adev, 10U, 11U);
  return (tmp);
}
}
static int ci_reset_to_default(struct amdgpu_device *adev )
{
  PPSMC_Result tmp ;
  {
  tmp = amdgpu_ci_send_msg_to_smc(adev, 132);
  return ((unsigned int )tmp == 1U ? 0 : -22);
}
}
static int ci_force_switch_to_arb_f0(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 2147484708U);
  tmp = (tmp___0 & 65280U) >> 8;
  if (tmp == 10U) {
    return (0);
  } else {
  }
  tmp___1 = ci_copy_and_switch_arb_sets(adev, tmp, 10U);
  return (tmp___1);
}
}
static void ci_register_patching_mc_arb(struct amdgpu_device *adev , u32 const engine_clock ,
                                        u32 const memory_clock , u32 *dram_timimg2 )
{
  bool patch ;
  u32 tmp ;
  u32 tmp2 ;
  {
  tmp = amdgpu_mm_rreg(adev, 2688U, 0);
  patch = (tmp & 3840U) == 768U;
  if ((int )patch && ((unsigned int )(adev->pdev)->device == 26544U || (unsigned int )(adev->pdev)->device == 26545U)) {
    if ((unsigned int )memory_clock > 100000U && (unsigned int )memory_clock <= 125000U) {
      tmp2 = (((unsigned int )engine_clock * 49U) / 125000U - 1U) & 255U;
      *dram_timimg2 = *dram_timimg2 & 4278255615U;
      *dram_timimg2 = *dram_timimg2 | (tmp2 << 16);
    } else
    if ((unsigned int )memory_clock > 125000U && (unsigned int )memory_clock <= 137500U) {
      tmp2 = (((unsigned int )engine_clock * 54U) / 137500U - 1U) & 255U;
      *dram_timimg2 = *dram_timimg2 & 4278255615U;
      *dram_timimg2 = *dram_timimg2 | (tmp2 << 16);
    } else {
    }
  } else {
  }
  return;
}
}
static int ci_populate_memory_timing_parameters(struct amdgpu_device *adev , u32 sclk ,
                                                u32 mclk , SMU7_Discrete_MCArbDramTimingTableEntry *arb_regs )
{
  u32 dram_timing ;
  u32 dram_timing2 ;
  u32 burst_time ;
  u32 tmp ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  {
  amdgpu_atombios_set_engine_dram_timings(adev, sclk, mclk);
  dram_timing = amdgpu_mm_rreg(adev, 2525U, 0);
  dram_timing2 = amdgpu_mm_rreg(adev, 2526U, 0);
  tmp = amdgpu_mm_rreg(adev, 2562U, 0);
  burst_time = tmp & 31U;
  ci_register_patching_mc_arb(adev, sclk, mclk, & dram_timing2);
  tmp___0 = __fswab32(dram_timing);
  arb_regs->McArbDramTiming = tmp___0;
  tmp___1 = __fswab32(dram_timing2);
  arb_regs->McArbDramTiming2 = tmp___1;
  arb_regs->McArbBurstTime = (unsigned char )burst_time;
  return (0);
}
}
static int ci_do_program_memory_timing_parameters(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  SMU7_Discrete_MCArbDramTimingTable arb_regs ;
  u32 i ;
  u32 j ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = 0;
  memset((void *)(& arb_regs), 0, 576UL);
  i = 0U;
  goto ldv_49415;
  ldv_49414:
  j = 0U;
  goto ldv_49413;
  ldv_49412:
  ret = ci_populate_memory_timing_parameters(adev, pi->dpm_table.sclk_table.dpm_levels[i].value,
                                             pi->dpm_table.mclk_table.dpm_levels[j].value,
                                             (SMU7_Discrete_MCArbDramTimingTableEntry *)(& arb_regs.entries) + ((unsigned long )i + (unsigned long )j));
  if (ret != 0) {
    goto ldv_49411;
  } else {
  }
  j = j + 1U;
  ldv_49413: ;
  if (pi->dpm_table.mclk_table.count > j) {
    goto ldv_49412;
  } else {
  }
  ldv_49411:
  i = i + 1U;
  ldv_49415: ;
  if (pi->dpm_table.sclk_table.count > i) {
    goto ldv_49414;
  } else {
  }
  if (ret == 0) {
    ret = amdgpu_ci_copy_bytes_to_smc(adev, pi->arb_table_start, (u8 const *)(& arb_regs),
                                      576U, pi->sram_end);
  } else {
  }
  return (ret);
}
}
static int ci_program_memory_timing_parameters(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (pi->need_update_smu7_dpm_table == 0U) {
    return (0);
  } else {
  }
  tmp___0 = ci_do_program_memory_timing_parameters(adev);
  return (tmp___0);
}
}
static void ci_populate_smc_initial_state(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_boot_state )
{
  struct ci_ps *boot_state ;
  struct ci_ps *tmp ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  u32 level ;
  {
  tmp = ci_get_ps(amdgpu_boot_state);
  boot_state = tmp;
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  level = 0U;
  level = 0U;
  goto ldv_49430;
  ldv_49429: ;
  if ((adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries + (unsigned long )level)->clk >= boot_state->performance_levels[0].sclk) {
    pi->smc_state_table.GraphicsBootLevel = (uint8_t )level;
    goto ldv_49428;
  } else {
  }
  level = level + 1U;
  ldv_49430: ;
  if (adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.count > level) {
    goto ldv_49429;
  } else {
  }
  ldv_49428:
  level = 0U;
  goto ldv_49433;
  ldv_49432: ;
  if ((adev->pm.dpm.dyn_state.vddc_dependency_on_mclk.entries + (unsigned long )level)->clk >= boot_state->performance_levels[0].mclk) {
    pi->smc_state_table.MemoryBootLevel = (uint8_t )level;
    goto ldv_49431;
  } else {
  }
  level = level + 1U;
  ldv_49433: ;
  if (adev->pm.dpm.dyn_state.vddc_dependency_on_mclk.count > level) {
    goto ldv_49432;
  } else {
  }
  ldv_49431: ;
  return;
}
}
static u32 ci_get_dpm_level_enable_mask_value(struct ci_single_dpm_table *dpm_table )
{
  u32 i ;
  u32 mask_value ;
  {
  mask_value = 0U;
  i = dpm_table->count;
  goto ldv_49440;
  ldv_49439:
  mask_value = mask_value << 1;
  if ((int )dpm_table->dpm_levels[i - 1U].enabled) {
    mask_value = mask_value | 1U;
  } else {
    mask_value = mask_value & 4294967294U;
  }
  i = i - 1U;
  ldv_49440: ;
  if (i != 0U) {
    goto ldv_49439;
  } else {
  }
  return (mask_value);
}
}
static void ci_populate_smc_link_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_dpm_table *dpm_table ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  dpm_table = & pi->dpm_table;
  i = 0U;
  goto ldv_49450;
  ldv_49449:
  table->LinkLevel[i].PcieGenSpeed = (unsigned char )dpm_table->pcie_speed_table.dpm_levels[i].value;
  table->LinkLevel[i].PcieLaneCount = amdgpu_encode_pci_lane_width(dpm_table->pcie_speed_table.dpm_levels[i].param1);
  table->LinkLevel[i].EnabledForActivity = 1U;
  table->LinkLevel[i].DownT = 83886080U;
  table->LinkLevel[i].UpT = 503316480U;
  i = i + 1U;
  ldv_49450: ;
  if (dpm_table->pcie_speed_table.count > i) {
    goto ldv_49449;
  } else {
  }
  pi->smc_state_table.LinkLevelCount = (unsigned char )dpm_table->pcie_speed_table.count;
  pi->dpm_level_enable_mask.pcie_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& dpm_table->pcie_speed_table);
  return;
}
}
static int ci_populate_smc_uvd_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  u32 count ;
  struct atom_clock_dividers dividers ;
  int ret ;
  __u32 tmp ;
  __u32 tmp___0 ;
  __u16 tmp___1 ;
  {
  ret = -22;
  table->UvdLevelCount = adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.count;
  count = 0U;
  goto ldv_49460;
  ldv_49459:
  table->UvdLevel[count].VclkFrequency = (adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )count)->vclk;
  table->UvdLevel[count].DclkFrequency = (adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )count)->dclk;
  table->UvdLevel[count].MinVddc = (unsigned int )(adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )count)->v * 4U;
  table->UvdLevel[count].MinVddcPhases = 1U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, table->UvdLevel[count].VclkFrequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->UvdLevel[count].VclkDivider = (unsigned char )dividers.post_divider;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, table->UvdLevel[count].DclkFrequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->UvdLevel[count].DclkDivider = (unsigned char )dividers.post_divider;
  tmp = __fswab32(table->UvdLevel[count].VclkFrequency);
  table->UvdLevel[count].VclkFrequency = tmp;
  tmp___0 = __fswab32(table->UvdLevel[count].DclkFrequency);
  table->UvdLevel[count].DclkFrequency = tmp___0;
  tmp___1 = __fswab16((int )table->UvdLevel[count].MinVddc);
  table->UvdLevel[count].MinVddc = tmp___1;
  count = count + 1U;
  ldv_49460: ;
  if ((u32 )table->UvdLevelCount > count) {
    goto ldv_49459;
  } else {
  }
  return (ret);
}
}
static int ci_populate_smc_vce_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  u32 count ;
  struct atom_clock_dividers dividers ;
  int ret ;
  __u32 tmp ;
  __u16 tmp___0 ;
  {
  ret = -22;
  table->VceLevelCount = adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.count;
  count = 0U;
  goto ldv_49470;
  ldv_49469:
  table->VceLevel[count].Frequency = (adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )count)->evclk;
  table->VceLevel[count].MinVoltage = (unsigned int )(adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )count)->v * 4U;
  table->VceLevel[count].MinPhases = 1U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, table->VceLevel[count].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->VceLevel[count].Divider = (unsigned char )dividers.post_divider;
  tmp = __fswab32(table->VceLevel[count].Frequency);
  table->VceLevel[count].Frequency = tmp;
  tmp___0 = __fswab16((int )table->VceLevel[count].MinVoltage);
  table->VceLevel[count].MinVoltage = tmp___0;
  count = count + 1U;
  ldv_49470: ;
  if ((u32 )table->VceLevelCount > count) {
    goto ldv_49469;
  } else {
  }
  return (ret);
}
}
static int ci_populate_smc_acp_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  u32 count ;
  struct atom_clock_dividers dividers ;
  int ret ;
  __u32 tmp ;
  __u16 tmp___0 ;
  {
  ret = -22;
  table->AcpLevelCount = (unsigned char )adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.count;
  count = 0U;
  goto ldv_49480;
  ldv_49479:
  table->AcpLevel[count].Frequency = (adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries + (unsigned long )count)->clk;
  table->AcpLevel[count].MinVoltage = (adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries + (unsigned long )count)->v;
  table->AcpLevel[count].MinPhases = 1U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, table->AcpLevel[count].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->AcpLevel[count].Divider = (unsigned char )dividers.post_divider;
  tmp = __fswab32(table->AcpLevel[count].Frequency);
  table->AcpLevel[count].Frequency = tmp;
  tmp___0 = __fswab16((int )table->AcpLevel[count].MinVoltage);
  table->AcpLevel[count].MinVoltage = tmp___0;
  count = count + 1U;
  ldv_49480: ;
  if ((u32 )table->AcpLevelCount > count) {
    goto ldv_49479;
  } else {
  }
  return (ret);
}
}
static int ci_populate_smc_samu_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  u32 count ;
  struct atom_clock_dividers dividers ;
  int ret ;
  __u32 tmp ;
  __u16 tmp___0 ;
  {
  ret = -22;
  table->SamuLevelCount = (uint8_t )adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.count;
  count = 0U;
  goto ldv_49490;
  ldv_49489:
  table->SamuLevel[count].Frequency = (adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries + (unsigned long )count)->clk;
  table->SamuLevel[count].MinVoltage = (unsigned int )(adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries + (unsigned long )count)->v * 4U;
  table->SamuLevel[count].MinPhases = 1U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, table->SamuLevel[count].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->SamuLevel[count].Divider = (unsigned char )dividers.post_divider;
  tmp = __fswab32(table->SamuLevel[count].Frequency);
  table->SamuLevel[count].Frequency = tmp;
  tmp___0 = __fswab16((int )table->SamuLevel[count].MinVoltage);
  table->SamuLevel[count].MinVoltage = tmp___0;
  count = count + 1U;
  ldv_49490: ;
  if ((u32 )table->SamuLevelCount > count) {
    goto ldv_49489;
  } else {
  }
  return (ret);
}
}
static int ci_calculate_mclk_params(struct amdgpu_device *adev , u32 memory_clock ,
                                    SMU7_Discrete_MemoryLevel *mclk , bool strobe_mode ,
                                    bool dll_state_on )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 dll_cntl ;
  u32 mclk_pwrmgt_cntl ;
  u32 mpll_ad_func_cntl ;
  u32 mpll_dq_func_cntl ;
  u32 mpll_func_cntl ;
  u32 mpll_func_cntl_1 ;
  u32 mpll_func_cntl_2 ;
  u32 mpll_ss1 ;
  u32 mpll_ss2 ;
  struct atom_mpll_param mpll_param ;
  int ret ;
  struct amdgpu_atom_ss ss ;
  u32 freq_nom ;
  u32 tmp___0 ;
  u32 reference_clock ;
  u32 clks ;
  u32 clkv ;
  bool tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  dll_cntl = pi->clock_registers.dll_cntl;
  mclk_pwrmgt_cntl = pi->clock_registers.mclk_pwrmgt_cntl;
  mpll_ad_func_cntl = pi->clock_registers.mpll_ad_func_cntl;
  mpll_dq_func_cntl = pi->clock_registers.mpll_dq_func_cntl;
  mpll_func_cntl = pi->clock_registers.mpll_func_cntl;
  mpll_func_cntl_1 = pi->clock_registers.mpll_func_cntl_1;
  mpll_func_cntl_2 = pi->clock_registers.mpll_func_cntl_2;
  mpll_ss1 = pi->clock_registers.mpll_ss1;
  mpll_ss2 = pi->clock_registers.mpll_ss2;
  ret = amdgpu_atombios_get_memory_pll_dividers(adev, memory_clock, (int )strobe_mode,
                                                & mpll_param);
  if (ret != 0) {
    return (ret);
  } else {
  }
  mpll_func_cntl = mpll_func_cntl & 4027580415U;
  mpll_func_cntl = (mpll_param.bwcntl << 20) | mpll_func_cntl;
  mpll_func_cntl_1 = mpll_func_cntl_1 & 4026531852U;
  mpll_func_cntl_1 = ((u32 )(((int )mpll_param.__annonCompField84.__annonCompField83.clkf << 16) | ((int )mpll_param.__annonCompField84.__annonCompField83.clkfrac << 4)) | mpll_param.vco_mode) | mpll_func_cntl_1;
  mpll_ad_func_cntl = mpll_ad_func_cntl & 4294967288U;
  mpll_ad_func_cntl = mpll_param.post_div | mpll_ad_func_cntl;
  if (adev->mc.vram_type == 5U) {
    mpll_dq_func_cntl = mpll_dq_func_cntl & 4294967272U;
    mpll_dq_func_cntl = ((mpll_param.yclk_sel << 4) | mpll_param.post_div) | mpll_dq_func_cntl;
  } else {
  }
  if ((int )pi->caps_mclk_ss_support) {
    reference_clock = adev->clock.mpll.reference_freq;
    if (mpll_param.qdr == 1U) {
      freq_nom = memory_clock * 4U << (int )mpll_param.post_div;
    } else {
      freq_nom = memory_clock * 2U << (int )mpll_param.post_div;
    }
    tmp___0 = freq_nom / reference_clock;
    tmp___0 = tmp___0 * tmp___0;
    tmp___1 = amdgpu_atombios_get_asic_ss_info(adev, & ss, 1, freq_nom);
    if ((int )tmp___1) {
      clks = (reference_clock * 5U) / (u32 )ss.rate;
      clkv = ((u32 )((((int )ss.percentage * 131) * (int )ss.rate) / 100) * tmp___0) / freq_nom;
      mpll_ss1 = mpll_ss1 & 4227858432U;
      mpll_ss1 = mpll_ss1 | clkv;
      mpll_ss2 = mpll_ss2 & 4294963200U;
      mpll_ss2 = mpll_ss2 | clks;
    } else {
    }
  } else {
  }
  mclk_pwrmgt_cntl = mclk_pwrmgt_cntl & 4294967264U;
  mclk_pwrmgt_cntl = mpll_param.dll_speed | mclk_pwrmgt_cntl;
  if ((int )dll_state_on) {
    mclk_pwrmgt_cntl = mclk_pwrmgt_cntl | 768U;
  } else {
    mclk_pwrmgt_cntl = mclk_pwrmgt_cntl & 4294966527U;
  }
  mclk->MclkFrequency = memory_clock;
  mclk->MpllFuncCntl = mpll_func_cntl;
  mclk->MpllFuncCntl_1 = mpll_func_cntl_1;
  mclk->MpllFuncCntl_2 = mpll_func_cntl_2;
  mclk->MpllAdFuncCntl = mpll_ad_func_cntl;
  mclk->MpllDqFuncCntl = mpll_dq_func_cntl;
  mclk->MclkPwrmgtCntl = mclk_pwrmgt_cntl;
  mclk->DllCntl = dll_cntl;
  mclk->MpllSs1 = mpll_ss1;
  mclk->MpllSs2 = mpll_ss2;
  return (0);
}
}
static int ci_populate_single_memory_level(struct amdgpu_device *adev , u32 memory_clock ,
                                           SMU7_Discrete_MemoryLevel *memory_level )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  bool dll_state_on ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u8 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  __u32 tmp___6 ;
  __u32 tmp___7 ;
  __u32 tmp___8 ;
  __u32 tmp___9 ;
  __u32 tmp___10 ;
  __u16 tmp___11 ;
  __u32 tmp___12 ;
  __u32 tmp___13 ;
  __u32 tmp___14 ;
  __u32 tmp___15 ;
  __u32 tmp___16 ;
  __u32 tmp___17 ;
  __u32 tmp___18 ;
  __u32 tmp___19 ;
  __u32 tmp___20 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((unsigned long )adev->pm.dpm.dyn_state.vddc_dependency_on_mclk.entries != (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    ret = ci_get_dependency_volt_by_clk(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_mclk,
                                        memory_clock, & memory_level->MinVddc);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->pm.dpm.dyn_state.vddci_dependency_on_mclk.entries != (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    ret = ci_get_dependency_volt_by_clk(adev, & adev->pm.dpm.dyn_state.vddci_dependency_on_mclk,
                                        memory_clock, & memory_level->MinVddci);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if ((unsigned long )adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk.entries != (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    ret = ci_get_dependency_volt_by_clk(adev, & adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk,
                                        memory_clock, & memory_level->MinMvdd);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  memory_level->MinVddcPhases = 1U;
  if ((int )pi->vddc_phase_shed_control) {
    ci_populate_phase_value_based_on_mclk(adev, (struct amdgpu_phase_shedding_limits_table const *)(& adev->pm.dpm.dyn_state.phase_shedding_limits_table),
                                          memory_clock, & memory_level->MinVddcPhases);
  } else {
  }
  memory_level->EnabledForThrottle = 1U;
  memory_level->EnabledForActivity = 1U;
  memory_level->UpH = 0U;
  memory_level->DownH = 100U;
  memory_level->VoltageDownH = 0U;
  memory_level->ActivityLevel = (unsigned short )pi->mclk_activity_target;
  memory_level->StutterEnable = 0U;
  memory_level->StrobeEnable = 0U;
  memory_level->EdcReadEnable = 0U;
  memory_level->EdcWriteEnable = 0U;
  memory_level->RttEnable = 0U;
  memory_level->DisplayWatermark = 0U;
  if ((pi->mclk_stutter_mode_threshold != 0U && pi->mclk_stutter_mode_threshold >= memory_clock) && ! pi->uvd_enabled) {
    tmp___0 = amdgpu_mm_rreg(adev, 6965U, 0);
    if ((int )tmp___0 & 1) {
      if (adev->pm.dpm.new_active_crtc_count <= 2) {
        memory_level->StutterEnable = 1U;
      } else {
      }
    } else {
    }
  } else {
  }
  if (pi->mclk_strobe_mode_threshold != 0U && pi->mclk_strobe_mode_threshold >= memory_clock) {
    memory_level->StrobeEnable = 1U;
  } else {
  }
  if (adev->mc.vram_type == 5U) {
    memory_level->StrobeRatio = ci_get_mclk_frequency_ratio(memory_clock, (unsigned int )memory_level->StrobeEnable != 0U);
    if (pi->mclk_edc_enable_threshold != 0U && pi->mclk_edc_enable_threshold < memory_clock) {
      memory_level->EdcReadEnable = 1U;
    } else {
    }
    if (pi->mclk_edc_wr_enable_threshold != 0U && pi->mclk_edc_wr_enable_threshold < memory_clock) {
      memory_level->EdcWriteEnable = 1U;
    } else {
    }
    if ((unsigned int )memory_level->StrobeEnable != 0U) {
      tmp___3 = ci_get_mclk_frequency_ratio(memory_clock, 1);
      tmp___4 = amdgpu_mm_rreg(adev, 2713U, 0);
      if ((u32 )tmp___3 >= ((tmp___4 >> 16) & 15U)) {
        tmp___1 = amdgpu_mm_rreg(adev, 2709U, 0);
        dll_state_on = (tmp___1 & 2U) != 0U;
      } else {
        tmp___2 = amdgpu_mm_rreg(adev, 2710U, 0);
        dll_state_on = (tmp___2 & 2U) != 0U;
      }
    } else {
      dll_state_on = pi->dll_default_on;
    }
  } else {
    memory_level->StrobeRatio = ci_get_ddr3_mclk_frequency_ratio(memory_clock);
    tmp___5 = amdgpu_mm_rreg(adev, 2709U, 0);
    dll_state_on = (tmp___5 & 2U) != 0U;
  }
  ret = ci_calculate_mclk_params(adev, memory_clock, memory_level, (unsigned int )memory_level->StrobeEnable != 0U,
                                 (int )dll_state_on);
  if (ret != 0) {
    return (ret);
  } else {
  }
  tmp___6 = __fswab32(memory_level->MinVddc * 4U);
  memory_level->MinVddc = tmp___6;
  tmp___7 = __fswab32(memory_level->MinVddcPhases);
  memory_level->MinVddcPhases = tmp___7;
  tmp___8 = __fswab32(memory_level->MinVddci * 4U);
  memory_level->MinVddci = tmp___8;
  tmp___9 = __fswab32(memory_level->MinMvdd * 4U);
  memory_level->MinMvdd = tmp___9;
  tmp___10 = __fswab32(memory_level->MclkFrequency);
  memory_level->MclkFrequency = tmp___10;
  tmp___11 = __fswab16((int )memory_level->ActivityLevel);
  memory_level->ActivityLevel = tmp___11;
  tmp___12 = __fswab32(memory_level->MpllFuncCntl);
  memory_level->MpllFuncCntl = tmp___12;
  tmp___13 = __fswab32(memory_level->MpllFuncCntl_1);
  memory_level->MpllFuncCntl_1 = tmp___13;
  tmp___14 = __fswab32(memory_level->MpllFuncCntl_2);
  memory_level->MpllFuncCntl_2 = tmp___14;
  tmp___15 = __fswab32(memory_level->MpllAdFuncCntl);
  memory_level->MpllAdFuncCntl = tmp___15;
  tmp___16 = __fswab32(memory_level->MpllDqFuncCntl);
  memory_level->MpllDqFuncCntl = tmp___16;
  tmp___17 = __fswab32(memory_level->MclkPwrmgtCntl);
  memory_level->MclkPwrmgtCntl = tmp___17;
  tmp___18 = __fswab32(memory_level->DllCntl);
  memory_level->DllCntl = tmp___18;
  tmp___19 = __fswab32(memory_level->MpllSs1);
  memory_level->MpllSs1 = tmp___19;
  tmp___20 = __fswab32(memory_level->MpllSs2);
  memory_level->MpllSs2 = tmp___20;
  return (0);
}
}
static int ci_populate_smc_acpi_level(struct amdgpu_device *adev , SMU7_Discrete_DpmTable *table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct atom_clock_dividers dividers ;
  SMU7_Discrete_VoltageLevel voltage_level ;
  u32 spll_func_cntl ;
  u32 spll_func_cntl_2 ;
  u32 dll_cntl ;
  u32 mclk_pwrmgt_cntl ;
  int ret ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  __u32 tmp___2 ;
  __u32 tmp___3 ;
  __u32 tmp___4 ;
  __u32 tmp___5 ;
  __u32 tmp___6 ;
  __u32 tmp___7 ;
  __u32 tmp___8 ;
  __u32 tmp___9 ;
  __u32 tmp___10 ;
  __u32 tmp___11 ;
  __u32 tmp___12 ;
  __u32 tmp___13 ;
  __u32 tmp___14 ;
  __u32 tmp___15 ;
  int tmp___16 ;
  __u32 tmp___17 ;
  __u32 tmp___18 ;
  __u32 tmp___19 ;
  __u32 tmp___20 ;
  __u32 tmp___21 ;
  __u32 tmp___22 ;
  __u32 tmp___23 ;
  __u32 tmp___24 ;
  __u32 tmp___25 ;
  __u16 tmp___26 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  spll_func_cntl = pi->clock_registers.cg_spll_func_cntl;
  spll_func_cntl_2 = pi->clock_registers.cg_spll_func_cntl_2;
  dll_cntl = pi->clock_registers.dll_cntl;
  mclk_pwrmgt_cntl = pi->clock_registers.mclk_pwrmgt_cntl;
  table->ACPILevel.Flags = table->ACPILevel.Flags & 4294967294U;
  if ((unsigned int )pi->acpi_vddc != 0U) {
    tmp___0 = __fswab32((__u32 )((int )pi->acpi_vddc * 4));
    table->ACPILevel.MinVddc = tmp___0;
  } else {
    tmp___1 = __fswab32((__u32 )((int )pi->min_vddc_in_pp_table * 4));
    table->ACPILevel.MinVddc = tmp___1;
  }
  table->ACPILevel.MinVddcPhases = (int )pi->vddc_phase_shed_control ? 0U : 1U;
  table->ACPILevel.SclkFrequency = adev->clock.spll.reference_freq;
  ret = amdgpu_atombios_get_clock_dividers(adev, 1, table->ACPILevel.SclkFrequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->ACPILevel.SclkDid = (unsigned char )dividers.post_divider;
  table->ACPILevel.DisplayWatermark = 0U;
  table->ACPILevel.DeepSleepDivId = 0U;
  spll_func_cntl = spll_func_cntl & 4294967293U;
  spll_func_cntl = spll_func_cntl | 1U;
  spll_func_cntl_2 = spll_func_cntl_2 & 4294966784U;
  spll_func_cntl_2 = spll_func_cntl_2 | 4U;
  table->ACPILevel.CgSpllFuncCntl = spll_func_cntl;
  table->ACPILevel.CgSpllFuncCntl2 = spll_func_cntl_2;
  table->ACPILevel.CgSpllFuncCntl3 = pi->clock_registers.cg_spll_func_cntl_3;
  table->ACPILevel.CgSpllFuncCntl4 = pi->clock_registers.cg_spll_func_cntl_4;
  table->ACPILevel.SpllSpreadSpectrum = pi->clock_registers.cg_spll_spread_spectrum;
  table->ACPILevel.SpllSpreadSpectrum2 = pi->clock_registers.cg_spll_spread_spectrum_2;
  table->ACPILevel.CcPwrDynRm = 0U;
  table->ACPILevel.CcPwrDynRm1 = 0U;
  tmp___2 = __fswab32(table->ACPILevel.Flags);
  table->ACPILevel.Flags = tmp___2;
  tmp___3 = __fswab32(table->ACPILevel.MinVddcPhases);
  table->ACPILevel.MinVddcPhases = tmp___3;
  tmp___4 = __fswab32(table->ACPILevel.SclkFrequency);
  table->ACPILevel.SclkFrequency = tmp___4;
  tmp___5 = __fswab32(table->ACPILevel.CgSpllFuncCntl);
  table->ACPILevel.CgSpllFuncCntl = tmp___5;
  tmp___6 = __fswab32(table->ACPILevel.CgSpllFuncCntl2);
  table->ACPILevel.CgSpllFuncCntl2 = tmp___6;
  tmp___7 = __fswab32(table->ACPILevel.CgSpllFuncCntl3);
  table->ACPILevel.CgSpllFuncCntl3 = tmp___7;
  tmp___8 = __fswab32(table->ACPILevel.CgSpllFuncCntl4);
  table->ACPILevel.CgSpllFuncCntl4 = tmp___8;
  tmp___9 = __fswab32(table->ACPILevel.SpllSpreadSpectrum);
  table->ACPILevel.SpllSpreadSpectrum = tmp___9;
  tmp___10 = __fswab32(table->ACPILevel.SpllSpreadSpectrum2);
  table->ACPILevel.SpllSpreadSpectrum2 = tmp___10;
  tmp___11 = __fswab32(table->ACPILevel.CcPwrDynRm);
  table->ACPILevel.CcPwrDynRm = tmp___11;
  tmp___12 = __fswab32(table->ACPILevel.CcPwrDynRm1);
  table->ACPILevel.CcPwrDynRm1 = tmp___12;
  table->MemoryACPILevel.MinVddc = table->ACPILevel.MinVddc;
  table->MemoryACPILevel.MinVddcPhases = table->ACPILevel.MinVddcPhases;
  if (pi->vddci_control != 0U) {
    if ((unsigned int )pi->acpi_vddci != 0U) {
      tmp___13 = __fswab32((__u32 )((int )pi->acpi_vddci * 4));
      table->MemoryACPILevel.MinVddci = tmp___13;
    } else {
      tmp___14 = __fswab32((__u32 )((int )pi->min_vddci_in_pp_table * 4));
      table->MemoryACPILevel.MinVddci = tmp___14;
    }
  } else {
  }
  tmp___16 = ci_populate_mvdd_value(adev, 0U, & voltage_level);
  if (tmp___16 != 0) {
    table->MemoryACPILevel.MinMvdd = 0U;
  } else {
    tmp___15 = __fswab32((__u32 )((int )voltage_level.Voltage * 4));
    table->MemoryACPILevel.MinMvdd = tmp___15;
  }
  mclk_pwrmgt_cntl = mclk_pwrmgt_cntl | 196608U;
  mclk_pwrmgt_cntl = mclk_pwrmgt_cntl & 4294966527U;
  dll_cntl = dll_cntl & 4244635647U;
  tmp___17 = __fswab32(dll_cntl);
  table->MemoryACPILevel.DllCntl = tmp___17;
  tmp___18 = __fswab32(mclk_pwrmgt_cntl);
  table->MemoryACPILevel.MclkPwrmgtCntl = tmp___18;
  tmp___19 = __fswab32(pi->clock_registers.mpll_ad_func_cntl);
  table->MemoryACPILevel.MpllAdFuncCntl = tmp___19;
  tmp___20 = __fswab32(pi->clock_registers.mpll_dq_func_cntl);
  table->MemoryACPILevel.MpllDqFuncCntl = tmp___20;
  tmp___21 = __fswab32(pi->clock_registers.mpll_func_cntl);
  table->MemoryACPILevel.MpllFuncCntl = tmp___21;
  tmp___22 = __fswab32(pi->clock_registers.mpll_func_cntl_1);
  table->MemoryACPILevel.MpllFuncCntl_1 = tmp___22;
  tmp___23 = __fswab32(pi->clock_registers.mpll_func_cntl_2);
  table->MemoryACPILevel.MpllFuncCntl_2 = tmp___23;
  tmp___24 = __fswab32(pi->clock_registers.mpll_ss1);
  table->MemoryACPILevel.MpllSs1 = tmp___24;
  tmp___25 = __fswab32(pi->clock_registers.mpll_ss2);
  table->MemoryACPILevel.MpllSs2 = tmp___25;
  table->MemoryACPILevel.EnabledForThrottle = 0U;
  table->MemoryACPILevel.EnabledForActivity = 0U;
  table->MemoryACPILevel.UpH = 0U;
  table->MemoryACPILevel.DownH = 100U;
  table->MemoryACPILevel.VoltageDownH = 0U;
  tmp___26 = __fswab16((int )((unsigned short )pi->mclk_activity_target));
  table->MemoryACPILevel.ActivityLevel = tmp___26;
  table->MemoryACPILevel.StutterEnable = 0U;
  table->MemoryACPILevel.StrobeEnable = 0U;
  table->MemoryACPILevel.EdcReadEnable = 0U;
  table->MemoryACPILevel.EdcWriteEnable = 0U;
  table->MemoryACPILevel.RttEnable = 0U;
  return (0);
}
}
static int ci_enable_ulv(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ulv_parm *ulv ;
  PPSMC_Result tmp___0 ;
  PPSMC_Result tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ulv = & pi->ulv;
  if ((int )ulv->supported) {
    if ((int )enable) {
      tmp___0 = amdgpu_ci_send_msg_to_smc(adev, 98);
      return ((unsigned int )tmp___0 == 1U ? 0 : -22);
    } else {
      tmp___1 = amdgpu_ci_send_msg_to_smc(adev, 99);
      return ((unsigned int )tmp___1 == 1U ? 0 : -22);
    }
  } else {
  }
  return (0);
}
}
static int ci_populate_ulv_level(struct amdgpu_device *adev , SMU7_Discrete_Ulv *state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u16 ulv_voltage ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  __u16 tmp___2 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ulv_voltage = (u16 )adev->pm.dpm.backbias_response_time;
  state->CcPwrDynRm = 0U;
  state->CcPwrDynRm1 = 0U;
  if ((unsigned int )ulv_voltage == 0U) {
    pi->ulv.supported = 0;
    return (0);
  } else {
  }
  if (pi->voltage_control != 2U) {
    if ((int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries)->v < (int )ulv_voltage) {
      state->VddcOffset = 0U;
    } else {
      state->VddcOffset = (int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries)->v - (int )ulv_voltage;
    }
  } else
  if ((int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries)->v < (int )ulv_voltage) {
    state->VddcOffsetVid = 0U;
  } else {
    state->VddcOffsetVid = (unsigned char )((((int )(adev->pm.dpm.dyn_state.vddc_dependency_on_sclk.entries)->v - (int )ulv_voltage) * 100) / 625);
  }
  state->VddcPhase = (int )pi->vddc_phase_shed_control ? 0U : 1U;
  tmp___0 = __fswab32(state->CcPwrDynRm);
  state->CcPwrDynRm = tmp___0;
  tmp___1 = __fswab32(state->CcPwrDynRm1);
  state->CcPwrDynRm1 = tmp___1;
  tmp___2 = __fswab16((int )state->VddcOffset);
  state->VddcOffset = tmp___2;
  return (0);
}
}
static int ci_calculate_sclk_params(struct amdgpu_device *adev , u32 engine_clock ,
                                    SMU7_Discrete_GraphicsLevel *sclk )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct atom_clock_dividers dividers ;
  u32 spll_func_cntl_3 ;
  u32 spll_func_cntl_4 ;
  u32 cg_spll_spread_spectrum ;
  u32 cg_spll_spread_spectrum_2 ;
  u32 reference_clock ;
  u32 reference_divider ;
  u32 fbdiv ;
  int ret ;
  struct amdgpu_atom_ss ss ;
  u32 vco_freq ;
  u32 clk_s ;
  u32 clk_v ;
  bool tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  spll_func_cntl_3 = pi->clock_registers.cg_spll_func_cntl_3;
  spll_func_cntl_4 = pi->clock_registers.cg_spll_func_cntl_4;
  cg_spll_spread_spectrum = pi->clock_registers.cg_spll_spread_spectrum;
  cg_spll_spread_spectrum_2 = pi->clock_registers.cg_spll_spread_spectrum_2;
  reference_clock = adev->clock.spll.reference_freq;
  ret = amdgpu_atombios_get_clock_dividers(adev, 1, engine_clock, 0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  reference_divider = dividers.ref_div + 1U;
  fbdiv = dividers.__annonCompField82.fb_div & 67108863U;
  spll_func_cntl_3 = spll_func_cntl_3 & 4227858432U;
  spll_func_cntl_3 = spll_func_cntl_3 | fbdiv;
  spll_func_cntl_3 = spll_func_cntl_3 | 268435456U;
  if ((int )pi->caps_sclk_ss_support) {
    vco_freq = dividers.post_div * engine_clock;
    tmp___0 = amdgpu_atombios_get_asic_ss_info(adev, & ss, 2, vco_freq);
    if ((int )tmp___0) {
      clk_s = (reference_clock * 5U) / ((u32 )ss.rate * reference_divider);
      clk_v = (((u32 )ss.percentage * fbdiv) * 4U) / (clk_s * 10000U);
      cg_spll_spread_spectrum = cg_spll_spread_spectrum & 4294901774U;
      cg_spll_spread_spectrum = (clk_s << 4) | cg_spll_spread_spectrum;
      cg_spll_spread_spectrum = cg_spll_spread_spectrum | 1U;
      cg_spll_spread_spectrum_2 = cg_spll_spread_spectrum_2 & 4227858432U;
      cg_spll_spread_spectrum_2 = cg_spll_spread_spectrum_2 | clk_v;
    } else {
    }
  } else {
  }
  sclk->SclkFrequency = engine_clock;
  sclk->CgSpllFuncCntl3 = spll_func_cntl_3;
  sclk->CgSpllFuncCntl4 = spll_func_cntl_4;
  sclk->SpllSpreadSpectrum = cg_spll_spread_spectrum;
  sclk->SpllSpreadSpectrum2 = cg_spll_spread_spectrum_2;
  sclk->SclkDid = (unsigned char )dividers.post_divider;
  return (0);
}
}
static int ci_populate_single_graphic_level(struct amdgpu_device *adev , u32 engine_clock ,
                                            u16 sclk_activity_level_t , SMU7_Discrete_GraphicsLevel *graphic_level )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  __u32 tmp___2 ;
  __u32 tmp___3 ;
  __u16 tmp___4 ;
  __u32 tmp___5 ;
  __u32 tmp___6 ;
  __u32 tmp___7 ;
  __u32 tmp___8 ;
  __u32 tmp___9 ;
  __u32 tmp___10 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = ci_calculate_sclk_params(adev, engine_clock, graphic_level);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_get_dependency_volt_by_clk(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk,
                                      engine_clock, & graphic_level->MinVddc);
  if (ret != 0) {
    return (ret);
  } else {
  }
  graphic_level->SclkFrequency = engine_clock;
  graphic_level->Flags = 0U;
  graphic_level->MinVddcPhases = 1U;
  if ((int )pi->vddc_phase_shed_control) {
    ci_populate_phase_value_based_on_sclk(adev, (struct amdgpu_phase_shedding_limits_table const *)(& adev->pm.dpm.dyn_state.phase_shedding_limits_table),
                                          engine_clock, & graphic_level->MinVddcPhases);
  } else {
  }
  graphic_level->ActivityLevel = sclk_activity_level_t;
  graphic_level->CcPwrDynRm = 0U;
  graphic_level->CcPwrDynRm1 = 0U;
  graphic_level->EnabledForThrottle = 1U;
  graphic_level->UpH = 0U;
  graphic_level->DownH = 0U;
  graphic_level->VoltageDownH = 0U;
  graphic_level->PowerThrottle = 0U;
  if ((int )pi->caps_sclk_ds) {
    graphic_level->DeepSleepDivId = ci_get_sleep_divider_id_from_clock(adev, engine_clock,
                                                                       800U);
  } else {
  }
  graphic_level->DisplayWatermark = 0U;
  tmp___0 = __fswab32(graphic_level->Flags);
  graphic_level->Flags = tmp___0;
  tmp___1 = __fswab32(graphic_level->MinVddc * 4U);
  graphic_level->MinVddc = tmp___1;
  tmp___2 = __fswab32(graphic_level->MinVddcPhases);
  graphic_level->MinVddcPhases = tmp___2;
  tmp___3 = __fswab32(graphic_level->SclkFrequency);
  graphic_level->SclkFrequency = tmp___3;
  tmp___4 = __fswab16((int )graphic_level->ActivityLevel);
  graphic_level->ActivityLevel = tmp___4;
  tmp___5 = __fswab32(graphic_level->CgSpllFuncCntl3);
  graphic_level->CgSpllFuncCntl3 = tmp___5;
  tmp___6 = __fswab32(graphic_level->CgSpllFuncCntl4);
  graphic_level->CgSpllFuncCntl4 = tmp___6;
  tmp___7 = __fswab32(graphic_level->SpllSpreadSpectrum);
  graphic_level->SpllSpreadSpectrum = tmp___7;
  tmp___8 = __fswab32(graphic_level->SpllSpreadSpectrum2);
  graphic_level->SpllSpreadSpectrum2 = tmp___8;
  tmp___9 = __fswab32(graphic_level->CcPwrDynRm);
  graphic_level->CcPwrDynRm = tmp___9;
  tmp___10 = __fswab32(graphic_level->CcPwrDynRm1);
  graphic_level->CcPwrDynRm1 = tmp___10;
  graphic_level->EnabledForActivity = 1U;
  return (0);
}
}
static int ci_populate_all_graphic_levels(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_dpm_table *dpm_table ;
  u32 level_array_address ;
  u32 level_array_size ;
  SMU7_Discrete_GraphicsLevel *levels ;
  u32 i ;
  u32 ret ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  dpm_table = & pi->dpm_table;
  level_array_address = pi->dpm_table_start + 296U;
  level_array_size = 448U;
  levels = (SMU7_Discrete_GraphicsLevel *)(& pi->smc_state_table.GraphicsLevel);
  memset((void *)levels, 0, (size_t )level_array_size);
  i = 0U;
  goto ldv_49587;
  ldv_49586:
  tmp___0 = ci_populate_single_graphic_level(adev, dpm_table->sclk_table.dpm_levels[i].value,
                                             (int )((unsigned short )pi->activity_target[i]),
                                             (SMU7_Discrete_GraphicsLevel *)(& pi->smc_state_table.GraphicsLevel) + (unsigned long )i);
  ret = (u32 )tmp___0;
  if (ret != 0U) {
    return ((int )ret);
  } else {
  }
  if (i > 1U) {
    pi->smc_state_table.GraphicsLevel[i].DeepSleepDivId = 0U;
  } else {
  }
  if (dpm_table->sclk_table.count - 1U == i) {
    pi->smc_state_table.GraphicsLevel[i].DisplayWatermark = 1U;
  } else {
  }
  i = i + 1U;
  ldv_49587: ;
  if (dpm_table->sclk_table.count > i) {
    goto ldv_49586;
  } else {
  }
  pi->smc_state_table.GraphicsDpmLevelCount = (unsigned char )dpm_table->sclk_table.count;
  pi->dpm_level_enable_mask.sclk_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& dpm_table->sclk_table);
  tmp___1 = amdgpu_ci_copy_bytes_to_smc(adev, level_array_address, (u8 const *)levels,
                                        level_array_size, pi->sram_end);
  ret = (u32 )tmp___1;
  if (ret != 0U) {
    return ((int )ret);
  } else {
  }
  return (0);
}
}
static int ci_populate_ulv_state(struct amdgpu_device *adev , SMU7_Discrete_Ulv *ulv_level )
{
  int tmp ;
  {
  tmp = ci_populate_ulv_level(adev, ulv_level);
  return (tmp);
}
}
static int ci_populate_all_memory_levels(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_dpm_table *dpm_table ;
  u32 level_array_address ;
  u32 level_array_size ;
  SMU7_Discrete_MemoryLevel *levels ;
  u32 i ;
  u32 ret ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  dpm_table = & pi->dpm_table;
  level_array_address = pi->dpm_table_start + 816U;
  level_array_size = 432U;
  levels = (SMU7_Discrete_MemoryLevel *)(& pi->smc_state_table.MemoryLevel);
  memset((void *)levels, 0, (size_t )level_array_size);
  i = 0U;
  goto ldv_49604;
  ldv_49603: ;
  if (dpm_table->mclk_table.dpm_levels[i].value == 0U) {
    return (-22);
  } else {
  }
  tmp___0 = ci_populate_single_memory_level(adev, dpm_table->mclk_table.dpm_levels[i].value,
                                            (SMU7_Discrete_MemoryLevel *)(& pi->smc_state_table.MemoryLevel) + (unsigned long )i);
  ret = (u32 )tmp___0;
  if (ret != 0U) {
    return ((int )ret);
  } else {
  }
  i = i + 1U;
  ldv_49604: ;
  if (dpm_table->mclk_table.count > i) {
    goto ldv_49603;
  } else {
  }
  if (dpm_table->mclk_table.count > 1U && ((unsigned int )(adev->pdev)->device == 26544U || (unsigned int )(adev->pdev)->device == 26545U)) {
    pi->smc_state_table.MemoryLevel[1].MinVddc = pi->smc_state_table.MemoryLevel[0].MinVddc;
    pi->smc_state_table.MemoryLevel[1].MinVddcPhases = pi->smc_state_table.MemoryLevel[0].MinVddcPhases;
  } else {
  }
  pi->smc_state_table.MemoryLevel[0].ActivityLevel = 7936U;
  pi->smc_state_table.MemoryDpmLevelCount = (unsigned char )dpm_table->mclk_table.count;
  pi->dpm_level_enable_mask.mclk_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& dpm_table->mclk_table);
  pi->smc_state_table.MemoryLevel[dpm_table->mclk_table.count - 1U].DisplayWatermark = 1U;
  tmp___1 = amdgpu_ci_copy_bytes_to_smc(adev, level_array_address, (u8 const *)levels,
                                        level_array_size, pi->sram_end);
  ret = (u32 )tmp___1;
  if (ret != 0U) {
    return ((int )ret);
  } else {
  }
  return (0);
}
}
static void ci_reset_single_dpm_table(struct amdgpu_device *adev , struct ci_single_dpm_table *dpm_table ,
                                      u32 count )
{
  u32 i ;
  {
  dpm_table->count = count;
  i = 0U;
  goto ldv_49613;
  ldv_49612:
  dpm_table->dpm_levels[i].enabled = 0;
  i = i + 1U;
  ldv_49613: ;
  if (i <= 7U) {
    goto ldv_49612;
  } else {
  }
  return;
}
}
static void ci_setup_pcie_table_entry(struct ci_single_dpm_table *dpm_table , u32 index ,
                                      u32 pcie_gen , u32 pcie_lanes )
{
  {
  dpm_table->dpm_levels[index].value = pcie_gen;
  dpm_table->dpm_levels[index].param1 = pcie_lanes;
  dpm_table->dpm_levels[index].enabled = 1;
  return;
}
}
static int ci_setup_default_pcie_tables(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (! pi->use_pcie_performance_levels && ! pi->use_pcie_powersaving_levels) {
    return (-22);
  } else {
  }
  if ((int )pi->use_pcie_performance_levels && ! pi->use_pcie_powersaving_levels) {
    pi->pcie_gen_powersaving = pi->pcie_gen_performance;
    pi->pcie_lane_powersaving = pi->pcie_lane_performance;
  } else
  if (! pi->use_pcie_performance_levels && (int )pi->use_pcie_powersaving_levels) {
    pi->pcie_gen_performance = pi->pcie_gen_powersaving;
    pi->pcie_lane_performance = pi->pcie_lane_powersaving;
  } else {
  }
  ci_reset_single_dpm_table(adev, & pi->dpm_table.pcie_speed_table, 8U);
  if ((unsigned int )adev->asic_type == 0U) {
    ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 0U, (u32 )pi->pcie_gen_powersaving.min,
                              (u32 )pi->pcie_lane_powersaving.max);
  } else {
    ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 0U, (u32 )pi->pcie_gen_powersaving.min,
                              (u32 )pi->pcie_lane_powersaving.min);
  }
  ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 1U, (u32 )pi->pcie_gen_performance.min,
                            (u32 )pi->pcie_lane_performance.min);
  ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 2U, (u32 )pi->pcie_gen_powersaving.min,
                            (u32 )pi->pcie_lane_powersaving.max);
  ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 3U, (u32 )pi->pcie_gen_performance.min,
                            (u32 )pi->pcie_lane_performance.max);
  ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 4U, (u32 )pi->pcie_gen_powersaving.max,
                            (u32 )pi->pcie_lane_powersaving.max);
  ci_setup_pcie_table_entry(& pi->dpm_table.pcie_speed_table, 5U, (u32 )pi->pcie_gen_performance.max,
                            (u32 )pi->pcie_lane_performance.max);
  pi->dpm_table.pcie_speed_table.count = 6U;
  return (0);
}
}
static int ci_setup_default_dpm_tables(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *allowed_sclk_vddc_table ;
  struct amdgpu_clock_voltage_dependency_table *allowed_mclk_table ;
  struct amdgpu_cac_leakage_table *std_voltage_table ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  allowed_sclk_vddc_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  allowed_mclk_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_mclk;
  std_voltage_table = & adev->pm.dpm.dyn_state.cac_leakage_table;
  if ((unsigned long )allowed_sclk_vddc_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (allowed_sclk_vddc_table->count == 0U) {
    return (-22);
  } else {
  }
  if ((unsigned long )allowed_mclk_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (allowed_mclk_table->count == 0U) {
    return (-22);
  } else {
  }
  memset((void *)(& pi->dpm_table), 0, 600UL);
  ci_reset_single_dpm_table(adev, & pi->dpm_table.sclk_table, 8U);
  ci_reset_single_dpm_table(adev, & pi->dpm_table.mclk_table, 6U);
  ci_reset_single_dpm_table(adev, & pi->dpm_table.vddc_table, 8U);
  ci_reset_single_dpm_table(adev, & pi->dpm_table.vddci_table, 4U);
  ci_reset_single_dpm_table(adev, & pi->dpm_table.mvdd_table, 4U);
  pi->dpm_table.sclk_table.count = 0U;
  i = 0U;
  goto ldv_49634;
  ldv_49633: ;
  if (i == 0U || pi->dpm_table.sclk_table.dpm_levels[pi->dpm_table.sclk_table.count - 1U].value != (allowed_sclk_vddc_table->entries + (unsigned long )i)->clk) {
    pi->dpm_table.sclk_table.dpm_levels[pi->dpm_table.sclk_table.count].value = (allowed_sclk_vddc_table->entries + (unsigned long )i)->clk;
    pi->dpm_table.sclk_table.dpm_levels[pi->dpm_table.sclk_table.count].enabled = i == 0U;
    pi->dpm_table.sclk_table.count = pi->dpm_table.sclk_table.count + 1U;
  } else {
  }
  i = i + 1U;
  ldv_49634: ;
  if (allowed_sclk_vddc_table->count > i) {
    goto ldv_49633;
  } else {
  }
  pi->dpm_table.mclk_table.count = 0U;
  i = 0U;
  goto ldv_49637;
  ldv_49636: ;
  if (i == 0U || pi->dpm_table.mclk_table.dpm_levels[pi->dpm_table.mclk_table.count - 1U].value != (allowed_mclk_table->entries + (unsigned long )i)->clk) {
    pi->dpm_table.mclk_table.dpm_levels[pi->dpm_table.mclk_table.count].value = (allowed_mclk_table->entries + (unsigned long )i)->clk;
    pi->dpm_table.mclk_table.dpm_levels[pi->dpm_table.mclk_table.count].enabled = i == 0U;
    pi->dpm_table.mclk_table.count = pi->dpm_table.mclk_table.count + 1U;
  } else {
  }
  i = i + 1U;
  ldv_49637: ;
  if (allowed_mclk_table->count > i) {
    goto ldv_49636;
  } else {
  }
  i = 0U;
  goto ldv_49640;
  ldv_49639:
  pi->dpm_table.vddc_table.dpm_levels[i].value = (u32 )(allowed_sclk_vddc_table->entries + (unsigned long )i)->v;
  pi->dpm_table.vddc_table.dpm_levels[i].param1 = (std_voltage_table->entries + (unsigned long )i)->__annonCompField79.leakage;
  pi->dpm_table.vddc_table.dpm_levels[i].enabled = 1;
  i = i + 1U;
  ldv_49640: ;
  if (allowed_sclk_vddc_table->count > i) {
    goto ldv_49639;
  } else {
  }
  pi->dpm_table.vddc_table.count = allowed_sclk_vddc_table->count;
  allowed_mclk_table = & adev->pm.dpm.dyn_state.vddci_dependency_on_mclk;
  if ((unsigned long )allowed_mclk_table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_49643;
    ldv_49642:
    pi->dpm_table.vddci_table.dpm_levels[i].value = (u32 )(allowed_mclk_table->entries + (unsigned long )i)->v;
    pi->dpm_table.vddci_table.dpm_levels[i].enabled = 1;
    i = i + 1U;
    ldv_49643: ;
    if (allowed_mclk_table->count > i) {
      goto ldv_49642;
    } else {
    }
    pi->dpm_table.vddci_table.count = allowed_mclk_table->count;
  } else {
  }
  allowed_mclk_table = & adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk;
  if ((unsigned long )allowed_mclk_table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_49646;
    ldv_49645:
    pi->dpm_table.mvdd_table.dpm_levels[i].value = (u32 )(allowed_mclk_table->entries + (unsigned long )i)->v;
    pi->dpm_table.mvdd_table.dpm_levels[i].enabled = 1;
    i = i + 1U;
    ldv_49646: ;
    if (allowed_mclk_table->count > i) {
      goto ldv_49645;
    } else {
    }
    pi->dpm_table.mvdd_table.count = allowed_mclk_table->count;
  } else {
  }
  ci_setup_default_pcie_tables(adev);
  return (0);
}
}
static int ci_find_boot_level(struct ci_single_dpm_table *table , u32 value , u32 *boot_level )
{
  u32 i ;
  int ret ;
  {
  ret = -22;
  i = 0U;
  goto ldv_49656;
  ldv_49655: ;
  if (table->dpm_levels[i].value == value) {
    *boot_level = i;
    ret = 0;
  } else {
  }
  i = i + 1U;
  ldv_49656: ;
  if (table->count > i) {
    goto ldv_49655;
  } else {
  }
  return (ret);
}
}
static int ci_init_smc_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ulv_parm *ulv ;
  struct amdgpu_ps *amdgpu_boot_state ;
  SMU7_Discrete_DpmTable *table ;
  int ret ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  __u32 tmp___2 ;
  __u32 tmp___3 ;
  __u32 tmp___4 ;
  __u32 tmp___5 ;
  __u16 tmp___6 ;
  __u16 tmp___7 ;
  __u16 tmp___8 ;
  __u16 tmp___9 ;
  __u16 tmp___10 ;
  __u16 tmp___11 ;
  __u16 tmp___12 ;
  __u16 tmp___13 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ulv = & pi->ulv;
  amdgpu_boot_state = adev->pm.dpm.boot_ps;
  table = & pi->smc_state_table;
  ret = ci_setup_default_dpm_tables(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  if (pi->voltage_control != 0U) {
    ci_populate_smc_voltage_tables(adev, table);
  } else {
  }
  ci_init_fps_limits(adev);
  if ((adev->pm.dpm.platform_caps & 32U) != 0U) {
    table->SystemFlags = table->SystemFlags | 1U;
  } else {
  }
  if ((adev->pm.dpm.platform_caps & 128U) != 0U) {
    table->SystemFlags = table->SystemFlags | 2U;
  } else {
  }
  if (adev->mc.vram_type == 5U) {
    table->SystemFlags = table->SystemFlags | 4U;
  } else {
  }
  if ((int )ulv->supported) {
    ret = ci_populate_ulv_state(adev, & pi->smc_state_table.Ulv);
    if (ret != 0) {
      return (ret);
    } else {
    }
    (*(adev->smc_wreg))(adev, 3223322972U, ulv->cg_ulv_parameter);
  } else {
  }
  ret = ci_populate_all_graphic_levels(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_all_memory_levels(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ci_populate_smc_link_level(adev, table);
  ret = ci_populate_smc_acpi_level(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_vce_level(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_acp_level(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_samu_level(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_do_program_memory_timing_parameters(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_populate_smc_uvd_level(adev, table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->UvdBootLevel = 0U;
  table->VceBootLevel = 0U;
  table->AcpBootLevel = 0U;
  table->SamuBootLevel = 0U;
  table->GraphicsBootLevel = 0U;
  table->MemoryBootLevel = 0U;
  ret = ci_find_boot_level(& pi->dpm_table.sclk_table, pi->vbios_boot_state.sclk_bootup_value,
                           (u32 *)(& pi->smc_state_table.GraphicsBootLevel));
  ret = ci_find_boot_level(& pi->dpm_table.mclk_table, pi->vbios_boot_state.mclk_bootup_value,
                           (u32 *)(& pi->smc_state_table.MemoryBootLevel));
  table->BootVddc = pi->vbios_boot_state.vddc_bootup_value;
  table->BootVddci = pi->vbios_boot_state.vddci_bootup_value;
  table->BootMVdd = pi->vbios_boot_state.mvdd_bootup_value;
  ci_populate_smc_initial_state(adev, amdgpu_boot_state);
  ret = ci_populate_bapm_parameters_in_dpm_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  table->UVDInterval = 1U;
  table->VCEInterval = 1U;
  table->ACPInterval = 1U;
  table->SAMUInterval = 1U;
  table->GraphicsVoltageChangeEnable = 1U;
  table->GraphicsThermThrottleEnable = 1U;
  table->GraphicsInterval = 1U;
  table->VoltageInterval = 1U;
  table->ThermalInterval = 1U;
  table->TemperatureLimitHigh = (unsigned short )((pi->thermal_temp_setting.temperature_high * 256) / 1000);
  table->TemperatureLimitLow = (unsigned short )((pi->thermal_temp_setting.temperature_low * 256) / 1000);
  table->MemoryVoltageChangeEnable = 1U;
  table->MemoryInterval = 1U;
  table->VoltageResponseTime = 0U;
  table->VddcVddciDelta = 4000U;
  table->PhaseResponseTime = 0U;
  table->MemoryThermThrottleEnable = 1U;
  table->PCIeBootLinkLevel = (unsigned int )((uint8_t )pi->dpm_table.pcie_speed_table.count) - 1U;
  table->PCIeGenInterval = 1U;
  if (pi->voltage_control == 2U) {
    table->SVI2Enable = 1U;
  } else {
    table->SVI2Enable = 0U;
  }
  table->ThermGpio = 17U;
  table->SclkStepSize = 16384U;
  tmp___0 = __fswab32(table->SystemFlags);
  table->SystemFlags = tmp___0;
  tmp___1 = __fswab32(table->SmioMaskVddcVid);
  table->SmioMaskVddcVid = tmp___1;
  tmp___2 = __fswab32(table->SmioMaskVddcPhase);
  table->SmioMaskVddcPhase = tmp___2;
  tmp___3 = __fswab32(table->SmioMaskVddciVid);
  table->SmioMaskVddciVid = tmp___3;
  tmp___4 = __fswab32(table->SmioMaskMvddVid);
  table->SmioMaskMvddVid = tmp___4;
  tmp___5 = __fswab32(table->SclkStepSize);
  table->SclkStepSize = tmp___5;
  tmp___6 = __fswab16((int )table->TemperatureLimitHigh);
  table->TemperatureLimitHigh = tmp___6;
  tmp___7 = __fswab16((int )table->TemperatureLimitLow);
  table->TemperatureLimitLow = tmp___7;
  tmp___8 = __fswab16((int )table->VddcVddciDelta);
  table->VddcVddciDelta = tmp___8;
  tmp___9 = __fswab16((int )table->VoltageResponseTime);
  table->VoltageResponseTime = tmp___9;
  tmp___10 = __fswab16((int )table->PhaseResponseTime);
  table->PhaseResponseTime = tmp___10;
  tmp___11 = __fswab16((int )((unsigned int )table->BootVddc * 4U));
  table->BootVddc = tmp___11;
  tmp___12 = __fswab16((int )((unsigned int )table->BootVddci * 4U));
  table->BootVddci = tmp___12;
  tmp___13 = __fswab16((int )((unsigned int )table->BootMVdd * 4U));
  table->BootMVdd = tmp___13;
  ret = amdgpu_ci_copy_bytes_to_smc(adev, pi->dpm_table_start + 108U, (u8 const *)(& table->SystemFlags),
                                    1916U, pi->sram_end);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (0);
}
}
static void ci_trim_single_dpm_states(struct amdgpu_device *adev , struct ci_single_dpm_table *dpm_table ,
                                      u32 low_limit , u32 high_limit )
{
  u32 i ;
  {
  i = 0U;
  goto ldv_49674;
  ldv_49673: ;
  if (dpm_table->dpm_levels[i].value < low_limit || dpm_table->dpm_levels[i].value > high_limit) {
    dpm_table->dpm_levels[i].enabled = 0;
  } else {
    dpm_table->dpm_levels[i].enabled = 1;
  }
  i = i + 1U;
  ldv_49674: ;
  if (dpm_table->count > i) {
    goto ldv_49673;
  } else {
  }
  return;
}
}
static void ci_trim_pcie_dpm_states(struct amdgpu_device *adev , u32 speed_low , u32 lanes_low ,
                                    u32 speed_high , u32 lanes_high )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_single_dpm_table *pcie_table ;
  u32 i ;
  u32 j ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  pcie_table = & pi->dpm_table.pcie_speed_table;
  i = 0U;
  goto ldv_49688;
  ldv_49687: ;
  if (((pcie_table->dpm_levels[i].value < speed_low || pcie_table->dpm_levels[i].param1 < lanes_low) || pcie_table->dpm_levels[i].value > speed_high) || pcie_table->dpm_levels[i].param1 > lanes_high) {
    pcie_table->dpm_levels[i].enabled = 0;
  } else {
    pcie_table->dpm_levels[i].enabled = 1;
  }
  i = i + 1U;
  ldv_49688: ;
  if (pcie_table->count > i) {
    goto ldv_49687;
  } else {
  }
  i = 0U;
  goto ldv_49694;
  ldv_49693: ;
  if ((int )pcie_table->dpm_levels[i].enabled) {
    j = i + 1U;
    goto ldv_49691;
    ldv_49690: ;
    if ((int )pcie_table->dpm_levels[j].enabled) {
      if (pcie_table->dpm_levels[i].value == pcie_table->dpm_levels[j].value && pcie_table->dpm_levels[i].param1 == pcie_table->dpm_levels[j].param1) {
        pcie_table->dpm_levels[j].enabled = 0;
      } else {
      }
    } else {
    }
    j = j + 1U;
    ldv_49691: ;
    if (pcie_table->count > j) {
      goto ldv_49690;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_49694: ;
  if (pcie_table->count > i) {
    goto ldv_49693;
  } else {
  }
  return;
}
}
static int ci_trim_dpm_states(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_state )
{
  struct ci_ps *state ;
  struct ci_ps *tmp ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  u32 high_limit_count ;
  {
  tmp = ci_get_ps(amdgpu_state);
  state = tmp;
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  if ((unsigned int )state->performance_level_count == 0U) {
    return (-22);
  } else {
  }
  if ((unsigned int )state->performance_level_count == 1U) {
    high_limit_count = 0U;
  } else {
    high_limit_count = 1U;
  }
  ci_trim_single_dpm_states(adev, & pi->dpm_table.sclk_table, state->performance_levels[0].sclk,
                            state->performance_levels[high_limit_count].sclk);
  ci_trim_single_dpm_states(adev, & pi->dpm_table.mclk_table, state->performance_levels[0].mclk,
                            state->performance_levels[high_limit_count].mclk);
  ci_trim_pcie_dpm_states(adev, (u32 )state->performance_levels[0].pcie_gen, (u32 )state->performance_levels[0].pcie_lane,
                          (u32 )state->performance_levels[high_limit_count].pcie_gen,
                          (u32 )state->performance_levels[high_limit_count].pcie_lane);
  return (0);
}
}
static int ci_apply_disp_minimum_voltage_request(struct amdgpu_device *adev )
{
  struct amdgpu_clock_voltage_dependency_table *disp_voltage_table ;
  struct amdgpu_clock_voltage_dependency_table *vddc_table ;
  u32 requested_voltage ;
  u32 i ;
  PPSMC_Result tmp ;
  {
  disp_voltage_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk;
  vddc_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  requested_voltage = 0U;
  if ((unsigned long )disp_voltage_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (disp_voltage_table->count == 0U) {
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_49711;
  ldv_49710: ;
  if (adev->clock.current_dispclk == (disp_voltage_table->entries + (unsigned long )i)->clk) {
    requested_voltage = (u32 )(disp_voltage_table->entries + (unsigned long )i)->v;
  } else {
  }
  i = i + 1U;
  ldv_49711: ;
  if (disp_voltage_table->count > i) {
    goto ldv_49710;
  } else {
  }
  i = 0U;
  goto ldv_49714;
  ldv_49713: ;
  if ((u32 )(vddc_table->entries + (unsigned long )i)->v >= requested_voltage) {
    requested_voltage = (u32 )(vddc_table->entries + (unsigned long )i)->v;
    tmp = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 351, requested_voltage * 4U);
    return ((unsigned int )tmp == 1U ? 0 : -22);
  } else {
  }
  i = i + 1U;
  ldv_49714: ;
  if (vddc_table->count > i) {
    goto ldv_49713;
  } else {
  }
  return (-22);
}
}
static int ci_upload_dpm_level_enable_mask(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  PPSMC_Result result ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ci_apply_disp_minimum_voltage_request(adev);
  if (pi->sclk_dpm_key_disabled == 0U) {
    if (pi->dpm_level_enable_mask.sclk_dpm_enable_mask != 0U) {
      result = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 325, pi->dpm_level_enable_mask.sclk_dpm_enable_mask);
      if ((unsigned int )result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
  } else {
  }
  if (pi->mclk_dpm_key_disabled == 0U) {
    if (pi->dpm_level_enable_mask.mclk_dpm_enable_mask != 0U) {
      result = amdgpu_ci_send_msg_to_smc_with_parameter(adev, 326, pi->dpm_level_enable_mask.mclk_dpm_enable_mask);
      if ((unsigned int )result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
  } else {
  }
  return (0);
}
}
static void ci_find_dpm_states_clocks_in_dpm_table(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ps *state ;
  struct ci_ps *tmp___0 ;
  struct ci_single_dpm_table *sclk_table ;
  u32 sclk ;
  struct ci_single_dpm_table *mclk_table ;
  u32 mclk ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_ps(amdgpu_state);
  state = tmp___0;
  sclk_table = & pi->dpm_table.sclk_table;
  sclk = state->performance_levels[(int )state->performance_level_count + -1].sclk;
  mclk_table = & pi->dpm_table.mclk_table;
  mclk = state->performance_levels[(int )state->performance_level_count + -1].mclk;
  pi->need_update_smu7_dpm_table = 0U;
  i = 0U;
  goto ldv_49734;
  ldv_49733: ;
  if (sclk_table->dpm_levels[i].value == sclk) {
    goto ldv_49732;
  } else {
  }
  i = i + 1U;
  ldv_49734: ;
  if (sclk_table->count > i) {
    goto ldv_49733;
  } else {
  }
  ldv_49732: ;
  if (sclk_table->count <= i) {
    pi->need_update_smu7_dpm_table = pi->need_update_smu7_dpm_table | 1U;
  } else {
  }
  i = 0U;
  goto ldv_49737;
  ldv_49736: ;
  if (mclk_table->dpm_levels[i].value == mclk) {
    goto ldv_49735;
  } else {
  }
  i = i + 1U;
  ldv_49737: ;
  if (mclk_table->count > i) {
    goto ldv_49736;
  } else {
  }
  ldv_49735: ;
  if (mclk_table->count <= i) {
    pi->need_update_smu7_dpm_table = pi->need_update_smu7_dpm_table | 2U;
  } else {
  }
  if (adev->pm.dpm.current_active_crtc_count != adev->pm.dpm.new_active_crtc_count) {
    pi->need_update_smu7_dpm_table = pi->need_update_smu7_dpm_table | 8U;
  } else {
  }
  return;
}
}
static int ci_populate_and_upload_sclk_mclk_dpm_levels(struct amdgpu_device *adev ,
                                                       struct amdgpu_ps *amdgpu_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ps *state ;
  struct ci_ps *tmp___0 ;
  u32 sclk ;
  u32 mclk ;
  struct ci_dpm_table *dpm_table ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_ps(amdgpu_state);
  state = tmp___0;
  sclk = state->performance_levels[(int )state->performance_level_count + -1].sclk;
  mclk = state->performance_levels[(int )state->performance_level_count + -1].mclk;
  dpm_table = & pi->dpm_table;
  if (pi->need_update_smu7_dpm_table == 0U) {
    return (0);
  } else {
  }
  if ((int )pi->need_update_smu7_dpm_table & 1) {
    dpm_table->sclk_table.dpm_levels[dpm_table->sclk_table.count - 1U].value = sclk;
  } else {
  }
  if ((pi->need_update_smu7_dpm_table & 2U) != 0U) {
    dpm_table->mclk_table.dpm_levels[dpm_table->mclk_table.count - 1U].value = mclk;
  } else {
  }
  if ((pi->need_update_smu7_dpm_table & 5U) != 0U) {
    ret = ci_populate_all_graphic_levels(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  if ((pi->need_update_smu7_dpm_table & 10U) != 0U) {
    ret = ci_populate_all_memory_levels(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int ci_enable_uvd_dpm(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_clock_and_voltage_limits const *max_limits ;
  int i ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )adev->pm.dpm.ac_power) {
    max_limits = (struct amdgpu_clock_and_voltage_limits const *)(& adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);
  } else {
    max_limits = (struct amdgpu_clock_and_voltage_limits const *)(& adev->pm.dpm.dyn_state.max_clock_voltage_on_dc);
  }
  if ((int )enable) {
    pi->dpm_level_enable_mask.uvd_dpm_enable_mask = 0U;
    i = (int )adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.count + -1;
    goto ldv_49757;
    ldv_49756: ;
    if ((int )(adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )i)->v <= (int )((unsigned short )max_limits->vddc)) {
      pi->dpm_level_enable_mask.uvd_dpm_enable_mask = pi->dpm_level_enable_mask.uvd_dpm_enable_mask | (u32 )(1 << i);
      if (! pi->caps_uvd_dpm) {
        goto ldv_49755;
      } else {
      }
    } else {
    }
    i = i - 1;
    ldv_49757: ;
    if (i >= 0) {
      goto ldv_49756;
    } else {
    }
    ldv_49755:
    amdgpu_ci_send_msg_to_smc_with_parameter(adev, 301, pi->dpm_level_enable_mask.uvd_dpm_enable_mask);
    if ((int )pi->last_mclk_dpm_enable_mask & 1) {
      pi->uvd_enabled = 1;
      pi->dpm_level_enable_mask.mclk_dpm_enable_mask = pi->dpm_level_enable_mask.mclk_dpm_enable_mask & 4294967294U;
      amdgpu_ci_send_msg_to_smc_with_parameter(adev, 326, pi->dpm_level_enable_mask.mclk_dpm_enable_mask);
    } else {
    }
  } else
  if ((int )pi->last_mclk_dpm_enable_mask & 1) {
    pi->uvd_enabled = 0;
    pi->dpm_level_enable_mask.mclk_dpm_enable_mask = pi->dpm_level_enable_mask.mclk_dpm_enable_mask | 1U;
    amdgpu_ci_send_msg_to_smc_with_parameter(adev, 326, pi->dpm_level_enable_mask.mclk_dpm_enable_mask);
  } else {
  }
  tmp___0 = amdgpu_ci_send_msg_to_smc(adev, (int )enable ? 340 : 341);
  return ((unsigned int )tmp___0 == 1U ? 0 : -22);
}
}
static int ci_enable_vce_dpm(struct amdgpu_device *adev , bool enable )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_clock_and_voltage_limits const *max_limits ;
  int i ;
  PPSMC_Result tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((int )adev->pm.dpm.ac_power) {
    max_limits = (struct amdgpu_clock_and_voltage_limits const *)(& adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);
  } else {
    max_limits = (struct amdgpu_clock_and_voltage_limits const *)(& adev->pm.dpm.dyn_state.max_clock_voltage_on_dc);
  }
  if ((int )enable) {
    pi->dpm_level_enable_mask.vce_dpm_enable_mask = 0U;
    i = (int )adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.count + -1;
    goto ldv_49767;
    ldv_49766: ;
    if ((int )(adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )i)->v <= (int )((unsigned short )max_limits->vddc)) {
      pi->dpm_level_enable_mask.vce_dpm_enable_mask = pi->dpm_level_enable_mask.vce_dpm_enable_mask | (u32 )(1 << i);
      if (! pi->caps_vce_dpm) {
        goto ldv_49765;
      } else {
      }
    } else {
    }
    i = i - 1;
    ldv_49767: ;
    if (i >= 0) {
      goto ldv_49766;
    } else {
    }
    ldv_49765:
    amdgpu_ci_send_msg_to_smc_with_parameter(adev, 302, pi->dpm_level_enable_mask.vce_dpm_enable_mask);
  } else {
  }
  tmp___0 = amdgpu_ci_send_msg_to_smc(adev, (int )enable ? 346 : 347);
  return ((unsigned int )tmp___0 == 1U ? 0 : -22);
}
}
static int ci_update_uvd_dpm(struct amdgpu_device *adev , bool gate )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if (! gate) {
    if ((int )pi->caps_uvd_dpm || (unsigned int )adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.count == 0U) {
      pi->smc_state_table.UvdBootLevel = 0U;
    } else {
      pi->smc_state_table.UvdBootLevel = (unsigned int )adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.count + 255U;
    }
    tmp___0 = (*(adev->smc_rreg))(adev, 259944U);
    tmp___0 = tmp___0 & 16777215U;
    tmp___0 = (u32 )((int )pi->smc_state_table.UvdBootLevel << 24) | tmp___0;
    (*(adev->smc_wreg))(adev, 259944U, tmp___0);
  } else {
  }
  tmp___1 = ci_enable_uvd_dpm(adev, (int )((bool )(! ((int )gate != 0))));
  return (tmp___1);
}
}
static u8 ci_get_vce_boot_level(struct amdgpu_device *adev )
{
  u8 i ;
  u32 min_evclk ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  {
  min_evclk = 30000U;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  i = 0U;
  goto ldv_49781;
  ldv_49780: ;
  if ((table->entries + (unsigned long )i)->evclk >= min_evclk) {
    return (i);
  } else {
  }
  i = (u8 )((int )i + 1);
  ldv_49781: ;
  if ((int )table->count > (int )i) {
    goto ldv_49780;
  } else {
  }
  return ((unsigned int )table->count + 255U);
}
}
static int ci_update_vce_dpm(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_new_state ,
                             struct amdgpu_ps *amdgpu_current_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  u32 tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = 0;
  if (amdgpu_current_state->evclk != amdgpu_new_state->evclk) {
    if (amdgpu_new_state->evclk != 0U) {
      ret = amdgpu_set_clockgating_state(adev, 8, 1);
      if (ret != 0) {
        return (ret);
      } else {
      }
      pi->smc_state_table.VceBootLevel = ci_get_vce_boot_level(adev);
      tmp___0 = (*(adev->smc_rreg))(adev, 259944U);
      tmp___0 = tmp___0 & 4278255615U;
      tmp___0 = (u32 )((int )pi->smc_state_table.VceBootLevel << 16) | tmp___0;
      (*(adev->smc_wreg))(adev, 259944U, tmp___0);
      ret = ci_enable_vce_dpm(adev, 1);
    } else {
      ret = amdgpu_set_clockgating_state(adev, 8, 0);
      if (ret != 0) {
        return (ret);
      } else {
      }
      ret = ci_enable_vce_dpm(adev, 0);
    }
  } else {
  }
  return (ret);
}
}
static int ci_generate_dpm_level_enable_mask(struct amdgpu_device *adev , struct amdgpu_ps *amdgpu_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ret = ci_trim_dpm_states(adev, amdgpu_state);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->dpm_level_enable_mask.sclk_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& pi->dpm_table.sclk_table);
  pi->dpm_level_enable_mask.mclk_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& pi->dpm_table.mclk_table);
  pi->last_mclk_dpm_enable_mask = pi->dpm_level_enable_mask.mclk_dpm_enable_mask;
  if ((int )pi->uvd_enabled) {
    if ((int )pi->dpm_level_enable_mask.mclk_dpm_enable_mask & 1) {
      pi->dpm_level_enable_mask.mclk_dpm_enable_mask = pi->dpm_level_enable_mask.mclk_dpm_enable_mask & 4294967294U;
    } else {
    }
  } else {
  }
  pi->dpm_level_enable_mask.pcie_dpm_enable_mask = ci_get_dpm_level_enable_mask_value(& pi->dpm_table.pcie_speed_table);
  return (0);
}
}
static u32 ci_get_lowest_enabled_level(struct amdgpu_device *adev , u32 level_mask )
{
  u32 level ;
  {
  level = 0U;
  goto ldv_49803;
  ldv_49802:
  level = level + 1U;
  ldv_49803: ;
  if (((u32 )(1 << (int )level) & level_mask) == 0U) {
    goto ldv_49802;
  } else {
  }
  return (level);
}
}
static int ci_dpm_force_performance_level(struct amdgpu_device *adev , enum amdgpu_dpm_forced_level level )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 tmp___0 ;
  u32 levels ;
  u32 i ;
  int ret ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  PPSMC_Result smc_result ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((unsigned int )level == 2U) {
    if (pi->pcie_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.pcie_dpm_enable_mask != 0U) {
      levels = 0U;
      tmp___0 = pi->dpm_level_enable_mask.pcie_dpm_enable_mask;
      goto ldv_49815;
      ldv_49814:
      levels = levels + 1U;
      ldv_49815:
      tmp___0 = tmp___0 >> 1;
      if (tmp___0 != 0U) {
        goto ldv_49814;
      } else {
      }
      if (levels != 0U) {
        ret = ci_dpm_force_state_pcie(adev, (u32 )level);
        if (ret != 0) {
          return (ret);
        } else {
        }
        i = 0U;
        goto ldv_49819;
        ldv_49818:
        tmp___1 = (*(adev->smc_rreg))(adev, 3223322864U);
        tmp___0 = (tmp___1 & 251658240U) >> 24;
        if (tmp___0 == levels) {
          goto ldv_49817;
        } else {
        }
        __const_udelay(4295UL);
        i = i + 1U;
        ldv_49819: ;
        if ((u32 )adev->usec_timeout > i) {
          goto ldv_49818;
        } else {
        }
        ldv_49817: ;
      } else {
      }
    } else {
    }
    if (pi->sclk_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.sclk_dpm_enable_mask != 0U) {
      levels = 0U;
      tmp___0 = pi->dpm_level_enable_mask.sclk_dpm_enable_mask;
      goto ldv_49821;
      ldv_49820:
      levels = levels + 1U;
      ldv_49821:
      tmp___0 = tmp___0 >> 1;
      if (tmp___0 != 0U) {
        goto ldv_49820;
      } else {
      }
      if (levels != 0U) {
        ret = ci_dpm_force_state_sclk(adev, levels);
        if (ret != 0) {
          return (ret);
        } else {
        }
        i = 0U;
        goto ldv_49825;
        ldv_49824:
        tmp___2 = (*(adev->smc_rreg))(adev, 3223322644U);
        tmp___0 = (tmp___2 & 2031616U) >> 16;
        if (tmp___0 == levels) {
          goto ldv_49823;
        } else {
        }
        __const_udelay(4295UL);
        i = i + 1U;
        ldv_49825: ;
        if ((u32 )adev->usec_timeout > i) {
          goto ldv_49824;
        } else {
        }
        ldv_49823: ;
      } else {
      }
    } else {
    }
    if (pi->mclk_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.mclk_dpm_enable_mask != 0U) {
      levels = 0U;
      tmp___0 = pi->dpm_level_enable_mask.mclk_dpm_enable_mask;
      goto ldv_49827;
      ldv_49826:
      levels = levels + 1U;
      ldv_49827:
      tmp___0 = tmp___0 >> 1;
      if (tmp___0 != 0U) {
        goto ldv_49826;
      } else {
      }
      if (levels != 0U) {
        ret = ci_dpm_force_state_mclk(adev, levels);
        if (ret != 0) {
          return (ret);
        } else {
        }
        i = 0U;
        goto ldv_49831;
        ldv_49830:
        tmp___3 = (*(adev->smc_rreg))(adev, 3223322644U);
        tmp___0 = (tmp___3 & 3840U) >> 8;
        if (tmp___0 == levels) {
          goto ldv_49829;
        } else {
        }
        __const_udelay(4295UL);
        i = i + 1U;
        ldv_49831: ;
        if ((u32 )adev->usec_timeout > i) {
          goto ldv_49830;
        } else {
        }
        ldv_49829: ;
      } else {
      }
    } else {
    }
    if (pi->pcie_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.pcie_dpm_enable_mask != 0U) {
      levels = 0U;
      tmp___0 = pi->dpm_level_enable_mask.pcie_dpm_enable_mask;
      goto ldv_49833;
      ldv_49832:
      levels = levels + 1U;
      ldv_49833:
      tmp___0 = tmp___0 >> 1;
      if (tmp___0 != 0U) {
        goto ldv_49832;
      } else {
      }
      if (levels != 0U) {
        ret = ci_dpm_force_state_pcie(adev, (u32 )level);
        if (ret != 0) {
          return (ret);
        } else {
        }
        i = 0U;
        goto ldv_49837;
        ldv_49836:
        tmp___4 = (*(adev->smc_rreg))(adev, 3223322864U);
        tmp___0 = (tmp___4 & 251658240U) >> 24;
        if (tmp___0 == levels) {
          goto ldv_49835;
        } else {
        }
        __const_udelay(4295UL);
        i = i + 1U;
        ldv_49837: ;
        if ((u32 )adev->usec_timeout > i) {
          goto ldv_49836;
        } else {
        }
        ldv_49835: ;
      } else {
      }
    } else {
    }
  } else
  if ((unsigned int )level == 1U) {
    if (pi->sclk_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.sclk_dpm_enable_mask != 0U) {
      levels = ci_get_lowest_enabled_level(adev, pi->dpm_level_enable_mask.sclk_dpm_enable_mask);
      ret = ci_dpm_force_state_sclk(adev, levels);
      if (ret != 0) {
        return (ret);
      } else {
      }
      i = 0U;
      goto ldv_49840;
      ldv_49839:
      tmp___5 = (*(adev->smc_rreg))(adev, 3223322644U);
      tmp___0 = (tmp___5 & 2031616U) >> 16;
      if (tmp___0 == levels) {
        goto ldv_49838;
      } else {
      }
      __const_udelay(4295UL);
      i = i + 1U;
      ldv_49840: ;
      if ((u32 )adev->usec_timeout > i) {
        goto ldv_49839;
      } else {
      }
      ldv_49838: ;
    } else {
    }
    if (pi->mclk_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.mclk_dpm_enable_mask != 0U) {
      levels = ci_get_lowest_enabled_level(adev, pi->dpm_level_enable_mask.mclk_dpm_enable_mask);
      ret = ci_dpm_force_state_mclk(adev, levels);
      if (ret != 0) {
        return (ret);
      } else {
      }
      i = 0U;
      goto ldv_49843;
      ldv_49842:
      tmp___6 = (*(adev->smc_rreg))(adev, 3223322644U);
      tmp___0 = (tmp___6 & 3840U) >> 8;
      if (tmp___0 == levels) {
        goto ldv_49841;
      } else {
      }
      __const_udelay(4295UL);
      i = i + 1U;
      ldv_49843: ;
      if ((u32 )adev->usec_timeout > i) {
        goto ldv_49842;
      } else {
      }
      ldv_49841: ;
    } else {
    }
    if (pi->pcie_dpm_key_disabled == 0U && pi->dpm_level_enable_mask.pcie_dpm_enable_mask != 0U) {
      levels = ci_get_lowest_enabled_level(adev, pi->dpm_level_enable_mask.pcie_dpm_enable_mask);
      ret = ci_dpm_force_state_pcie(adev, levels);
      if (ret != 0) {
        return (ret);
      } else {
      }
      i = 0U;
      goto ldv_49846;
      ldv_49845:
      tmp___7 = (*(adev->smc_rreg))(adev, 3223322864U);
      tmp___0 = (tmp___7 & 251658240U) >> 24;
      if (tmp___0 == levels) {
        goto ldv_49844;
      } else {
      }
      __const_udelay(4295UL);
      i = i + 1U;
      ldv_49846: ;
      if ((u32 )adev->usec_timeout > i) {
        goto ldv_49845;
      } else {
      }
      ldv_49844: ;
    } else {
    }
  } else
  if ((unsigned int )level == 0U) {
    if (pi->pcie_dpm_key_disabled == 0U) {
      smc_result = amdgpu_ci_send_msg_to_smc(adev, 328);
      if ((unsigned int )smc_result != 1U) {
        return (-22);
      } else {
      }
    } else {
    }
    ret = ci_upload_dpm_level_enable_mask(adev);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  adev->pm.dpm.forced_level = level;
  return (0);
}
}
static int ci_set_mc_special_registers(struct amdgpu_device *adev , struct ci_mc_reg_table *table )
{
  u8 i ;
  u8 j ;
  u8 k ;
  u32 temp_reg ;
  {
  i = 0U;
  j = table->last;
  goto ldv_49873;
  ldv_49872: ;
  if ((unsigned int )j > 15U) {
    return (-22);
  } else {
  }
  switch ((int )table->mc_reg_address[(int )i].s1) {
  case 2689:
  temp_reg = amdgpu_mm_rreg(adev, 2691U, 0);
  table->mc_reg_address[(int )j].s1 = 2691U;
  table->mc_reg_address[(int )j].s0 = 2721U;
  k = 0U;
  goto ldv_49858;
  ldv_49857:
  table->mc_reg_table_entry[(int )k].mc_data[(int )j] = (temp_reg & 4294901760U) | (table->mc_reg_table_entry[(int )k].mc_data[(int )i] >> 16);
  k = (u8 )((int )k + 1);
  ldv_49858: ;
  if ((int )table->num_entries > (int )k) {
    goto ldv_49857;
  } else {
  }
  j = (u8 )((int )j + 1);
  if ((unsigned int )j > 15U) {
    return (-22);
  } else {
  }
  temp_reg = amdgpu_mm_rreg(adev, 2731U, 0);
  table->mc_reg_address[(int )j].s1 = 2731U;
  table->mc_reg_address[(int )j].s0 = 2722U;
  k = 0U;
  goto ldv_49861;
  ldv_49860:
  table->mc_reg_table_entry[(int )k].mc_data[(int )j] = (temp_reg & 4294901760U) | (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 65535U);
  if (adev->mc.vram_type != 5U) {
    table->mc_reg_table_entry[(int )k].mc_data[(int )j] = table->mc_reg_table_entry[(int )k].mc_data[(int )j] | 256U;
  } else {
  }
  k = (u8 )((int )k + 1);
  ldv_49861: ;
  if ((int )table->num_entries > (int )k) {
    goto ldv_49860;
  } else {
  }
  j = (u8 )((int )j + 1);
  if ((unsigned int )j > 16U) {
    return (-22);
  } else {
  }
  if (adev->mc.vram_type != 5U) {
    table->mc_reg_address[(int )j].s1 = 2612U;
    table->mc_reg_address[(int )j].s0 = 2612U;
    k = 0U;
    goto ldv_49864;
    ldv_49863:
    table->mc_reg_table_entry[(int )k].mc_data[(int )j] = table->mc_reg_table_entry[(int )k].mc_data[(int )i] >> 16;
    k = (u8 )((int )k + 1);
    ldv_49864: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49863;
    } else {
    }
    j = (u8 )((int )j + 1);
    if ((unsigned int )j > 16U) {
      return (-22);
    } else {
    }
  } else {
  }
  goto ldv_49866;
  case 2690:
  temp_reg = amdgpu_mm_rreg(adev, 2769U, 0);
  table->mc_reg_address[(int )j].s1 = 2769U;
  table->mc_reg_address[(int )j].s0 = 2770U;
  k = 0U;
  goto ldv_49869;
  ldv_49868:
  table->mc_reg_table_entry[(int )k].mc_data[(int )j] = (temp_reg & 4294901760U) | (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 65535U);
  k = (u8 )((int )k + 1);
  ldv_49869: ;
  if ((int )table->num_entries > (int )k) {
    goto ldv_49868;
  } else {
  }
  j = (u8 )((int )j + 1);
  if ((unsigned int )j > 16U) {
    return (-22);
  } else {
  }
  goto ldv_49866;
  default: ;
  goto ldv_49866;
  }
  ldv_49866:
  i = (u8 )((int )i + 1);
  ldv_49873: ;
  if ((int )table->last > (int )i) {
    goto ldv_49872;
  } else {
  }
  table->last = j;
  return (0);
}
}
static bool ci_check_s0_mc_reg_index(u16 in_reg , u16 *out_reg )
{
  bool result ;
  {
  result = 1;
  switch ((int )in_reg) {
  case 2600:
  *out_reg = 2715U;
  goto ldv_49881;
  case 3470:
  *out_reg = 3471U;
  goto ldv_49881;
  case 3459:
  *out_reg = 3460U;
  goto ldv_49881;
  case 3461:
  *out_reg = 3462U;
  goto ldv_49881;
  case 3457:
  *out_reg = 3458U;
  goto ldv_49881;
  case 2601:
  *out_reg = 2716U;
  goto ldv_49881;
  case 2602:
  *out_reg = 2717U;
  goto ldv_49881;
  case 2603:
  *out_reg = 2718U;
  goto ldv_49881;
  case 3468:
  *out_reg = 3469U;
  goto ldv_49881;
  case 3466:
  *out_reg = 3467U;
  goto ldv_49881;
  case 2605:
  *out_reg = 2759U;
  goto ldv_49881;
  case 2606:
  *out_reg = 2760U;
  goto ldv_49881;
  case 2607:
  *out_reg = 2719U;
  goto ldv_49881;
  case 2608:
  *out_reg = 2720U;
  goto ldv_49881;
  case 2691:
  *out_reg = 2721U;
  goto ldv_49881;
  case 2731:
  *out_reg = 2722U;
  goto ldv_49881;
  case 2769:
  *out_reg = 2770U;
  goto ldv_49881;
  case 2604:
  *out_reg = 2771U;
  goto ldv_49881;
  case 2775:
  *out_reg = 2776U;
  goto ldv_49881;
  case 2773:
  *out_reg = 2774U;
  goto ldv_49881;
  default:
  result = 0;
  goto ldv_49881;
  }
  ldv_49881: ;
  return (result);
}
}
static void ci_set_valid_flag(struct ci_mc_reg_table *table )
{
  u8 i ;
  u8 j ;
  {
  i = 0U;
  goto ldv_49911;
  ldv_49910:
  j = 1U;
  goto ldv_49909;
  ldv_49908: ;
  if (table->mc_reg_table_entry[(int )j + -1].mc_data[(int )i] != table->mc_reg_table_entry[(int )j].mc_data[(int )i]) {
    table->valid_flag = (u16 )((int )((short )table->valid_flag) | (int )((short )(1 << (int )i)));
    goto ldv_49907;
  } else {
  }
  j = (u8 )((int )j + 1);
  ldv_49909: ;
  if ((int )table->num_entries > (int )j) {
    goto ldv_49908;
  } else {
  }
  ldv_49907:
  i = (u8 )((int )i + 1);
  ldv_49911: ;
  if ((int )table->last > (int )i) {
    goto ldv_49910;
  } else {
  }
  return;
}
}
static void ci_set_s0_mc_reg_index(struct ci_mc_reg_table *table )
{
  u32 i ;
  u16 address ;
  bool tmp ;
  {
  i = 0U;
  goto ldv_49919;
  ldv_49918:
  tmp = ci_check_s0_mc_reg_index((int )table->mc_reg_address[i].s1, & address);
  table->mc_reg_address[i].s0 = (int )tmp ? address : table->mc_reg_address[i].s1;
  i = i + 1U;
  ldv_49919: ;
  if ((u32 )table->last > i) {
    goto ldv_49918;
  } else {
  }
  return;
}
}
static int ci_copy_vbios_mc_reg_table(struct atom_mc_reg_table const *table , struct ci_mc_reg_table *ci_table )
{
  u8 i ;
  u8 j ;
  {
  if ((unsigned int )((unsigned char )table->last) > 16U) {
    return (-22);
  } else {
  }
  if ((unsigned int )((unsigned char )table->num_entries) > 16U) {
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_49928;
  ldv_49927:
  ci_table->mc_reg_address[(int )i].s1 = table->mc_reg_address[(int )i].s1;
  i = (u8 )((int )i + 1);
  ldv_49928: ;
  if ((int )((unsigned char )table->last) > (int )i) {
    goto ldv_49927;
  } else {
  }
  ci_table->last = table->last;
  i = 0U;
  goto ldv_49934;
  ldv_49933:
  ci_table->mc_reg_table_entry[(int )i].mclk_max = table->mc_reg_table_entry[(int )i].mclk_max;
  j = 0U;
  goto ldv_49931;
  ldv_49930:
  ci_table->mc_reg_table_entry[(int )i].mc_data[(int )j] = table->mc_reg_table_entry[(int )i].mc_data[(int )j];
  j = (u8 )((int )j + 1);
  ldv_49931: ;
  if ((int )((unsigned char )table->last) > (int )j) {
    goto ldv_49930;
  } else {
  }
  i = (u8 )((int )i + 1);
  ldv_49934: ;
  if ((int )((unsigned char )table->num_entries) > (int )i) {
    goto ldv_49933;
  } else {
  }
  ci_table->num_entries = table->num_entries;
  return (0);
}
}
static int ci_register_patching_mc_seq(struct amdgpu_device *adev , struct ci_mc_reg_table *table )
{
  u8 i ;
  u8 k ;
  u32 tmp ;
  bool patch ;
  {
  tmp = amdgpu_mm_rreg(adev, 2688U, 0);
  patch = (tmp & 3840U) == 768U;
  if ((int )patch && ((unsigned int )(adev->pdev)->device == 26544U || (unsigned int )(adev->pdev)->device == 26545U)) {
    i = 0U;
    goto ldv_49971;
    ldv_49970: ;
    if ((unsigned int )table->last > 15U) {
      return (-22);
    } else {
    }
    switch ((int )table->mc_reg_address[(int )i].s1) {
    case 2689:
    k = 0U;
    goto ldv_49946;
    ldv_49945: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U || table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = table->mc_reg_table_entry[(int )k].mc_data[(int )i] | 7U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49946: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49945;
    } else {
    }
    goto ldv_49948;
    case 2607:
    k = 0U;
    goto ldv_49951;
    ldv_49950: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U || table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4294905600U) | 53469U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49951: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49950;
    } else {
    }
    goto ldv_49948;
    case 2608:
    k = 0U;
    goto ldv_49955;
    ldv_49954: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U || table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4294905600U) | 53469U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49955: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49954;
    } else {
    }
    goto ldv_49948;
    case 2773:
    k = 0U;
    goto ldv_49959;
    ldv_49958: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U || table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = 0U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49959: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49958;
    } else {
    }
    goto ldv_49948;
    case 2601:
    k = 0U;
    goto ldv_49963;
    ldv_49962: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4292935183U) | 786752U;
    } else
    if (table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4292935183U) | 786768U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49963: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49962;
    } else {
    }
    goto ldv_49948;
    case 2602:
    k = 0U;
    goto ldv_49967;
    ldv_49966: ;
    if (table->mc_reg_table_entry[(int )k].mclk_max == 125000U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4294967232U) | 48U;
    } else
    if (table->mc_reg_table_entry[(int )k].mclk_max == 137500U) {
      table->mc_reg_table_entry[(int )k].mc_data[(int )i] = (table->mc_reg_table_entry[(int )k].mc_data[(int )i] & 4294967232U) | 53U;
    } else {
    }
    k = (u8 )((int )k + 1);
    ldv_49967: ;
    if ((int )table->num_entries > (int )k) {
      goto ldv_49966;
    } else {
    }
    goto ldv_49948;
    default: ;
    goto ldv_49948;
    }
    ldv_49948:
    i = (u8 )((int )i + 1);
    ldv_49971: ;
    if ((int )table->last > (int )i) {
      goto ldv_49970;
    } else {
    }
    amdgpu_mm_wreg(adev, 2705U, 3U, 0);
    tmp = amdgpu_mm_rreg(adev, 2706U, 0);
    tmp = (tmp & 4294508543U) | 65536U;
    amdgpu_mm_wreg(adev, 2705U, 3U, 0);
    amdgpu_mm_wreg(adev, 2706U, tmp, 0);
  } else {
  }
  return (0);
}
}
static int ci_initialize_mc_reg_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct atom_mc_reg_table *table ;
  struct ci_mc_reg_table *ci_table ;
  u8 module_index ;
  u8 tmp___0 ;
  int ret ;
  void *tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  ci_table = & pi->mc_reg_table;
  tmp___0 = ci_get_memory_module_index(adev);
  module_index = tmp___0;
  tmp___1 = kzalloc(2772UL, 208U);
  table = (struct atom_mc_reg_table *)tmp___1;
  if ((unsigned long )table == (unsigned long )((struct atom_mc_reg_table *)0)) {
    return (-12);
  } else {
  }
  tmp___2 = amdgpu_mm_rreg(adev, 2600U, 0);
  amdgpu_mm_wreg(adev, 2715U, tmp___2, 0);
  tmp___3 = amdgpu_mm_rreg(adev, 2601U, 0);
  amdgpu_mm_wreg(adev, 2716U, tmp___3, 0);
  tmp___4 = amdgpu_mm_rreg(adev, 3470U, 0);
  amdgpu_mm_wreg(adev, 3471U, tmp___4, 0);
  tmp___5 = amdgpu_mm_rreg(adev, 3459U, 0);
  amdgpu_mm_wreg(adev, 3460U, tmp___5, 0);
  tmp___6 = amdgpu_mm_rreg(adev, 3461U, 0);
  amdgpu_mm_wreg(adev, 3462U, tmp___6, 0);
  tmp___7 = amdgpu_mm_rreg(adev, 3457U, 0);
  amdgpu_mm_wreg(adev, 3458U, tmp___7, 0);
  tmp___8 = amdgpu_mm_rreg(adev, 3468U, 0);
  amdgpu_mm_wreg(adev, 3469U, tmp___8, 0);
  tmp___9 = amdgpu_mm_rreg(adev, 3466U, 0);
  amdgpu_mm_wreg(adev, 3467U, tmp___9, 0);
  tmp___10 = amdgpu_mm_rreg(adev, 2602U, 0);
  amdgpu_mm_wreg(adev, 2717U, tmp___10, 0);
  tmp___11 = amdgpu_mm_rreg(adev, 2603U, 0);
  amdgpu_mm_wreg(adev, 2718U, tmp___11, 0);
  tmp___12 = amdgpu_mm_rreg(adev, 2691U, 0);
  amdgpu_mm_wreg(adev, 2721U, tmp___12, 0);
  tmp___13 = amdgpu_mm_rreg(adev, 2731U, 0);
  amdgpu_mm_wreg(adev, 2722U, tmp___13, 0);
  tmp___14 = amdgpu_mm_rreg(adev, 2769U, 0);
  amdgpu_mm_wreg(adev, 2770U, tmp___14, 0);
  tmp___15 = amdgpu_mm_rreg(adev, 2607U, 0);
  amdgpu_mm_wreg(adev, 2719U, tmp___15, 0);
  tmp___16 = amdgpu_mm_rreg(adev, 2608U, 0);
  amdgpu_mm_wreg(adev, 2720U, tmp___16, 0);
  tmp___17 = amdgpu_mm_rreg(adev, 2605U, 0);
  amdgpu_mm_wreg(adev, 2759U, tmp___17, 0);
  tmp___18 = amdgpu_mm_rreg(adev, 2606U, 0);
  amdgpu_mm_wreg(adev, 2760U, tmp___18, 0);
  tmp___19 = amdgpu_mm_rreg(adev, 2604U, 0);
  amdgpu_mm_wreg(adev, 2771U, tmp___19, 0);
  tmp___20 = amdgpu_mm_rreg(adev, 2775U, 0);
  amdgpu_mm_wreg(adev, 2776U, tmp___20, 0);
  tmp___21 = amdgpu_mm_rreg(adev, 2773U, 0);
  amdgpu_mm_wreg(adev, 2774U, tmp___21, 0);
  ret = amdgpu_atombios_init_mc_reg_table(adev, (int )module_index, table);
  if (ret != 0) {
    goto init_mc_done;
  } else {
  }
  ret = ci_copy_vbios_mc_reg_table((struct atom_mc_reg_table const *)table, ci_table);
  if (ret != 0) {
    goto init_mc_done;
  } else {
  }
  ci_set_s0_mc_reg_index(ci_table);
  ret = ci_register_patching_mc_seq(adev, ci_table);
  if (ret != 0) {
    goto init_mc_done;
  } else {
  }
  ret = ci_set_mc_special_registers(adev, ci_table);
  if (ret != 0) {
    goto init_mc_done;
  } else {
  }
  ci_set_valid_flag(ci_table);
  init_mc_done:
  kfree((void const *)table);
  return (ret);
}
}
static int ci_populate_mc_reg_addresses(struct amdgpu_device *adev , SMU7_Discrete_MCRegisters *mc_reg_table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 i ;
  u32 j ;
  __u16 tmp___0 ;
  __u16 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  i = 0U;
  j = 0U;
  goto ldv_49990;
  ldv_49989: ;
  if (((int )pi->mc_reg_table.valid_flag >> (int )j) & 1) {
    if (i > 15U) {
      return (-22);
    } else {
    }
    tmp___0 = __fswab16((int )pi->mc_reg_table.mc_reg_address[j].s0);
    mc_reg_table->address[i].s0 = tmp___0;
    tmp___1 = __fswab16((int )pi->mc_reg_table.mc_reg_address[j].s1);
    mc_reg_table->address[i].s1 = tmp___1;
    i = i + 1U;
  } else {
  }
  j = j + 1U;
  ldv_49990: ;
  if ((u32 )pi->mc_reg_table.last > j) {
    goto ldv_49989;
  } else {
  }
  mc_reg_table->last = (unsigned char )i;
  return (0);
}
}
static void ci_convert_mc_registers(struct ci_mc_reg_entry const *entry , SMU7_Discrete_MCRegisterSet *data ,
                                    u32 num_entries , u32 valid_flag )
{
  u32 i ;
  u32 j ;
  __u32 tmp ;
  {
  i = 0U;
  j = 0U;
  goto ldv_50001;
  ldv_50000: ;
  if (((u32 )(1 << (int )j) & valid_flag) != 0U) {
    tmp = __fswab32(entry->mc_data[j]);
    data->value[i] = tmp;
    i = i + 1U;
  } else {
  }
  j = j + 1U;
  ldv_50001: ;
  if (j < num_entries) {
    goto ldv_50000;
  } else {
  }
  return;
}
}
static void ci_convert_mc_reg_table_entry_to_smc(struct amdgpu_device *adev , u32 const memory_clock ,
                                                 SMU7_Discrete_MCRegisterSet *mc_reg_table_data )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  i = 0U;
  i = 0U;
  goto ldv_50012;
  ldv_50011: ;
  if (pi->mc_reg_table.mc_reg_table_entry[i].mclk_max >= (unsigned int )memory_clock) {
    goto ldv_50010;
  } else {
  }
  i = i + 1U;
  ldv_50012: ;
  if ((u32 )pi->mc_reg_table.num_entries > i) {
    goto ldv_50011;
  } else {
  }
  ldv_50010: ;
  if ((u32 )pi->mc_reg_table.num_entries == i && i != 0U) {
    i = i - 1U;
  } else {
  }
  ci_convert_mc_registers((struct ci_mc_reg_entry const *)(& pi->mc_reg_table.mc_reg_table_entry) + (unsigned long )i,
                          mc_reg_table_data, (u32 )pi->mc_reg_table.last, (u32 )pi->mc_reg_table.valid_flag);
  return;
}
}
static void ci_convert_mc_reg_table_to_smc(struct amdgpu_device *adev , SMU7_Discrete_MCRegisters *mc_reg_table )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  u32 i ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  i = 0U;
  goto ldv_50020;
  ldv_50019:
  ci_convert_mc_reg_table_entry_to_smc(adev, pi->dpm_table.mclk_table.dpm_levels[i].value,
                                       (SMU7_Discrete_MCRegisterSet *)(& mc_reg_table->data) + (unsigned long )i);
  i = i + 1U;
  ldv_50020: ;
  if (pi->dpm_table.mclk_table.count > i) {
    goto ldv_50019;
  } else {
  }
  return;
}
}
static int ci_populate_initial_mc_reg_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int ret ;
  int tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  memset((void *)(& pi->smc_mc_reg_table), 0, 452UL);
  ret = ci_populate_mc_reg_addresses(adev, & pi->smc_mc_reg_table);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ci_convert_mc_reg_table_to_smc(adev, & pi->smc_mc_reg_table);
  tmp___0 = amdgpu_ci_copy_bytes_to_smc(adev, pi->mc_reg_table_start, (u8 const *)(& pi->smc_mc_reg_table),
                                        452U, pi->sram_end);
  return (tmp___0);
}
}
static int ci_update_and_upload_mc_reg_table(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  int tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  if ((pi->need_update_smu7_dpm_table & 2U) == 0U) {
    return (0);
  } else {
  }
  memset((void *)(& pi->smc_mc_reg_table), 0, 452UL);
  ci_convert_mc_reg_table_to_smc(adev, & pi->smc_mc_reg_table);
  tmp___0 = amdgpu_ci_copy_bytes_to_smc(adev, pi->mc_reg_table_start + 68U, (u8 const *)(& pi->smc_mc_reg_table.data),
                                        pi->dpm_table.mclk_table.count * 64U, pi->sram_end);
  return (tmp___0);
}
}
static void ci_enable_voltage_control(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = (*(adev->smc_rreg))(adev, 3223322624U);
  tmp = tmp___0;
  tmp = tmp | 1024U;
  (*(adev->smc_wreg))(adev, 3223322624U, tmp);
  return;
}
}
static enum amdgpu_pcie_gen ci_get_maximum_link_speed(struct amdgpu_device *adev ,
                                                      struct amdgpu_ps *amdgpu_state )
{
  struct ci_ps *state ;
  struct ci_ps *tmp ;
  int i ;
  u16 pcie_speed ;
  u16 max_speed ;
  {
  tmp = ci_get_ps(amdgpu_state);
  state = tmp;
  max_speed = 0U;
  i = 0;
  goto ldv_50044;
  ldv_50043:
  pcie_speed = (u16 )state->performance_levels[i].pcie_gen;
  if ((int )max_speed < (int )pcie_speed) {
    max_speed = pcie_speed;
  } else {
  }
  i = i + 1;
  ldv_50044: ;
  if ((int )state->performance_level_count > i) {
    goto ldv_50043;
  } else {
  }
  return ((enum amdgpu_pcie_gen )max_speed);
}
}
static u16 ci_get_current_pcie_speed(struct amdgpu_device *adev )
{
  u32 speed_cntl ;
  u32 tmp ;
  {
  speed_cntl = 0U;
  tmp = (*(adev->pcie_rreg))(adev, 268501156U);
  speed_cntl = tmp & 24576U;
  speed_cntl = speed_cntl >> 13;
  return ((u16 )speed_cntl);
}
}
static int ci_get_current_pcie_lane_number(struct amdgpu_device *adev )
{
  u32 link_width ;
  u32 tmp ;
  {
  link_width = 0U;
  tmp = (*(adev->pcie_rreg))(adev, 268501154U);
  link_width = tmp & 112U;
  link_width = link_width >> 4;
  switch (link_width) {
  case 1U: ;
  return (1);
  case 2U: ;
  return (2);
  case 3U: ;
  return (4);
  case 4U: ;
  return (8);
  case 0U: ;
  case 6U: ;
  default: ;
  return (16);
  }
}
}
static void ci_request_link_speed_change_before_state_change(struct amdgpu_device *adev ,
                                                             struct amdgpu_ps *amdgpu_new_state ,
                                                             struct amdgpu_ps *amdgpu_current_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  enum amdgpu_pcie_gen target_link_speed ;
  enum amdgpu_pcie_gen tmp___0 ;
  enum amdgpu_pcie_gen current_link_speed ;
  int tmp___1 ;
  int tmp___2 ;
  u16 tmp___3 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_maximum_link_speed(adev, amdgpu_new_state);
  target_link_speed = tmp___0;
  if ((unsigned int )pi->force_pcie_gen == 65535U) {
    current_link_speed = ci_get_maximum_link_speed(adev, amdgpu_current_state);
  } else {
    current_link_speed = pi->force_pcie_gen;
  }
  pi->force_pcie_gen = 65535;
  pi->pspp_notify_required = 0;
  if ((unsigned int )target_link_speed > (unsigned int )current_link_speed) {
    switch ((unsigned int )target_link_speed) {
    case 2U:
    tmp___1 = amdgpu_acpi_pcie_performance_request(adev, 4, 0);
    if (tmp___1 == 0) {
      goto ldv_50070;
    } else {
    }
    pi->force_pcie_gen = 1;
    if ((unsigned int )current_link_speed == 1U) {
      goto ldv_50070;
    } else {
    }
    case 1U:
    tmp___2 = amdgpu_acpi_pcie_performance_request(adev, 3, 0);
    if (tmp___2 == 0) {
      goto ldv_50070;
    } else {
    }
    default:
    tmp___3 = ci_get_current_pcie_speed(adev);
    pi->force_pcie_gen = (enum amdgpu_pcie_gen )tmp___3;
    goto ldv_50070;
    }
    ldv_50070: ;
  } else
  if ((unsigned int )target_link_speed < (unsigned int )current_link_speed) {
    pi->pspp_notify_required = 1;
  } else {
  }
  return;
}
}
static void ci_notify_link_speed_change_after_state_change(struct amdgpu_device *adev ,
                                                           struct amdgpu_ps *amdgpu_new_state ,
                                                           struct amdgpu_ps *amdgpu_current_state )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  enum amdgpu_pcie_gen target_link_speed ;
  enum amdgpu_pcie_gen tmp___0 ;
  u8 request ;
  u16 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_maximum_link_speed(adev, amdgpu_new_state);
  target_link_speed = tmp___0;
  if ((int )pi->pspp_notify_required) {
    if ((unsigned int )target_link_speed == 2U) {
      request = 4U;
    } else
    if ((unsigned int )target_link_speed == 1U) {
      request = 3U;
    } else {
      request = 2U;
    }
    if ((unsigned int )request == 2U) {
      tmp___1 = ci_get_current_pcie_speed(adev);
      if ((unsigned int )tmp___1 != 0U) {
        return;
      } else {
      }
    } else {
    }
    amdgpu_acpi_pcie_performance_request(adev, (int )request, 0);
  } else {
  }
  return;
}
}
static int ci_set_private_data_variables_based_on_pptable(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *allowed_sclk_vddc_table ;
  struct amdgpu_clock_voltage_dependency_table *allowed_mclk_vddc_table ;
  struct amdgpu_clock_voltage_dependency_table *allowed_mclk_vddci_table ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  allowed_sclk_vddc_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  allowed_mclk_vddc_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_mclk;
  allowed_mclk_vddci_table = & adev->pm.dpm.dyn_state.vddci_dependency_on_mclk;
  if ((unsigned long )allowed_sclk_vddc_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (allowed_sclk_vddc_table->count == 0U) {
    return (-22);
  } else {
  }
  if ((unsigned long )allowed_mclk_vddc_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (allowed_mclk_vddc_table->count == 0U) {
    return (-22);
  } else {
  }
  if ((unsigned long )allowed_mclk_vddci_table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    return (-22);
  } else {
  }
  if (allowed_mclk_vddci_table->count == 0U) {
    return (-22);
  } else {
  }
  pi->min_vddc_in_pp_table = (allowed_sclk_vddc_table->entries)->v;
  pi->max_vddc_in_pp_table = (allowed_sclk_vddc_table->entries + (unsigned long )(allowed_sclk_vddc_table->count - 1U))->v;
  pi->min_vddci_in_pp_table = (allowed_mclk_vddci_table->entries)->v;
  pi->max_vddci_in_pp_table = (allowed_mclk_vddci_table->entries + (unsigned long )(allowed_mclk_vddci_table->count - 1U))->v;
  adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.sclk = (allowed_sclk_vddc_table->entries + (unsigned long )(allowed_sclk_vddc_table->count - 1U))->clk;
  adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.mclk = (allowed_mclk_vddc_table->entries + (unsigned long )(allowed_sclk_vddc_table->count - 1U))->clk;
  adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.vddc = (allowed_sclk_vddc_table->entries + (unsigned long )(allowed_sclk_vddc_table->count - 1U))->v;
  adev->pm.dpm.dyn_state.max_clock_voltage_on_ac.vddci = (allowed_mclk_vddci_table->entries + (unsigned long )(allowed_mclk_vddci_table->count - 1U))->v;
  return (0);
}
}
static void ci_patch_with_vddc_leakage(struct amdgpu_device *adev , u16 *vddc )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_leakage_voltage *leakage_table ;
  u32 leakage_index ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  leakage_table = & pi->vddc_leakage;
  leakage_index = 0U;
  goto ldv_50097;
  ldv_50096: ;
  if ((int )leakage_table->leakage_id[leakage_index] == (int )*vddc) {
    *vddc = leakage_table->actual_voltage[leakage_index];
    goto ldv_50095;
  } else {
  }
  leakage_index = leakage_index + 1U;
  ldv_50097: ;
  if ((u32 )leakage_table->count > leakage_index) {
    goto ldv_50096;
  } else {
  }
  ldv_50095: ;
  return;
}
}
static void ci_patch_with_vddci_leakage(struct amdgpu_device *adev , u16 *vddci )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_leakage_voltage *leakage_table ;
  u32 leakage_index ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  leakage_table = & pi->vddci_leakage;
  leakage_index = 0U;
  goto ldv_50107;
  ldv_50106: ;
  if ((int )leakage_table->leakage_id[leakage_index] == (int )*vddci) {
    *vddci = leakage_table->actual_voltage[leakage_index];
    goto ldv_50105;
  } else {
  }
  leakage_index = leakage_index + 1U;
  ldv_50107: ;
  if ((u32 )leakage_table->count > leakage_index) {
    goto ldv_50106;
  } else {
  }
  ldv_50105: ;
  return;
}
}
static void ci_patch_clock_voltage_dependency_table_with_vddc_leakage(struct amdgpu_device *adev ,
                                                                      struct amdgpu_clock_voltage_dependency_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_50114;
    ldv_50113:
    ci_patch_with_vddc_leakage(adev, & (table->entries + (unsigned long )i)->v);
    i = i + 1U;
    ldv_50114: ;
    if (table->count > i) {
      goto ldv_50113;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_clock_voltage_dependency_table_with_vddci_leakage(struct amdgpu_device *adev ,
                                                                       struct amdgpu_clock_voltage_dependency_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_50122;
    ldv_50121:
    ci_patch_with_vddci_leakage(adev, & (table->entries + (unsigned long )i)->v);
    i = i + 1U;
    ldv_50122: ;
    if (table->count > i) {
      goto ldv_50121;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_vce_clock_voltage_dependency_table_with_vddc_leakage(struct amdgpu_device *adev ,
                                                                          struct amdgpu_vce_clock_voltage_dependency_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_vce_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_50130;
    ldv_50129:
    ci_patch_with_vddc_leakage(adev, & (table->entries + (unsigned long )i)->v);
    i = i + 1U;
    ldv_50130: ;
    if ((u32 )table->count > i) {
      goto ldv_50129;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_uvd_clock_voltage_dependency_table_with_vddc_leakage(struct amdgpu_device *adev ,
                                                                          struct amdgpu_uvd_clock_voltage_dependency_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_uvd_clock_voltage_dependency_table *)0)) {
    i = 0U;
    goto ldv_50138;
    ldv_50137:
    ci_patch_with_vddc_leakage(adev, & (table->entries + (unsigned long )i)->v);
    i = i + 1U;
    ldv_50138: ;
    if ((u32 )table->count > i) {
      goto ldv_50137;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_vddc_phase_shed_limit_table_with_vddc_leakage(struct amdgpu_device *adev ,
                                                                   struct amdgpu_phase_shedding_limits_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_phase_shedding_limits_table *)0)) {
    i = 0U;
    goto ldv_50146;
    ldv_50145:
    ci_patch_with_vddc_leakage(adev, & (table->entries + (unsigned long )i)->voltage);
    i = i + 1U;
    ldv_50146: ;
    if (table->count > i) {
      goto ldv_50145;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_clock_voltage_limits_with_vddc_leakage(struct amdgpu_device *adev ,
                                                            struct amdgpu_clock_and_voltage_limits *table )
{
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_clock_and_voltage_limits *)0)) {
    ci_patch_with_vddc_leakage(adev, & table->vddc);
    ci_patch_with_vddci_leakage(adev, & table->vddci);
  } else {
  }
  return;
}
}
static void ci_patch_cac_leakage_table_with_vddc_leakage(struct amdgpu_device *adev ,
                                                         struct amdgpu_cac_leakage_table *table )
{
  u32 i ;
  {
  if ((unsigned long )table != (unsigned long )((struct amdgpu_cac_leakage_table *)0)) {
    i = 0U;
    goto ldv_50158;
    ldv_50157:
    ci_patch_with_vddc_leakage(adev, & (table->entries + (unsigned long )i)->__annonCompField79.vddc);
    i = i + 1U;
    ldv_50158: ;
    if (table->count > i) {
      goto ldv_50157;
    } else {
    }
  } else {
  }
  return;
}
}
static void ci_patch_dependency_tables_with_leakage(struct amdgpu_device *adev )
{
  {
  ci_patch_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk);
  ci_patch_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_mclk);
  ci_patch_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk);
  ci_patch_clock_voltage_dependency_table_with_vddci_leakage(adev, & adev->pm.dpm.dyn_state.vddci_dependency_on_mclk);
  ci_patch_vce_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table);
  ci_patch_uvd_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table);
  ci_patch_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table);
  ci_patch_clock_voltage_dependency_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table);
  ci_patch_vddc_phase_shed_limit_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.phase_shedding_limits_table);
  ci_patch_clock_voltage_limits_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);
  ci_patch_clock_voltage_limits_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.max_clock_voltage_on_dc);
  ci_patch_cac_leakage_table_with_vddc_leakage(adev, & adev->pm.dpm.dyn_state.cac_leakage_table);
  return;
}
}
static void ci_update_current_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct ci_ps *new_ps ;
  struct ci_ps *tmp ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  {
  tmp = ci_get_ps(rps);
  new_ps = tmp;
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  pi->current_rps = *rps;
  pi->current_ps = *new_ps;
  pi->current_rps.ps_priv = (void *)(& pi->current_ps);
  return;
}
}
static void ci_update_requested_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct ci_ps *new_ps ;
  struct ci_ps *tmp ;
  struct ci_power_info *pi ;
  struct ci_power_info *tmp___0 ;
  {
  tmp = ci_get_ps(rps);
  new_ps = tmp;
  tmp___0 = ci_get_pi(adev);
  pi = tmp___0;
  pi->requested_rps = *rps;
  pi->requested_ps = *new_ps;
  pi->requested_rps.ps_priv = (void *)(& pi->requested_ps);
  return;
}
}
static int ci_dpm_pre_set_power_state(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps requested_ps ;
  struct amdgpu_ps *new_ps ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  requested_ps = *(adev->pm.dpm.requested_ps);
  new_ps = & requested_ps;
  ci_update_requested_ps(adev, new_ps);
  ci_apply_state_adjust_rules(adev, & pi->requested_rps);
  return (0);
}
}
static void ci_dpm_post_set_power_state(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps *new_ps ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  new_ps = & pi->requested_rps;
  ci_update_current_ps(adev, new_ps);
  return;
}
}
static void ci_dpm_setup_asic(struct amdgpu_device *adev )
{
  {
  ci_read_clock_registers(adev);
  ci_enable_acpi_power_management(adev);
  ci_init_sclk_t(adev);
  return;
}
}
static int ci_dpm_enable(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps *boot_ps ;
  int ret ;
  bool tmp___0 ;
  bool tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  boot_ps = adev->pm.dpm.boot_ps;
  tmp___0 = amdgpu_ci_is_smc_running(adev);
  if ((int )tmp___0) {
    return (-22);
  } else {
  }
  if (pi->voltage_control != 0U) {
    ci_enable_voltage_control(adev);
    ret = ci_construct_voltage_tables(adev);
    if (ret != 0) {
      drm_err("ci_construct_voltage_tables failed\n");
      return (ret);
    } else {
    }
  } else {
  }
  if ((int )pi->caps_dynamic_ac_timing) {
    ret = ci_initialize_mc_reg_table(adev);
    if (ret != 0) {
      pi->caps_dynamic_ac_timing = 0;
    } else {
    }
  } else {
  }
  if ((int )pi->dynamic_ss) {
    ci_enable_spread_spectrum(adev, 1);
  } else {
  }
  if ((int )pi->thermal_protection) {
    ci_enable_thermal_protection(adev, 1);
  } else {
  }
  ci_program_sstp(adev);
  ci_enable_display_gap(adev);
  ci_program_vc(adev);
  ret = ci_upload_firmware(adev);
  if (ret != 0) {
    drm_err("ci_upload_firmware failed\n");
    return (ret);
  } else {
  }
  ret = ci_process_firmware_header(adev);
  if (ret != 0) {
    drm_err("ci_process_firmware_header failed\n");
    return (ret);
  } else {
  }
  ret = ci_initial_switch_from_arb_f0_to_f1(adev);
  if (ret != 0) {
    drm_err("ci_initial_switch_from_arb_f0_to_f1 failed\n");
    return (ret);
  } else {
  }
  ret = ci_init_smc_table(adev);
  if (ret != 0) {
    drm_err("ci_init_smc_table failed\n");
    return (ret);
  } else {
  }
  ret = ci_init_arb_table_index(adev);
  if (ret != 0) {
    drm_err("ci_init_arb_table_index failed\n");
    return (ret);
  } else {
  }
  if ((int )pi->caps_dynamic_ac_timing) {
    ret = ci_populate_initial_mc_reg_table(adev);
    if (ret != 0) {
      drm_err("ci_populate_initial_mc_reg_table failed\n");
      return (ret);
    } else {
    }
  } else {
  }
  ret = ci_populate_pm_base(adev);
  if (ret != 0) {
    drm_err("ci_populate_pm_base failed\n");
    return (ret);
  } else {
  }
  ci_dpm_start_smc(adev);
  ci_enable_vr_hot_gpio_interrupt(adev);
  ret = ci_notify_smc_display_change(adev, 0);
  if (ret != 0) {
    drm_err("ci_notify_smc_display_change failed\n");
    return (ret);
  } else {
  }
  ci_enable_sclk_control(adev, 1);
  ret = ci_enable_ulv(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_ulv failed\n");
    return (ret);
  } else {
  }
  ret = ci_enable_ds_master_switch(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_ds_master_switch failed\n");
    return (ret);
  } else {
  }
  ret = ci_start_dpm(adev);
  if (ret != 0) {
    drm_err("ci_start_dpm failed\n");
    return (ret);
  } else {
  }
  ret = ci_enable_didt(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_didt failed\n");
    return (ret);
  } else {
  }
  ret = ci_enable_smc_cac(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_smc_cac failed\n");
    return (ret);
  } else {
  }
  ret = ci_enable_power_containment(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_power_containment failed\n");
    return (ret);
  } else {
  }
  ret = ci_power_control_set_level(adev);
  if (ret != 0) {
    drm_err("ci_power_control_set_level failed\n");
    return (ret);
  } else {
  }
  ci_enable_auto_throttle_source(adev, 0, 1);
  ret = ci_enable_thermal_based_sclk_dpm(adev, 1);
  if (ret != 0) {
    drm_err("ci_enable_thermal_based_sclk_dpm failed\n");
    return (ret);
  } else {
  }
  ci_thermal_start_thermal_controller(adev);
  ci_update_current_ps(adev, boot_ps);
  if ((int )adev->irq.installed) {
    tmp___1 = amdgpu_is_internal_thermal_sensor(adev->pm.int_thermal_type);
    if ((int )tmp___1) {
      ret = ci_thermal_set_temperature_range(adev, 90000, 120000);
      if (ret != 0) {
        drm_err("ci_thermal_set_temperature_range failed\n");
        return (ret);
      } else {
      }
      amdgpu_irq_get(adev, & adev->pm.dpm.thermal.irq, 0U);
      amdgpu_irq_get(adev, & adev->pm.dpm.thermal.irq, 1U);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void ci_dpm_disable(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps *boot_ps ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  boot_ps = adev->pm.dpm.boot_ps;
  amdgpu_irq_put(adev, & adev->pm.dpm.thermal.irq, 0U);
  amdgpu_irq_put(adev, & adev->pm.dpm.thermal.irq, 1U);
  ci_dpm_powergate_uvd(adev, 0);
  tmp___0 = amdgpu_ci_is_smc_running(adev);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    return;
  } else {
  }
  ci_thermal_stop_thermal_controller(adev);
  if ((int )pi->thermal_protection) {
    ci_enable_thermal_protection(adev, 0);
  } else {
  }
  ci_enable_power_containment(adev, 0);
  ci_enable_smc_cac(adev, 0);
  ci_enable_didt(adev, 0);
  ci_enable_spread_spectrum(adev, 0);
  ci_enable_auto_throttle_source(adev, 0, 0);
  ci_stop_dpm(adev);
  ci_enable_ds_master_switch(adev, 0);
  ci_enable_ulv(adev, 0);
  ci_clear_vc(adev);
  ci_reset_to_default(adev);
  ci_dpm_stop_smc(adev);
  ci_force_switch_to_arb_f0(adev);
  ci_enable_thermal_based_sclk_dpm(adev, 0);
  ci_update_current_ps(adev, boot_ps);
  return;
}
}
static int ci_dpm_set_power_state(struct amdgpu_device *adev )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps *new_ps ;
  struct amdgpu_ps *old_ps ;
  int ret ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  new_ps = & pi->requested_rps;
  old_ps = & pi->current_rps;
  ci_find_dpm_states_clocks_in_dpm_table(adev, new_ps);
  if ((int )pi->pcie_performance_request) {
    ci_request_link_speed_change_before_state_change(adev, new_ps, old_ps);
  } else {
  }
  ret = ci_freeze_sclk_mclk_dpm(adev);
  if (ret != 0) {
    drm_err("ci_freeze_sclk_mclk_dpm failed\n");
    return (ret);
  } else {
  }
  ret = ci_populate_and_upload_sclk_mclk_dpm_levels(adev, new_ps);
  if (ret != 0) {
    drm_err("ci_populate_and_upload_sclk_mclk_dpm_levels failed\n");
    return (ret);
  } else {
  }
  ret = ci_generate_dpm_level_enable_mask(adev, new_ps);
  if (ret != 0) {
    drm_err("ci_generate_dpm_level_enable_mask failed\n");
    return (ret);
  } else {
  }
  ret = ci_update_vce_dpm(adev, new_ps, old_ps);
  if (ret != 0) {
    drm_err("ci_update_vce_dpm failed\n");
    return (ret);
  } else {
  }
  ret = ci_update_sclk_t(adev);
  if (ret != 0) {
    drm_err("ci_update_sclk_t failed\n");
    return (ret);
  } else {
  }
  if ((int )pi->caps_dynamic_ac_timing) {
    ret = ci_update_and_upload_mc_reg_table(adev);
    if (ret != 0) {
      drm_err("ci_update_and_upload_mc_reg_table failed\n");
      return (ret);
    } else {
    }
  } else {
  }
  ret = ci_program_memory_timing_parameters(adev);
  if (ret != 0) {
    drm_err("ci_program_memory_timing_parameters failed\n");
    return (ret);
  } else {
  }
  ret = ci_unfreeze_sclk_mclk_dpm(adev);
  if (ret != 0) {
    drm_err("ci_unfreeze_sclk_mclk_dpm failed\n");
    return (ret);
  } else {
  }
  ret = ci_upload_dpm_level_enable_mask(adev);
  if (ret != 0) {
    drm_err("ci_upload_dpm_level_enable_mask failed\n");
    return (ret);
  } else {
  }
  if ((int )pi->pcie_performance_request) {
    ci_notify_link_speed_change_after_state_change(adev, new_ps, old_ps);
  } else {
  }
  return (0);
}
}
static void ci_dpm_display_configuration_changed(struct amdgpu_device *adev )
{
  {
  ci_program_display_gap(adev);
  return;
}
}
static void ci_parse_pplib_non_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                          struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ,
                                          u8 table_rev )
{
  {
  rps->caps = non_clock_info->ulCapsAndSettings;
  rps->class = (u32 )non_clock_info->usClassification;
  rps->class2 = (u32 )non_clock_info->usClassification2;
  if ((unsigned int )table_rev > 12U) {
    rps->vclk = non_clock_info->ulVCLK;
    rps->dclk = non_clock_info->ulDCLK;
  } else {
    rps->vclk = 0U;
    rps->dclk = 0U;
  }
  if ((rps->class & 8U) != 0U) {
    adev->pm.dpm.boot_ps = rps;
  } else {
  }
  if ((rps->class & 1024U) != 0U) {
    adev->pm.dpm.uvd_ps = rps;
  } else {
  }
  return;
}
}
static void ci_parse_pplib_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                      int index , union pplib_clock_info___0 *clock_info )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ps *ps ;
  struct ci_ps *tmp___0 ;
  struct ci_pl *pl ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_ps(rps);
  ps = tmp___0;
  pl = (struct ci_pl *)(& ps->performance_levels) + (unsigned long )index;
  ps->performance_level_count = (unsigned int )((u16 )index) + 1U;
  pl->sclk = (u32 )clock_info->ci.usEngineClockLow;
  pl->sclk = pl->sclk | (u32 )((int )clock_info->ci.ucEngineClockHigh << 16);
  pl->mclk = (u32 )clock_info->ci.usMemoryClockLow;
  pl->mclk = pl->mclk | (u32 )((int )clock_info->ci.ucMemoryClockHigh << 16);
  pl->pcie_gen = amdgpu_get_pcie_gen_support(adev, pi->sys_pcie_mask, (enum amdgpu_pcie_gen )pi->vbios_boot_state.pcie_gen_bootup_value,
                                             (enum amdgpu_pcie_gen )clock_info->ci.ucPCIEGen);
  pl->pcie_lane = amdgpu_get_pcie_lane_support(adev, (int )pi->vbios_boot_state.pcie_lane_bootup_value,
                                               (int )clock_info->ci.usPCIELane);
  if ((rps->class & 4096U) != 0U) {
    pi->acpi_pcie_gen = pl->pcie_gen;
  } else {
  }
  if ((rps->class2 & 2U) != 0U) {
    pi->ulv.supported = 1;
    pi->ulv.pl = *pl;
    pi->ulv.cg_ulv_parameter = 262197U;
  } else {
  }
  if ((rps->class & 8U) != 0U) {
    pl->mclk = pi->vbios_boot_state.mclk_bootup_value;
    pl->sclk = pi->vbios_boot_state.sclk_bootup_value;
    pl->pcie_gen = (enum amdgpu_pcie_gen )pi->vbios_boot_state.pcie_gen_bootup_value;
    pl->pcie_lane = pi->vbios_boot_state.pcie_lane_bootup_value;
  } else {
  }
  switch (rps->class & 7U) {
  case 1U:
  pi->use_pcie_powersaving_levels = 1;
  if ((unsigned int )pi->pcie_gen_powersaving.max < (unsigned int )pl->pcie_gen) {
    pi->pcie_gen_powersaving.max = (u16 )pl->pcie_gen;
  } else {
  }
  if ((unsigned int )pi->pcie_gen_powersaving.min > (unsigned int )pl->pcie_gen) {
    pi->pcie_gen_powersaving.min = (u16 )pl->pcie_gen;
  } else {
  }
  if ((int )pi->pcie_lane_powersaving.max < (int )pl->pcie_lane) {
    pi->pcie_lane_powersaving.max = pl->pcie_lane;
  } else {
  }
  if ((int )pi->pcie_lane_powersaving.min > (int )pl->pcie_lane) {
    pi->pcie_lane_powersaving.min = pl->pcie_lane;
  } else {
  }
  goto ldv_50243;
  case 5U:
  pi->use_pcie_performance_levels = 1;
  if ((unsigned int )pi->pcie_gen_performance.max < (unsigned int )pl->pcie_gen) {
    pi->pcie_gen_performance.max = (u16 )pl->pcie_gen;
  } else {
  }
  if ((unsigned int )pi->pcie_gen_performance.min > (unsigned int )pl->pcie_gen) {
    pi->pcie_gen_performance.min = (u16 )pl->pcie_gen;
  } else {
  }
  if ((int )pi->pcie_lane_performance.max < (int )pl->pcie_lane) {
    pi->pcie_lane_performance.max = pl->pcie_lane;
  } else {
  }
  if ((int )pi->pcie_lane_performance.min > (int )pl->pcie_lane) {
    pi->pcie_lane_performance.min = pl->pcie_lane;
  } else {
  }
  goto ldv_50243;
  default: ;
  goto ldv_50243;
  }
  ldv_50243: ;
  return;
}
}
static int ci_parse_power_table(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ;
  union pplib_power_state *power_state ;
  int i ;
  int j ;
  int k ;
  int non_clock_array_index ;
  int clock_array_index ;
  union pplib_clock_info___0 *clock_info ;
  struct _StateArray *state_array ;
  struct _ClockInfoArray *clock_info_array ;
  struct _NonClockInfoArray *non_clock_info_array ;
  union power_info *power_info ;
  int index ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  u8 *power_state_offset ;
  struct ci_ps *ps ;
  bool tmp ;
  int tmp___0 ;
  void *tmp___1 ;
  u8 *idx ;
  void *tmp___2 ;
  u32 sclk ;
  u32 mclk ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  power_info = (union power_info *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  amdgpu_add_thermal_controller(adev);
  state_array = (struct _StateArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usStateArrayOffset));
  clock_info_array = (struct _ClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usClockInfoArrayOffset));
  non_clock_info_array = (struct _NonClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usNonClockInfoArrayOffset));
  tmp___1 = kzalloc((unsigned long )state_array->ucNumEntries * 48UL, 208U);
  adev->pm.dpm.ps = (struct amdgpu_ps *)tmp___1;
  if ((unsigned long )adev->pm.dpm.ps == (unsigned long )((struct amdgpu_ps *)0)) {
    return (-12);
  } else {
  }
  power_state_offset = (u8 *)(& state_array->states);
  i = 0;
  goto ldv_50274;
  ldv_50273:
  power_state = (union pplib_power_state *)power_state_offset;
  non_clock_array_index = (int )power_state->v2.nonClockInfoIndex;
  non_clock_info = (struct _ATOM_PPLIB_NONCLOCK_INFO *)(& non_clock_info_array->nonClockInfo) + (unsigned long )non_clock_array_index;
  tmp___2 = kzalloc(40UL, 208U);
  ps = (struct ci_ps *)tmp___2;
  if ((unsigned long )ps == (unsigned long )((struct ci_ps *)0)) {
    kfree((void const *)adev->pm.dpm.ps);
    return (-12);
  } else {
  }
  (adev->pm.dpm.ps + (unsigned long )i)->ps_priv = (void *)ps;
  ci_parse_pplib_non_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, non_clock_info,
                                (int )non_clock_info_array->ucEntrySize);
  k = 0;
  idx = (u8 *)(& power_state->v2.clockInfoIndex);
  j = 0;
  goto ldv_50272;
  ldv_50271:
  clock_array_index = (int )*(idx + (unsigned long )j);
  if ((int )clock_info_array->ucNumEntries <= clock_array_index) {
    goto ldv_50269;
  } else {
  }
  if (k > 1) {
    goto ldv_50270;
  } else {
  }
  clock_info = (union pplib_clock_info___0 *)(& clock_info_array->clockInfo) + (unsigned long )((int )clock_info_array->ucEntrySize * clock_array_index);
  ci_parse_pplib_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, k, clock_info);
  k = k + 1;
  ldv_50269:
  j = j + 1;
  ldv_50272: ;
  if ((int )power_state->v2.ucNumDPMLevels > j) {
    goto ldv_50271;
  } else {
  }
  ldv_50270:
  power_state_offset = power_state_offset + (unsigned long )((int )power_state->v2.ucNumDPMLevels + 2);
  i = i + 1;
  ldv_50274: ;
  if ((int )state_array->ucNumEntries > i) {
    goto ldv_50273;
  } else {
  }
  adev->pm.dpm.num_ps = (int )state_array->ucNumEntries;
  i = 0;
  goto ldv_50279;
  ldv_50278:
  clock_array_index = (int )adev->pm.dpm.vce_states[i].clk_idx;
  clock_info = (union pplib_clock_info___0 *)(& clock_info_array->clockInfo) + (unsigned long )((int )clock_info_array->ucEntrySize * clock_array_index);
  sclk = (u32 )clock_info->ci.usEngineClockLow;
  sclk = (u32 )((int )clock_info->ci.ucEngineClockHigh << 16) | sclk;
  mclk = (u32 )clock_info->ci.usMemoryClockLow;
  mclk = (u32 )((int )clock_info->ci.ucMemoryClockHigh << 16) | mclk;
  adev->pm.dpm.vce_states[i].sclk = sclk;
  adev->pm.dpm.vce_states[i].mclk = mclk;
  i = i + 1;
  ldv_50279: ;
  if (i <= 5) {
    goto ldv_50278;
  } else {
  }
  return (0);
}
}
static int ci_get_vbios_boot_values(struct amdgpu_device *adev , struct ci_vbios_boot_state *boot_state )
{
  struct amdgpu_mode_info *mode_info ;
  int index ;
  ATOM_FIRMWARE_INFO_V2_2 *firmware_info ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  int tmp ;
  bool tmp___0 ;
  {
  mode_info = & adev->mode_info;
  index = 4;
  tmp___0 = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___0) {
    firmware_info = (ATOM_FIRMWARE_INFO_V2_2 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    boot_state->mvdd_bootup_value = firmware_info->usBootUpMVDDCVoltage;
    boot_state->vddc_bootup_value = firmware_info->usBootUpVDDCVoltage;
    boot_state->vddci_bootup_value = firmware_info->usBootUpVDDCIVoltage;
    boot_state->pcie_gen_bootup_value = ci_get_current_pcie_speed(adev);
    tmp = ci_get_current_pcie_lane_number(adev);
    boot_state->pcie_lane_bootup_value = (u16 )tmp;
    boot_state->sclk_bootup_value = firmware_info->ulDefaultEngineClock;
    boot_state->mclk_bootup_value = firmware_info->ulDefaultMemoryClock;
    return (0);
  } else {
  }
  return (-22);
}
}
static void ci_dpm_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_50296;
  ldv_50295:
  kfree((void const *)(adev->pm.dpm.ps + (unsigned long )i)->ps_priv);
  i = i + 1;
  ldv_50296: ;
  if (adev->pm.dpm.num_ps > i) {
    goto ldv_50295;
  } else {
  }
  kfree((void const *)adev->pm.dpm.ps);
  kfree((void const *)adev->pm.dpm.priv);
  kfree((void const *)adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries);
  amdgpu_free_extended_power_table(adev);
  return;
}
}
static int ci_dpm_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("ci_dpm_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  chip_name = "bonaire";
  goto ldv_50306;
  case 3U:
  chip_name = "hawaii";
  goto ldv_50306;
  case 1U: ;
  case 2U: ;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/ci_dpm.c"),
                       "i" (5811), "i" (12UL));
  ldv_50311: ;
  goto ldv_50311;
  }
  ldv_50306:
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_smc.bin", chip_name);
  err = request_firmware(& adev->pm.fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->pm.fw);
  out: ;
  if (err != 0) {
    printk("\vcik_smc: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    release_firmware(adev->pm.fw);
    adev->pm.fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static int ci_dpm_init(struct amdgpu_device *adev )
{
  int index ;
  SMU7_Discrete_DpmTable *dpm_table ;
  struct amdgpu_gpio_rec gpio ;
  u16 data_offset ;
  u16 size ;
  u8 frev ;
  u8 crev ;
  struct ci_power_info *pi ;
  int ret ;
  u32 mask ;
  void *tmp ;
  void *tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  bool tmp___3 ;
  bool tmp___4 ;
  bool tmp___5 ;
  bool tmp___6 ;
  bool tmp___7 ;
  bool tmp___8 ;
  bool tmp___9 ;
  {
  index = 26;
  tmp = kzalloc(5736UL, 208U);
  pi = (struct ci_power_info *)tmp;
  if ((unsigned long )pi == (unsigned long )((struct ci_power_info *)0)) {
    return (-12);
  } else {
  }
  adev->pm.dpm.priv = (void *)pi;
  ret = drm_pcie_get_speed_cap_mask(adev->ddev, & mask);
  if (ret != 0) {
    pi->sys_pcie_mask = 0U;
  } else {
    pi->sys_pcie_mask = mask;
  }
  pi->force_pcie_gen = 65535;
  pi->pcie_gen_performance.max = 0U;
  pi->pcie_gen_performance.min = 2U;
  pi->pcie_gen_powersaving.max = 0U;
  pi->pcie_gen_powersaving.min = 2U;
  pi->pcie_lane_performance.max = 0U;
  pi->pcie_lane_performance.min = 16U;
  pi->pcie_lane_powersaving.max = 0U;
  pi->pcie_lane_powersaving.min = 16U;
  ret = ci_get_vbios_boot_values(adev, & pi->vbios_boot_state);
  if (ret != 0) {
    ci_dpm_fini(adev);
    return (ret);
  } else {
  }
  ret = amdgpu_get_platform_caps(adev);
  if (ret != 0) {
    ci_dpm_fini(adev);
    return (ret);
  } else {
  }
  ret = amdgpu_parse_extended_power_table(adev);
  if (ret != 0) {
    ci_dpm_fini(adev);
    return (ret);
  } else {
  }
  ret = ci_parse_power_table(adev);
  if (ret != 0) {
    ci_dpm_fini(adev);
    return (ret);
  } else {
  }
  pi->dll_default_on = 0;
  pi->sram_end = 262144U;
  pi->activity_target[0] = 30U;
  pi->activity_target[1] = 30U;
  pi->activity_target[2] = 30U;
  pi->activity_target[3] = 30U;
  pi->activity_target[4] = 30U;
  pi->activity_target[5] = 30U;
  pi->activity_target[6] = 30U;
  pi->activity_target[7] = 30U;
  pi->mclk_activity_target = 10U;
  pi->sclk_dpm_key_disabled = 0U;
  pi->mclk_dpm_key_disabled = 0U;
  pi->pcie_dpm_key_disabled = 0U;
  pi->thermal_sclk_dpm_enabled = 0U;
  pi->caps_sclk_ds = 1;
  pi->mclk_strobe_mode_threshold = 40000U;
  pi->mclk_stutter_mode_threshold = 40000U;
  pi->mclk_edc_enable_threshold = 40000U;
  pi->mclk_edc_wr_enable_threshold = 40000U;
  ci_initialize_powertune_defaults(adev);
  pi->caps_fps = 0;
  pi->caps_sclk_throttle_low_notification = 0;
  pi->caps_uvd_dpm = 1;
  pi->caps_vce_dpm = 1;
  ci_get_leakage_voltages(adev);
  ci_patch_dependency_tables_with_leakage(adev);
  ci_set_private_data_variables_based_on_pptable(adev);
  tmp___0 = kzalloc(32UL, 208U);
  adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries = (struct amdgpu_clock_voltage_dependency_entry *)tmp___0;
  if ((unsigned long )adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries == (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    ci_dpm_fini(adev);
    return (-12);
  } else {
  }
  adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.count = 4U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries)->clk = 0U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries)->v = 0U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 1UL)->clk = 36000U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 1UL)->v = 720U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 2UL)->clk = 54000U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 2UL)->v = 810U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 3UL)->clk = 72000U;
  (adev->pm.dpm.dyn_state.vddc_dependency_on_dispclk.entries + 3UL)->v = 900U;
  adev->pm.dpm.dyn_state.mclk_sclk_ratio = 4U;
  adev->pm.dpm.dyn_state.sclk_mclk_delta = 15000U;
  adev->pm.dpm.dyn_state.vddc_vddci_delta = 200U;
  adev->pm.dpm.dyn_state.valid_sclk_values.count = 0U;
  adev->pm.dpm.dyn_state.valid_sclk_values.values = (u32 *)0U;
  adev->pm.dpm.dyn_state.valid_mclk_values.count = 0U;
  adev->pm.dpm.dyn_state.valid_mclk_values.values = (u32 *)0U;
  if ((unsigned int )adev->asic_type == 3U) {
    pi->thermal_temp_setting.temperature_low = 94500;
    pi->thermal_temp_setting.temperature_high = 95000;
    pi->thermal_temp_setting.temperature_shutdown = 104000;
  } else {
    pi->thermal_temp_setting.temperature_low = 99500;
    pi->thermal_temp_setting.temperature_high = 100000;
    pi->thermal_temp_setting.temperature_shutdown = 104000;
  }
  pi->uvd_enabled = 0;
  dpm_table = & pi->smc_state_table;
  gpio = amdgpu_atombios_lookup_gpio(adev, 61);
  if ((int )gpio.valid) {
    dpm_table->VRHotGpio = (uint8_t )gpio.shift;
    adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps | 65536U;
  } else {
    dpm_table->VRHotGpio = 127U;
    adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps & 4294901759U;
  }
  gpio = amdgpu_atombios_lookup_gpio(adev, 60);
  if ((int )gpio.valid) {
    dpm_table->AcDcGpio = (uint8_t )gpio.shift;
    adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps | 32U;
  } else {
    dpm_table->AcDcGpio = 127U;
    adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps & 4294967263U;
  }
  gpio = amdgpu_atombios_lookup_gpio(adev, 62);
  if ((int )gpio.valid) {
    tmp___2 = (*(adev->smc_rreg))(adev, 3223322628U);
    tmp___1 = tmp___2;
    switch (gpio.shift) {
    case 0U:
    tmp___1 = tmp___1 & 4294967292U;
    tmp___1 = tmp___1 | 1U;
    goto ldv_50328;
    case 1U:
    tmp___1 = tmp___1 & 4294967292U;
    tmp___1 = tmp___1 | 2U;
    goto ldv_50328;
    case 2U:
    tmp___1 = tmp___1 | 4U;
    goto ldv_50328;
    case 3U:
    tmp___1 = tmp___1 | 8U;
    goto ldv_50328;
    case 4U:
    tmp___1 = tmp___1 | 16U;
    goto ldv_50328;
    default:
    drm_err("Invalid PCC GPIO: %u!\n", gpio.shift);
    goto ldv_50328;
    }
    ldv_50328:
    (*(adev->smc_wreg))(adev, 3223322628U, tmp___1);
  } else {
  }
  pi->voltage_control = 0U;
  pi->vddci_control = 0U;
  pi->mvdd_control = 0U;
  tmp___4 = amdgpu_atombios_is_voltage_gpio(adev, 1, 0);
  if ((int )tmp___4) {
    pi->voltage_control = 1U;
  } else {
    tmp___3 = amdgpu_atombios_is_voltage_gpio(adev, 1, 7);
    if ((int )tmp___3) {
      pi->voltage_control = 2U;
    } else {
    }
  }
  if ((adev->pm.dpm.platform_caps & 32768U) != 0U) {
    tmp___6 = amdgpu_atombios_is_voltage_gpio(adev, 4, 0);
    if ((int )tmp___6) {
      pi->vddci_control = 1U;
    } else {
      tmp___5 = amdgpu_atombios_is_voltage_gpio(adev, 4, 7);
      if ((int )tmp___5) {
        pi->vddci_control = 2U;
      } else {
        adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps & 4294934527U;
      }
    }
  } else {
  }
  if ((adev->pm.dpm.platform_caps & 4096U) != 0U) {
    tmp___8 = amdgpu_atombios_is_voltage_gpio(adev, 2, 0);
    if ((int )tmp___8) {
      pi->mvdd_control = 1U;
    } else {
      tmp___7 = amdgpu_atombios_is_voltage_gpio(adev, 2, 7);
      if ((int )tmp___7) {
        pi->mvdd_control = 2U;
      } else {
        adev->pm.dpm.platform_caps = adev->pm.dpm.platform_caps & 4294963199U;
      }
    }
  } else {
  }
  pi->vddc_phase_shed_control = 1;
  pi->pcie_performance_request = amdgpu_acpi_is_pcie_performance_request_supported(adev);
  tmp___9 = amdgpu_atom_parse_data_header(adev->mode_info.atom_context, index, & size,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___9) {
    pi->caps_sclk_ss_support = 1;
    pi->caps_mclk_ss_support = 1;
    pi->dynamic_ss = 1;
  } else {
    pi->caps_sclk_ss_support = 0;
    pi->caps_mclk_ss_support = 0;
    pi->dynamic_ss = 1;
  }
  if ((unsigned int )adev->pm.int_thermal_type != 0U) {
    pi->thermal_protection = 1;
  } else {
    pi->thermal_protection = 0;
  }
  pi->caps_dynamic_ac_timing = 1;
  pi->uvd_power_gated = 0;
  if (adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.sclk == 0U || adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.mclk == 0U) {
    adev->pm.dpm.dyn_state.max_clock_voltage_on_dc = adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  } else {
  }
  pi->fan_ctrl_is_in_default_mode = 1;
  return (0);
}
}
static void ci_dpm_debugfs_print_current_performance_level(struct amdgpu_device *adev ,
                                                           struct seq_file *m )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct amdgpu_ps *rps ;
  u32 sclk ;
  u32 tmp___0 ;
  u32 mclk ;
  u32 tmp___1 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  rps = & pi->current_rps;
  tmp___0 = ci_get_average_sclk_freq(adev);
  sclk = tmp___0;
  tmp___1 = ci_get_average_mclk_freq(adev);
  mclk = tmp___1;
  seq_printf(m, "uvd %sabled\n", (int )pi->uvd_enabled ? (char *)"en" : (char *)"dis");
  seq_printf(m, "vce %sabled\n", (int )rps->vce_active ? (char *)"en" : (char *)"dis");
  seq_printf(m, "power level avg    sclk: %u mclk: %u\n", sclk, mclk);
  return;
}
}
static void ci_dpm_print_power_state(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct ci_ps *ps ;
  struct ci_ps *tmp ;
  struct ci_pl *pl ;
  int i ;
  {
  tmp = ci_get_ps(rps);
  ps = tmp;
  amdgpu_dpm_print_class_info(rps->class, rps->class2);
  amdgpu_dpm_print_cap_info(rps->caps);
  printk("\tuvd    vclk: %d dclk: %d\n", rps->vclk, rps->dclk);
  i = 0;
  goto ldv_50350;
  ldv_50349:
  pl = (struct ci_pl *)(& ps->performance_levels) + (unsigned long )i;
  printk("\t\tpower level %d    sclk: %u mclk: %u pcie gen: %u pcie lanes: %u\n",
         i, pl->sclk, pl->mclk, (unsigned int )pl->pcie_gen + 1U, (int )pl->pcie_lane);
  i = i + 1;
  ldv_50350: ;
  if ((int )ps->performance_level_count > i) {
    goto ldv_50349;
  } else {
  }
  amdgpu_dpm_print_ps_status(adev, rps);
  return;
}
}
static u32 ci_dpm_get_sclk(struct amdgpu_device *adev , bool low )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ps *requested_state ;
  struct ci_ps *tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_ps(& pi->requested_rps);
  requested_state = tmp___0;
  if ((int )low) {
    return (requested_state->performance_levels[0].sclk);
  } else {
    return (requested_state->performance_levels[(int )requested_state->performance_level_count + -1].sclk);
  }
}
}
static u32 ci_dpm_get_mclk(struct amdgpu_device *adev , bool low )
{
  struct ci_power_info *pi ;
  struct ci_power_info *tmp ;
  struct ci_ps *requested_state ;
  struct ci_ps *tmp___0 ;
  {
  tmp = ci_get_pi(adev);
  pi = tmp;
  tmp___0 = ci_get_ps(& pi->requested_rps);
  requested_state = tmp___0;
  if ((int )low) {
    return (requested_state->performance_levels[0].mclk);
  } else {
    return (requested_state->performance_levels[(int )requested_state->performance_level_count + -1].mclk);
  }
}
}
static int ci_dpm_get_temp(struct amdgpu_device *adev )
{
  u32 temp ;
  int actual_temp ;
  u32 tmp ;
  {
  actual_temp = 0;
  tmp = (*(adev->smc_rreg))(adev, 3224371220U);
  temp = (tmp & 261632U) >> 9;
  if ((temp & 512U) != 0U) {
    actual_temp = 255;
  } else {
    actual_temp = (int )temp & 511;
  }
  actual_temp = actual_temp * 1000;
  return (actual_temp);
}
}
static int ci_set_temperature_range(struct amdgpu_device *adev )
{
  int ret ;
  {
  ret = ci_thermal_enable_alert(adev, 0);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_thermal_set_temperature_range(adev, 90000, 120000);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = ci_thermal_enable_alert(adev, 1);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (ret);
}
}
static int ci_dpm_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  ci_dpm_set_dpm_funcs(adev);
  ci_dpm_set_irq_funcs(adev);
  return (0);
}
}
static int ci_dpm_late_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if (amdgpu_dpm == 0) {
    return (0);
  } else {
  }
  ret = ci_set_temperature_range(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ci_dpm_powergate_uvd(adev, 1);
  return (0);
}
}
static int ci_dpm_sw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct amdgpu_ps *tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = amdgpu_irq_add_id(adev, 230U, & adev->pm.dpm.thermal.irq);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_irq_add_id(adev, 231U, & adev->pm.dpm.thermal.irq);
  if (ret != 0) {
    return (ret);
  } else {
  }
  adev->pm.dpm.state = 3;
  adev->pm.dpm.user_state = 3;
  adev->pm.dpm.forced_level = 0;
  adev->pm.default_sclk = adev->clock.default_sclk;
  adev->pm.default_mclk = adev->clock.default_mclk;
  adev->pm.current_sclk = adev->clock.default_sclk;
  adev->pm.current_mclk = adev->clock.default_mclk;
  adev->pm.int_thermal_type = 0;
  if (amdgpu_dpm == 0) {
    return (0);
  } else {
  }
  ret = ci_dpm_init_microcode(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  __init_work(& adev->pm.dpm.thermal.work, 0);
  __constr_expr_0.counter = 137438953408L;
  adev->pm.dpm.thermal.work.data = __constr_expr_0;
  lockdep_init_map(& adev->pm.dpm.thermal.work.lockdep_map, "(&adev->pm.dpm.thermal.work)",
                   & __key, 0);
  INIT_LIST_HEAD(& adev->pm.dpm.thermal.work.entry);
  adev->pm.dpm.thermal.work.func = & amdgpu_dpm_thermal_work_handler;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = ci_dpm_init(adev);
  if (ret != 0) {
    goto dpm_failed;
  } else {
  }
  tmp = adev->pm.dpm.boot_ps;
  adev->pm.dpm.requested_ps = tmp;
  adev->pm.dpm.current_ps = tmp;
  if (amdgpu_dpm == 1) {
    amdgpu_pm_print_power_states(adev);
  } else {
  }
  ret = amdgpu_pm_sysfs_init(adev);
  if (ret != 0) {
    goto dpm_failed;
  } else {
  }
  mutex_unlock(& adev->pm.mutex);
  printk("\016[drm] amdgpu: dpm initialized\n");
  return (0);
  dpm_failed:
  ci_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  drm_err("amdgpu: dpm initialization failed\n");
  return (ret);
}
}
static int ci_dpm_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  amdgpu_pm_sysfs_fini(adev);
  ci_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static int ci_dpm_hw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if (amdgpu_dpm == 0) {
    return (0);
  } else {
  }
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ci_dpm_setup_asic(adev);
  ret = ci_dpm_enable(adev);
  if (ret != 0) {
    adev->pm.dpm_enabled = 0;
  } else {
    adev->pm.dpm_enabled = 1;
  }
  mutex_unlock(& adev->pm.mutex);
  return (ret);
}
}
static int ci_dpm_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    ci_dpm_disable(adev);
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (0);
}
}
static int ci_dpm_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ps *tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    ci_dpm_disable(adev);
    tmp = adev->pm.dpm.boot_ps;
    adev->pm.dpm.requested_ps = tmp;
    adev->pm.dpm.current_ps = tmp;
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (0);
}
}
static int ci_dpm_resume(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    ci_dpm_setup_asic(adev);
    ret = ci_dpm_enable(adev);
    if (ret != 0) {
      adev->pm.dpm_enabled = 0;
    } else {
      adev->pm.dpm_enabled = 1;
    }
    mutex_unlock(& adev->pm.mutex);
    if ((int )adev->pm.dpm_enabled) {
      amdgpu_pm_compute_clocks(adev);
    } else {
    }
  } else {
  }
  return (0);
}
}
static bool ci_dpm_is_idle(void *handle )
{
  {
  return (1);
}
}
static int ci_dpm_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void ci_dpm_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  u32 tmp___46 ;
  u32 tmp___47 ;
  u32 tmp___48 ;
  u32 tmp___49 ;
  u32 tmp___50 ;
  u32 tmp___51 ;
  u32 tmp___52 ;
  u32 tmp___53 ;
  u32 tmp___54 ;
  u32 tmp___55 ;
  u32 tmp___56 ;
  u32 tmp___57 ;
  u32 tmp___58 ;
  u32 tmp___59 ;
  u32 tmp___60 ;
  u32 tmp___61 ;
  u32 tmp___62 ;
  u32 tmp___63 ;
  u32 tmp___64 ;
  u32 tmp___65 ;
  u32 tmp___66 ;
  u32 tmp___67 ;
  u32 tmp___68 ;
  u32 tmp___69 ;
  u32 tmp___70 ;
  u32 tmp___71 ;
  u32 tmp___72 ;
  u32 tmp___73 ;
  u32 tmp___74 ;
  u32 tmp___75 ;
  u32 tmp___76 ;
  u32 tmp___77 ;
  u32 tmp___78 ;
  u32 tmp___79 ;
  u32 tmp___80 ;
  u32 tmp___81 ;
  u32 tmp___82 ;
  u32 tmp___83 ;
  u32 tmp___84 ;
  u32 tmp___85 ;
  u32 tmp___86 ;
  u32 tmp___87 ;
  u32 tmp___88 ;
  u32 tmp___89 ;
  u32 tmp___90 ;
  u32 tmp___91 ;
  u32 tmp___92 ;
  u32 tmp___93 ;
  u32 tmp___94 ;
  u32 tmp___95 ;
  u32 tmp___96 ;
  u32 tmp___97 ;
  u32 tmp___98 ;
  u32 tmp___99 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "CIK DPM registers\n");
  tmp = amdgpu_mm_rreg(adev, 1485U, 0);
  _dev_info((struct device const *)adev->dev, "  BIOS_SCRATCH_4=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 2525U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_DRAM_TIMING=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 2526U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_DRAM_TIMING2=0x%08X\n",
            tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 2562U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_BURST_TIME=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 2556U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_DRAM_TIMING_1=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 2559U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_DRAM_TIMING2_1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 2415U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_CG_CONFIG=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 2554U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_ARB_CG=0x%08X\n", tmp___6);
  tmp___7 = (*(adev->didt_rreg))(adev, 0U);
  _dev_info((struct device const *)adev->dev, "  DIDT_SQ_CTRL0=0x%08X\n", tmp___7);
  tmp___8 = (*(adev->didt_rreg))(adev, 32U);
  _dev_info((struct device const *)adev->dev, "  DIDT_DB_CTRL0=0x%08X\n", tmp___8);
  tmp___9 = (*(adev->didt_rreg))(adev, 64U);
  _dev_info((struct device const *)adev->dev, "  DIDT_TD_CTRL0=0x%08X\n", tmp___9);
  tmp___10 = (*(adev->didt_rreg))(adev, 96U);
  _dev_info((struct device const *)adev->dev, "  DIDT_TCP_CTRL0=0x%08X\n", tmp___10);
  tmp___11 = (*(adev->smc_rreg))(adev, 3224371212U);
  _dev_info((struct device const *)adev->dev, "  CG_THERMAL_INT=0x%08X\n", tmp___11);
  tmp___12 = (*(adev->smc_rreg))(adev, 3224371204U);
  _dev_info((struct device const *)adev->dev, "  CG_THERMAL_CTRL=0x%08X\n", tmp___12);
  tmp___13 = (*(adev->smc_rreg))(adev, 3223322624U);
  _dev_info((struct device const *)adev->dev, "  GENERAL_PWRMGT=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 3456U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_CNTL_3=0x%08X\n", tmp___14);
  tmp___15 = (*(adev->smc_rreg))(adev, 3225423152U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC0_CNTL=0x%08X\n", tmp___15);
  tmp___16 = (*(adev->smc_rreg))(adev, 3225423164U);
  _dev_info((struct device const *)adev->dev, "  LCAC_MC1_CNTL=0x%08X\n", tmp___16);
  tmp___17 = (*(adev->smc_rreg))(adev, 3225423232U);
  _dev_info((struct device const *)adev->dev, "  LCAC_CPL_CNTL=0x%08X\n", tmp___17);
  tmp___18 = (*(adev->smc_rreg))(adev, 3223322632U);
  _dev_info((struct device const *)adev->dev, "  SCLK_PWRMGT_CNTL=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 5256U, 0);
  _dev_info((struct device const *)adev->dev, "  BIF_LNCNT_RESET=0x%08X\n", tmp___19);
  tmp___20 = (*(adev->smc_rreg))(adev, 260096U);
  _dev_info((struct device const *)adev->dev, "  FIRMWARE_FLAGS=0x%08X\n", tmp___20);
  tmp___21 = (*(adev->smc_rreg))(adev, 3226468672U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_FUNC_CNTL=0x%08X\n", tmp___21);
  tmp___22 = (*(adev->smc_rreg))(adev, 3226468676U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_FUNC_CNTL_2=0x%08X\n",
            tmp___22);
  tmp___23 = (*(adev->smc_rreg))(adev, 3226468680U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_FUNC_CNTL_3=0x%08X\n",
            tmp___23);
  tmp___24 = (*(adev->smc_rreg))(adev, 3226468684U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_FUNC_CNTL_4=0x%08X\n",
            tmp___24);
  tmp___25 = (*(adev->smc_rreg))(adev, 3226468708U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_SPREAD_SPECTRUM=0x%08X\n",
            tmp___25);
  tmp___26 = (*(adev->smc_rreg))(adev, 3226468712U);
  _dev_info((struct device const *)adev->dev, "  CG_SPLL_SPREAD_SPECTRUM_2=0x%08X\n",
            tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 2793U, 0);
  _dev_info((struct device const *)adev->dev, "  DLL_CNTL=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 2792U, 0);
  _dev_info((struct device const *)adev->dev, "  MCLK_PWRMGT_CNTL=0x%08X\n", tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 2800U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_AD_FUNC_CNTL=0x%08X\n", tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 2801U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_DQ_FUNC_CNTL=0x%08X\n", tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 2797U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_FUNC_CNTL=0x%08X\n", tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 2798U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_FUNC_CNTL_1=0x%08X\n", tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 2799U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_FUNC_CNTL_2=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 2803U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_SS1=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 2804U, 0);
  _dev_info((struct device const *)adev->dev, "  MPLL_SS2=0x%08X\n", tmp___35);
  tmp___36 = (*(adev->smc_rreg))(adev, 3223322720U);
  _dev_info((struct device const *)adev->dev, "  CG_DISPLAY_GAP_CNTL=0x%08X\n",
            tmp___36);
  tmp___37 = (*(adev->smc_rreg))(adev, 3223323184U);
  _dev_info((struct device const *)adev->dev, "  CG_DISPLAY_GAP_CNTL2=0x%08X\n",
            tmp___37);
  tmp___38 = (*(adev->smc_rreg))(adev, 3223322692U);
  _dev_info((struct device const *)adev->dev, "  CG_STATIC_SCREEN_PARAMETER=0x%08X\n",
            tmp___38);
  tmp___39 = (*(adev->smc_rreg))(adev, 3223323048U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_0=0x%08X\n",
            tmp___39);
  tmp___40 = (*(adev->smc_rreg))(adev, 3223323052U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_1=0x%08X\n",
            tmp___40);
  tmp___41 = (*(adev->smc_rreg))(adev, 3223323056U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_2=0x%08X\n",
            tmp___41);
  tmp___42 = (*(adev->smc_rreg))(adev, 3223323060U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_3=0x%08X\n",
            tmp___42);
  tmp___43 = (*(adev->smc_rreg))(adev, 3223323064U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_4=0x%08X\n",
            tmp___43);
  tmp___44 = (*(adev->smc_rreg))(adev, 3223323068U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_5=0x%08X\n",
            tmp___44);
  tmp___45 = (*(adev->smc_rreg))(adev, 3223323072U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_6=0x%08X\n",
            tmp___45);
  tmp___46 = (*(adev->smc_rreg))(adev, 3223323076U);
  _dev_info((struct device const *)adev->dev, "  CG_FREQ_TRAN_VOTING_7=0x%08X\n",
            tmp___46);
  tmp___47 = (*(adev->smc_rreg))(adev, 3221225476U);
  _dev_info((struct device const *)adev->dev, "  RCU_UC_EVENTS=0x%08X\n", tmp___47);
  tmp___48 = (*(adev->smc_rreg))(adev, 259944U);
  _dev_info((struct device const *)adev->dev, "  DPM_TABLE_475=0x%08X\n", tmp___48);
  tmp___49 = amdgpu_mm_rreg(adev, 2715U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RAS_TIMING_LP=0x%08X\n",
            tmp___49);
  tmp___50 = amdgpu_mm_rreg(adev, 2600U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RAS_TIMING=0x%08X\n", tmp___50);
  tmp___51 = amdgpu_mm_rreg(adev, 2716U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_CAS_TIMING_LP=0x%08X\n",
            tmp___51);
  tmp___52 = amdgpu_mm_rreg(adev, 2601U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_CAS_TIMING=0x%08X\n", tmp___52);
  tmp___53 = amdgpu_mm_rreg(adev, 3471U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_DLL_STBY_LP=0x%08X\n", tmp___53);
  tmp___54 = amdgpu_mm_rreg(adev, 3470U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_DLL_STBY=0x%08X\n", tmp___54);
  tmp___55 = amdgpu_mm_rreg(adev, 3460U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CMD0_LP=0x%08X\n",
            tmp___55);
  tmp___56 = amdgpu_mm_rreg(adev, 3459U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CMD0=0x%08X\n", tmp___56);
  tmp___57 = amdgpu_mm_rreg(adev, 3462U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CMD1_LP=0x%08X\n",
            tmp___57);
  tmp___58 = amdgpu_mm_rreg(adev, 3461U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CMD1=0x%08X\n", tmp___58);
  tmp___59 = amdgpu_mm_rreg(adev, 3458U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CTRL_LP=0x%08X\n",
            tmp___59);
  tmp___60 = amdgpu_mm_rreg(adev, 3457U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_G5PDX_CTRL=0x%08X\n", tmp___60);
  tmp___61 = amdgpu_mm_rreg(adev, 3469U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_DVS_CMD_LP=0x%08X\n",
            tmp___61);
  tmp___62 = amdgpu_mm_rreg(adev, 3468U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_DVS_CMD=0x%08X\n", tmp___62);
  tmp___63 = amdgpu_mm_rreg(adev, 3467U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_DVS_CTL_LP=0x%08X\n",
            tmp___63);
  tmp___64 = amdgpu_mm_rreg(adev, 3466U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_DVS_CTL=0x%08X\n", tmp___64);
  tmp___65 = amdgpu_mm_rreg(adev, 2717U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_MISC_TIMING_LP=0x%08X\n",
            tmp___65);
  tmp___66 = amdgpu_mm_rreg(adev, 2602U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_MISC_TIMING=0x%08X\n", tmp___66);
  tmp___67 = amdgpu_mm_rreg(adev, 2718U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_MISC_TIMING2_LP=0x%08X\n",
            tmp___67);
  tmp___68 = amdgpu_mm_rreg(adev, 2603U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_MISC_TIMING2=0x%08X\n",
            tmp___68);
  tmp___69 = amdgpu_mm_rreg(adev, 2721U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_CMD_EMRS_LP=0x%08X\n",
            tmp___69);
  tmp___70 = amdgpu_mm_rreg(adev, 2691U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_PMG_CMD_EMRS=0x%08X\n", tmp___70);
  tmp___71 = amdgpu_mm_rreg(adev, 2722U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_CMD_MRS_LP=0x%08X\n",
            tmp___71);
  tmp___72 = amdgpu_mm_rreg(adev, 2731U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_PMG_CMD_MRS=0x%08X\n", tmp___72);
  tmp___73 = amdgpu_mm_rreg(adev, 2770U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_CMD_MRS1_LP=0x%08X\n",
            tmp___73);
  tmp___74 = amdgpu_mm_rreg(adev, 2769U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_PMG_CMD_MRS1=0x%08X\n", tmp___74);
  tmp___75 = amdgpu_mm_rreg(adev, 2719U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_D0_LP=0x%08X\n",
            tmp___75);
  tmp___76 = amdgpu_mm_rreg(adev, 2607U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_D0=0x%08X\n", tmp___76);
  tmp___77 = amdgpu_mm_rreg(adev, 2720U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_D1_LP=0x%08X\n",
            tmp___77);
  tmp___78 = amdgpu_mm_rreg(adev, 2608U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_D1=0x%08X\n", tmp___78);
  tmp___79 = amdgpu_mm_rreg(adev, 2759U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RD_CTL_D0_LP=0x%08X\n",
            tmp___79);
  tmp___80 = amdgpu_mm_rreg(adev, 2605U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RD_CTL_D0=0x%08X\n", tmp___80);
  tmp___81 = amdgpu_mm_rreg(adev, 2760U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RD_CTL_D1_LP=0x%08X\n",
            tmp___81);
  tmp___82 = amdgpu_mm_rreg(adev, 2606U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_RD_CTL_D1=0x%08X\n", tmp___82);
  tmp___83 = amdgpu_mm_rreg(adev, 2771U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_TIMING_LP=0x%08X\n",
            tmp___83);
  tmp___84 = amdgpu_mm_rreg(adev, 2604U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_TIMING=0x%08X\n", tmp___84);
  tmp___85 = amdgpu_mm_rreg(adev, 2776U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_PMG_CMD_MRS2_LP=0x%08X\n",
            tmp___85);
  tmp___86 = amdgpu_mm_rreg(adev, 2775U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_PMG_CMD_MRS2=0x%08X\n", tmp___86);
  tmp___87 = amdgpu_mm_rreg(adev, 2774U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_2_LP=0x%08X\n", tmp___87);
  tmp___88 = amdgpu_mm_rreg(adev, 2773U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_SEQ_WR_CTL_2=0x%08X\n", tmp___88);
  tmp___89 = (*(adev->pcie_rreg))(adev, 268501156U);
  _dev_info((struct device const *)adev->dev, "  PCIE_LC_SPEED_CNTL=0x%08X\n", tmp___89);
  tmp___90 = (*(adev->pcie_rreg))(adev, 268501154U);
  _dev_info((struct device const *)adev->dev, "  PCIE_LC_LINK_WIDTH_CNTL=0x%08X\n",
            tmp___90);
  tmp___91 = amdgpu_mm_rreg(adev, 128U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_INDEX_0=0x%08X\n", tmp___91);
  tmp___92 = amdgpu_mm_rreg(adev, 129U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_DATA_0=0x%08X\n", tmp___92);
  tmp___93 = amdgpu_mm_rreg(adev, 144U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_IND_ACCESS_CNTL=0x%08X\n",
            tmp___93);
  tmp___94 = amdgpu_mm_rreg(adev, 149U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_RESP_0=0x%08X\n", tmp___94);
  tmp___95 = amdgpu_mm_rreg(adev, 148U, 0);
  _dev_info((struct device const *)adev->dev, "  SMC_MESSAGE_0=0x%08X\n", tmp___95);
  tmp___96 = (*(adev->smc_rreg))(adev, 2147483648U);
  _dev_info((struct device const *)adev->dev, "  SMC_SYSCON_RESET_CNTL=0x%08X\n",
            tmp___96);
  tmp___97 = (*(adev->smc_rreg))(adev, 2147483652U);
  _dev_info((struct device const *)adev->dev, "  SMC_SYSCON_CLOCK_CNTL_0=0x%08X\n",
            tmp___97);
  tmp___98 = (*(adev->smc_rreg))(adev, 2147483664U);
  _dev_info((struct device const *)adev->dev, "  SMC_SYSCON_MISC_CNTL=0x%08X\n",
            tmp___98);
  tmp___99 = (*(adev->smc_rreg))(adev, 2147484528U);
  _dev_info((struct device const *)adev->dev, "  SMC_PC_C=0x%08X\n", tmp___99);
  return;
}
}
static int ci_dpm_soft_reset(void *handle )
{
  {
  return (0);
}
}
static int ci_dpm_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cg_thermal_int ;
  {
  switch (type) {
  case 0U: ;
  switch ((unsigned int )state) {
  case 0U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3224371212U);
  cg_thermal_int = cg_thermal_int & 4278190079U;
  (*(adev->smc_wreg))(adev, 3224371212U, cg_thermal_int);
  goto ldv_50434;
  case 1U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3224371212U);
  cg_thermal_int = cg_thermal_int | 16777216U;
  (*(adev->smc_wreg))(adev, 3224371212U, cg_thermal_int);
  goto ldv_50434;
  default: ;
  goto ldv_50434;
  }
  ldv_50434: ;
  goto ldv_50437;
  case 1U: ;
  switch ((unsigned int )state) {
  case 0U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3224371212U);
  cg_thermal_int = cg_thermal_int & 4261412863U;
  (*(adev->smc_wreg))(adev, 3224371212U, cg_thermal_int);
  goto ldv_50440;
  case 1U:
  cg_thermal_int = (*(adev->smc_rreg))(adev, 3224371212U);
  cg_thermal_int = cg_thermal_int | 33554432U;
  (*(adev->smc_wreg))(adev, 3224371212U, cg_thermal_int);
  goto ldv_50440;
  default: ;
  goto ldv_50440;
  }
  ldv_50440: ;
  goto ldv_50437;
  default: ;
  goto ldv_50437;
  }
  ldv_50437: ;
  return (0);
}
}
static int ci_dpm_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                    struct amdgpu_iv_entry *entry )
{
  bool queue_thermal ;
  long tmp ;
  long tmp___0 ;
  {
  queue_thermal = 0;
  if ((unsigned long )entry == (unsigned long )((struct amdgpu_iv_entry *)0)) {
    return (-22);
  } else {
  }
  switch (entry->src_id) {
  case 230U:
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("ci_dpm_process_interrupt", "IH: thermal low to high\n");
  } else {
  }
  adev->pm.dpm.thermal.high_to_low = 0;
  queue_thermal = 1;
  goto ldv_50452;
  case 231U:
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("ci_dpm_process_interrupt", "IH: thermal high to low\n");
  } else {
  }
  adev->pm.dpm.thermal.high_to_low = 1;
  queue_thermal = 1;
  goto ldv_50452;
  default: ;
  goto ldv_50452;
  }
  ldv_50452: ;
  if ((int )queue_thermal) {
    schedule_work___0(& adev->pm.dpm.thermal.work);
  } else {
  }
  return (0);
}
}
static int ci_dpm_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int ci_dpm_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const ci_dpm_ip_funcs =
     {& ci_dpm_early_init, & ci_dpm_late_init, & ci_dpm_sw_init, & ci_dpm_sw_fini, & ci_dpm_hw_init,
    & ci_dpm_hw_fini, & ci_dpm_suspend, & ci_dpm_resume, & ci_dpm_is_idle, & ci_dpm_wait_for_idle,
    & ci_dpm_soft_reset, & ci_dpm_print_status, & ci_dpm_set_clockgating_state, & ci_dpm_set_powergating_state};
static struct amdgpu_dpm_funcs const ci_dpm_funcs =
     {& ci_dpm_get_temp, & ci_dpm_pre_set_power_state, & ci_dpm_set_power_state, & ci_dpm_post_set_power_state,
    & ci_dpm_display_configuration_changed, & ci_dpm_get_sclk, & ci_dpm_get_mclk,
    & ci_dpm_print_power_state, & ci_dpm_debugfs_print_current_performance_level,
    & ci_dpm_force_performance_level, & ci_dpm_vblank_too_short, & ci_dpm_powergate_uvd,
    0, 0, & ci_dpm_set_fan_control_mode, & ci_dpm_get_fan_control_mode, & ci_dpm_set_fan_speed_percent,
    & ci_dpm_get_fan_speed_percent};
static void ci_dpm_set_dpm_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.funcs == (unsigned long )((struct amdgpu_dpm_funcs const *)0)) {
    adev->pm.funcs = & ci_dpm_funcs;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const ci_dpm_irq_funcs = {& ci_dpm_set_interrupt_state, & ci_dpm_process_interrupt};
static void ci_dpm_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->pm.dpm.thermal.irq.num_types = 2U;
  adev->pm.dpm.thermal.irq.funcs = & ci_dpm_irq_funcs;
  return;
}
}
extern int ldv_probe_111(void) ;
int ldv_retval_79 ;
extern int ldv_release_111(void) ;
int ldv_retval_80 ;
void activate_work_5(struct work_struct *work , int state )
{
  {
  if (ldv_work_5_0 == 0) {
    ldv_work_struct_5_0 = work;
    ldv_work_5_0 = state;
    return;
  } else {
  }
  if (ldv_work_5_1 == 0) {
    ldv_work_struct_5_1 = work;
    ldv_work_5_1 = state;
    return;
  } else {
  }
  if (ldv_work_5_2 == 0) {
    ldv_work_struct_5_2 = work;
    ldv_work_5_2 = state;
    return;
  } else {
  }
  if (ldv_work_5_3 == 0) {
    ldv_work_struct_5_3 = work;
    ldv_work_5_3 = state;
    return;
  } else {
  }
  return;
}
}
void invoke_work_5(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_5_0 == 2 || ldv_work_5_0 == 3) {
    ldv_work_5_0 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_5_0);
    ldv_work_5_0 = 1;
  } else {
  }
  goto ldv_50488;
  case 1: ;
  if (ldv_work_5_1 == 2 || ldv_work_5_1 == 3) {
    ldv_work_5_1 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_5_0);
    ldv_work_5_1 = 1;
  } else {
  }
  goto ldv_50488;
  case 2: ;
  if (ldv_work_5_2 == 2 || ldv_work_5_2 == 3) {
    ldv_work_5_2 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_5_0);
    ldv_work_5_2 = 1;
  } else {
  }
  goto ldv_50488;
  case 3: ;
  if (ldv_work_5_3 == 2 || ldv_work_5_3 == 3) {
    ldv_work_5_3 = 4;
    amdgpu_dpm_thermal_work_handler(ldv_work_struct_5_0);
    ldv_work_5_3 = 1;
  } else {
  }
  goto ldv_50488;
  default:
  ldv_stop();
  }
  ldv_50488: ;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_109(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  ci_dpm_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ci_dpm_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_dpm_funcs_110(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  ci_dpm_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void disable_work_5(struct work_struct *work )
{
  {
  if ((ldv_work_5_0 == 3 || ldv_work_5_0 == 2) && (unsigned long )ldv_work_struct_5_0 == (unsigned long )work) {
    ldv_work_5_0 = 1;
  } else {
  }
  if ((ldv_work_5_1 == 3 || ldv_work_5_1 == 2) && (unsigned long )ldv_work_struct_5_1 == (unsigned long )work) {
    ldv_work_5_1 = 1;
  } else {
  }
  if ((ldv_work_5_2 == 3 || ldv_work_5_2 == 2) && (unsigned long )ldv_work_struct_5_2 == (unsigned long )work) {
    ldv_work_5_2 = 1;
  } else {
  }
  if ((ldv_work_5_3 == 3 || ldv_work_5_3 == 2) && (unsigned long )ldv_work_struct_5_3 == (unsigned long )work) {
    ldv_work_5_3 = 1;
  } else {
  }
  return;
}
}
void call_and_disable_all_5(int state )
{
  {
  if (ldv_work_5_0 == state) {
    call_and_disable_work_5(ldv_work_struct_5_0);
  } else {
  }
  if (ldv_work_5_1 == state) {
    call_and_disable_work_5(ldv_work_struct_5_1);
  } else {
  }
  if (ldv_work_5_2 == state) {
    call_and_disable_work_5(ldv_work_struct_5_2);
  } else {
  }
  if (ldv_work_5_3 == state) {
    call_and_disable_work_5(ldv_work_struct_5_3);
  } else {
  }
  return;
}
}
void work_init_5(void)
{
  {
  ldv_work_5_0 = 0;
  ldv_work_5_1 = 0;
  ldv_work_5_2 = 0;
  ldv_work_5_3 = 0;
  return;
}
}
void call_and_disable_work_5(struct work_struct *work )
{
  {
  if ((ldv_work_5_0 == 2 || ldv_work_5_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_5_0) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_5_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_5_1 == 2 || ldv_work_5_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_5_1) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_5_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_5_2 == 2 || ldv_work_5_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_5_2) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_5_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_5_3 == 2 || ldv_work_5_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_5_3) {
    amdgpu_dpm_thermal_work_handler(work);
    ldv_work_5_3 = 1;
    return;
  } else {
  }
  return;
}
}
void ldv_main_exported_109(void)
{
  struct amdgpu_iv_entry *ldvarg103 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg105 ;
  unsigned int ldvarg104 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg103 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg105), 0, 4UL);
  ldv_memset((void *)(& ldvarg104), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_109 == 1) {
    ci_dpm_set_interrupt_state(ci_dpm_irq_funcs_group0, ci_dpm_irq_funcs_group1, ldvarg104,
                               ldvarg105);
    ldv_state_variable_109 = 1;
  } else {
  }
  goto ldv_50520;
  case 1: ;
  if (ldv_state_variable_109 == 1) {
    ci_dpm_process_interrupt(ci_dpm_irq_funcs_group0, ci_dpm_irq_funcs_group1, ldvarg103);
    ldv_state_variable_109 = 1;
  } else {
  }
  goto ldv_50520;
  default:
  ldv_stop();
  }
  ldv_50520: ;
  return;
}
}
void ldv_main_exported_111(void)
{
  void *ldvarg1069 ;
  void *tmp ;
  void *ldvarg1083 ;
  void *tmp___0 ;
  enum amd_powergating_state ldvarg1077 ;
  void *ldvarg1071 ;
  void *tmp___1 ;
  void *ldvarg1080 ;
  void *tmp___2 ;
  enum amd_clockgating_state ldvarg1073 ;
  void *ldvarg1075 ;
  void *tmp___3 ;
  void *ldvarg1074 ;
  void *tmp___4 ;
  void *ldvarg1072 ;
  void *tmp___5 ;
  void *ldvarg1076 ;
  void *tmp___6 ;
  void *ldvarg1078 ;
  void *tmp___7 ;
  void *ldvarg1070 ;
  void *tmp___8 ;
  void *ldvarg1084 ;
  void *tmp___9 ;
  void *ldvarg1081 ;
  void *tmp___10 ;
  void *ldvarg1079 ;
  void *tmp___11 ;
  void *ldvarg1082 ;
  void *tmp___12 ;
  int tmp___13 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg1069 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg1083 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg1071 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg1080 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg1075 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg1074 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg1072 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg1076 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg1078 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg1070 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg1084 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg1081 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg1079 = tmp___11;
  tmp___12 = ldv_init_zalloc(1UL);
  ldvarg1082 = tmp___12;
  ldv_memset((void *)(& ldvarg1077), 0, 4UL);
  ldv_memset((void *)(& ldvarg1073), 0, 4UL);
  tmp___13 = __VERIFIER_nondet_int();
  switch (tmp___13) {
  case 0: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_hw_fini(ldvarg1084);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_hw_fini(ldvarg1084);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_hw_fini(ldvarg1084);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 1: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_print_status(ldvarg1083);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_print_status(ldvarg1083);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_print_status(ldvarg1083);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 2: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_early_init(ldvarg1082);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_early_init(ldvarg1082);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_early_init(ldvarg1082);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 3: ;
  if (ldv_state_variable_111 == 2) {
    ldv_retval_80 = ci_dpm_suspend(ldvarg1081);
    if (ldv_retval_80 == 0) {
      ldv_state_variable_111 = 3;
    } else {
    }
  } else {
  }
  goto ldv_50543;
  case 4: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_late_init(ldvarg1080);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_late_init(ldvarg1080);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_late_init(ldvarg1080);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 5: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_sw_init(ldvarg1079);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_sw_init(ldvarg1079);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_sw_init(ldvarg1079);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 6: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_set_powergating_state(ldvarg1078, ldvarg1077);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_set_powergating_state(ldvarg1078, ldvarg1077);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_set_powergating_state(ldvarg1078, ldvarg1077);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 7: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_wait_for_idle(ldvarg1076);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_wait_for_idle(ldvarg1076);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_wait_for_idle(ldvarg1076);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 8: ;
  if (ldv_state_variable_111 == 3) {
    ldv_retval_79 = ci_dpm_resume(ldvarg1075);
    if (ldv_retval_79 == 0) {
      ldv_state_variable_111 = 2;
    } else {
    }
  } else {
  }
  goto ldv_50543;
  case 9: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_set_clockgating_state(ldvarg1074, ldvarg1073);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_set_clockgating_state(ldvarg1074, ldvarg1073);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_set_clockgating_state(ldvarg1074, ldvarg1073);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 10: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_hw_init(ldvarg1072);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_hw_init(ldvarg1072);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_hw_init(ldvarg1072);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 11: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_soft_reset(ldvarg1071);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_soft_reset(ldvarg1071);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_soft_reset(ldvarg1071);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 12: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_sw_fini(ldvarg1070);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_sw_fini(ldvarg1070);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_sw_fini(ldvarg1070);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 13: ;
  if (ldv_state_variable_111 == 2) {
    ci_dpm_is_idle(ldvarg1069);
    ldv_state_variable_111 = 2;
  } else {
  }
  if (ldv_state_variable_111 == 1) {
    ci_dpm_is_idle(ldvarg1069);
    ldv_state_variable_111 = 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ci_dpm_is_idle(ldvarg1069);
    ldv_state_variable_111 = 3;
  } else {
  }
  goto ldv_50543;
  case 14: ;
  if (ldv_state_variable_111 == 2) {
    ldv_release_111();
    ldv_state_variable_111 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_111 == 3) {
    ldv_release_111();
    ldv_state_variable_111 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50543;
  case 15: ;
  if (ldv_state_variable_111 == 1) {
    ldv_probe_111();
    ldv_state_variable_111 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50543;
  default:
  ldv_stop();
  }
  ldv_50543: ;
  return;
}
}
void ldv_main_exported_110(void)
{
  u32 ldvarg316 ;
  bool ldvarg311 ;
  struct seq_file *ldvarg317 ;
  void *tmp ;
  enum amdgpu_dpm_forced_level ldvarg313 ;
  bool ldvarg318 ;
  struct amdgpu_ps *ldvarg312 ;
  void *tmp___0 ;
  u32 ldvarg315 ;
  bool ldvarg310 ;
  u32 *ldvarg314 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(256UL);
  ldvarg317 = (struct seq_file *)tmp;
  tmp___0 = ldv_init_zalloc(48UL);
  ldvarg312 = (struct amdgpu_ps *)tmp___0;
  tmp___1 = ldv_init_zalloc(4UL);
  ldvarg314 = (u32 *)tmp___1;
  ldv_memset((void *)(& ldvarg316), 0, 4UL);
  ldv_memset((void *)(& ldvarg311), 0, 1UL);
  ldv_memset((void *)(& ldvarg313), 0, 4UL);
  ldv_memset((void *)(& ldvarg318), 0, 1UL);
  ldv_memset((void *)(& ldvarg315), 0, 4UL);
  ldv_memset((void *)(& ldvarg310), 0, 1UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_get_sclk(ci_dpm_funcs_group0, (int )ldvarg318);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 1: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_debugfs_print_current_performance_level(ci_dpm_funcs_group0, ldvarg317);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 2: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_vblank_too_short(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 3: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_set_fan_speed_percent(ci_dpm_funcs_group0, ldvarg316);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 4: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_get_temp(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 5: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_set_fan_control_mode(ci_dpm_funcs_group0, ldvarg315);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 6: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_get_fan_control_mode(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 7: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_get_fan_speed_percent(ci_dpm_funcs_group0, ldvarg314);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 8: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_force_performance_level(ci_dpm_funcs_group0, ldvarg313);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 9: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_post_set_power_state(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 10: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_display_configuration_changed(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 11: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_print_power_state(ci_dpm_funcs_group0, ldvarg312);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 12: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_pre_set_power_state(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 13: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_set_power_state(ci_dpm_funcs_group0);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 14: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_get_mclk(ci_dpm_funcs_group0, (int )ldvarg311);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  case 15: ;
  if (ldv_state_variable_110 == 1) {
    ci_dpm_powergate_uvd(ci_dpm_funcs_group0, (int )ldvarg310);
    ldv_state_variable_110 = 1;
  } else {
  }
  goto ldv_50573;
  default:
  ldv_stop();
  }
  ldv_50573: ;
  return;
}
}
bool ldv_queue_work_on_607(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_608(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_609(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_610(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_611(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static int __atomic_add_unless___4(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___4(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___4(v, a, u);
  return (tmp != u);
}
}
extern struct workqueue_struct *__alloc_workqueue_key(char const * , unsigned int ,
                                                      int , struct lock_class_key * ,
                                                      char const * , ...) ;
extern void destroy_workqueue(struct workqueue_struct * ) ;
void ldv_destroy_workqueue_626(struct workqueue_struct *ldv_func_arg1 ) ;
bool ldv_queue_work_on_621(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_623(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_622(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_625(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_624(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___2(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_621(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___1(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___2(system_wq, work);
  return (tmp);
}
}
__inline static int kref_put_mutex___4(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___4(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
extern ssize_t hdmi_avi_infoframe_pack(struct hdmi_avi_infoframe * , void * , size_t ) ;
extern void drm_crtc_cleanup(struct drm_crtc * ) ;
extern int drm_encoder_init(struct drm_device * , struct drm_encoder * , struct drm_encoder_funcs const * ,
                            int ) ;
extern void drm_encoder_cleanup(struct drm_encoder * ) ;
extern void drm_mode_config_cleanup(struct drm_device * ) ;
extern int drm_mode_crtc_set_gamma_size(struct drm_crtc * , int ) ;
extern char const *drm_get_format_name(u32 ) ;
extern void drm_send_vblank_event(struct drm_device * , int , struct drm_pending_vblank_event * ) ;
extern bool drm_handle_vblank(struct drm_device * , int ) ;
extern void drm_vblank_pre_modeset(struct drm_device * , int ) ;
extern void drm_vblank_post_modeset(struct drm_device * , int ) ;
__inline static void drm_gem_object_unreference_unlocked___4(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___4(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
extern int drm_edid_to_sad(struct edid * , struct cea_sad ** ) ;
extern int drm_edid_to_speaker_allocation(struct edid * , u8 ** ) ;
extern int drm_hdmi_avi_infoframe_from_display_mode(struct hdmi_avi_infoframe * ,
                                                    struct drm_display_mode const * ) ;
__inline static void drm_crtc_helper_add(struct drm_crtc *crtc , struct drm_crtc_helper_funcs const *funcs )
{
  {
  crtc->helper_private = (void const *)funcs;
  return;
}
}
__inline static void drm_encoder_helper_add(struct drm_encoder *encoder , struct drm_encoder_helper_funcs const *funcs )
{
  {
  encoder->helper_private = (void const *)funcs;
  return;
}
}
extern void drm_kms_helper_poll_init(struct drm_device * ) ;
extern void drm_kms_helper_poll_fini(struct drm_device * ) ;
extern int drm_crtc_init(struct drm_device * , struct drm_crtc * , struct drm_crtc_funcs const * ) ;
bool amdgpu_irq_enabled(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                        unsigned int type ) ;
static void dce_v8_0_set_display_funcs(struct amdgpu_device *adev ) ;
static void dce_v8_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const crtc_offsets[6U] = { 0U, 768U, 9728U, 10496U,
        11264U, 12032U};
static u32 const dig_offsets[7U] = { 0U, 768U, 9728U, 10496U,
        11264U, 12032U, 12800U};
static struct __anonstruct_interrupt_status_offsets_324 const interrupt_status_offsets[6U] = { {6205U,
      8U, 4U, 131072U},
        {6206U, 8U, 4U, 131072U},
        {6207U, 8U, 4U, 131072U},
        {6208U, 8U, 4U, 131072U},
        {6227U, 8U, 4U, 131072U},
        {6228U, 8U, 4U, 131072U}};
static u32 const hpd_int_control_offsets[6U] = { 6152U, 6155U, 6158U, 6161U,
        6164U, 6167U};
static u32 dce_v8_0_audio_endpt_rreg(struct amdgpu_device *adev , u32 block_offset ,
                                     u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6016U, reg, 0);
  r = amdgpu_mm_rreg(adev, block_offset + 6017U, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return (r);
}
}
static void dce_v8_0_audio_endpt_wreg(struct amdgpu_device *adev , u32 block_offset ,
                                      u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6016U, reg, 0);
  amdgpu_mm_wreg(adev, block_offset + 6017U, v, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return;
}
}
static bool dce_v8_0_is_in_vblank(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7075U, 0);
  if ((tmp & 8191U) != 0U) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v8_0_is_counter_moving(struct amdgpu_device *adev , int crtc )
{
  u32 pos1 ;
  u32 pos2 ;
  {
  pos1 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7076U, 0);
  pos2 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7076U, 0);
  if (pos1 != pos2) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v8_0_vblank_wait(struct amdgpu_device *adev , int crtc )
{
  unsigned int i ;
  u32 tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  unsigned int tmp___2 ;
  bool tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  unsigned int tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  {
  i = 0U;
  if (adev->mode_info.num_crtc <= crtc) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7068U, 0);
  if ((tmp & 1U) == 0U) {
    return;
  } else {
  }
  goto ldv_53101;
  ldv_53100:
  tmp___2 = i;
  i = i + 1U;
  if (tmp___2 % 100U == 0U) {
    tmp___0 = dce_v8_0_is_counter_moving(adev, crtc);
    if (tmp___0) {
      tmp___1 = 0;
    } else {
      tmp___1 = 1;
    }
    if (tmp___1) {
      goto ldv_53099;
    } else {
    }
  } else {
  }
  ldv_53101:
  tmp___3 = dce_v8_0_is_in_vblank(adev, crtc);
  if ((int )tmp___3) {
    goto ldv_53100;
  } else {
  }
  ldv_53099: ;
  goto ldv_53104;
  ldv_53103:
  tmp___6 = i;
  i = i + 1U;
  if (tmp___6 % 100U == 0U) {
    tmp___4 = dce_v8_0_is_counter_moving(adev, crtc);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto ldv_53102;
    } else {
    }
  } else {
  }
  ldv_53104:
  tmp___7 = dce_v8_0_is_in_vblank(adev, crtc);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
    goto ldv_53103;
  } else {
  }
  ldv_53102: ;
  return;
}
}
static u32 dce_v8_0_vblank_get_counter(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    return (0U);
  } else {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7078U, 0);
    return (tmp);
  }
}
}
static void dce_v8_0_page_flip(struct amdgpu_device *adev , int crtc_id , u64 crtc_base )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  u32 tmp ;
  u32 tmp___0 ;
  int i ;
  u32 tmp___1 ;
  long tmp___2 ;
  {
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  tmp___0 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  tmp = tmp___0;
  tmp = tmp | 65536U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )crtc_base,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )crtc_base,
                 0);
  i = 0;
  goto ldv_53119;
  ldv_53118:
  tmp___1 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  if ((tmp___1 & 4U) != 0U) {
    goto ldv_53117;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_53119: ;
  if (adev->usec_timeout > i) {
    goto ldv_53118;
  } else {
  }
  ldv_53117:
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v8_0_page_flip", "Update pending now high. Unlocking vupdate_lock.\n");
  } else {
  }
  tmp = tmp & 4294901759U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  return;
}
}
static int dce_v8_0_crtc_get_scanoutpos(struct amdgpu_device *adev , int crtc , u32 *vbl ,
                                        u32 *position )
{
  {
  if (crtc < 0 || adev->mode_info.num_crtc <= crtc) {
    return (-22);
  } else {
  }
  *vbl = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7053U, 0);
  *position = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[crtc] + 7076U, 0);
  return (0);
}
}
static bool dce_v8_0_hpd_sense(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  bool connected ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  connected = 0;
  switch ((unsigned int )hpd) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, 6151U, 0);
  if ((tmp & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  case 1U:
  tmp___0 = amdgpu_mm_rreg(adev, 6154U, 0);
  if ((tmp___0 & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  case 2U:
  tmp___1 = amdgpu_mm_rreg(adev, 6157U, 0);
  if ((tmp___1 & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  case 3U:
  tmp___2 = amdgpu_mm_rreg(adev, 6160U, 0);
  if ((tmp___2 & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  case 4U:
  tmp___3 = amdgpu_mm_rreg(adev, 6163U, 0);
  if ((tmp___3 & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  case 5U:
  tmp___4 = amdgpu_mm_rreg(adev, 6166U, 0);
  if ((tmp___4 & 2U) != 0U) {
    connected = 1;
  } else {
  }
  goto ldv_53133;
  default: ;
  goto ldv_53133;
  }
  ldv_53133: ;
  return (connected);
}
}
static void dce_v8_0_hpd_set_polarity(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  u32 tmp ;
  bool connected ;
  bool tmp___0 ;
  {
  tmp___0 = dce_v8_0_hpd_sense(adev, hpd);
  connected = tmp___0;
  switch ((unsigned int )hpd) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, 6152U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6152U, tmp, 0);
  goto ldv_53147;
  case 1U:
  tmp = amdgpu_mm_rreg(adev, 6155U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6155U, tmp, 0);
  goto ldv_53147;
  case 2U:
  tmp = amdgpu_mm_rreg(adev, 6158U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6158U, tmp, 0);
  goto ldv_53147;
  case 3U:
  tmp = amdgpu_mm_rreg(adev, 6161U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6161U, tmp, 0);
  goto ldv_53147;
  case 4U:
  tmp = amdgpu_mm_rreg(adev, 6164U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6164U, tmp, 0);
  goto ldv_53147;
  case 5U:
  tmp = amdgpu_mm_rreg(adev, 6167U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, 6167U, tmp, 0);
  goto ldv_53147;
  default: ;
  goto ldv_53147;
  }
  ldv_53147: ;
  return;
}
}
static void dce_v8_0_hpd_init(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  u32 tmp ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  tmp = 284821956U;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_53177;
  ldv_53176:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    goto ldv_53167;
  } else {
  }
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  amdgpu_mm_wreg(adev, 6153U, tmp, 0);
  goto ldv_53169;
  case 1U:
  amdgpu_mm_wreg(adev, 6156U, tmp, 0);
  goto ldv_53169;
  case 2U:
  amdgpu_mm_wreg(adev, 6159U, tmp, 0);
  goto ldv_53169;
  case 3U:
  amdgpu_mm_wreg(adev, 6162U, tmp, 0);
  goto ldv_53169;
  case 4U:
  amdgpu_mm_wreg(adev, 6165U, tmp, 0);
  goto ldv_53169;
  case 5U:
  amdgpu_mm_wreg(adev, 6168U, tmp, 0);
  goto ldv_53169;
  default: ;
  goto ldv_53169;
  }
  ldv_53169:
  dce_v8_0_hpd_set_polarity(adev, amdgpu_connector->hpd.hpd);
  amdgpu_irq_get(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  ldv_53167:
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_53177: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_53176;
  } else {
  }
  return;
}
}
static void dce_v8_0_hpd_fini(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_53200;
  ldv_53199:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  amdgpu_mm_wreg(adev, 6153U, 0U, 0);
  goto ldv_53192;
  case 1U:
  amdgpu_mm_wreg(adev, 6156U, 0U, 0);
  goto ldv_53192;
  case 2U:
  amdgpu_mm_wreg(adev, 6159U, 0U, 0);
  goto ldv_53192;
  case 3U:
  amdgpu_mm_wreg(adev, 6162U, 0U, 0);
  goto ldv_53192;
  case 4U:
  amdgpu_mm_wreg(adev, 6165U, 0U, 0);
  goto ldv_53192;
  case 5U:
  amdgpu_mm_wreg(adev, 6168U, 0U, 0);
  goto ldv_53192;
  default: ;
  goto ldv_53192;
  }
  ldv_53192:
  amdgpu_irq_put(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_53200: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_53199;
  } else {
  }
  return;
}
}
static u32 dce_v8_0_hpd_get_gpio_reg(struct amdgpu_device *adev )
{
  {
  return (6509U);
}
}
static bool dce_v8_0_is_display_hung(struct amdgpu_device *adev )
{
  u32 crtc_hung ;
  u32 crtc_status[6U] ;
  u32 i ;
  u32 j ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  crtc_hung = 0U;
  i = 0U;
  goto ldv_53214;
  ldv_53213:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7068U, 0);
  if ((int )tmp___0 & 1) {
    crtc_status[i] = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7080U,
                                    0);
    crtc_hung = (u32 )(1 << (int )i) | crtc_hung;
  } else {
  }
  i = i + 1U;
  ldv_53214: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_53213;
  } else {
  }
  j = 0U;
  goto ldv_53220;
  ldv_53219:
  i = 0U;
  goto ldv_53217;
  ldv_53216: ;
  if (((u32 )(1 << (int )i) & crtc_hung) != 0U) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7080U, 0);
    if (crtc_status[i] != tmp) {
      crtc_hung = (u32 )(~ (1 << (int )i)) & crtc_hung;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_53217: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_53216;
  } else {
  }
  if (crtc_hung == 0U) {
    return (0);
  } else {
  }
  __const_udelay(429500UL);
  j = j + 1U;
  ldv_53220: ;
  if (j <= 9U) {
    goto ldv_53219;
  } else {
  }
  return (1);
}
}
static void dce_v8_0_stop_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 crtc_enabled ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  {
  save->vga_render_control = amdgpu_mm_rreg(adev, 192U, 0);
  save->vga_hdp_control = amdgpu_mm_rreg(adev, 202U, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  tmp = tmp & 4294770687U;
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  i = 0;
  goto ldv_53230;
  ldv_53229:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7068U, 0);
  crtc_enabled = tmp___0 & 1U;
  if (crtc_enabled != 0U) {
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7093U, 1U, 0);
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7068U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7068U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7093U, 0U, 0);
    save->crtc_enabled[i] = 0;
  } else {
    save->crtc_enabled[i] = 0;
  }
  i = i + 1;
  ldv_53230: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_53229;
  } else {
  }
  return;
}
}
static void dce_v8_0_resume_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 tmp ;
  u32 frame_count ;
  int i ;
  int j ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  {
  i = 0;
  goto ldv_53247;
  ldv_53246:
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 6663U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 6664U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 6660U, (unsigned int )adev->mc.vram_start,
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 6661U, (unsigned int )adev->mc.vram_start,
                 0);
  if ((int )save->crtc_enabled[i]) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7102U, 0);
    if ((tmp & 7U) != 3U) {
      tmp = (tmp & 4294967288U) | 3U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7102U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 6673U, 0);
    if ((tmp & 65536U) >> 16 != 0U) {
      tmp = tmp & 4294901759U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 6673U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7101U, 0);
    if ((int )tmp & 1) {
      tmp = tmp & 4294967294U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7101U, tmp, 0);
    } else {
    }
    j = 0;
    goto ldv_53242;
    ldv_53241:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 6673U, 0);
    if ((tmp & 4U) >> 2 == 0U) {
      goto ldv_53240;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_53242: ;
    if (adev->usec_timeout > j) {
      goto ldv_53241;
    } else {
    }
    ldv_53240:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets[i] + 7069U, 0);
    tmp = tmp & 4294967039U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7093U, 1U, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7069U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[i] + 7093U, 0U, 0);
    frame_count = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    j = 0;
    goto ldv_53245;
    ldv_53244:
    tmp___0 = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    if (tmp___0 != frame_count) {
      goto ldv_53243;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_53245: ;
    if (adev->usec_timeout > j) {
      goto ldv_53244;
    } else {
    }
    ldv_53243: ;
  } else {
  }
  i = i + 1;
  ldv_53247: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_53246;
  } else {
  }
  amdgpu_mm_wreg(adev, 201U, (unsigned int )(adev->mc.vram_start >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 196U, (unsigned int )adev->mc.vram_start, 0);
  amdgpu_mm_wreg(adev, 202U, save->vga_hdp_control, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_53251;
    ldv_53250:
    __const_udelay(4295000UL);
    ldv_53251:
    tmp___1 = __ms;
    __ms = __ms - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_53250;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 192U, save->vga_render_control, 0);
  return;
}
}
static void dce_v8_0_set_vga_render_state(struct amdgpu_device *adev , bool render )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 202U, 0);
  if ((int )render) {
    tmp = tmp & 4294967279U;
  } else {
    tmp = tmp | 16U;
  }
  amdgpu_mm_wreg(adev, 202U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  if ((int )render) {
    tmp = (tmp & 4294770687U) | 65536U;
  } else {
    tmp = tmp & 4294770687U;
  }
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  return;
}
}
static void dce_v8_0_program_fmt(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  int bpc ;
  u32 tmp___0 ;
  enum amdgpu_connector_dither dither ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 0;
  tmp___0 = 0U;
  dither = 0;
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    bpc = amdgpu_connector_get_monitor_bpc(connector);
    dither = amdgpu_connector->dither;
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    return;
  } else {
  }
  if (amdgpu_encoder->encoder_id == 21U || amdgpu_encoder->encoder_id == 22U) {
    return;
  } else {
  }
  if (bpc == 0) {
    return;
  } else {
  }
  switch (bpc) {
  case 6: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 41216U;
  } else {
    tmp___0 = tmp___0 | 1U;
  }
  goto ldv_53277;
  case 8: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 59648U;
  } else {
    tmp___0 = tmp___0 | 17U;
  }
  goto ldv_53277;
  case 10: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 61696U;
  } else {
    tmp___0 = tmp___0 | 33U;
  }
  goto ldv_53277;
  default: ;
  goto ldv_53277;
  }
  ldv_53277:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7154U, tmp___0, 0);
  return;
}
}
static u32 dce_v8_0_line_buffer_adjust(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                       struct drm_display_mode *mode )
{
  u32 tmp ;
  u32 buffer_alloc ;
  u32 i ;
  u32 pipe_offset ;
  long tmp___0 ;
  u32 tmp___1 ;
  {
  pipe_offset = (u32 )(amdgpu_crtc->crtc_id * 8);
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    if (mode->crtc_hdisplay <= 1919) {
      tmp = 1U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 2559) {
      tmp = 2U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 4095) {
      tmp = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    } else {
      tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("dce_v8_0_line_buffer_adjust", "Mode too big for LB!\n");
      } else {
      }
      tmp = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    }
  } else {
    tmp = 1U;
    buffer_alloc = 0U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6849U, (tmp << 20) | 1712U, 0);
  amdgpu_mm_wreg(adev, pipe_offset + 808U, buffer_alloc, 0);
  i = 0U;
  goto ldv_53293;
  ldv_53292:
  tmp___1 = amdgpu_mm_rreg(adev, pipe_offset + 808U, 0);
  if ((tmp___1 & 16U) != 0U) {
    goto ldv_53291;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53293: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_53292;
  } else {
  }
  ldv_53291: ;
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    switch (tmp) {
    case 0U: ;
    default: ;
    return (8192U);
    case 1U: ;
    return (3840U);
    case 2U: ;
    return (5120U);
    }
  } else {
  }
  return (0U);
}
}
static u32 cik_get_number_of_dram_channels(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 2049U, 0);
  tmp = tmp___0;
  switch ((tmp & 61440U) >> 12) {
  case 0U: ;
  default: ;
  return (1U);
  case 1U: ;
  return (2U);
  case 2U: ;
  return (4U);
  case 3U: ;
  return (8U);
  case 4U: ;
  return (3U);
  case 5U: ;
  return (6U);
  case 6U: ;
  return (10U);
  case 7U: ;
  return (12U);
  case 8U: ;
  return (16U);
  }
}
}
static u32 dce_v8_0_dram_bandwidth(struct dce8_wm_params *wm )
{
  fixed20_12 dram_efficiency ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  dram_efficiency.full = 28672U;
  dram_efficiency.full = dfixed_div(dram_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )dram_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v8_0_dram_bandwidth_for_display(struct dce8_wm_params *wm )
{
  fixed20_12 disp_dram_allocation ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  disp_dram_allocation.full = 12288U;
  disp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )disp_dram_allocation.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v8_0_data_return_bandwidth(struct dce8_wm_params *wm )
{
  fixed20_12 return_efficiency ;
  fixed20_12 sclk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  sclk.full = wm->sclk << 12;
  sclk.full = dfixed_div(sclk, a);
  a.full = 40960U;
  return_efficiency.full = 32768U;
  return_efficiency.full = dfixed_div(return_efficiency, a);
  a.full = 131072U;
  bandwidth.full = (u32 )(((unsigned long long )a.full * (unsigned long long )sclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )return_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v8_0_dmif_request_bandwidth(struct dce8_wm_params *wm )
{
  fixed20_12 disp_clk_request_efficiency ;
  fixed20_12 disp_clk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  fixed20_12 b ;
  {
  a.full = 4096000U;
  disp_clk.full = wm->disp_clk << 12;
  disp_clk.full = dfixed_div(disp_clk, a);
  a.full = 131072U;
  b.full = (u32 )(((unsigned long long )a.full * (unsigned long long )disp_clk.full + 2048ULL) >> 12);
  a.full = 40960U;
  disp_clk_request_efficiency.full = 32768U;
  disp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )b.full * (unsigned long long )disp_clk_request_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v8_0_available_bandwidth(struct dce8_wm_params *wm )
{
  u32 dram_bandwidth ;
  u32 tmp ;
  u32 data_return_bandwidth ;
  u32 tmp___0 ;
  u32 dmif_req_bandwidth ;
  u32 tmp___1 ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  tmp = dce_v8_0_dram_bandwidth(wm);
  dram_bandwidth = tmp;
  tmp___0 = dce_v8_0_data_return_bandwidth(wm);
  data_return_bandwidth = tmp___0;
  tmp___1 = dce_v8_0_dmif_request_bandwidth(wm);
  dmif_req_bandwidth = tmp___1;
  _min1 = dram_bandwidth;
  _min1___0 = data_return_bandwidth;
  _min2___0 = dmif_req_bandwidth;
  _min2 = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  return (_min1 < _min2 ? _min1 : _min2);
}
}
static u32 dce_v8_0_average_bandwidth(struct dce8_wm_params *wm )
{
  fixed20_12 bpp ;
  fixed20_12 line_time ;
  fixed20_12 src_width ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  line_time.full = (wm->active_time + wm->blank_time) << 12;
  line_time.full = dfixed_div(line_time, a);
  bpp.full = wm->bytes_per_pixel << 12;
  src_width.full = wm->src_width << 12;
  bandwidth.full = (u32 )(((unsigned long long )src_width.full * (unsigned long long )bpp.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )wm->vsc.full + 2048ULL) >> 12);
  bandwidth.full = dfixed_div(bandwidth, line_time);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v8_0_latency_watermark(struct dce8_wm_params *wm )
{
  u32 mc_latency ;
  u32 available_bandwidth ;
  u32 tmp ;
  u32 worst_chunk_return_time ;
  u32 cursor_line_pair_return_time ;
  u32 dc_latency ;
  u32 other_heads_data_return_time ;
  u32 latency ;
  u32 max_src_lines_per_dst_line ;
  u32 lb_fill_bw ;
  u32 line_fill_time ;
  u32 tmp___0 ;
  u32 dmif_size ;
  fixed20_12 a ;
  fixed20_12 b ;
  fixed20_12 c ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  mc_latency = 2000U;
  tmp = dce_v8_0_available_bandwidth(wm);
  available_bandwidth = tmp;
  worst_chunk_return_time = 4096000U / available_bandwidth;
  cursor_line_pair_return_time = 512000U / available_bandwidth;
  dc_latency = 40000000U / wm->disp_clk;
  other_heads_data_return_time = (wm->num_heads + 1U) * worst_chunk_return_time + wm->num_heads * cursor_line_pair_return_time;
  latency = (mc_latency + other_heads_data_return_time) + dc_latency;
  dmif_size = 12288U;
  if (wm->num_heads == 0U) {
    return (0U);
  } else {
  }
  a.full = 8192U;
  b.full = 4096U;
  if (((wm->vsc.full > a.full || (wm->vsc.full > b.full && wm->vtaps > 2U)) || wm->vtaps > 4U) || (wm->vsc.full >= a.full && (int )wm->interlaced)) {
    max_src_lines_per_dst_line = 4U;
  } else {
    max_src_lines_per_dst_line = 2U;
  }
  a.full = available_bandwidth << 12;
  b.full = wm->num_heads << 12;
  a.full = dfixed_div(a, b);
  b.full = (mc_latency + 512U) << 12;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(b, c);
  c.full = dmif_size << 12;
  b.full = dfixed_div(c, b);
  _min1 = a.full >> 12;
  _min2 = b.full >> 12;
  tmp___0 = _min1 < _min2 ? _min1 : _min2;
  b.full = 4096000U;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(c, b);
  c.full = wm->bytes_per_pixel << 12;
  b.full = (u32 )(((unsigned long long )b.full * (unsigned long long )c.full + 2048ULL) >> 12);
  _min1___0 = tmp___0;
  _min2___0 = b.full >> 12;
  lb_fill_bw = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  a.full = (wm->src_width * max_src_lines_per_dst_line) * wm->bytes_per_pixel << 12;
  b.full = 4096000U;
  c.full = lb_fill_bw << 12;
  b.full = dfixed_div(c, b);
  a.full = dfixed_div(a, b);
  line_fill_time = a.full >> 12;
  if (wm->active_time > line_fill_time) {
    return (latency);
  } else {
    return ((line_fill_time - wm->active_time) + latency);
  }
}
}
static bool dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(struct dce8_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v8_0_average_bandwidth(wm);
  tmp___0 = dce_v8_0_dram_bandwidth_for_display(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v8_0_average_bandwidth_vs_available_bandwidth(struct dce8_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v8_0_average_bandwidth(wm);
  tmp___0 = dce_v8_0_available_bandwidth(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v8_0_check_latency_hiding(struct dce8_wm_params *wm )
{
  u32 lb_partitions ;
  u32 line_time ;
  u32 latency_tolerant_lines ;
  u32 latency_hiding ;
  fixed20_12 a ;
  u32 tmp ;
  {
  lb_partitions = wm->lb_size / wm->src_width;
  line_time = wm->active_time + wm->blank_time;
  a.full = 4096U;
  if (wm->vsc.full > a.full) {
    latency_tolerant_lines = 1U;
  } else
  if (wm->vtaps + 1U >= lb_partitions) {
    latency_tolerant_lines = 1U;
  } else {
    latency_tolerant_lines = 2U;
  }
  latency_hiding = latency_tolerant_lines * line_time + wm->blank_time;
  tmp = dce_v8_0_latency_watermark(wm);
  if (tmp <= latency_hiding) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v8_0_program_watermarks(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                        u32 lb_size , u32 num_heads )
{
  struct drm_display_mode *mode ;
  struct dce8_wm_params wm_low ;
  struct dce8_wm_params wm_high ;
  u32 pixel_period ;
  u32 line_time ;
  u32 latency_watermark_a ;
  u32 latency_watermark_b ;
  u32 tmp ;
  u32 wm_mask ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 _min1___0 ;
  u32 tmp___2 ;
  unsigned int _min2___0 ;
  long tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  bool tmp___6 ;
  int tmp___7 ;
  bool tmp___8 ;
  int tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 _min1___1 ;
  u32 tmp___12 ;
  unsigned int _min2___1 ;
  long tmp___13 ;
  bool tmp___14 ;
  int tmp___15 ;
  bool tmp___16 ;
  int tmp___17 ;
  bool tmp___18 ;
  int tmp___19 ;
  {
  mode = & amdgpu_crtc->base.mode;
  line_time = 0U;
  latency_watermark_a = 0U;
  latency_watermark_b = 0U;
  if (((int )amdgpu_crtc->base.enabled && num_heads != 0U) && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    pixel_period = 1000000U / (unsigned int )mode->clock;
    _min1 = (unsigned int )mode->crtc_htotal * pixel_period;
    _min2 = 65535U;
    line_time = _min1 < _min2 ? _min1 : _min2;
    if ((int )adev->pm.dpm_enabled) {
      tmp___0 = (*((adev->pm.funcs)->get_mclk))(adev, 0);
      wm_high.yclk = tmp___0 * 10U;
      tmp___1 = (*((adev->pm.funcs)->get_sclk))(adev, 0);
      wm_high.sclk = tmp___1 * 10U;
    } else {
      wm_high.yclk = adev->pm.current_mclk * 10U;
      wm_high.sclk = adev->pm.current_sclk * 10U;
    }
    wm_high.disp_clk = (u32 )mode->clock;
    wm_high.src_width = (u32 )mode->crtc_hdisplay;
    wm_high.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_high.blank_time = line_time - wm_high.active_time;
    wm_high.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_high.interlaced = 1;
    } else {
    }
    wm_high.vsc = amdgpu_crtc->vsc;
    wm_high.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_high.vtaps = 2U;
    } else {
    }
    wm_high.bytes_per_pixel = 4U;
    wm_high.lb_size = lb_size;
    wm_high.dram_channels = cik_get_number_of_dram_channels(adev);
    wm_high.num_heads = num_heads;
    tmp___2 = dce_v8_0_latency_watermark(& wm_high);
    _min1___0 = tmp___2;
    _min2___0 = 65535U;
    latency_watermark_a = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    tmp___4 = dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_high);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto _L;
    } else {
      tmp___6 = dce_v8_0_average_bandwidth_vs_available_bandwidth(& wm_high);
      if (tmp___6) {
        tmp___7 = 0;
      } else {
        tmp___7 = 1;
      }
      if (tmp___7) {
        goto _L;
      } else {
        tmp___8 = dce_v8_0_check_latency_hiding(& wm_high);
        if (tmp___8) {
          tmp___9 = 0;
        } else {
          tmp___9 = 1;
        }
        if (tmp___9) {
          goto _L;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L:
          tmp___3 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___3 != 0L) {
            drm_ut_debug_printk("dce_v8_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
    if ((int )adev->pm.dpm_enabled) {
      tmp___10 = (*((adev->pm.funcs)->get_mclk))(adev, 1);
      wm_low.yclk = tmp___10 * 10U;
      tmp___11 = (*((adev->pm.funcs)->get_sclk))(adev, 1);
      wm_low.sclk = tmp___11 * 10U;
    } else {
      wm_low.yclk = adev->pm.current_mclk * 10U;
      wm_low.sclk = adev->pm.current_sclk * 10U;
    }
    wm_low.disp_clk = (u32 )mode->clock;
    wm_low.src_width = (u32 )mode->crtc_hdisplay;
    wm_low.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_low.blank_time = line_time - wm_low.active_time;
    wm_low.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_low.interlaced = 1;
    } else {
    }
    wm_low.vsc = amdgpu_crtc->vsc;
    wm_low.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_low.vtaps = 2U;
    } else {
    }
    wm_low.bytes_per_pixel = 4U;
    wm_low.lb_size = lb_size;
    wm_low.dram_channels = cik_get_number_of_dram_channels(adev);
    wm_low.num_heads = num_heads;
    tmp___12 = dce_v8_0_latency_watermark(& wm_low);
    _min1___1 = tmp___12;
    _min2___1 = 65535U;
    latency_watermark_b = _min1___1 < _min2___1 ? _min1___1 : _min2___1;
    tmp___14 = dce_v8_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_low);
    if (tmp___14) {
      tmp___15 = 0;
    } else {
      tmp___15 = 1;
    }
    if (tmp___15) {
      goto _L___0;
    } else {
      tmp___16 = dce_v8_0_average_bandwidth_vs_available_bandwidth(& wm_low);
      if (tmp___16) {
        tmp___17 = 0;
      } else {
        tmp___17 = 1;
      }
      if (tmp___17) {
        goto _L___0;
      } else {
        tmp___18 = dce_v8_0_check_latency_hiding(& wm_low);
        if (tmp___18) {
          tmp___19 = 0;
        } else {
          tmp___19 = 1;
        }
        if (tmp___19) {
          goto _L___0;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L___0:
          tmp___13 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___13 != 0L) {
            drm_ut_debug_printk("dce_v8_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
  } else {
  }
  wm_mask = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6962U, 0);
  tmp = wm_mask;
  tmp = tmp & 4294966527U;
  tmp = tmp | 256U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, (line_time << 16) | latency_watermark_a,
                 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6962U, 0);
  tmp = tmp & 4294966527U;
  tmp = tmp | 512U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, (line_time << 16) | latency_watermark_b,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, wm_mask, 0);
  amdgpu_crtc->line_time = line_time;
  amdgpu_crtc->wm_high = latency_watermark_a;
  amdgpu_crtc->wm_low = latency_watermark_b;
  return;
}
}
static void dce_v8_0_bandwidth_update(struct amdgpu_device *adev )
{
  struct drm_display_mode *mode ;
  u32 num_heads ;
  u32 lb_size ;
  int i ;
  {
  mode = (struct drm_display_mode *)0;
  num_heads = 0U;
  amdgpu_update_display_priority(adev);
  i = 0;
  goto ldv_53451;
  ldv_53450: ;
  if ((int )(adev->mode_info.crtcs[i])->base.enabled) {
    num_heads = num_heads + 1U;
  } else {
  }
  i = i + 1;
  ldv_53451: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_53450;
  } else {
  }
  i = 0;
  goto ldv_53454;
  ldv_53453:
  mode = & (adev->mode_info.crtcs[i])->base.mode;
  lb_size = dce_v8_0_line_buffer_adjust(adev, adev->mode_info.crtcs[i], mode);
  dce_v8_0_program_watermarks(adev, adev->mode_info.crtcs[i], lb_size, num_heads);
  i = i + 1;
  ldv_53454: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_53453;
  } else {
  }
  return;
}
}
static void dce_v8_0_audio_get_connected_pins(struct amdgpu_device *adev )
{
  int i ;
  u32 offset ;
  u32 tmp ;
  {
  i = 0;
  goto ldv_53463;
  ldv_53462:
  offset = adev->mode_info.audio.pin[i].offset;
  tmp = (*(adev->audio_endpt_rreg))(adev, offset, 86U);
  if (tmp >> 30 == 1U) {
    adev->mode_info.audio.pin[i].connected = 0;
  } else {
    adev->mode_info.audio.pin[i].connected = 1;
  }
  i = i + 1;
  ldv_53463: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_53462;
  } else {
  }
  return;
}
}
static struct amdgpu_audio_pin *dce_v8_0_audio_get_pin(struct amdgpu_device *adev )
{
  int i ;
  {
  dce_v8_0_audio_get_connected_pins(adev);
  i = 0;
  goto ldv_53470;
  ldv_53469: ;
  if ((int )adev->mode_info.audio.pin[i].connected) {
    return ((struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i);
  } else {
  }
  i = i + 1;
  ldv_53470: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_53469;
  } else {
  }
  drm_err("No connected audio pins found!\n");
  return ((struct amdgpu_audio_pin *)0);
}
}
static void dce_v8_0_afmt_audio_select_pin(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 offset ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  offset = (u32 )(dig->afmt)->offset;
  amdgpu_mm_wreg(adev, offset + 7247U, ((dig->afmt)->pin)->id, 0);
  return;
}
}
static void dce_v8_0_audio_write_latency_fields(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 tmp ;
  u32 offset ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  tmp = 0U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  offset = ((dig->afmt)->pin)->offset;
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_53502;
  ldv_53501: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_53500;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_53502: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_53501;
  } else {
  }
  ldv_53500: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  if ((mode->flags & 16U) != 0U) {
    if ((int )connector->latency_present[1]) {
      tmp = (u32 )(connector->video_latency[1] | (connector->audio_latency[1] << 8));
    } else {
      tmp = 0U;
    }
  } else
  if ((int )connector->latency_present[0]) {
    tmp = (u32 )(connector->video_latency[0] | (connector->audio_latency[0] << 8));
  } else {
    tmp = 0U;
  }
  (*(adev->audio_endpt_wreg))(adev, offset, 55U, tmp);
  return;
}
}
static void dce_v8_0_audio_write_speaker_allocation(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 offset ;
  u32 tmp ;
  u8 *sadb ;
  int sad_count ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  sadb = (u8 *)0U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  offset = ((dig->afmt)->pin)->offset;
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_53525;
  ldv_53524: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_53523;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_53525: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_53524;
  } else {
  }
  ldv_53523: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp___0 = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_speaker_allocation(tmp___0, & sadb);
  if (sad_count < 0) {
    drm_err("Couldn\'t read Speaker Allocation Data Block: %d\n", sad_count);
    sad_count = 0;
  } else {
  }
  tmp = (*(adev->audio_endpt_rreg))(adev, offset, 37U);
  tmp = tmp & 4294836096U;
  tmp = tmp | 65536U;
  if (sad_count != 0) {
    tmp = (u32 )*sadb | tmp;
  } else {
    tmp = tmp | 5U;
  }
  (*(adev->audio_endpt_wreg))(adev, offset, 37U, tmp);
  kfree((void const *)sadb);
  return;
}
}
static void dce_v8_0_audio_write_sad_regs(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 offset ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct cea_sad *sads ;
  int i ;
  int sad_count ;
  u16 eld_reg_to_type[12U][2U] ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp ;
  long tmp___0 ;
  u32 value ;
  u8 stereo_freqs ;
  int max_channels ;
  int j ;
  struct cea_sad *sad ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  eld_reg_to_type[0][0] = 40U;
  eld_reg_to_type[0][1] = 1U;
  eld_reg_to_type[1][0] = 41U;
  eld_reg_to_type[1][1] = 2U;
  eld_reg_to_type[2][0] = 42U;
  eld_reg_to_type[2][1] = 3U;
  eld_reg_to_type[3][0] = 43U;
  eld_reg_to_type[3][1] = 4U;
  eld_reg_to_type[4][0] = 44U;
  eld_reg_to_type[4][1] = 5U;
  eld_reg_to_type[5][0] = 45U;
  eld_reg_to_type[5][1] = 6U;
  eld_reg_to_type[6][0] = 46U;
  eld_reg_to_type[6][1] = 7U;
  eld_reg_to_type[7][0] = 47U;
  eld_reg_to_type[7][1] = 8U;
  eld_reg_to_type[8][0] = 49U;
  eld_reg_to_type[8][1] = 10U;
  eld_reg_to_type[9][0] = 50U;
  eld_reg_to_type[9][1] = 11U;
  eld_reg_to_type[10][0] = 51U;
  eld_reg_to_type[10][1] = 12U;
  eld_reg_to_type[11][0] = 53U;
  eld_reg_to_type[11][1] = 14U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  offset = ((dig->afmt)->pin)->offset;
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_53549;
  ldv_53548: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_53547;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_53549: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_53548;
  } else {
  }
  ldv_53547: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_sad(tmp, & sads);
  if (sad_count <= 0) {
    drm_err("Couldn\'t read SADs: %d\n", sad_count);
    return;
  } else {
  }
  tmp___0 = ldv__builtin_expect((unsigned long )sads == (unsigned long )((struct cea_sad *)0),
                             0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/dce_v8_0.c"),
                         "i" (1544), "i" (12UL));
    ldv_53550: ;
    goto ldv_53550;
  } else {
  }
  i = 0;
  goto ldv_53562;
  ldv_53561:
  value = 0U;
  stereo_freqs = 0U;
  max_channels = -1;
  j = 0;
  goto ldv_53560;
  ldv_53559:
  sad = sads + (unsigned long )j;
  if ((int )((unsigned short )sad->format) == (int )eld_reg_to_type[i][1]) {
    if ((int )sad->channels > max_channels) {
      value = (u32 )(((int )sad->channels | ((int )sad->byte2 << 16)) | ((int )sad->freq << 8));
      max_channels = (int )sad->channels;
    } else {
    }
    if ((unsigned int )sad->format == 1U) {
      stereo_freqs = (u8 )((int )sad->freq | (int )stereo_freqs);
    } else {
      goto ldv_53558;
    }
  } else {
  }
  j = j + 1;
  ldv_53560: ;
  if (j < sad_count) {
    goto ldv_53559;
  } else {
  }
  ldv_53558:
  value = (u32 )((int )stereo_freqs << 24) | value;
  (*(adev->audio_endpt_wreg))(adev, offset, (u32 )eld_reg_to_type[i][0], value);
  i = i + 1;
  ldv_53562: ;
  if ((unsigned int )i <= 11U) {
    goto ldv_53561;
  } else {
  }
  kfree((void const *)sads);
  return;
}
}
static void dce_v8_0_audio_enable(struct amdgpu_device *adev , struct amdgpu_audio_pin *pin ,
                                  bool enable )
{
  {
  if ((unsigned long )pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  (*(adev->audio_endpt_wreg))(adev, pin->offset, 84U, (int )enable ? 2147483648U : 0U);
  return;
}
}
static u32 const pin_offsets[7U] = { 0U, 6U, 12U, 18U,
        24U, 29U, 36U};
static int dce_v8_0_audio_init(struct amdgpu_device *adev )
{
  int i ;
  {
  if (amdgpu_audio == 0) {
    return (0);
  } else {
  }
  adev->mode_info.audio.enabled = 1;
  if ((unsigned int )adev->asic_type == 1U) {
    adev->mode_info.audio.num_pins = 7;
  } else
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    adev->mode_info.audio.num_pins = 3;
  } else
  if ((unsigned int )adev->asic_type == 0U || (unsigned int )adev->asic_type == 3U) {
    adev->mode_info.audio.num_pins = 7;
  } else {
    adev->mode_info.audio.num_pins = 3;
  }
  i = 0;
  goto ldv_53575;
  ldv_53574:
  adev->mode_info.audio.pin[i].channels = -1;
  adev->mode_info.audio.pin[i].rate = -1;
  adev->mode_info.audio.pin[i].bits_per_sample = -1;
  adev->mode_info.audio.pin[i].status_bits = 0U;
  adev->mode_info.audio.pin[i].category_code = 0U;
  adev->mode_info.audio.pin[i].connected = 0;
  adev->mode_info.audio.pin[i].offset = pin_offsets[i];
  adev->mode_info.audio.pin[i].id = (u32 )i;
  dce_v8_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                        0);
  i = i + 1;
  ldv_53575: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_53574;
  } else {
  }
  return (0);
}
}
static void dce_v8_0_audio_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  if (! adev->mode_info.audio.enabled) {
    return;
  } else {
  }
  i = 0;
  goto ldv_53582;
  ldv_53581:
  dce_v8_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                        0);
  i = i + 1;
  ldv_53582: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_53581;
  } else {
  }
  adev->mode_info.audio.enabled = 0;
  return;
}
}
static void dce_v8_0_afmt_update_ACR(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_afmt_acr acr ;
  struct amdgpu_afmt_acr tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 offset ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_afmt_acr(clock);
  acr = tmp;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  offset = (u32 )(dig->afmt)->offset;
  amdgpu_mm_wreg(adev, offset + 7223U, (u32 )(acr.cts_32khz << 12), 0);
  amdgpu_mm_wreg(adev, offset + 7224U, (u32 )acr.n_32khz, 0);
  amdgpu_mm_wreg(adev, offset + 7225U, (u32 )(acr.cts_44_1khz << 12), 0);
  amdgpu_mm_wreg(adev, offset + 7226U, (u32 )acr.n_44_1khz, 0);
  amdgpu_mm_wreg(adev, offset + 7227U, (u32 )(acr.cts_48khz << 12), 0);
  amdgpu_mm_wreg(adev, offset + 7228U, (u32 )acr.n_48khz, 0);
  return;
}
}
static void dce_v8_0_afmt_update_avi_infoframe(struct drm_encoder *encoder , void *buffer ,
                                               size_t size )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 offset ;
  uint8_t *frame ;
  uint8_t *header ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  offset = (u32 )(dig->afmt)->offset;
  frame = (uint8_t *)buffer + 3U;
  header = (uint8_t *)buffer;
  amdgpu_mm_wreg(adev, offset + 7201U, (u32 )((((int )*frame | ((int )*(frame + 1UL) << 8)) | ((int )*(frame + 2UL) << 16)) | ((int )*(frame + 3UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, offset + 7202U, (u32 )((((int )*(frame + 4UL) | ((int )*(frame + 5UL) << 8)) | ((int )*(frame + 6UL) << 16)) | ((int )*(frame + 7UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, offset + 7203U, (u32 )((((int )*(frame + 8UL) | ((int )*(frame + 9UL) << 8)) | ((int )*(frame + 10UL) << 16)) | ((int )*(frame + 11UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, offset + 7204U, (u32 )(((int )*(frame + 12UL) | ((int )*(frame + 13UL) << 8)) | ((int )*(header + 1UL) << 24)),
                 0);
  return;
}
}
static void dce_v8_0_audio_set_dto(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  u32 dto_phase ;
  u32 dto_modulo ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  dto_phase = 24000U;
  dto_modulo = clock;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  amdgpu_mm_wreg(adev, 363U, (u32 )amdgpu_crtc->crtc_id, 0);
  amdgpu_mm_wreg(adev, 364U, dto_phase, 0);
  amdgpu_mm_wreg(adev, 365U, dto_modulo, 0);
  return;
}
}
static void dce_v8_0_afmt_setmode(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  u8 buffer[17U] ;
  struct hdmi_avi_infoframe frame ;
  u32 offset ;
  u32 val ;
  ssize_t err ;
  int bpc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  int tmp___3 ;
  u32 tmp_ ;
  u32 tmp___4 ;
  u32 tmp____0 ;
  u32 tmp___5 ;
  u32 tmp____1 ;
  u32 tmp___6 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 8;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if (! (dig->afmt)->enabled) {
    return;
  } else {
  }
  offset = (u32 )(dig->afmt)->offset;
  if ((unsigned long )encoder->crtc != (unsigned long )((struct drm_crtc *)0)) {
    __mptr___0 = (struct drm_crtc const *)encoder->crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    bpc = amdgpu_crtc->bpc;
  } else {
  }
  (dig->afmt)->pin = dce_v8_0_audio_get_pin(adev);
  dce_v8_0_audio_enable(adev, (dig->afmt)->pin, 0);
  dce_v8_0_audio_set_dto(encoder, (u32 )mode->clock);
  amdgpu_mm_wreg(adev, offset + 7184U, 1U, 0);
  amdgpu_mm_wreg(adev, offset + 7235U, 4096U, 0);
  val = amdgpu_mm_rreg(adev, offset + 7180U, 0);
  val = val & 4278190079U;
  val = val & 3489660927U;
  switch (bpc) {
  case 0: ;
  case 6: ;
  case 8: ;
  case 16: ;
  default:
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("dce_v8_0_afmt_setmode", "%s: Disabling hdmi deep color for %d bpc.\n",
                        connector->name, bpc);
  } else {
  }
  goto ldv_53651;
  case 10:
  val = val | 16777216U;
  val = val | 268435456U;
  tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___1 != 0L) {
    drm_ut_debug_printk("dce_v8_0_afmt_setmode", "%s: Enabling hdmi deep color 30 for 10 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_53651;
  case 12:
  val = val | 16777216U;
  val = val | 536870912U;
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v8_0_afmt_setmode", "%s: Enabling hdmi deep color 36 for 12 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_53651;
  }
  ldv_53651:
  amdgpu_mm_wreg(adev, offset + 7180U, val, 0);
  amdgpu_mm_wreg(adev, offset + 7184U, 49U, 0);
  amdgpu_mm_wreg(adev, offset + 7185U, 48U, 0);
  amdgpu_mm_wreg(adev, offset + 7245U, 128U, 0);
  amdgpu_mm_wreg(adev, offset + 7186U, 512U, 0);
  amdgpu_mm_wreg(adev, offset + 7190U, 0U, 0);
  amdgpu_mm_wreg(adev, offset + 7182U, 196624U, 0);
  amdgpu_mm_wreg(adev, offset + 7243U, 67108864U, 0);
  if (bpc > 8) {
    amdgpu_mm_wreg(adev, offset + 7183U, 4096U, 0);
  } else {
    amdgpu_mm_wreg(adev, offset + 7183U, 4352U, 0);
  }
  dce_v8_0_afmt_update_ACR(encoder, (u32 )mode->clock);
  amdgpu_mm_wreg(adev, offset + 7233U, 1048576U, 0);
  amdgpu_mm_wreg(adev, offset + 7234U, 2097152U, 0);
  amdgpu_mm_wreg(adev, offset + 7240U, 8873283U, 0);
  dce_v8_0_audio_write_speaker_allocation(encoder);
  amdgpu_mm_wreg(adev, offset + 7191U, 65280U, 0);
  dce_v8_0_afmt_audio_select_pin(encoder);
  dce_v8_0_audio_write_sad_regs(encoder);
  dce_v8_0_audio_write_latency_fields(encoder, mode);
  tmp___3 = drm_hdmi_avi_infoframe_from_display_mode(& frame, (struct drm_display_mode const *)mode);
  err = (ssize_t )tmp___3;
  if (err < 0L) {
    drm_err("failed to setup AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  err = hdmi_avi_infoframe_pack(& frame, (void *)(& buffer), 17UL);
  if (err < 0L) {
    drm_err("failed to pack AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  dce_v8_0_afmt_update_avi_infoframe(encoder, (void *)(& buffer), 17UL);
  tmp___4 = amdgpu_mm_rreg(adev, offset + 7185U, 0);
  tmp_ = tmp___4;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_ | 1U;
  amdgpu_mm_wreg(adev, offset + 7185U, tmp_, 0);
  tmp___5 = amdgpu_mm_rreg(adev, offset + 7186U, 0);
  tmp____0 = tmp___5;
  tmp____0 = tmp____0 & 4294967232U;
  tmp____0 = tmp____0 | 2U;
  amdgpu_mm_wreg(adev, offset + 7186U, tmp____0, 0);
  tmp___6 = amdgpu_mm_rreg(adev, offset + 7243U, 0);
  tmp____1 = tmp___6;
  tmp____1 = tmp____1 & 4294967294U;
  tmp____1 = tmp____1 | 1U;
  amdgpu_mm_wreg(adev, offset + 7243U, tmp____1, 0);
  amdgpu_mm_wreg(adev, offset + 7236U, 16777215U, 0);
  amdgpu_mm_wreg(adev, offset + 7237U, 8388607U, 0);
  amdgpu_mm_wreg(adev, offset + 7238U, 1U, 0);
  amdgpu_mm_wreg(adev, offset + 7239U, 1U, 0);
  dce_v8_0_audio_enable(adev, (dig->afmt)->pin, 1);
  return;
}
}
static void dce_v8_0_afmt_enable(struct drm_encoder *encoder , bool enable )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  long tmp ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if ((int )enable && (int )(dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && ! (dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && (unsigned long )(dig->afmt)->pin != (unsigned long )((struct amdgpu_audio_pin *)0)) {
    dce_v8_0_audio_enable(adev, (dig->afmt)->pin, 0);
    (dig->afmt)->pin = (struct amdgpu_audio_pin *)0;
  } else {
  }
  (dig->afmt)->enabled = enable;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v8_0_afmt_enable", "%sabling AFMT interface @ 0x%04X for encoder 0x%x\n",
                        (int )enable ? (char *)"En" : (char *)"Dis", (dig->afmt)->offset,
                        amdgpu_encoder->encoder_id);
  } else {
  }
  return;
}
}
static void dce_v8_0_afmt_init(struct amdgpu_device *adev )
{
  int i ;
  void *tmp ;
  {
  i = 0;
  goto ldv_53673;
  ldv_53672:
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_53673: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_53672;
  } else {
  }
  i = 0;
  goto ldv_53676;
  ldv_53675:
  tmp = kzalloc(24UL, 208U);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)tmp;
  if ((unsigned long )adev->mode_info.afmt[i] != (unsigned long )((struct amdgpu_afmt *)0)) {
    (adev->mode_info.afmt[i])->offset = (int )dig_offsets[i];
    (adev->mode_info.afmt[i])->id = i;
  } else {
  }
  i = i + 1;
  ldv_53676: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_53675;
  } else {
  }
  return;
}
}
static void dce_v8_0_afmt_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_53683;
  ldv_53682:
  kfree((void const *)adev->mode_info.afmt[i]);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_53683: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_53682;
  } else {
  }
  return;
}
}
static u32 const vga_control_regs[6U] = { 204U, 206U, 248U, 249U,
        250U, 251U};
static void dce_v8_0_vga_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 vga_control ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_mm_rreg(adev, vga_control_regs[amdgpu_crtc->crtc_id], 0);
  vga_control = tmp & 4294967294U;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, vga_control_regs[amdgpu_crtc->crtc_id], vga_control | 1U,
                   0);
  } else {
    amdgpu_mm_wreg(adev, vga_control_regs[amdgpu_crtc->crtc_id], vga_control, 0);
  }
  return;
}
}
static void dce_v8_0_grph_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 1U, 0);
  } else {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 0U, 0);
  }
  return;
}
}
static int dce_v8_0_crtc_do_set_base(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                     int x , int y , int atomic )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct drm_framebuffer *target_fb ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *rbo ;
  uint64_t fb_location ;
  uint64_t tiling_flags ;
  u32 fb_format ;
  u32 fb_pitch_pixels ;
  u32 fb_swap ;
  u32 pipe_config ;
  u32 tmp ;
  u32 viewport_w ;
  u32 viewport_h ;
  int r ;
  bool bypass_lut ;
  long tmp___0 ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_framebuffer const *__mptr___1 ;
  struct drm_gem_object const *__mptr___2 ;
  long tmp___1 ;
  long tmp___2 ;
  char const *tmp___3 ;
  unsigned int bankw ;
  unsigned int bankh ;
  unsigned int mtaspect ;
  unsigned int tile_split ;
  unsigned int num_banks ;
  u32 tmp_ ;
  u32 tmp___4 ;
  long tmp___5 ;
  struct drm_framebuffer const *__mptr___3 ;
  struct drm_gem_object const *__mptr___4 ;
  long tmp___6 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  fb_swap = 0U;
  bypass_lut = 0;
  if (atomic == 0 && (unsigned long )(crtc->primary)->fb == (unsigned long )((struct drm_framebuffer *)0)) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v8_0_crtc_do_set_base", "No FB bound\n");
    } else {
    }
    return (0);
  } else {
  }
  if (atomic != 0) {
    __mptr___0 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    target_fb = fb;
  } else {
    __mptr___1 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___1;
    target_fb = (crtc->primary)->fb;
  }
  obj = amdgpu_fb->obj;
  __mptr___2 = (struct drm_gem_object const *)obj;
  rbo = (struct amdgpu_bo *)__mptr___2 + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(rbo, 0);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    return (r);
  } else {
  }
  if (atomic != 0) {
    fb_location = amdgpu_bo_gpu_offset(rbo);
  } else {
    r = amdgpu_bo_pin(rbo, 4U, & fb_location);
    tmp___2 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___2 != 0L) {
      amdgpu_bo_unreserve(rbo);
      return (-22);
    } else {
    }
  }
  amdgpu_bo_get_tiling_flags(rbo, & tiling_flags);
  amdgpu_bo_unreserve(rbo);
  pipe_config = (u32 )(tiling_flags >> 4) & 31U;
  switch (target_fb->pixel_format) {
  case 538982467U:
  fb_format = 0U;
  goto ldv_53740;
  case 842093144U: ;
  case 842093121U:
  fb_format = 1U;
  goto ldv_53740;
  case 892424792U: ;
  case 892424769U:
  fb_format = 1U;
  goto ldv_53740;
  case 892426306U: ;
  case 892420418U:
  fb_format = 1281U;
  goto ldv_53740;
  case 909199186U:
  fb_format = 257U;
  goto ldv_53740;
  case 875713112U: ;
  case 875713089U:
  fb_format = 2U;
  goto ldv_53740;
  case 808669784U: ;
  case 808669761U:
  fb_format = 258U;
  bypass_lut = 1;
  goto ldv_53740;
  case 808671298U: ;
  case 808665410U:
  fb_format = 1026U;
  bypass_lut = 1;
  goto ldv_53740;
  default:
  tmp___3 = drm_get_format_name(target_fb->pixel_format);
  drm_err("Unsupported screen format %s\n", tmp___3);
  return (-22);
  }
  ldv_53740: ;
  if ((tiling_flags & 15ULL) == 4ULL) {
    bankw = (unsigned int )(tiling_flags >> 15) & 3U;
    bankh = (unsigned int )(tiling_flags >> 17) & 3U;
    mtaspect = (unsigned int )(tiling_flags >> 19) & 3U;
    tile_split = (unsigned int )(tiling_flags >> 9) & 7U;
    num_banks = (unsigned int )(tiling_flags >> 21) & 3U;
    fb_format = (num_banks << 2) | fb_format;
    fb_format = fb_format | 4194304U;
    fb_format = (tile_split << 13) | fb_format;
    fb_format = (bankw << 6) | fb_format;
    fb_format = (bankh << 11) | fb_format;
    fb_format = (mtaspect << 18) | fb_format;
    fb_format = fb_format;
  } else
  if ((tiling_flags & 15ULL) == 2ULL) {
    fb_format = fb_format | 2097152U;
  } else {
  }
  fb_format = (pipe_config << 24) | fb_format;
  dce_v8_0_vga_enable(crtc, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6657U, fb_format, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6659U, fb_swap, 0);
  tmp___4 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6658U, 0);
  tmp_ = tmp___4;
  tmp_ = tmp_ & 4294967039U;
  tmp_ = ((int )bypass_lut ? 256U : 0U) | tmp_;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6658U, tmp_, 0);
  if ((int )bypass_lut) {
    tmp___5 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___5 != 0L) {
      drm_ut_debug_printk("dce_v8_0_crtc_do_set_base", "Bypassing hardware LUT due to 10 bit fb scanout.\n");
    } else {
    }
  } else {
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6665U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6666U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6667U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6668U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6669U, target_fb->width, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6670U, target_fb->height, 0);
  fb_pitch_pixels = target_fb->pitches[0] / (unsigned int )(target_fb->bits_per_pixel / 8);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6662U, fb_pitch_pixels, 0);
  dce_v8_0_grph_enable(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6851U, target_fb->height, 0);
  x = x & -4;
  y = y & -2;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7004U, (u32 )((x << 16) | y), 0);
  viewport_w = (u32 )crtc->mode.hdisplay;
  viewport_h = (u32 )(crtc->mode.vdisplay + 1) & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7005U, (viewport_w << 16) | viewport_h,
                 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6674U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6674U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7102U, 3U, 0);
  if ((atomic == 0 && (unsigned long )fb != (unsigned long )((struct drm_framebuffer *)0)) && (unsigned long )(crtc->primary)->fb != (unsigned long )fb) {
    __mptr___3 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___3;
    __mptr___4 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___4 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp___6 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___6 != 0L) {
      return (r);
    } else {
    }
    amdgpu_bo_unpin(rbo);
    amdgpu_bo_unreserve(rbo);
  } else {
  }
  dce_v8_0_bandwidth_update(adev);
  return (0);
}
}
static void dce_v8_0_set_interleave(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if ((mode->flags & 16U) != 0U) {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6848U, 3U, 0);
  } else {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6848U, 0U, 0);
  }
  return;
}
}
static void dce_v8_0_crtc_load_lut(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int i ;
  long tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v8_0_crtc_load_lut", "%d\n", amdgpu_crtc->crtc_id);
  } else {
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6709U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6701U, 16U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6705U, 16U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6672U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6784U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6785U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6786U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6787U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6788U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6789U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6790U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6776U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6782U, 7U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6777U, 0U, 0);
  i = 0;
  goto ldv_53785;
  ldv_53784:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6780U, (u32 )((((int )amdgpu_crtc->lut_r[i] << 20) | ((int )amdgpu_crtc->lut_g[i] << 10)) | (int )amdgpu_crtc->lut_b[i]),
                 0);
  i = i + 1;
  ldv_53785: ;
  if (i <= 255) {
    goto ldv_53784;
  } else {
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6744U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6745U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6816U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6716U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6736U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6844U, 2U, 0);
  return;
}
}
static int dce_v8_0_pick_dig_encoder(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  if ((int )dig->linkb) {
    return (1);
  } else {
    return (0);
  }
  case 32U: ;
  if ((int )dig->linkb) {
    return (3);
  } else {
    return (2);
  }
  case 33U: ;
  if ((int )dig->linkb) {
    return (5);
  } else {
    return (4);
  }
  case 37U: ;
  return (6);
  default:
  drm_err("invalid encoder_id: 0x%x\n", amdgpu_encoder->encoder_id);
  return (0);
  }
}
}
static u32 dce_v8_0_pick_pll(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 pll_in_use ;
  int pll ;
  int tmp ;
  int tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
  if (tmp == 0) {
    goto _L;
  } else {
    tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___0 == 5) {
      _L:
      if (adev->clock.dp_extclk != 0U) {
        return (255U);
      } else {
        pll = amdgpu_pll_get_shared_dp_ppll(crtc);
        if (pll != 255) {
          return ((u32 )pll);
        } else {
        }
      }
    } else {
      pll = amdgpu_pll_get_shared_nondp_ppll(crtc);
      if (pll != 255) {
        return ((u32 )pll);
      } else {
      }
    }
  }
  if ((unsigned int )adev->asic_type == 2U || (unsigned int )adev->asic_type == 4U) {
    pll_in_use = amdgpu_pll_get_use_mask(crtc);
    if ((pll_in_use & 2U) == 0U) {
      return (1U);
    } else {
    }
    if ((pll_in_use & 1U) == 0U) {
      return (0U);
    } else {
    }
    drm_err("unable to allocate a PPLL\n");
    return (255U);
  } else {
    pll_in_use = amdgpu_pll_get_use_mask(crtc);
    if ((pll_in_use & 2U) == 0U) {
      return (1U);
    } else {
    }
    if ((pll_in_use & 1U) == 0U) {
      return (0U);
    } else {
    }
    if ((pll_in_use & 4U) == 0U) {
      return (2U);
    } else {
    }
    drm_err("unable to allocate a PPLL\n");
    return (255U);
  }
  return (255U);
}
}
static void dce_v8_0_lock_cursor(struct drm_crtc *crtc , bool lock )
{
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  u32 cur_lock ;
  {
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  cur_lock = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6766U, 0);
  if ((int )lock) {
    cur_lock = cur_lock | 65536U;
  } else {
    cur_lock = cur_lock & 4294901759U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6766U, cur_lock, 0);
  return;
}
}
static void dce_v8_0_hide_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, 67109376U, 1);
  return;
}
}
static void dce_v8_0_show_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, 67109377U, 1);
  return;
}
}
static void dce_v8_0_set_cursor(struct drm_crtc *crtc , struct drm_gem_object *obj ,
                                uint64_t gpu_addr )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6761U, (unsigned int )(gpu_addr >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6759U, (u32 )gpu_addr, 0);
  return;
}
}
static int dce_v8_0_crtc_cursor_move(struct drm_crtc *crtc , int x , int y )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  int xorigin ;
  int yorigin ;
  long tmp ;
  int _min1 ;
  int _min2 ;
  int _min1___0 ;
  int _min2___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  xorigin = 0;
  yorigin = 0;
  x = crtc->x + x;
  y = crtc->y + y;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v8_0_crtc_cursor_move", "x %d y %d c->x %d c->y %d\n",
                        x, y, crtc->x, crtc->y);
  } else {
  }
  if (x < 0) {
    _min1 = - x;
    _min2 = amdgpu_crtc->max_cursor_width + -1;
    xorigin = _min1 < _min2 ? _min1 : _min2;
    x = 0;
  } else {
  }
  if (y < 0) {
    _min1___0 = - y;
    _min2___0 = amdgpu_crtc->max_cursor_height + -1;
    yorigin = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    y = 0;
  } else {
  }
  dce_v8_0_lock_cursor(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6762U, (u32 )((x << 16) | y), 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6763U, (u32 )((xorigin << 16) | yorigin),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6760U, (u32 )(((amdgpu_crtc->cursor_width + -1) << 16) | (amdgpu_crtc->cursor_height + -1)),
                 0);
  dce_v8_0_lock_cursor(crtc, 0);
  return (0);
}
}
static int dce_v8_0_crtc_cursor_set(struct drm_crtc *crtc , struct drm_file *file_priv ,
                                    u32 handle , u32 width , u32 height )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *robj ;
  uint64_t gpu_addr ;
  int ret ;
  struct drm_gem_object const *__mptr___0 ;
  long tmp ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (handle == 0U) {
    dce_v8_0_hide_cursor(crtc);
    obj = (struct drm_gem_object *)0;
    goto unpin;
  } else {
  }
  if ((u32 )amdgpu_crtc->max_cursor_width < width || (u32 )amdgpu_crtc->max_cursor_height < height) {
    drm_err("bad cursor width or height %d x %d\n", width, height);
    return (-22);
  } else {
  }
  obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    drm_err("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
    return (-2);
  } else {
  }
  __mptr___0 = (struct drm_gem_object const *)obj;
  robj = (struct amdgpu_bo *)__mptr___0 + 0xfffffffffffffbc0UL;
  ret = amdgpu_bo_reserve(robj, 0);
  tmp = ldv__builtin_expect(ret != 0, 0L);
  if (tmp != 0L) {
    goto fail;
  } else {
  }
  ret = amdgpu_bo_pin_restricted(robj, 4U, 0ULL, 0ULL, & gpu_addr);
  amdgpu_bo_unreserve(robj);
  if (ret != 0) {
    goto fail;
  } else {
  }
  amdgpu_crtc->cursor_width = (int )width;
  amdgpu_crtc->cursor_height = (int )height;
  dce_v8_0_lock_cursor(crtc, 1);
  dce_v8_0_set_cursor(crtc, obj, gpu_addr);
  dce_v8_0_show_cursor(crtc);
  dce_v8_0_lock_cursor(crtc, 0);
  unpin: ;
  if ((unsigned long )amdgpu_crtc->cursor_bo != (unsigned long )((struct drm_gem_object *)0)) {
    __mptr___1 = (struct drm_gem_object const *)amdgpu_crtc->cursor_bo;
    robj = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    ret = amdgpu_bo_reserve(robj, 0);
    tmp___0 = ldv__builtin_expect(ret == 0, 1L);
    if (tmp___0 != 0L) {
      amdgpu_bo_unpin(robj);
      amdgpu_bo_unreserve(robj);
    } else {
    }
    drm_gem_object_unreference_unlocked___4(amdgpu_crtc->cursor_bo);
  } else {
  }
  amdgpu_crtc->cursor_bo = obj;
  return (0);
  fail:
  drm_gem_object_unreference_unlocked___4(obj);
  return (ret);
}
}
static void dce_v8_0_crtc_gamma_set(struct drm_crtc *crtc , u16 *red , u16 *green ,
                                    u16 *blue , u32 start , u32 size )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  int end ;
  int i ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  end = (int )(256U < start + size ? 256U : start + size);
  i = (int )start;
  goto ldv_53893;
  ldv_53892:
  amdgpu_crtc->lut_r[i] = (u16 )((int )*(red + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_g[i] = (u16 )((int )*(green + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_b[i] = (u16 )((int )*(blue + (unsigned long )i) >> 6);
  i = i + 1;
  ldv_53893: ;
  if (i < end) {
    goto ldv_53892;
  } else {
  }
  dce_v8_0_crtc_load_lut(crtc);
  return;
}
}
static void dce_v8_0_crtc_destroy(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  drm_crtc_cleanup(crtc);
  ldv_destroy_workqueue_626(amdgpu_crtc->pflip_queue);
  kfree((void const *)amdgpu_crtc);
  return;
}
}
static struct drm_crtc_funcs const dce_v8_0_crtc_funcs =
     {0, 0, 0, & dce_v8_0_crtc_cursor_set, 0, & dce_v8_0_crtc_cursor_move, & dce_v8_0_crtc_gamma_set,
    & dce_v8_0_crtc_destroy, & amdgpu_crtc_set_config, & amdgpu_crtc_page_flip, 0,
    0, 0, 0, 0};
static void dce_v8_0_crtc_dpms(struct drm_crtc *crtc , int mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  switch (mode) {
  case 0:
  amdgpu_crtc->enabled = 1;
  amdgpu_atombios_crtc_enable(crtc, 1);
  dce_v8_0_vga_enable(crtc, 1);
  amdgpu_atombios_crtc_blank(crtc, 0);
  dce_v8_0_vga_enable(crtc, 0);
  drm_vblank_post_modeset(dev, amdgpu_crtc->crtc_id);
  dce_v8_0_crtc_load_lut(crtc);
  goto ldv_53912;
  case 1: ;
  case 2: ;
  case 3:
  drm_vblank_pre_modeset(dev, amdgpu_crtc->crtc_id);
  if ((int )amdgpu_crtc->enabled) {
    dce_v8_0_vga_enable(crtc, 1);
    amdgpu_atombios_crtc_blank(crtc, 1);
    dce_v8_0_vga_enable(crtc, 0);
  } else {
  }
  amdgpu_atombios_crtc_enable(crtc, 0);
  amdgpu_crtc->enabled = 0;
  goto ldv_53912;
  }
  ldv_53912:
  amdgpu_pm_compute_clocks(adev);
  return;
}
}
static void dce_v8_0_crtc_prepare(struct drm_crtc *crtc )
{
  {
  amdgpu_atombios_crtc_powergate(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 1);
  dce_v8_0_crtc_dpms(crtc, 3);
  return;
}
}
static void dce_v8_0_crtc_commit(struct drm_crtc *crtc )
{
  {
  dce_v8_0_crtc_dpms(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 0);
  return;
}
}
static void dce_v8_0_crtc_disable(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_atom_ss ss ;
  int i ;
  int r ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct amdgpu_bo *rbo ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  dce_v8_0_crtc_dpms(crtc, 3);
  if ((unsigned long )(crtc->primary)->fb != (unsigned long )((struct drm_framebuffer *)0)) {
    __mptr___0 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    __mptr___1 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      drm_err("failed to reserve rbo before unpin\n");
    } else {
      amdgpu_bo_unpin(rbo);
      amdgpu_bo_unreserve(rbo);
    }
  } else {
  }
  dce_v8_0_grph_enable(crtc, 0);
  amdgpu_atombios_crtc_powergate(crtc, 1);
  i = 0;
  goto ldv_53941;
  ldv_53940: ;
  if ((((unsigned long )adev->mode_info.crtcs[i] != (unsigned long )((struct amdgpu_crtc *)0) && (int )(adev->mode_info.crtcs[i])->enabled) && amdgpu_crtc->crtc_id != i) && amdgpu_crtc->pll_id == (adev->mode_info.crtcs[i])->pll_id) {
    goto done;
  } else {
  }
  i = i + 1;
  ldv_53941: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_53940;
  } else {
  }
  switch (amdgpu_crtc->pll_id) {
  case 0U: ;
  case 1U:
  amdgpu_atombios_crtc_program_pll(crtc, (u32 )amdgpu_crtc->crtc_id, (int )amdgpu_crtc->pll_id,
                                   0U, 0U, 0U, 0U, 0U, 0U, 0U, 0, 0, & ss);
  goto ldv_53945;
  case 2U: ;
  if (((unsigned int )adev->asic_type == 1U || (unsigned int )adev->asic_type == 0U) || (unsigned int )adev->asic_type == 3U) {
    amdgpu_atombios_crtc_program_pll(crtc, (u32 )amdgpu_crtc->crtc_id, (int )amdgpu_crtc->pll_id,
                                     0U, 0U, 0U, 0U, 0U, 0U, 0U, 0, 0, & ss);
  } else {
  }
  goto ldv_53945;
  default: ;
  goto ldv_53945;
  }
  ldv_53945: ;
  done:
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  return;
}
}
static int dce_v8_0_crtc_mode_set(struct drm_crtc *crtc , struct drm_display_mode *mode ,
                                  struct drm_display_mode *adjusted_mode , int x ,
                                  int y , struct drm_framebuffer *old_fb )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (amdgpu_crtc->adjusted_clock == 0U) {
    return (-22);
  } else {
  }
  amdgpu_atombios_crtc_set_pll(crtc, adjusted_mode);
  amdgpu_atombios_crtc_set_dtd_timing(crtc, adjusted_mode);
  dce_v8_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  amdgpu_atombios_crtc_overscan_setup(crtc, mode, adjusted_mode);
  amdgpu_atombios_crtc_scaler_setup(crtc);
  amdgpu_crtc->hw_mode = *adjusted_mode;
  return (0);
}
}
static bool dce_v8_0_crtc_mode_fixup(struct drm_crtc *crtc , struct drm_display_mode const *mode ,
                                     struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  __mptr___0 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___0 + 0xfffffffffffffff8UL;
  goto ldv_53975;
  ldv_53974: ;
  if ((unsigned long )encoder->crtc == (unsigned long )crtc) {
    amdgpu_crtc->encoder = encoder;
    amdgpu_crtc->connector = amdgpu_get_connector_for_encoder(encoder);
    goto ldv_53973;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_53975: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_53974;
  } else {
  }
  ldv_53973: ;
  if ((unsigned long )amdgpu_crtc->encoder == (unsigned long )((struct drm_encoder *)0) || (unsigned long )amdgpu_crtc->connector == (unsigned long )((struct drm_connector *)0)) {
    amdgpu_crtc->encoder = (struct drm_encoder *)0;
    amdgpu_crtc->connector = (struct drm_connector *)0;
    return (0);
  } else {
  }
  tmp = amdgpu_crtc_scaling_mode_fixup(crtc, mode, adjusted_mode);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (0);
  } else {
  }
  tmp___1 = amdgpu_atombios_crtc_prepare_pll(crtc, adjusted_mode);
  if (tmp___1 != 0) {
    return (0);
  } else {
  }
  amdgpu_crtc->pll_id = dce_v8_0_pick_pll(crtc);
  if (amdgpu_crtc->pll_id == 255U) {
    tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___2 != 0) {
      tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
      if (tmp___3 != 5) {
        return (0);
      } else {
      }
    } else {
    }
  } else {
  }
  return (1);
}
}
static int dce_v8_0_crtc_set_base(struct drm_crtc *crtc , int x , int y , struct drm_framebuffer *old_fb )
{
  int tmp ;
  {
  tmp = dce_v8_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  return (tmp);
}
}
static int dce_v8_0_crtc_set_base_atomic(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                         int x , int y , enum mode_set_atomic state )
{
  int tmp ;
  {
  tmp = dce_v8_0_crtc_do_set_base(crtc, fb, x, y, 1);
  return (tmp);
}
}
static struct drm_crtc_helper_funcs const dce_v8_0_crtc_helper_funcs =
     {& dce_v8_0_crtc_dpms, & dce_v8_0_crtc_prepare, & dce_v8_0_crtc_commit, & dce_v8_0_crtc_mode_fixup,
    & dce_v8_0_crtc_mode_set, 0, & dce_v8_0_crtc_set_base, & dce_v8_0_crtc_set_base_atomic,
    & dce_v8_0_crtc_load_lut, & dce_v8_0_crtc_disable, 0, 0, 0, 0};
static int dce_v8_0_crtc_init(struct amdgpu_device *adev , int index )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  int i ;
  void *tmp ;
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp___0 ;
  {
  tmp = kzalloc(3312UL, 208U);
  amdgpu_crtc = (struct amdgpu_crtc *)tmp;
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (-12);
  } else {
  }
  drm_crtc_init(adev->ddev, & amdgpu_crtc->base, & dce_v8_0_crtc_funcs);
  drm_mode_crtc_set_gamma_size(& amdgpu_crtc->base, 256);
  amdgpu_crtc->crtc_id = index;
  __lock_name = "\"%s\"\"amdgpu-pageflip-queue\"";
  tmp___0 = __alloc_workqueue_key("%s", 131082U, 1, & __key, __lock_name, (char *)"amdgpu-pageflip-queue");
  amdgpu_crtc->pflip_queue = tmp___0;
  adev->mode_info.crtcs[index] = amdgpu_crtc;
  amdgpu_crtc->max_cursor_width = 128;
  amdgpu_crtc->max_cursor_height = 128;
  (adev->ddev)->mode_config.cursor_width = (u32 )amdgpu_crtc->max_cursor_width;
  (adev->ddev)->mode_config.cursor_height = (u32 )amdgpu_crtc->max_cursor_height;
  i = 0;
  goto ldv_54000;
  ldv_53999:
  amdgpu_crtc->lut_r[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_g[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_b[i] = (int )((u16 )i) << 2U;
  i = i + 1;
  ldv_54000: ;
  if (i <= 255) {
    goto ldv_53999;
  } else {
  }
  amdgpu_crtc->crtc_offset = crtc_offsets[amdgpu_crtc->crtc_id];
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  drm_crtc_helper_add(& amdgpu_crtc->base, & dce_v8_0_crtc_helper_funcs);
  return (0);
}
}
static int dce_v8_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->audio_endpt_rreg = & dce_v8_0_audio_endpt_rreg;
  adev->audio_endpt_wreg = & dce_v8_0_audio_endpt_wreg;
  dce_v8_0_set_display_funcs(adev);
  dce_v8_0_set_irq_funcs(adev);
  switch ((unsigned int )adev->asic_type) {
  case 0U: ;
  case 3U:
  adev->mode_info.num_crtc = 6;
  adev->mode_info.num_hpd = 6;
  adev->mode_info.num_dig = 6;
  goto ldv_54008;
  case 1U:
  adev->mode_info.num_crtc = 4;
  adev->mode_info.num_hpd = 6;
  adev->mode_info.num_dig = 7;
  goto ldv_54008;
  case 2U: ;
  case 4U:
  adev->mode_info.num_crtc = 2;
  adev->mode_info.num_hpd = 6;
  adev->mode_info.num_dig = 6;
  goto ldv_54008;
  default: ;
  return (-22);
  }
  ldv_54008: ;
  return (0);
}
}
static int dce_v8_0_sw_init(void *handle )
{
  int r ;
  int i ;
  struct amdgpu_device *adev ;
  bool tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0;
  goto ldv_54020;
  ldv_54019:
  r = amdgpu_irq_add_id(adev, (unsigned int )(i + 1), & adev->crtc_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_54020: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54019;
  } else {
  }
  i = 8;
  goto ldv_54023;
  ldv_54022:
  r = amdgpu_irq_add_id(adev, (unsigned int )i, & adev->pageflip_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 2;
  ldv_54023: ;
  if (i <= 19) {
    goto ldv_54022;
  } else {
  }
  r = amdgpu_irq_add_id(adev, 42U, & adev->hpd_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->mode_info.mode_config_initialized = 1;
  (adev->ddev)->mode_config.funcs = & amdgpu_mode_funcs;
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  (adev->ddev)->mode_config.preferred_depth = 24U;
  (adev->ddev)->mode_config.prefer_shadow = 1U;
  (adev->ddev)->mode_config.fb_base = adev->mc.aper_base;
  r = amdgpu_modeset_create_props(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  i = 0;
  goto ldv_54026;
  ldv_54025:
  r = dce_v8_0_crtc_init(adev, i);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_54026: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54025;
  } else {
  }
  tmp = amdgpu_atombios_get_connector_info_from_object_table(adev);
  if ((int )tmp) {
    amdgpu_print_display_setup(adev->ddev);
  } else {
    return (-22);
  }
  dce_v8_0_afmt_init(adev);
  r = dce_v8_0_audio_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  drm_kms_helper_poll_init(adev->ddev);
  return (r);
}
}
static int dce_v8_0_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  kfree((void const *)adev->mode_info.bios_hardcoded_edid);
  drm_kms_helper_poll_fini(adev->ddev);
  dce_v8_0_audio_fini(adev);
  dce_v8_0_afmt_fini(adev);
  drm_mode_config_cleanup(adev->ddev);
  adev->mode_info.mode_config_initialized = 0;
  return (0);
}
}
static int dce_v8_0_hw_init(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  dce_v8_0_hpd_init(adev);
  i = 0;
  goto ldv_54038;
  ldv_54037:
  dce_v8_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                        0);
  i = i + 1;
  ldv_54038: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54037;
  } else {
  }
  return (0);
}
}
static int dce_v8_0_hw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v8_0_hpd_fini(adev);
  i = 0;
  goto ldv_54046;
  ldv_54045:
  dce_v8_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                        0);
  i = i + 1;
  ldv_54046: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54045;
  } else {
  }
  return (0);
}
}
static int dce_v8_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_atombios_scratch_regs_save(adev);
  dce_v8_0_hpd_fini(adev);
  return (0);
}
}
static int dce_v8_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  u8 bl_level ;
  u8 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_atombios_scratch_regs_restore(adev);
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  if ((unsigned long )adev->mode_info.bl_encoder != (unsigned long )((struct amdgpu_encoder *)0)) {
    tmp = (*((adev->mode_info.funcs)->backlight_get_level))(adev->mode_info.bl_encoder);
    bl_level = tmp;
    (*((adev->mode_info.funcs)->backlight_set_level))(adev->mode_info.bl_encoder,
                                                      (int )bl_level);
  } else {
  }
  dce_v8_0_hpd_init(adev);
  return (0);
}
}
static bool dce_v8_0_is_idle(void *handle )
{
  {
  return (1);
}
}
static int dce_v8_0_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void dce_v8_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "DCE 8.x registers\n");
  return;
}
}
static int dce_v8_0_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  bool tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = dce_v8_0_is_display_hung(adev);
  if ((int )tmp___0) {
    srbm_soft_reset = srbm_soft_reset | 32U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    dce_v8_0_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    dce_v8_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static void dce_v8_0_set_crtc_vblank_interrupt_state(struct amdgpu_device *adev ,
                                                     int crtc , enum amdgpu_interrupt_state state )
{
  u32 reg_block ;
  u32 lb_interrupt_mask ;
  long tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v8_0_set_crtc_vblank_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch (crtc) {
  case 0:
  reg_block = 0U;
  goto ldv_54082;
  case 1:
  reg_block = 768U;
  goto ldv_54082;
  case 2:
  reg_block = 9728U;
  goto ldv_54082;
  case 3:
  reg_block = 10496U;
  goto ldv_54082;
  case 4:
  reg_block = 11264U;
  goto ldv_54082;
  case 5:
  reg_block = 12032U;
  goto ldv_54082;
  default:
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("dce_v8_0_set_crtc_vblank_interrupt_state", "invalid crtc %d\n",
                        crtc);
  } else {
  }
  return;
  }
  ldv_54082: ;
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, reg_block + 6856U, 0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967294U;
  amdgpu_mm_wreg(adev, reg_block + 6856U, lb_interrupt_mask, 0);
  goto ldv_54090;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, reg_block + 6856U, 0);
  lb_interrupt_mask = lb_interrupt_mask | 1U;
  amdgpu_mm_wreg(adev, reg_block + 6856U, lb_interrupt_mask, 0);
  goto ldv_54090;
  default: ;
  goto ldv_54090;
  }
  ldv_54090: ;
  return;
}
}
static void dce_v8_0_set_crtc_vline_interrupt_state(struct amdgpu_device *adev , int crtc ,
                                                    enum amdgpu_interrupt_state state )
{
  u32 reg_block ;
  u32 lb_interrupt_mask ;
  long tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v8_0_set_crtc_vline_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch (crtc) {
  case 0:
  reg_block = 0U;
  goto ldv_54102;
  case 1:
  reg_block = 768U;
  goto ldv_54102;
  case 2:
  reg_block = 9728U;
  goto ldv_54102;
  case 3:
  reg_block = 10496U;
  goto ldv_54102;
  case 4:
  reg_block = 11264U;
  goto ldv_54102;
  case 5:
  reg_block = 12032U;
  goto ldv_54102;
  default:
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("dce_v8_0_set_crtc_vline_interrupt_state", "invalid crtc %d\n",
                        crtc);
  } else {
  }
  return;
  }
  ldv_54102: ;
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, reg_block + 6856U, 0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967279U;
  amdgpu_mm_wreg(adev, reg_block + 6856U, lb_interrupt_mask, 0);
  goto ldv_54110;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, reg_block + 6856U, 0);
  lb_interrupt_mask = lb_interrupt_mask | 16U;
  amdgpu_mm_wreg(adev, reg_block + 6856U, lb_interrupt_mask, 0);
  goto ldv_54110;
  default: ;
  goto ldv_54110;
  }
  ldv_54110: ;
  return;
}
}
static int dce_v8_0_set_hpd_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                            unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 dc_hpd_int_cntl_reg ;
  u32 dc_hpd_int_cntl ;
  long tmp ;
  {
  switch (type) {
  case 0U:
  dc_hpd_int_cntl_reg = 6152U;
  goto ldv_54122;
  case 1U:
  dc_hpd_int_cntl_reg = 6155U;
  goto ldv_54122;
  case 2U:
  dc_hpd_int_cntl_reg = 6158U;
  goto ldv_54122;
  case 3U:
  dc_hpd_int_cntl_reg = 6161U;
  goto ldv_54122;
  case 4U:
  dc_hpd_int_cntl_reg = 6164U;
  goto ldv_54122;
  case 5U:
  dc_hpd_int_cntl_reg = 6167U;
  goto ldv_54122;
  default:
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v8_0_set_hpd_interrupt_state", "invalid hdp %d\n", type);
  } else {
  }
  return (0);
  }
  ldv_54122: ;
  switch ((unsigned int )state) {
  case 0U:
  dc_hpd_int_cntl = amdgpu_mm_rreg(adev, dc_hpd_int_cntl_reg, 0);
  dc_hpd_int_cntl = dc_hpd_int_cntl & 4294901759U;
  amdgpu_mm_wreg(adev, dc_hpd_int_cntl_reg, dc_hpd_int_cntl, 0);
  goto ldv_54131;
  case 1U:
  dc_hpd_int_cntl = amdgpu_mm_rreg(adev, dc_hpd_int_cntl_reg, 0);
  dc_hpd_int_cntl = dc_hpd_int_cntl | 65536U;
  amdgpu_mm_wreg(adev, dc_hpd_int_cntl_reg, dc_hpd_int_cntl, 0);
  goto ldv_54131;
  default: ;
  goto ldv_54131;
  }
  ldv_54131: ;
  return (0);
}
}
static int dce_v8_0_set_crtc_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                             unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  switch (type) {
  case 0U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 0, state);
  goto ldv_54141;
  case 1U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 1, state);
  goto ldv_54141;
  case 2U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 2, state);
  goto ldv_54141;
  case 3U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 3, state);
  goto ldv_54141;
  case 4U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 4, state);
  goto ldv_54141;
  case 5U:
  dce_v8_0_set_crtc_vblank_interrupt_state(adev, 5, state);
  goto ldv_54141;
  case 6U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 0, state);
  goto ldv_54141;
  case 7U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 1, state);
  goto ldv_54141;
  case 8U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 2, state);
  goto ldv_54141;
  case 9U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 3, state);
  goto ldv_54141;
  case 10U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 4, state);
  goto ldv_54141;
  case 11U:
  dce_v8_0_set_crtc_vline_interrupt_state(adev, 5, state);
  goto ldv_54141;
  default: ;
  goto ldv_54141;
  }
  ldv_54141: ;
  return (0);
}
}
static int dce_v8_0_crtc_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                             struct amdgpu_iv_entry *entry )
{
  unsigned int crtc ;
  u32 disp_int ;
  u32 tmp ;
  unsigned int irq_type ;
  int tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  {
  crtc = entry->src_id - 1U;
  tmp = amdgpu_mm_rreg(adev, interrupt_status_offsets[crtc].reg, 0);
  disp_int = tmp;
  tmp___0 = amdgpu_crtc_idx_to_irq_type(adev, (int )crtc);
  irq_type = (unsigned int )tmp___0;
  switch (entry->src_data) {
  case 0U: ;
  if (((u32 )interrupt_status_offsets[crtc].vblank & disp_int) != 0U) {
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[crtc] + 6859U, 16U, 0);
    tmp___1 = amdgpu_irq_enabled(adev, source, irq_type);
    if ((int )tmp___1) {
      drm_handle_vblank(adev->ddev, (int )crtc);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("dce_v8_0_crtc_irq", "IH: D%d vblank\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_54164;
  case 1U: ;
  if (((u32 )interrupt_status_offsets[crtc].vline & disp_int) != 0U) {
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets[crtc] + 6857U, 16U, 0);
    tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___3 != 0L) {
      drm_ut_debug_printk("dce_v8_0_crtc_irq", "IH: D%d vline\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_54164;
  default:
  tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___4 != 0L) {
    drm_ut_debug_printk("dce_v8_0_crtc_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                        entry->src_data);
  } else {
  }
  goto ldv_54164;
  }
  ldv_54164: ;
  return (0);
}
}
static int dce_v8_0_set_pageflip_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                                 unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 reg ;
  u32 reg_block ;
  {
  switch (type) {
  case 0U:
  reg_block = 0U;
  goto ldv_54176;
  case 1U:
  reg_block = 768U;
  goto ldv_54176;
  case 2U:
  reg_block = 9728U;
  goto ldv_54176;
  case 3U:
  reg_block = 10496U;
  goto ldv_54176;
  case 4U:
  reg_block = 11264U;
  goto ldv_54176;
  case 5U:
  reg_block = 12032U;
  goto ldv_54176;
  default:
  drm_err("invalid pageflip crtc %d\n", type);
  return (-22);
  }
  ldv_54176:
  reg = amdgpu_mm_rreg(adev, reg_block + 6679U, 0);
  if ((unsigned int )state == 0U) {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg & 4294967294U, 0);
  } else {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg | 1U, 0);
  }
  return (0);
}
}
static int dce_v8_0_pageflip_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                 struct amdgpu_iv_entry *entry )
{
  int reg_block ;
  unsigned long flags ;
  unsigned int crtc_id ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct amdgpu_flip_work *works ;
  u32 tmp ;
  raw_spinlock_t *tmp___0 ;
  long tmp___1 ;
  {
  crtc_id = (entry->src_id - 8U) >> 1;
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  switch (crtc_id) {
  case 0U:
  reg_block = 0;
  goto ldv_54194;
  case 1U:
  reg_block = 768;
  goto ldv_54194;
  case 2U:
  reg_block = 9728;
  goto ldv_54194;
  case 3U:
  reg_block = 10496;
  goto ldv_54194;
  case 4U:
  reg_block = 11264;
  goto ldv_54194;
  case 5U:
  reg_block = 12032;
  goto ldv_54194;
  default:
  drm_err("invalid pageflip crtc %d\n", crtc_id);
  return (-22);
  }
  ldv_54194:
  tmp = amdgpu_mm_rreg(adev, (u32 )(reg_block + 6678), 0);
  if ((int )tmp & 1) {
    amdgpu_mm_wreg(adev, (u32 )(reg_block + 6678), 256U, 0);
  } else {
  }
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (0);
  } else {
  }
  tmp___0 = spinlock_check(& (adev->ddev)->event_lock);
  flags = _raw_spin_lock_irqsave(tmp___0);
  works = amdgpu_crtc->pflip_works;
  if ((unsigned int )amdgpu_crtc->pflip_status != 2U) {
    tmp___1 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("dce_v8_0_pageflip_irq", "amdgpu_crtc->pflip_status = %d != AMDGPU_FLIP_SUBMITTED(%d)\n",
                          (unsigned int )amdgpu_crtc->pflip_status, 2);
    } else {
    }
    spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
    return (0);
  } else {
  }
  amdgpu_crtc->pflip_status = 0;
  amdgpu_crtc->pflip_works = (struct amdgpu_flip_work *)0;
  if ((unsigned long )works->event != (unsigned long )((struct drm_pending_vblank_event *)0)) {
    drm_send_vblank_event(adev->ddev, (int )crtc_id, works->event);
  } else {
  }
  spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
  drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
  amdgpu_irq_put(adev, & adev->pageflip_irq, crtc_id);
  queue_work___2(amdgpu_crtc->pflip_queue, & works->unpin_work);
  return (0);
}
}
static int dce_v8_0_hpd_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                            struct amdgpu_iv_entry *entry )
{
  u32 disp_int ;
  u32 mask ;
  u32 int_control ;
  u32 tmp ;
  unsigned int hpd ;
  long tmp___0 ;
  long tmp___1 ;
  {
  if (entry->src_data >= (unsigned int )adev->mode_info.num_hpd) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v8_0_hpd_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                          entry->src_data);
    } else {
    }
    return (0);
  } else {
  }
  hpd = entry->src_data;
  disp_int = amdgpu_mm_rreg(adev, interrupt_status_offsets[hpd].reg, 0);
  mask = interrupt_status_offsets[hpd].hpd;
  int_control = hpd_int_control_offsets[hpd];
  if ((disp_int & mask) != 0U) {
    tmp = amdgpu_mm_rreg(adev, int_control, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, int_control, tmp, 0);
    schedule_work___1(& adev->hotplug_work);
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("dce_v8_0_hpd_irq", "IH: HPD%d\n", hpd + 1U);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int dce_v8_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int dce_v8_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const dce_v8_0_ip_funcs =
     {& dce_v8_0_early_init, (int (*)(void * ))0, & dce_v8_0_sw_init, & dce_v8_0_sw_fini,
    & dce_v8_0_hw_init, & dce_v8_0_hw_fini, & dce_v8_0_suspend, & dce_v8_0_resume,
    & dce_v8_0_is_idle, & dce_v8_0_wait_for_idle, & dce_v8_0_soft_reset, & dce_v8_0_print_status,
    & dce_v8_0_set_clockgating_state, & dce_v8_0_set_powergating_state};
static void dce_v8_0_encoder_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                      struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  int tmp ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_encoder->pixel_clock = (u32 )adjusted_mode->clock;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  dce_v8_0_set_interleave(encoder->crtc, mode);
  tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  if (tmp == 3) {
    dce_v8_0_afmt_enable(encoder, 1);
    dce_v8_0_afmt_setmode(encoder, adjusted_mode);
  } else {
  }
  return;
}
}
static void dce_v8_0_encoder_prepare(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  struct amdgpu_encoder_atom_dig *dig ;
  u16 tmp___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  if (((long )amdgpu_encoder->active_device & 3818L) != 0L) {
    goto _L;
  } else {
    tmp___0 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    if ((unsigned int )tmp___0 != 0U) {
      _L:
      dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
      if ((unsigned long )dig != (unsigned long )((struct amdgpu_encoder_atom_dig *)0)) {
        dig->dig_encoder = dce_v8_0_pick_dig_encoder(encoder);
        if (((long )amdgpu_encoder->active_device & 3784L) != 0L) {
          dig->afmt = adev->mode_info.afmt[dig->dig_encoder];
        } else {
        }
      } else {
      }
    } else {
    }
  }
  amdgpu_atombios_scratch_regs_lock(adev, 1);
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    if ((int )amdgpu_connector->router.cd_valid) {
      amdgpu_i2c_router_select_cd_port(amdgpu_connector);
    } else {
    }
    if (connector->connector_type == 14) {
      amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
    } else {
    }
  } else {
  }
  amdgpu_atombios_encoder_set_crtc_source(encoder);
  dce_v8_0_program_fmt(encoder);
  return;
}
}
static void dce_v8_0_encoder_commit(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_atombios_encoder_dpms(encoder, 0);
  amdgpu_atombios_scratch_regs_lock(adev, 0);
  return;
}
}
static void dce_v8_0_encoder_disable(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  int tmp ;
  bool tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  tmp___0 = amdgpu_atombios_encoder_is_digital(encoder);
  if ((int )tmp___0) {
    tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp == 3) {
      dce_v8_0_afmt_enable(encoder, 0);
    } else {
    }
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    dig->dig_encoder = -1;
  } else {
  }
  amdgpu_encoder->active_device = 0U;
  return;
}
}
static void dce_v8_0_ext_prepare(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v8_0_ext_commit(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v8_0_ext_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                  struct drm_display_mode *adjusted_mode )
{
  {
  return;
}
}
static void dce_v8_0_ext_disable(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v8_0_ext_dpms(struct drm_encoder *encoder , int mode )
{
  {
  return;
}
}
static bool dce_v8_0_ext_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode const *mode ,
                                    struct drm_display_mode *adjusted_mode )
{
  {
  return (1);
}
}
static struct drm_encoder_helper_funcs const dce_v8_0_ext_helper_funcs =
     {& dce_v8_0_ext_dpms, 0, 0, & dce_v8_0_ext_mode_fixup, & dce_v8_0_ext_prepare,
    & dce_v8_0_ext_commit, & dce_v8_0_ext_mode_set, 0, 0, & dce_v8_0_ext_disable,
    0, 0};
static struct drm_encoder_helper_funcs const dce_v8_0_dig_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v8_0_encoder_prepare,
    & dce_v8_0_encoder_commit, & dce_v8_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dig_detect,
    & dce_v8_0_encoder_disable, 0, 0};
static struct drm_encoder_helper_funcs const dce_v8_0_dac_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v8_0_encoder_prepare,
    & dce_v8_0_encoder_commit, & dce_v8_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dac_detect,
    0, 0, 0};
static void dce_v8_0_encoder_destroy(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_atombios_encoder_fini_backlight(amdgpu_encoder);
  } else {
  }
  kfree((void const *)amdgpu_encoder->enc_priv);
  drm_encoder_cleanup(encoder);
  kfree((void const *)amdgpu_encoder);
  return;
}
}
static struct drm_encoder_funcs const dce_v8_0_encoder_funcs = {0, & dce_v8_0_encoder_destroy};
static void dce_v8_0_encoder_add(struct amdgpu_device *adev , u32 encoder_enum , u32 supported_device ,
                                 u16 caps )
{
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct list_head const *__mptr ;
  struct drm_encoder const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  void *tmp ;
  struct amdgpu_encoder_atom_dig *tmp___0 ;
  struct amdgpu_encoder_atom_dig *tmp___1 ;
  struct amdgpu_encoder_atom_dig *tmp___2 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_54306;
  ldv_54305:
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  if (amdgpu_encoder->encoder_enum == encoder_enum) {
    amdgpu_encoder->devices = amdgpu_encoder->devices | supported_device;
    return;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_54306: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_54305;
  } else {
  }
  tmp = kzalloc(360UL, 208U);
  amdgpu_encoder = (struct amdgpu_encoder *)tmp;
  if ((unsigned long )amdgpu_encoder == (unsigned long )((struct amdgpu_encoder *)0)) {
    return;
  } else {
  }
  encoder = & amdgpu_encoder->base;
  switch (adev->mode_info.num_crtc) {
  case 1:
  encoder->possible_crtcs = 1U;
  goto ldv_54309;
  case 2: ;
  default:
  encoder->possible_crtcs = 3U;
  goto ldv_54309;
  case 4:
  encoder->possible_crtcs = 15U;
  goto ldv_54309;
  case 6:
  encoder->possible_crtcs = 63U;
  goto ldv_54309;
  }
  ldv_54309:
  amdgpu_encoder->enc_priv = (void *)0;
  amdgpu_encoder->encoder_enum = encoder_enum;
  amdgpu_encoder->encoder_id = encoder_enum & 255U;
  amdgpu_encoder->devices = supported_device;
  amdgpu_encoder->rmx_type = 0;
  amdgpu_encoder->underscan_type = 0;
  amdgpu_encoder->is_ext_encoder = 0;
  amdgpu_encoder->caps = caps;
  switch (amdgpu_encoder->encoder_id) {
  case 21U: ;
  case 22U:
  drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 1);
  drm_encoder_helper_add(encoder, & dce_v8_0_dac_helper_funcs);
  goto ldv_54316;
  case 20U: ;
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_encoder->rmx_type = 1;
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 3);
    tmp___0 = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___0;
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 1);
    tmp___1 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___1;
  } else {
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 2);
    tmp___2 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___2;
  }
  drm_encoder_helper_add(encoder, & dce_v8_0_dig_helper_funcs);
  goto ldv_54316;
  case 8U: ;
  case 9U: ;
  case 12U: ;
  case 13U: ;
  case 14U: ;
  case 16U: ;
  case 17U: ;
  case 35U: ;
  case 34U:
  amdgpu_encoder->is_ext_encoder = 1;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 3);
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 1);
  } else {
    drm_encoder_init(dev, encoder, & dce_v8_0_encoder_funcs, 2);
  }
  drm_encoder_helper_add(encoder, & dce_v8_0_ext_helper_funcs);
  goto ldv_54316;
  }
  ldv_54316: ;
  return;
}
}
static struct amdgpu_display_funcs const dce_v8_0_display_funcs =
     {& dce_v8_0_set_vga_render_state, & dce_v8_0_bandwidth_update, & dce_v8_0_vblank_get_counter,
    & dce_v8_0_vblank_wait, & dce_v8_0_is_display_hung, & amdgpu_atombios_encoder_set_backlight_level,
    & amdgpu_atombios_encoder_get_backlight_level, & dce_v8_0_hpd_sense, & dce_v8_0_hpd_set_polarity,
    & dce_v8_0_hpd_get_gpio_reg, & dce_v8_0_page_flip, & dce_v8_0_crtc_get_scanoutpos,
    & dce_v8_0_encoder_add, & amdgpu_connector_add, & dce_v8_0_stop_mc_access, & dce_v8_0_resume_mc_access};
static void dce_v8_0_set_display_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.funcs == (unsigned long )((struct amdgpu_display_funcs const *)0)) {
    adev->mode_info.funcs = & dce_v8_0_display_funcs;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const dce_v8_0_crtc_irq_funcs = {& dce_v8_0_set_crtc_interrupt_state, & dce_v8_0_crtc_irq};
static struct amdgpu_irq_src_funcs const dce_v8_0_pageflip_irq_funcs = {& dce_v8_0_set_pageflip_interrupt_state, & dce_v8_0_pageflip_irq};
static struct amdgpu_irq_src_funcs const dce_v8_0_hpd_irq_funcs = {& dce_v8_0_set_hpd_interrupt_state, & dce_v8_0_hpd_irq};
static void dce_v8_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->crtc_irq.num_types = 12U;
  adev->crtc_irq.funcs = & dce_v8_0_crtc_irq_funcs;
  adev->pageflip_irq.num_types = 6U;
  adev->pageflip_irq.funcs = & dce_v8_0_pageflip_irq_funcs;
  adev->hpd_irq.num_types = 6U;
  adev->hpd_irq.funcs = & dce_v8_0_hpd_irq_funcs;
  return;
}
}
extern int ldv_connect_107(void) ;
extern int ldv_probe_106(void) ;
extern int ldv_release_107(void) ;
extern int ldv_bind_105(void) ;
extern int ldv_probe_102(void) ;
extern int ldv_release_106(void) ;
extern int ldv_bind_107(void) ;
extern int ldv_release_104(void) ;
extern int ldv_bind_104(void) ;
extern int ldv_connect_105(void) ;
extern int ldv_probe_108(void) ;
extern int ldv_release_105(void) ;
int ldv_retval_8 ;
extern int ldv_connect_104(void) ;
int ldv_retval_7 ;
void ldv_initialize_amdgpu_irq_src_funcs_98(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v8_0_hpd_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v8_0_hpd_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_display_funcs_101(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v8_0_display_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(16UL);
  dce_v8_0_display_funcs_group1 = (struct amdgpu_mode_mc_save *)tmp___0;
  tmp___1 = ldv_init_zalloc(360UL);
  dce_v8_0_display_funcs_group2 = (struct amdgpu_encoder *)tmp___1;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_104(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v8_0_dig_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v8_0_dig_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_drm_crtc_helper_funcs_107(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v8_0_crtc_helper_funcs_group0 = (struct drm_crtc *)tmp;
  tmp___0 = ldv_init_zalloc(168UL);
  dce_v8_0_crtc_helper_funcs_group1 = (struct drm_framebuffer *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  dce_v8_0_crtc_helper_funcs_group2 = (struct drm_display_mode *)tmp___1;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_103(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v8_0_dac_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v8_0_dac_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_100(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v8_0_crtc_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v8_0_crtc_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_99(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v8_0_pageflip_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v8_0_pageflip_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_105(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v8_0_ext_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v8_0_ext_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_drm_crtc_funcs_108(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v8_0_crtc_funcs_group0 = (struct drm_crtc *)tmp;
  return;
}
}
void ldv_main_exported_104(void)
{
  struct drm_display_mode *ldvarg394 ;
  void *tmp ;
  struct drm_display_mode *ldvarg396 ;
  void *tmp___0 ;
  struct drm_connector *ldvarg395 ;
  void *tmp___1 ;
  int ldvarg397 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg394 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg396 = (struct drm_display_mode *)tmp___0;
  tmp___1 = ldv_init_zalloc(936UL);
  ldvarg395 = (struct drm_connector *)tmp___1;
  ldv_memset((void *)(& ldvarg397), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_104 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v8_0_dig_helper_funcs_group0, ldvarg397);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    amdgpu_atombios_encoder_dpms(dce_v8_0_dig_helper_funcs_group0, ldvarg397);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    amdgpu_atombios_encoder_dpms(dce_v8_0_dig_helper_funcs_group0, ldvarg397);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 1: ;
  if (ldv_state_variable_104 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v8_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg396,
                                       dce_v8_0_dig_helper_funcs_group1);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    amdgpu_atombios_encoder_mode_fixup(dce_v8_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg396,
                                       dce_v8_0_dig_helper_funcs_group1);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    amdgpu_atombios_encoder_mode_fixup(dce_v8_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg396,
                                       dce_v8_0_dig_helper_funcs_group1);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 2: ;
  if (ldv_state_variable_104 == 1) {
    amdgpu_atombios_encoder_dig_detect(dce_v8_0_dig_helper_funcs_group0, ldvarg395);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    amdgpu_atombios_encoder_dig_detect(dce_v8_0_dig_helper_funcs_group0, ldvarg395);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    amdgpu_atombios_encoder_dig_detect(dce_v8_0_dig_helper_funcs_group0, ldvarg395);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 3: ;
  if (ldv_state_variable_104 == 1) {
    dce_v8_0_encoder_mode_set(dce_v8_0_dig_helper_funcs_group0, dce_v8_0_dig_helper_funcs_group1,
                              ldvarg394);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    dce_v8_0_encoder_mode_set(dce_v8_0_dig_helper_funcs_group0, dce_v8_0_dig_helper_funcs_group1,
                              ldvarg394);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    dce_v8_0_encoder_mode_set(dce_v8_0_dig_helper_funcs_group0, dce_v8_0_dig_helper_funcs_group1,
                              ldvarg394);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 4: ;
  if (ldv_state_variable_104 == 3) {
    dce_v8_0_encoder_disable(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 5: ;
  if (ldv_state_variable_104 == 1) {
    dce_v8_0_encoder_prepare(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    dce_v8_0_encoder_prepare(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    dce_v8_0_encoder_prepare(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 6: ;
  if (ldv_state_variable_104 == 1) {
    dce_v8_0_encoder_commit(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 1;
  } else {
  }
  if (ldv_state_variable_104 == 3) {
    dce_v8_0_encoder_commit(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 3;
  } else {
  }
  if (ldv_state_variable_104 == 2) {
    dce_v8_0_encoder_commit(dce_v8_0_dig_helper_funcs_group0);
    ldv_state_variable_104 = 2;
  } else {
  }
  goto ldv_54404;
  case 7: ;
  if (ldv_state_variable_104 == 2) {
    ldv_release_104();
    ldv_state_variable_104 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54404;
  case 8: ;
  if (ldv_state_variable_104 == 1) {
    ldv_bind_104();
    ldv_state_variable_104 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54404;
  case 9: ;
  if (ldv_state_variable_104 == 2) {
    ldv_connect_104();
    ldv_state_variable_104 = 3;
  } else {
  }
  goto ldv_54404;
  default:
  ldv_stop();
  }
  ldv_54404: ;
  return;
}
}
void ldv_main_exported_102(void)
{
  struct drm_encoder *ldvarg24 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  ldvarg24 = (struct drm_encoder *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_102 == 2) {
    dce_v8_0_encoder_destroy(ldvarg24);
    ldv_state_variable_102 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54420;
  case 1: ;
  if (ldv_state_variable_102 == 1) {
    ldv_probe_102();
    ldv_state_variable_102 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54420;
  default:
  ldv_stop();
  }
  ldv_54420: ;
  return;
}
}
void ldv_main_exported_108(void)
{
  u32 ldvarg636 ;
  struct drm_mode_set *ldvarg622 ;
  void *tmp ;
  u16 *ldvarg631 ;
  void *tmp___0 ;
  u32 ldvarg633 ;
  struct drm_framebuffer *ldvarg624 ;
  void *tmp___1 ;
  struct drm_file *ldvarg634 ;
  void *tmp___2 ;
  u16 *ldvarg630 ;
  void *tmp___3 ;
  u32 ldvarg628 ;
  u16 *ldvarg629 ;
  void *tmp___4 ;
  struct drm_pending_vblank_event *ldvarg623 ;
  void *tmp___5 ;
  u32 ldvarg625 ;
  u32 ldvarg635 ;
  int ldvarg627 ;
  int ldvarg626 ;
  u32 ldvarg632 ;
  int tmp___6 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg622 = (struct drm_mode_set *)tmp;
  tmp___0 = ldv_init_zalloc(2UL);
  ldvarg631 = (u16 *)tmp___0;
  tmp___1 = ldv_init_zalloc(168UL);
  ldvarg624 = (struct drm_framebuffer *)tmp___1;
  tmp___2 = ldv_init_zalloc(744UL);
  ldvarg634 = (struct drm_file *)tmp___2;
  tmp___3 = ldv_init_zalloc(2UL);
  ldvarg630 = (u16 *)tmp___3;
  tmp___4 = ldv_init_zalloc(2UL);
  ldvarg629 = (u16 *)tmp___4;
  tmp___5 = ldv_init_zalloc(88UL);
  ldvarg623 = (struct drm_pending_vblank_event *)tmp___5;
  ldv_memset((void *)(& ldvarg636), 0, 4UL);
  ldv_memset((void *)(& ldvarg633), 0, 4UL);
  ldv_memset((void *)(& ldvarg628), 0, 4UL);
  ldv_memset((void *)(& ldvarg625), 0, 4UL);
  ldv_memset((void *)(& ldvarg635), 0, 4UL);
  ldv_memset((void *)(& ldvarg627), 0, 4UL);
  ldv_memset((void *)(& ldvarg626), 0, 4UL);
  ldv_memset((void *)(& ldvarg632), 0, 4UL);
  tmp___6 = __VERIFIER_nondet_int();
  switch (tmp___6) {
  case 0: ;
  if (ldv_state_variable_108 == 2) {
    dce_v8_0_crtc_cursor_set(dce_v8_0_crtc_funcs_group0, ldvarg634, ldvarg633, ldvarg635,
                             ldvarg636);
    ldv_state_variable_108 = 2;
  } else {
  }
  if (ldv_state_variable_108 == 1) {
    dce_v8_0_crtc_cursor_set(dce_v8_0_crtc_funcs_group0, ldvarg634, ldvarg633, ldvarg635,
                             ldvarg636);
    ldv_state_variable_108 = 1;
  } else {
  }
  goto ldv_54442;
  case 1: ;
  if (ldv_state_variable_108 == 2) {
    dce_v8_0_crtc_destroy(dce_v8_0_crtc_funcs_group0);
    ldv_state_variable_108 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54442;
  case 2: ;
  if (ldv_state_variable_108 == 2) {
    dce_v8_0_crtc_gamma_set(dce_v8_0_crtc_funcs_group0, ldvarg630, ldvarg629, ldvarg631,
                            ldvarg632, ldvarg628);
    ldv_state_variable_108 = 2;
  } else {
  }
  if (ldv_state_variable_108 == 1) {
    dce_v8_0_crtc_gamma_set(dce_v8_0_crtc_funcs_group0, ldvarg630, ldvarg629, ldvarg631,
                            ldvarg632, ldvarg628);
    ldv_state_variable_108 = 1;
  } else {
  }
  goto ldv_54442;
  case 3: ;
  if (ldv_state_variable_108 == 2) {
    dce_v8_0_crtc_cursor_move(dce_v8_0_crtc_funcs_group0, ldvarg627, ldvarg626);
    ldv_state_variable_108 = 2;
  } else {
  }
  if (ldv_state_variable_108 == 1) {
    dce_v8_0_crtc_cursor_move(dce_v8_0_crtc_funcs_group0, ldvarg627, ldvarg626);
    ldv_state_variable_108 = 1;
  } else {
  }
  goto ldv_54442;
  case 4: ;
  if (ldv_state_variable_108 == 2) {
    amdgpu_crtc_page_flip(dce_v8_0_crtc_funcs_group0, ldvarg624, ldvarg623, ldvarg625);
    ldv_state_variable_108 = 2;
  } else {
  }
  if (ldv_state_variable_108 == 1) {
    amdgpu_crtc_page_flip(dce_v8_0_crtc_funcs_group0, ldvarg624, ldvarg623, ldvarg625);
    ldv_state_variable_108 = 1;
  } else {
  }
  goto ldv_54442;
  case 5: ;
  if (ldv_state_variable_108 == 2) {
    amdgpu_crtc_set_config(ldvarg622);
    ldv_state_variable_108 = 2;
  } else {
  }
  if (ldv_state_variable_108 == 1) {
    amdgpu_crtc_set_config(ldvarg622);
    ldv_state_variable_108 = 1;
  } else {
  }
  goto ldv_54442;
  case 6: ;
  if (ldv_state_variable_108 == 1) {
    ldv_probe_108();
    ldv_state_variable_108 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54442;
  default:
  ldv_stop();
  }
  ldv_54442: ;
  return;
}
}
void ldv_main_exported_107(void)
{
  int ldvarg698 ;
  int ldvarg689 ;
  int ldvarg696 ;
  int ldvarg693 ;
  struct drm_display_mode *ldvarg695 ;
  void *tmp ;
  int ldvarg697 ;
  struct drm_display_mode *ldvarg692 ;
  void *tmp___0 ;
  enum mode_set_atomic ldvarg691 ;
  int ldvarg694 ;
  int ldvarg690 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg695 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg692 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg698), 0, 4UL);
  ldv_memset((void *)(& ldvarg689), 0, 4UL);
  ldv_memset((void *)(& ldvarg696), 0, 4UL);
  ldv_memset((void *)(& ldvarg693), 0, 4UL);
  ldv_memset((void *)(& ldvarg697), 0, 4UL);
  ldv_memset((void *)(& ldvarg691), 0, 4UL);
  ldv_memset((void *)(& ldvarg694), 0, 4UL);
  ldv_memset((void *)(& ldvarg690), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_set_base(dce_v8_0_crtc_helper_funcs_group0, ldvarg698, ldvarg697,
                           dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_set_base(dce_v8_0_crtc_helper_funcs_group0, ldvarg698, ldvarg697,
                           dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_set_base(dce_v8_0_crtc_helper_funcs_group0, ldvarg698, ldvarg697,
                           dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 1: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_dpms(dce_v8_0_crtc_helper_funcs_group0, ldvarg696);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_dpms(dce_v8_0_crtc_helper_funcs_group0, ldvarg696);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_dpms(dce_v8_0_crtc_helper_funcs_group0, ldvarg696);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 2: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_mode_fixup(dce_v8_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg695,
                             dce_v8_0_crtc_helper_funcs_group2);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_mode_fixup(dce_v8_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg695,
                             dce_v8_0_crtc_helper_funcs_group2);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_mode_fixup(dce_v8_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg695,
                             dce_v8_0_crtc_helper_funcs_group2);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 3: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_mode_set(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group2,
                           ldvarg692, ldvarg693, ldvarg694, dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_mode_set(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group2,
                           ldvarg692, ldvarg693, ldvarg694, dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_mode_set(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group2,
                           ldvarg692, ldvarg693, ldvarg694, dce_v8_0_crtc_helper_funcs_group1);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 4: ;
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_disable(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 2;
  } else {
  }
  goto ldv_54464;
  case 5: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_prepare(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_prepare(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_prepare(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 6: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_load_lut(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_load_lut(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_load_lut(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 7: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_commit(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_commit(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_commit(dce_v8_0_crtc_helper_funcs_group0);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 8: ;
  if (ldv_state_variable_107 == 2) {
    dce_v8_0_crtc_set_base_atomic(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group1,
                                  ldvarg689, ldvarg690, ldvarg691);
    ldv_state_variable_107 = 2;
  } else {
  }
  if (ldv_state_variable_107 == 1) {
    dce_v8_0_crtc_set_base_atomic(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group1,
                                  ldvarg689, ldvarg690, ldvarg691);
    ldv_state_variable_107 = 1;
  } else {
  }
  if (ldv_state_variable_107 == 3) {
    dce_v8_0_crtc_set_base_atomic(dce_v8_0_crtc_helper_funcs_group0, dce_v8_0_crtc_helper_funcs_group1,
                                  ldvarg689, ldvarg690, ldvarg691);
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  case 9: ;
  if (ldv_state_variable_107 == 2) {
    ldv_release_107();
    ldv_state_variable_107 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54464;
  case 10: ;
  if (ldv_state_variable_107 == 1) {
    ldv_bind_107();
    ldv_state_variable_107 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54464;
  case 11: ;
  if (ldv_state_variable_107 == 2) {
    ldv_connect_107();
    ldv_state_variable_107 = 3;
  } else {
  }
  goto ldv_54464;
  default:
  ldv_stop();
  }
  ldv_54464: ;
  return;
}
}
void ldv_main_exported_98(void)
{
  struct amdgpu_iv_entry *ldvarg457 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg459 ;
  unsigned int ldvarg458 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg457 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg459), 0, 4UL);
  ldv_memset((void *)(& ldvarg458), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_98 == 1) {
    dce_v8_0_set_hpd_interrupt_state(dce_v8_0_hpd_irq_funcs_group0, dce_v8_0_hpd_irq_funcs_group1,
                                     ldvarg458, ldvarg459);
    ldv_state_variable_98 = 1;
  } else {
  }
  goto ldv_54484;
  case 1: ;
  if (ldv_state_variable_98 == 1) {
    dce_v8_0_hpd_irq(dce_v8_0_hpd_irq_funcs_group0, dce_v8_0_hpd_irq_funcs_group1,
                     ldvarg457);
    ldv_state_variable_98 = 1;
  } else {
  }
  goto ldv_54484;
  default:
  ldv_stop();
  }
  ldv_54484: ;
  return;
}
}
void ldv_main_exported_99(void)
{
  enum amdgpu_interrupt_state ldvarg565 ;
  struct amdgpu_iv_entry *ldvarg563 ;
  void *tmp ;
  unsigned int ldvarg564 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg563 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg565), 0, 4UL);
  ldv_memset((void *)(& ldvarg564), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_99 == 1) {
    dce_v8_0_set_pageflip_interrupt_state(dce_v8_0_pageflip_irq_funcs_group0, dce_v8_0_pageflip_irq_funcs_group1,
                                          ldvarg564, ldvarg565);
    ldv_state_variable_99 = 1;
  } else {
  }
  goto ldv_54494;
  case 1: ;
  if (ldv_state_variable_99 == 1) {
    dce_v8_0_pageflip_irq(dce_v8_0_pageflip_irq_funcs_group0, dce_v8_0_pageflip_irq_funcs_group1,
                          ldvarg563);
    ldv_state_variable_99 = 1;
  } else {
  }
  goto ldv_54494;
  default:
  ldv_stop();
  }
  ldv_54494: ;
  return;
}
}
void ldv_main_exported_103(void)
{
  struct drm_connector *ldvarg642 ;
  void *tmp ;
  struct drm_display_mode *ldvarg641 ;
  void *tmp___0 ;
  int ldvarg644 ;
  struct drm_display_mode *ldvarg643 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(936UL);
  ldvarg642 = (struct drm_connector *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg641 = (struct drm_display_mode *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  ldvarg643 = (struct drm_display_mode *)tmp___1;
  ldv_memset((void *)(& ldvarg644), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_103 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v8_0_dac_helper_funcs_group0, ldvarg644);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  case 1: ;
  if (ldv_state_variable_103 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v8_0_dac_helper_funcs_group0, (struct drm_display_mode const *)ldvarg643,
                                       dce_v8_0_dac_helper_funcs_group1);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  case 2: ;
  if (ldv_state_variable_103 == 1) {
    amdgpu_atombios_encoder_dac_detect(dce_v8_0_dac_helper_funcs_group0, ldvarg642);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  case 3: ;
  if (ldv_state_variable_103 == 1) {
    dce_v8_0_encoder_mode_set(dce_v8_0_dac_helper_funcs_group0, dce_v8_0_dac_helper_funcs_group1,
                              ldvarg641);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  case 4: ;
  if (ldv_state_variable_103 == 1) {
    dce_v8_0_encoder_prepare(dce_v8_0_dac_helper_funcs_group0);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  case 5: ;
  if (ldv_state_variable_103 == 1) {
    dce_v8_0_encoder_commit(dce_v8_0_dac_helper_funcs_group0);
    ldv_state_variable_103 = 1;
  } else {
  }
  goto ldv_54505;
  default:
  ldv_stop();
  }
  ldv_54505: ;
  return;
}
}
void ldv_main_exported_106(void)
{
  void *ldvarg148 ;
  void *tmp ;
  void *ldvarg142 ;
  void *tmp___0 ;
  void *ldvarg155 ;
  void *tmp___1 ;
  void *ldvarg149 ;
  void *tmp___2 ;
  void *ldvarg143 ;
  void *tmp___3 ;
  void *ldvarg156 ;
  void *tmp___4 ;
  void *ldvarg153 ;
  void *tmp___5 ;
  enum amd_powergating_state ldvarg150 ;
  void *ldvarg152 ;
  void *tmp___6 ;
  void *ldvarg144 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg146 ;
  void *ldvarg145 ;
  void *tmp___8 ;
  void *ldvarg151 ;
  void *tmp___9 ;
  void *ldvarg154 ;
  void *tmp___10 ;
  void *ldvarg147 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg148 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg142 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg155 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg149 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg143 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg156 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg153 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg152 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg144 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg145 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg151 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg154 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg147 = tmp___11;
  ldv_memset((void *)(& ldvarg150), 0, 4UL);
  ldv_memset((void *)(& ldvarg146), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_hw_fini(ldvarg156);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_hw_fini(ldvarg156);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_hw_fini(ldvarg156);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 1: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_print_status(ldvarg155);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_print_status(ldvarg155);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_print_status(ldvarg155);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 2: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_early_init(ldvarg154);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_early_init(ldvarg154);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_early_init(ldvarg154);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 3: ;
  if (ldv_state_variable_106 == 2) {
    ldv_retval_8 = dce_v8_0_suspend(ldvarg153);
    if (ldv_retval_8 == 0) {
      ldv_state_variable_106 = 3;
    } else {
    }
  } else {
  }
  goto ldv_54531;
  case 4: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_sw_init(ldvarg152);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_sw_init(ldvarg152);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_sw_init(ldvarg152);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 5: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_set_powergating_state(ldvarg151, ldvarg150);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_set_powergating_state(ldvarg151, ldvarg150);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_set_powergating_state(ldvarg151, ldvarg150);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 6: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_wait_for_idle(ldvarg149);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_wait_for_idle(ldvarg149);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_wait_for_idle(ldvarg149);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 7: ;
  if (ldv_state_variable_106 == 3) {
    ldv_retval_7 = dce_v8_0_resume(ldvarg148);
    if (ldv_retval_7 == 0) {
      ldv_state_variable_106 = 2;
    } else {
    }
  } else {
  }
  goto ldv_54531;
  case 8: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_set_clockgating_state(ldvarg147, ldvarg146);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_set_clockgating_state(ldvarg147, ldvarg146);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_set_clockgating_state(ldvarg147, ldvarg146);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 9: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_hw_init(ldvarg145);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_hw_init(ldvarg145);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_hw_init(ldvarg145);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 10: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_soft_reset(ldvarg144);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_soft_reset(ldvarg144);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_soft_reset(ldvarg144);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 11: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_sw_fini(ldvarg143);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_sw_fini(ldvarg143);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_sw_fini(ldvarg143);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 12: ;
  if (ldv_state_variable_106 == 1) {
    dce_v8_0_is_idle(ldvarg142);
    ldv_state_variable_106 = 1;
  } else {
  }
  if (ldv_state_variable_106 == 3) {
    dce_v8_0_is_idle(ldvarg142);
    ldv_state_variable_106 = 3;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    dce_v8_0_is_idle(ldvarg142);
    ldv_state_variable_106 = 2;
  } else {
  }
  goto ldv_54531;
  case 13: ;
  if (ldv_state_variable_106 == 3) {
    ldv_release_106();
    ldv_state_variable_106 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_106 == 2) {
    ldv_release_106();
    ldv_state_variable_106 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54531;
  case 14: ;
  if (ldv_state_variable_106 == 1) {
    ldv_probe_106();
    ldv_state_variable_106 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54531;
  default:
  ldv_stop();
  }
  ldv_54531: ;
  return;
}
}
void ldv_main_exported_101(void)
{
  u16 ldvarg232 ;
  u32 *ldvarg239 ;
  void *tmp ;
  int ldvarg228 ;
  struct amdgpu_hpd *ldvarg244 ;
  void *tmp___0 ;
  int ldvarg234 ;
  enum amdgpu_hpd_id ldvarg229 ;
  bool ldvarg235 ;
  u32 ldvarg245 ;
  u32 ldvarg243 ;
  struct amdgpu_i2c_bus_rec *ldvarg248 ;
  void *tmp___1 ;
  enum amdgpu_hpd_id ldvarg236 ;
  u64 ldvarg233 ;
  int ldvarg238 ;
  u32 ldvarg231 ;
  int ldvarg247 ;
  struct amdgpu_router *ldvarg246 ;
  void *tmp___2 ;
  u8 ldvarg241 ;
  int ldvarg240 ;
  u32 ldvarg230 ;
  u32 *ldvarg237 ;
  void *tmp___3 ;
  u16 ldvarg242 ;
  int tmp___4 ;
  {
  tmp = ldv_init_zalloc(4UL);
  ldvarg239 = (u32 *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg244 = (struct amdgpu_hpd *)tmp___0;
  tmp___1 = ldv_init_zalloc(76UL);
  ldvarg248 = (struct amdgpu_i2c_bus_rec *)tmp___1;
  tmp___2 = ldv_init_zalloc(92UL);
  ldvarg246 = (struct amdgpu_router *)tmp___2;
  tmp___3 = ldv_init_zalloc(4UL);
  ldvarg237 = (u32 *)tmp___3;
  ldv_memset((void *)(& ldvarg232), 0, 2UL);
  ldv_memset((void *)(& ldvarg228), 0, 4UL);
  ldv_memset((void *)(& ldvarg234), 0, 4UL);
  ldv_memset((void *)(& ldvarg229), 0, 4UL);
  ldv_memset((void *)(& ldvarg235), 0, 1UL);
  ldv_memset((void *)(& ldvarg245), 0, 4UL);
  ldv_memset((void *)(& ldvarg243), 0, 4UL);
  ldv_memset((void *)(& ldvarg236), 0, 4UL);
  ldv_memset((void *)(& ldvarg233), 0, 8UL);
  ldv_memset((void *)(& ldvarg238), 0, 4UL);
  ldv_memset((void *)(& ldvarg231), 0, 4UL);
  ldv_memset((void *)(& ldvarg247), 0, 4UL);
  ldv_memset((void *)(& ldvarg241), 0, 1UL);
  ldv_memset((void *)(& ldvarg240), 0, 4UL);
  ldv_memset((void *)(& ldvarg230), 0, 4UL);
  ldv_memset((void *)(& ldvarg242), 0, 2UL);
  tmp___4 = __VERIFIER_nondet_int();
  switch (tmp___4) {
  case 0: ;
  if (ldv_state_variable_101 == 1) {
    amdgpu_connector_add(dce_v8_0_display_funcs_group0, ldvarg245, ldvarg243, ldvarg247,
                         ldvarg248, (int )ldvarg242, ldvarg244, ldvarg246);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 1: ;
  if (ldv_state_variable_101 == 1) {
    amdgpu_atombios_encoder_set_backlight_level(dce_v8_0_display_funcs_group2, (int )ldvarg241);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 2: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_vblank_wait(dce_v8_0_display_funcs_group0, ldvarg240);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 3: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_stop_mc_access(dce_v8_0_display_funcs_group0, dce_v8_0_display_funcs_group1);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 4: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_crtc_get_scanoutpos(dce_v8_0_display_funcs_group0, ldvarg238, ldvarg237,
                                 ldvarg239);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 5: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_hpd_get_gpio_reg(dce_v8_0_display_funcs_group0);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 6: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_resume_mc_access(dce_v8_0_display_funcs_group0, dce_v8_0_display_funcs_group1);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 7: ;
  if (ldv_state_variable_101 == 1) {
    amdgpu_atombios_encoder_get_backlight_level(dce_v8_0_display_funcs_group2);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 8: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_hpd_set_polarity(dce_v8_0_display_funcs_group0, ldvarg236);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 9: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_is_display_hung(dce_v8_0_display_funcs_group0);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 10: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_set_vga_render_state(dce_v8_0_display_funcs_group0, (int )ldvarg235);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 11: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_page_flip(dce_v8_0_display_funcs_group0, ldvarg234, ldvarg233);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 12: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_encoder_add(dce_v8_0_display_funcs_group0, ldvarg231, ldvarg230, (int )ldvarg232);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 13: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_hpd_sense(dce_v8_0_display_funcs_group0, ldvarg229);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 14: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_bandwidth_update(dce_v8_0_display_funcs_group0);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  case 15: ;
  if (ldv_state_variable_101 == 1) {
    dce_v8_0_vblank_get_counter(dce_v8_0_display_funcs_group0, ldvarg228);
    ldv_state_variable_101 = 1;
  } else {
  }
  goto ldv_54572;
  default:
  ldv_stop();
  }
  ldv_54572: ;
  return;
}
}
void ldv_main_exported_100(void)
{
  struct amdgpu_iv_entry *ldvarg894 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg896 ;
  unsigned int ldvarg895 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg894 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg896), 0, 4UL);
  ldv_memset((void *)(& ldvarg895), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_100 == 1) {
    dce_v8_0_set_crtc_interrupt_state(dce_v8_0_crtc_irq_funcs_group0, dce_v8_0_crtc_irq_funcs_group1,
                                      ldvarg895, ldvarg896);
    ldv_state_variable_100 = 1;
  } else {
  }
  goto ldv_54596;
  case 1: ;
  if (ldv_state_variable_100 == 1) {
    dce_v8_0_crtc_irq(dce_v8_0_crtc_irq_funcs_group0, dce_v8_0_crtc_irq_funcs_group1,
                      ldvarg894);
    ldv_state_variable_100 = 1;
  } else {
  }
  goto ldv_54596;
  default:
  ldv_stop();
  }
  ldv_54596: ;
  return;
}
}
void ldv_main_exported_105(void)
{
  struct drm_display_mode *ldvarg1039 ;
  void *tmp ;
  struct drm_display_mode *ldvarg1040 ;
  void *tmp___0 ;
  int ldvarg1041 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg1039 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg1040 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg1041), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_105 == 2) {
    dce_v8_0_ext_dpms(dce_v8_0_ext_helper_funcs_group0, ldvarg1041);
    ldv_state_variable_105 = 2;
  } else {
  }
  if (ldv_state_variable_105 == 1) {
    dce_v8_0_ext_dpms(dce_v8_0_ext_helper_funcs_group0, ldvarg1041);
    ldv_state_variable_105 = 1;
  } else {
  }
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_dpms(dce_v8_0_ext_helper_funcs_group0, ldvarg1041);
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  case 1: ;
  if (ldv_state_variable_105 == 2) {
    dce_v8_0_ext_mode_fixup(dce_v8_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1040,
                            dce_v8_0_ext_helper_funcs_group1);
    ldv_state_variable_105 = 2;
  } else {
  }
  if (ldv_state_variable_105 == 1) {
    dce_v8_0_ext_mode_fixup(dce_v8_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1040,
                            dce_v8_0_ext_helper_funcs_group1);
    ldv_state_variable_105 = 1;
  } else {
  }
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_mode_fixup(dce_v8_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1040,
                            dce_v8_0_ext_helper_funcs_group1);
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  case 2: ;
  if (ldv_state_variable_105 == 2) {
    dce_v8_0_ext_mode_set(dce_v8_0_ext_helper_funcs_group0, dce_v8_0_ext_helper_funcs_group1,
                          ldvarg1039);
    ldv_state_variable_105 = 2;
  } else {
  }
  if (ldv_state_variable_105 == 1) {
    dce_v8_0_ext_mode_set(dce_v8_0_ext_helper_funcs_group0, dce_v8_0_ext_helper_funcs_group1,
                          ldvarg1039);
    ldv_state_variable_105 = 1;
  } else {
  }
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_mode_set(dce_v8_0_ext_helper_funcs_group0, dce_v8_0_ext_helper_funcs_group1,
                          ldvarg1039);
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  case 3: ;
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_disable(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 2;
  } else {
  }
  goto ldv_54606;
  case 4: ;
  if (ldv_state_variable_105 == 2) {
    dce_v8_0_ext_prepare(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 2;
  } else {
  }
  if (ldv_state_variable_105 == 1) {
    dce_v8_0_ext_prepare(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 1;
  } else {
  }
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_prepare(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  case 5: ;
  if (ldv_state_variable_105 == 2) {
    dce_v8_0_ext_commit(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 2;
  } else {
  }
  if (ldv_state_variable_105 == 1) {
    dce_v8_0_ext_commit(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 1;
  } else {
  }
  if (ldv_state_variable_105 == 3) {
    dce_v8_0_ext_commit(dce_v8_0_ext_helper_funcs_group0);
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  case 6: ;
  if (ldv_state_variable_105 == 2) {
    ldv_release_105();
    ldv_state_variable_105 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54606;
  case 7: ;
  if (ldv_state_variable_105 == 1) {
    ldv_bind_105();
    ldv_state_variable_105 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54606;
  case 8: ;
  if (ldv_state_variable_105 == 2) {
    ldv_connect_105();
    ldv_state_variable_105 = 3;
  } else {
  }
  goto ldv_54606;
  default:
  ldv_stop();
  }
  ldv_54606: ;
  return;
}
}
bool ldv_queue_work_on_621(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_622(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_623(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_624(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_625(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
void ldv_destroy_workqueue_626(struct workqueue_struct *ldv_func_arg1 )
{
  {
  destroy_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_637(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_639(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_638(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_641(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_640(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___3(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_637(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___2(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___3(system_wq, work);
  return (tmp);
}
}
int amdgpu_gfx_scratch_get(struct amdgpu_device *adev , u32 *reg ) ;
void amdgpu_gfx_scratch_free(struct amdgpu_device *adev , u32 reg ) ;
static unsigned int const ci_SECT_CONTEXT_def_1[212U] =
  { 0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 1073758208U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 2147483648U, 1073758208U, 65535U,
        0U, 1073758208U, 0U, 1073758208U,
        0U, 1073758208U, 0U, 1073758208U,
        2862197418U, 0U, 4294967295U, 4294967295U,
        2147483648U, 1073758208U, 0U, 0U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U};
static unsigned int const ci_SECT_CONTEXT_def_2[274U] =
  { 0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 4294967295U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        2U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U};
static unsigned int const ci_SECT_CONTEXT_def_3[6U] = { 0U, 0U, 0U, 0U,
        0U, 0U};
static unsigned int const ci_SECT_CONTEXT_def_4[157U] =
  { 0U, 0U, 0U, 0U,
        589824U, 4U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 256U, 128U, 2U,
        0U, 0U, 0U, 0U,
        0U};
static unsigned int const ci_SECT_CONTEXT_def_5[2U] = { 0U, 0U};
static unsigned int const ci_SECT_CONTEXT_def_6[1U] = { 0U};
static unsigned int const ci_SECT_CONTEXT_def_7[233U] =
  { 0U, 0U, 0U, 0U,
        0U, 255U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 4096U, 0U,
        5U, 1065353216U, 1065353216U, 1065353216U,
        1065353216U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 4294967295U, 4294967295U, 0U,
        0U, 0U, 0U, 0U,
        0U, 14U, 16U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U};
static struct cs_extent_def const ci_SECT_CONTEXT_defs[8U] =
  { {(unsigned int const *)(& ci_SECT_CONTEXT_def_1), 40960U, 212U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_2), 41174U, 274U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_3), 41461U, 6U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_4), 41472U, 157U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_5), 41632U, 2U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_6), 41635U, 1U},
        {(unsigned int const *)(& ci_SECT_CONTEXT_def_7), 41637U, 233U},
        {(unsigned int const *)0U, 0U, 0U}};
static struct cs_section_def const ci_cs_data[2U] = { {(struct cs_extent_def const *)(& ci_SECT_CONTEXT_defs), 1},
        {(struct cs_extent_def const *)0, 0}};
static void gfx_v7_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void gfx_v7_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static void gfx_v7_0_set_gds_init(struct amdgpu_device *adev ) ;
static struct amdgpu_gds_reg_offset const amdgpu_gds_reg_offset[16U] =
  { {13056U, 13057U, 13088U, 13104U},
        {13058U, 13059U, 13089U, 13105U},
        {13060U, 13061U, 13090U, 13106U},
        {13062U, 13063U, 13091U, 13107U},
        {13064U, 13065U, 13092U, 13108U},
        {13066U, 13067U, 13093U, 13109U},
        {13068U, 13069U, 13094U, 13110U},
        {13070U, 13071U, 13095U, 13111U},
        {13072U, 13073U, 13096U, 13112U},
        {13074U, 13075U, 13097U, 13113U},
        {13076U, 13077U, 13098U, 13114U},
        {13078U, 13079U, 13099U, 13115U},
        {13080U, 13081U, 13100U, 13116U},
        {13082U, 13083U, 13101U, 13117U},
        {13084U, 13085U, 13102U, 13118U},
        {13086U, 13087U, 13103U, 13119U}};
static u32 const spectre_rlc_save_restore_register_list[443U] =
  { 234893387U, 0U, 234893392U, 0U,
        234893396U, 0U, 234893399U, 0U,
        234893402U, 0U, 234893404U, 0U,
        234893406U, 0U, 234893441U, 0U,
        234893485U, 0U, 234893486U, 0U,
        234893487U, 0U, 234893488U, 0U,
        234889354U, 0U, 234889383U, 0U,
        234889639U, 0U, 100673085U, 0U,
        234890814U, 0U, 234890816U, 0U,
        234893464U, 0U, 234890298U, 0U,
        234942464U, 0U, 234942467U, 0U,
        234889991U, 0U, 234890688U, 0U,
        234894152U, 0U, 1308635976U, 0U,
        1577071432U, 0U, 1845506888U, 0U,
        2113942344U, 0U, 2382377800U, 0U,
        2650813256U, 0U, 2919248712U, 0U,
        3187684168U, 0U, 234889839U, 0U,
        234889792U, 0U, 3U, 234893388U,
        0U, 234893389U, 0U, 234893439U,
        0U, 234893442U, 0U, 234893465U,
        0U, 234893466U, 0U, 234893467U,
        0U, 234893468U, 0U, 234893469U,
        0U, 234893470U, 0U, 234893471U,
        0U, 234893472U, 0U, 234893473U,
        0U, 234893474U, 0U, 234893475U,
        0U, 234893476U, 0U, 234893477U,
        0U, 234893478U, 0U, 234893479U,
        0U, 234893480U, 0U, 234893481U,
        0U, 234893482U, 0U, 234893483U,
        0U, 234893484U, 0U, 234930292U,
        0U, 234930318U, 0U, 234930324U,
        0U, 234930325U, 0U, 234930326U,
        0U, 234930327U, 0U, 1308635712U,
        0U, 1577071168U, 0U, 1845506624U,
        0U, 2113942080U, 0U, 2382377536U,
        0U, 2650812992U, 0U, 2919248448U,
        0U, 3187683904U, 0U, 1308635713U,
        0U, 1577071169U, 0U, 1845506625U,
        0U, 2113942081U, 0U, 2382377537U,
        0U, 2650812993U, 0U, 2919248449U,
        0U, 3187683905U, 0U, 1308635714U,
        0U, 1577071170U, 0U, 1845506626U,
        0U, 2113942082U, 0U, 2382377538U,
        0U, 2650812994U, 0U, 2919248450U,
        0U, 3187683906U, 0U, 1308635715U,
        0U, 1577071171U, 0U, 1845506627U,
        0U, 2113942083U, 0U, 2382377539U,
        0U, 2650812995U, 0U, 2919248451U,
        0U, 3187683907U, 0U, 1308635716U,
        0U, 1577071172U, 0U, 1845506628U,
        0U, 2113942084U, 0U, 2382377540U,
        0U, 2650812996U, 0U, 2919248452U,
        0U, 3187683908U, 0U, 234893927U,
        0U, 234890765U, 0U, 50112U,
        0U, 115648U, 0U, 50113U,
        0U, 115649U, 0U, 50114U,
        0U, 115650U, 0U, 50115U,
        0U, 115651U, 0U, 100673247U,
        0U, 234889861U, 0U, 234889862U,
        0U, 100713088U, 0U, 234889980U,
        0U, 234889971U, 0U, 234889929U,
        0U, 234930817U, 0U, 100713092U,
        0U, 100713093U, 0U, 100713094U,
        0U, 100713099U, 0U, 234893760U,
        0U, 234893761U, 0U, 234893762U,
        0U, 234893786U, 0U, 67121628U,
        0U, 67121629U, 0U, 67121630U,
        0U, 67121631U, 0U, 67121632U,
        0U, 67121633U, 0U, 67121634U,
        0U, 67121635U, 0U, 67121638U,
        0U, 67121639U, 0U, 67121640U,
        0U, 67121641U, 0U, 67121642U,
        0U, 67121643U, 0U, 67121644U,
        0U, 67121645U, 0U, 234890304U,
        0U, 234942468U, 0U, 234890410U,
        0U, 234890411U, 0U, 234890413U,
        0U, 234890414U, 0U, 234890415U,
        0U, 234890416U, 0U, 234890417U,
        0U, 234890418U, 0U, 234890419U,
        0U, 234890420U, 0U, 234889984U,
        0U, 234889985U, 0U, 234889992U,
        0U, 234889998U, 0U, 234889999U,
        0U, 234892160U, 0U, 234890625U,
        0U, 234892034U, 0U, 234892035U,
        0U, 234892036U, 0U, 234892037U,
        0U, 234892054U, 0U, 234892058U,
        0U, 234892059U, 0U, 234892060U,
        0U, 234892061U, 0U, 234892062U,
        0U, 234892063U, 0U, 234892064U,
        0U, 234892065U, 0U, 234892066U,
        0U, 234892067U, 0U, 234890691U,
        0U, 234890693U, 0U, 234890694U,
        0U, 234890695U, 0U, 234931226U,
        0U, 1308673050U, 0U, 1577108506U,
        0U, 1845543962U, 0U, 2113979418U,
        0U, 2382414874U, 0U, 2650850330U,
        0U, 2919285786U, 0U, 3187721242U,
        0U, 234894148U, 0U, 234894149U,
        0U, 234889772U, 0U, 234889773U,
        0U, 234889774U, 0U, 234889775U,
        0U, 67117680U, 0U, 234889777U,
        0U, 234889778U, 0U, 234889780U,
        0U, 234889781U, 0U, 234889782U,
        0U, 234889824U, 0U, 234930766U,
        0U, 234930767U, 0U, 234930768U,
        0U, 234889832U, 0U, 234930752U,
        0U, 234930753U, 0U, 234889837U,
        0U, 234942596U, 0U, 234942597U,
        0U, 234942598U, 0U, 234889793U,
        0U, 5U, 234889994U, 234889995U,
        234889996U, 234889997U, 234890624U};
static u32 const kalindi_rlc_save_restore_register_list[321U] =
  { 234893387U, 0U, 234893392U, 0U,
        234893396U, 0U, 234893399U, 0U,
        234893402U, 0U, 234893404U, 0U,
        234893441U, 0U, 234893485U, 0U,
        234893486U, 0U, 234893487U, 0U,
        234893488U, 0U, 234889354U, 0U,
        234889383U, 0U, 234889639U, 0U,
        100673085U, 0U, 234890814U, 0U,
        234890816U, 0U, 234893464U, 0U,
        234890298U, 0U, 234942464U, 0U,
        234942467U, 0U, 234889991U, 0U,
        234890688U, 0U, 234894152U, 0U,
        1308635976U, 0U, 1577071432U, 0U,
        1845506888U, 0U, 2113942344U, 0U,
        234889839U, 0U, 234889792U, 0U,
        3U, 234893388U, 0U, 234893389U,
        0U, 234893439U, 0U, 234893442U,
        0U, 234893465U, 0U, 234893466U,
        0U, 234893467U, 0U, 234893468U,
        0U, 234893469U, 0U, 234893475U,
        0U, 234893476U, 0U, 234893477U,
        0U, 234893478U, 0U, 234893480U,
        0U, 234893481U, 0U, 234893482U,
        0U, 234893483U, 0U, 234930292U,
        0U, 234930318U, 0U, 234930324U,
        0U, 234930325U, 0U, 234930326U,
        0U, 234930327U, 0U, 1308635712U,
        0U, 1577071168U, 0U, 1845506624U,
        0U, 2113942080U, 0U, 1308635713U,
        0U, 1577071169U, 0U, 1845506625U,
        0U, 2113942081U, 0U, 1308635714U,
        0U, 1577071170U, 0U, 1845506626U,
        0U, 2113942082U, 0U, 1308635715U,
        0U, 1577071171U, 0U, 1845506627U,
        0U, 2113942083U, 0U, 1308635716U,
        0U, 1577071172U, 0U, 1845506628U,
        0U, 2113942084U, 0U, 234893927U,
        0U, 234890765U, 0U, 50112U,
        0U, 50113U, 0U, 50114U,
        0U, 50115U, 0U, 100673247U,
        0U, 234889861U, 0U, 234889862U,
        0U, 100713088U, 0U, 234889980U,
        0U, 234889971U, 0U, 234889929U,
        0U, 234930817U, 0U, 100713092U,
        0U, 100713093U, 0U, 100713094U,
        0U, 100713099U, 0U, 234893760U,
        0U, 234893761U, 0U, 234893762U,
        0U, 234893786U, 0U, 67121628U,
        0U, 67121629U, 0U, 67121638U,
        0U, 67121639U, 0U, 234890304U,
        0U, 234942468U, 0U, 234889984U,
        0U, 234889985U, 0U, 234889992U,
        0U, 234889998U, 0U, 234889999U,
        0U, 234892160U, 0U, 234890625U,
        0U, 234892034U, 0U, 234892035U,
        0U, 234892036U, 0U, 234892037U,
        0U, 234892054U, 0U, 234892058U,
        0U, 234892059U, 0U, 234892060U,
        0U, 234892061U, 0U, 234892062U,
        0U, 234892063U, 0U, 234892064U,
        0U, 234892065U, 0U, 234892066U,
        0U, 234892067U, 0U, 234890691U,
        0U, 234890693U, 0U, 234890694U,
        0U, 234890695U, 0U, 234931226U,
        0U, 1308673050U, 0U, 1577108506U,
        0U, 1845543962U, 0U, 2113979418U,
        0U, 234894148U, 0U, 234894149U,
        0U, 234889772U, 0U, 234889773U,
        0U, 234889774U, 0U, 234889775U,
        0U, 67117680U, 0U, 234889777U,
        0U, 234889778U, 0U, 234889780U,
        0U, 234889781U, 0U, 234889782U,
        0U, 234889824U, 0U, 234930766U,
        0U, 234930767U, 0U, 234930768U,
        0U, 234889832U, 0U, 234930752U,
        0U, 234930753U, 0U, 234889837U,
        0U, 234944639U, 0U, 234942596U,
        0U, 234942597U, 0U, 234942598U,
        0U, 234889793U, 0U, 5U,
        234889994U, 234889995U, 234889996U, 234889997U,
        234890624U};
static u32 gfx_v7_0_get_csb_size(struct amdgpu_device *adev ) ;
static void gfx_v7_0_get_csb_buffer(struct amdgpu_device *adev , u32 volatile *buffer ) ;
static void gfx_v7_0_init_cp_pg_table(struct amdgpu_device *adev ) ;
static void gfx_v7_0_init_pg(struct amdgpu_device *adev ) ;
static int gfx_v7_0_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gfx_v7_0_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  chip_name = "bonaire";
  goto ldv_52840;
  case 3U:
  chip_name = "hawaii";
  goto ldv_52840;
  case 1U:
  chip_name = "kaveri";
  goto ldv_52840;
  case 2U:
  chip_name = "kabini";
  goto ldv_52840;
  case 4U:
  chip_name = "mullins";
  goto ldv_52840;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gfx_v7_0.c"),
                       "i" (924), "i" (12UL));
  ldv_52846: ;
  goto ldv_52846;
  }
  ldv_52840:
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_pfp.bin", chip_name);
  err = request_firmware(& adev->gfx.pfp_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.pfp_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_me.bin", chip_name);
  err = request_firmware(& adev->gfx.me_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.me_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_ce.bin", chip_name);
  err = request_firmware(& adev->gfx.ce_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.ce_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_mec.bin", chip_name);
  err = request_firmware(& adev->gfx.mec_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.mec_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  if ((unsigned int )adev->asic_type == 1U) {
    snprintf((char *)(& fw_name), 30UL, "radeon/%s_mec2.bin", chip_name);
    err = request_firmware(& adev->gfx.mec2_fw, (char const *)(& fw_name), adev->dev);
    if (err != 0) {
      goto out;
    } else {
    }
    err = amdgpu_ucode_validate(adev->gfx.mec2_fw);
    if (err != 0) {
      goto out;
    } else {
    }
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "radeon/%s_rlc.bin", chip_name);
  err = request_firmware(& adev->gfx.rlc_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.rlc_fw);
  out: ;
  if (err != 0) {
    printk("\vgfx7: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    release_firmware(adev->gfx.pfp_fw);
    adev->gfx.pfp_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.me_fw);
    adev->gfx.me_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.ce_fw);
    adev->gfx.ce_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.mec_fw);
    adev->gfx.mec_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.mec2_fw);
    adev->gfx.mec2_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.rlc_fw);
    adev->gfx.rlc_fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static void gfx_v7_0_tiling_mode_table_init(struct amdgpu_device *adev )
{
  u32 num_tile_mode_states ;
  u32 num_secondary_tile_mode_states ;
  u32 reg_offset ;
  u32 gb_tile_moden ;
  u32 split_equal_to_row_size ;
  {
  num_tile_mode_states = 32U;
  num_secondary_tile_mode_states = 16U;
  switch (adev->gfx.config.mem_row_size_in_kb) {
  case 1U:
  split_equal_to_row_size = 4U;
  goto ldv_52857;
  case 2U: ;
  default:
  split_equal_to_row_size = 5U;
  goto ldv_52857;
  case 4U:
  split_equal_to_row_size = 6U;
  goto ldv_52857;
  }
  ldv_52857: ;
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  reg_offset = 0U;
  goto ldv_52896;
  ldv_52895: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8388944U;
  goto ldv_52863;
  case 1U:
  gb_tile_moden = 8390992U;
  goto ldv_52863;
  case 2U:
  gb_tile_moden = 8393040U;
  goto ldv_52863;
  case 3U:
  gb_tile_moden = 8395088U;
  goto ldv_52863;
  case 4U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8388944U;
  goto ldv_52863;
  case 5U:
  gb_tile_moden = 8388936U;
  goto ldv_52863;
  case 6U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8388948U;
  goto ldv_52863;
  case 7U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52863;
  case 8U:
  gb_tile_moden = 324U;
  goto ldv_52863;
  case 9U:
  gb_tile_moden = 328U;
  goto ldv_52863;
  case 10U:
  gb_tile_moden = 33554768U;
  goto ldv_52863;
  case 11U:
  gb_tile_moden = 100663636U;
  goto ldv_52863;
  case 12U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52863;
  case 13U:
  gb_tile_moden = 4194632U;
  goto ldv_52863;
  case 14U:
  gb_tile_moden = 37749072U;
  goto ldv_52863;
  case 15U:
  gb_tile_moden = 37749104U;
  goto ldv_52863;
  case 16U:
  gb_tile_moden = 104857940U;
  goto ldv_52863;
  case 17U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52863;
  case 18U:
  gb_tile_moden = 4194636U;
  goto ldv_52863;
  case 19U:
  gb_tile_moden = 4194636U;
  goto ldv_52863;
  case 20U:
  gb_tile_moden = 4194652U;
  goto ldv_52863;
  case 21U:
  gb_tile_moden = 4194676U;
  goto ldv_52863;
  case 22U:
  gb_tile_moden = 4194660U;
  goto ldv_52863;
  case 23U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52863;
  case 24U:
  gb_tile_moden = 4194652U;
  goto ldv_52863;
  case 25U:
  gb_tile_moden = 4194656U;
  goto ldv_52863;
  case 26U:
  gb_tile_moden = 4194680U;
  goto ldv_52863;
  case 27U:
  gb_tile_moden = 12583240U;
  goto ldv_52863;
  case 28U:
  gb_tile_moden = 46137680U;
  goto ldv_52863;
  case 29U:
  gb_tile_moden = 113246548U;
  goto ldv_52863;
  case 30U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52863;
  default:
  gb_tile_moden = 0U;
  goto ldv_52863;
  }
  ldv_52863:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_52896: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_52895;
  } else {
  }
  reg_offset = 0U;
  goto ldv_52915;
  ldv_52914: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 232U;
  goto ldv_52899;
  case 1U:
  gb_tile_moden = 228U;
  goto ldv_52899;
  case 2U:
  gb_tile_moden = 208U;
  goto ldv_52899;
  case 3U:
  gb_tile_moden = 208U;
  goto ldv_52899;
  case 4U:
  gb_tile_moden = 208U;
  goto ldv_52899;
  case 5U:
  gb_tile_moden = 144U;
  goto ldv_52899;
  case 6U:
  gb_tile_moden = 64U;
  goto ldv_52899;
  case 8U:
  gb_tile_moden = 237U;
  goto ldv_52899;
  case 9U:
  gb_tile_moden = 233U;
  goto ldv_52899;
  case 10U:
  gb_tile_moden = 232U;
  goto ldv_52899;
  case 11U:
  gb_tile_moden = 228U;
  goto ldv_52899;
  case 12U:
  gb_tile_moden = 208U;
  goto ldv_52899;
  case 13U:
  gb_tile_moden = 144U;
  goto ldv_52899;
  case 14U:
  gb_tile_moden = 64U;
  goto ldv_52899;
  default:
  gb_tile_moden = 0U;
  goto ldv_52899;
  }
  ldv_52899:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_52915: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_52914;
  } else {
  }
  goto ldv_52917;
  case 3U:
  reg_offset = 0U;
  goto ldv_52953;
  ldv_52952: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8389712U;
  goto ldv_52920;
  case 1U:
  gb_tile_moden = 8391760U;
  goto ldv_52920;
  case 2U:
  gb_tile_moden = 8393808U;
  goto ldv_52920;
  case 3U:
  gb_tile_moden = 8395856U;
  goto ldv_52920;
  case 4U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8389712U;
  goto ldv_52920;
  case 5U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8389704U;
  goto ldv_52920;
  case 6U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8389716U;
  goto ldv_52920;
  case 7U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8388948U;
  goto ldv_52920;
  case 8U:
  gb_tile_moden = 1092U;
  goto ldv_52920;
  case 9U:
  gb_tile_moden = 1096U;
  goto ldv_52920;
  case 10U:
  gb_tile_moden = 33555536U;
  goto ldv_52920;
  case 11U:
  gb_tile_moden = 100664404U;
  goto ldv_52920;
  case 12U:
  gb_tile_moden = 100663640U;
  goto ldv_52920;
  case 13U:
  gb_tile_moden = 4195400U;
  goto ldv_52920;
  case 14U:
  gb_tile_moden = 37749840U;
  goto ldv_52920;
  case 15U:
  gb_tile_moden = 37749872U;
  goto ldv_52920;
  case 16U:
  gb_tile_moden = 104858708U;
  goto ldv_52920;
  case 17U:
  gb_tile_moden = 104857940U;
  goto ldv_52920;
  case 18U:
  gb_tile_moden = 4195404U;
  goto ldv_52920;
  case 19U:
  gb_tile_moden = 16778316U;
  goto ldv_52920;
  case 20U:
  gb_tile_moden = 16778332U;
  goto ldv_52920;
  case 21U:
  gb_tile_moden = 16778356U;
  goto ldv_52920;
  case 22U:
  gb_tile_moden = 16778340U;
  goto ldv_52920;
  case 23U:
  gb_tile_moden = 16777572U;
  goto ldv_52920;
  case 24U:
  gb_tile_moden = 4195420U;
  goto ldv_52920;
  case 25U:
  gb_tile_moden = 16778336U;
  goto ldv_52920;
  case 26U:
  gb_tile_moden = 16778360U;
  goto ldv_52920;
  case 27U:
  gb_tile_moden = 12584008U;
  goto ldv_52920;
  case 28U:
  gb_tile_moden = 46138448U;
  goto ldv_52920;
  case 29U:
  gb_tile_moden = 113247316U;
  goto ldv_52920;
  case 30U:
  gb_tile_moden = 113246548U;
  goto ldv_52920;
  default:
  gb_tile_moden = 0U;
  goto ldv_52920;
  }
  ldv_52920:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_52953: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_52952;
  } else {
  }
  reg_offset = 0U;
  goto ldv_52972;
  ldv_52971: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 216U;
  goto ldv_52956;
  case 1U:
  gb_tile_moden = 212U;
  goto ldv_52956;
  case 2U:
  gb_tile_moden = 192U;
  goto ldv_52956;
  case 3U:
  gb_tile_moden = 192U;
  goto ldv_52956;
  case 4U:
  gb_tile_moden = 128U;
  goto ldv_52956;
  case 5U:
  gb_tile_moden = 64U;
  goto ldv_52956;
  case 6U:
  gb_tile_moden = 64U;
  goto ldv_52956;
  case 8U:
  gb_tile_moden = 216U;
  goto ldv_52956;
  case 9U:
  gb_tile_moden = 212U;
  goto ldv_52956;
  case 10U:
  gb_tile_moden = 192U;
  goto ldv_52956;
  case 11U:
  gb_tile_moden = 128U;
  goto ldv_52956;
  case 12U:
  gb_tile_moden = 208U;
  goto ldv_52956;
  case 13U:
  gb_tile_moden = 144U;
  goto ldv_52956;
  case 14U:
  gb_tile_moden = 64U;
  goto ldv_52956;
  default:
  gb_tile_moden = 0U;
  goto ldv_52956;
  }
  ldv_52956:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_52972: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_52971;
  } else {
  }
  goto ldv_52917;
  case 2U: ;
  case 1U: ;
  case 4U: ;
  default:
  reg_offset = 0U;
  goto ldv_53012;
  ldv_53011: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8388624U;
  goto ldv_52979;
  case 1U:
  gb_tile_moden = 8390672U;
  goto ldv_52979;
  case 2U:
  gb_tile_moden = 8392720U;
  goto ldv_52979;
  case 3U:
  gb_tile_moden = 8394768U;
  goto ldv_52979;
  case 4U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8388624U;
  goto ldv_52979;
  case 5U:
  gb_tile_moden = 8388616U;
  goto ldv_52979;
  case 6U:
  gb_tile_moden = (split_equal_to_row_size << 11) | 8388628U;
  goto ldv_52979;
  case 7U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52979;
  case 8U:
  gb_tile_moden = 4U;
  goto ldv_52979;
  case 9U:
  gb_tile_moden = 8U;
  goto ldv_52979;
  case 10U:
  gb_tile_moden = 33554448U;
  goto ldv_52979;
  case 11U:
  gb_tile_moden = 100663316U;
  goto ldv_52979;
  case 12U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52979;
  case 13U:
  gb_tile_moden = 4194312U;
  goto ldv_52979;
  case 14U:
  gb_tile_moden = 37748752U;
  goto ldv_52979;
  case 15U:
  gb_tile_moden = 37748784U;
  goto ldv_52979;
  case 16U:
  gb_tile_moden = 104857620U;
  goto ldv_52979;
  case 17U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52979;
  case 18U:
  gb_tile_moden = 4194316U;
  goto ldv_52979;
  case 19U:
  gb_tile_moden = 16777228U;
  goto ldv_52979;
  case 20U:
  gb_tile_moden = 16777244U;
  goto ldv_52979;
  case 21U:
  gb_tile_moden = 16777268U;
  goto ldv_52979;
  case 22U:
  gb_tile_moden = 16777252U;
  goto ldv_52979;
  case 23U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52979;
  case 24U:
  gb_tile_moden = 4194332U;
  goto ldv_52979;
  case 25U:
  gb_tile_moden = 16777248U;
  goto ldv_52979;
  case 26U:
  gb_tile_moden = 16777272U;
  goto ldv_52979;
  case 27U:
  gb_tile_moden = 12582920U;
  goto ldv_52979;
  case 28U:
  gb_tile_moden = 46137360U;
  goto ldv_52979;
  case 29U:
  gb_tile_moden = 113246228U;
  goto ldv_52979;
  case 30U:
  gb_tile_moden = split_equal_to_row_size << 11;
  goto ldv_52979;
  default:
  gb_tile_moden = 0U;
  goto ldv_52979;
  }
  ldv_52979:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_53012: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_53011;
  } else {
  }
  reg_offset = 0U;
  goto ldv_53031;
  ldv_53030: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 168U;
  goto ldv_53015;
  case 1U:
  gb_tile_moden = 164U;
  goto ldv_53015;
  case 2U:
  gb_tile_moden = 148U;
  goto ldv_53015;
  case 3U:
  gb_tile_moden = 144U;
  goto ldv_53015;
  case 4U:
  gb_tile_moden = 144U;
  goto ldv_53015;
  case 5U:
  gb_tile_moden = 144U;
  goto ldv_53015;
  case 6U:
  gb_tile_moden = 144U;
  goto ldv_53015;
  case 8U:
  gb_tile_moden = 238U;
  goto ldv_53015;
  case 9U:
  gb_tile_moden = 234U;
  goto ldv_53015;
  case 10U:
  gb_tile_moden = 233U;
  goto ldv_53015;
  case 11U:
  gb_tile_moden = 229U;
  goto ldv_53015;
  case 12U:
  gb_tile_moden = 228U;
  goto ldv_53015;
  case 13U:
  gb_tile_moden = 224U;
  goto ldv_53015;
  case 14U:
  gb_tile_moden = 144U;
  goto ldv_53015;
  default:
  gb_tile_moden = 0U;
  goto ldv_53015;
  }
  ldv_53015:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_53031: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_53030;
  } else {
  }
  goto ldv_52917;
  }
  ldv_52917: ;
  return;
}
}
void gfx_v7_0_select_se_sh(struct amdgpu_device *adev , u32 se_num , u32 sh_num )
{
  u32 data ;
  {
  data = 1073741824U;
  if (se_num == 4294967295U && sh_num == 4294967295U) {
    data = data | 2684354560U;
  } else
  if (se_num == 4294967295U) {
    data = ((sh_num << 8) | data) | 2147483648U;
  } else
  if (sh_num == 4294967295U) {
    data = ((se_num << 16) | data) | 536870912U;
  } else {
    data = ((sh_num << 8) | (se_num << 16)) | data;
  }
  amdgpu_mm_wreg(adev, 49664U, data, 0);
  return;
}
}
static u32 gfx_v7_0_create_bitmask(u32 bit_width )
{
  u32 i ;
  u32 mask ;
  {
  mask = 0U;
  i = 0U;
  goto ldv_53045;
  ldv_53044:
  mask = mask << 1;
  mask = mask | 1U;
  i = i + 1U;
  ldv_53045: ;
  if (i < bit_width) {
    goto ldv_53044;
  } else {
  }
  return (mask);
}
}
static u32 gfx_v7_0_get_rb_disabled(struct amdgpu_device *adev , u32 max_rb_num_per_se ,
                                    u32 sh_per_se )
{
  u32 data ;
  u32 mask ;
  u32 tmp ;
  {
  data = amdgpu_mm_rreg(adev, 9789U, 0);
  if ((int )data & 1) {
    data = data & 16711680U;
  } else {
    data = 0U;
  }
  tmp = amdgpu_mm_rreg(adev, 9951U, 0);
  data = tmp | data;
  data = data >> 16;
  mask = gfx_v7_0_create_bitmask(max_rb_num_per_se / sh_per_se);
  return (data & mask);
}
}
static void gfx_v7_0_setup_rb(struct amdgpu_device *adev , u32 se_num , u32 sh_per_se ,
                              u32 max_rb_num_per_se )
{
  int i ;
  int j ;
  u32 data ;
  u32 mask ;
  u32 disabled_rbs ;
  u32 enabled_rbs ;
  {
  disabled_rbs = 0U;
  enabled_rbs = 0U;
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_53070;
  ldv_53069:
  j = 0;
  goto ldv_53067;
  ldv_53066:
  gfx_v7_0_select_se_sh(adev, (u32 )i, (u32 )j);
  data = gfx_v7_0_get_rb_disabled(adev, max_rb_num_per_se, sh_per_se);
  if ((unsigned int )adev->asic_type == 3U) {
    disabled_rbs = (data << (int )(((u32 )i * sh_per_se + (u32 )j) * 4U)) | disabled_rbs;
  } else {
    disabled_rbs = (data << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | disabled_rbs;
  }
  j = j + 1;
  ldv_53067: ;
  if ((u32 )j < sh_per_se) {
    goto ldv_53066;
  } else {
  }
  i = i + 1;
  ldv_53070: ;
  if ((u32 )i < se_num) {
    goto ldv_53069;
  } else {
  }
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  mask = 1U;
  i = 0;
  goto ldv_53073;
  ldv_53072: ;
  if ((disabled_rbs & mask) == 0U) {
    enabled_rbs = enabled_rbs | mask;
  } else {
  }
  mask = mask << 1;
  i = i + 1;
  ldv_53073: ;
  if ((u32 )i < max_rb_num_per_se * se_num) {
    goto ldv_53072;
  } else {
  }
  adev->gfx.config.backend_enable_mask = enabled_rbs;
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_53085;
  ldv_53084:
  gfx_v7_0_select_se_sh(adev, (u32 )i, 4294967295U);
  data = 0U;
  j = 0;
  goto ldv_53082;
  ldv_53081: ;
  switch (enabled_rbs & 3U) {
  case 0U: ;
  if (j == 0) {
    data = data | 768U;
  } else {
    data = data;
  }
  goto ldv_53076;
  case 1U:
  data = data;
  goto ldv_53076;
  case 2U:
  data = (u32 )(3 << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | data;
  goto ldv_53076;
  case 3U: ;
  default:
  data = (u32 )(2 << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | data;
  goto ldv_53076;
  }
  ldv_53076:
  enabled_rbs = enabled_rbs >> 2;
  j = j + 1;
  ldv_53082: ;
  if ((u32 )j < sh_per_se) {
    goto ldv_53081;
  } else {
  }
  amdgpu_mm_wreg(adev, 41172U, data, 0);
  i = i + 1;
  ldv_53085: ;
  if ((u32 )i < se_num) {
    goto ldv_53084;
  } else {
  }
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  return;
}
}
static void gmc_v7_0_init_compute_vmid(struct amdgpu_device *adev )
{
  int i ;
  u32 sh_mem_config ;
  u32 sh_mem_bases ;
  {
  sh_mem_bases = 1610637312U;
  sh_mem_config = 12U;
  sh_mem_config = sh_mem_config | 48U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 8;
  goto ldv_53094;
  ldv_53093:
  cik_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  amdgpu_mm_wreg(adev, 8973U, sh_mem_config, 0);
  amdgpu_mm_wreg(adev, 8971U, 1U, 0);
  amdgpu_mm_wreg(adev, 8972U, 0U, 0);
  amdgpu_mm_wreg(adev, 8970U, sh_mem_bases, 0);
  i = i + 1;
  ldv_53094: ;
  if (i <= 15) {
    goto ldv_53093;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  return;
}
}
static void gfx_v7_0_gpu_init(struct amdgpu_device *adev )
{
  u32 gb_addr_config ;
  u32 mc_shared_chmap ;
  u32 mc_arb_ramcfg ;
  u32 dimm00_addr_map ;
  u32 dimm01_addr_map ;
  u32 dimm10_addr_map ;
  u32 dimm11_addr_map ;
  u32 sh_mem_cfg ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  {
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  adev->gfx.config.max_shader_engines = 2U;
  adev->gfx.config.max_tile_pipes = 4U;
  adev->gfx.config.max_cu_per_sh = 7U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 2U;
  adev->gfx.config.max_texture_channel_caches = 4U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 302055425U;
  goto ldv_53110;
  case 3U:
  adev->gfx.config.max_shader_engines = 4U;
  adev->gfx.config.max_tile_pipes = 16U;
  adev->gfx.config.max_cu_per_sh = 11U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 4U;
  adev->gfx.config.max_texture_channel_caches = 16U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 302059523U;
  goto ldv_53110;
  case 1U:
  adev->gfx.config.max_shader_engines = 1U;
  adev->gfx.config.max_tile_pipes = 4U;
  if (((((((unsigned int )(adev->pdev)->device == 4868U || (unsigned int )(adev->pdev)->device == 4869U) || (unsigned int )(adev->pdev)->device == 4876U) || (unsigned int )(adev->pdev)->device == 4879U) || (unsigned int )(adev->pdev)->device == 4880U) || (unsigned int )(adev->pdev)->device == 4881U) || (unsigned int )(adev->pdev)->device == 4892U) {
    adev->gfx.config.max_cu_per_sh = 8U;
    adev->gfx.config.max_backends_per_se = 2U;
  } else
  if (((((unsigned int )(adev->pdev)->device == 4873U || (unsigned int )(adev->pdev)->device == 4874U) || (unsigned int )(adev->pdev)->device == 4877U) || (unsigned int )(adev->pdev)->device == 4883U) || (unsigned int )(adev->pdev)->device == 4893U) {
    adev->gfx.config.max_cu_per_sh = 6U;
    adev->gfx.config.max_backends_per_se = 2U;
  } else
  if ((((((unsigned int )(adev->pdev)->device == 4870U || (unsigned int )(adev->pdev)->device == 4871U) || (unsigned int )(adev->pdev)->device == 4875U) || (unsigned int )(adev->pdev)->device == 4878U) || (unsigned int )(adev->pdev)->device == 4885U) || (unsigned int )(adev->pdev)->device == 4891U) {
    adev->gfx.config.max_cu_per_sh = 4U;
    adev->gfx.config.max_backends_per_se = 1U;
  } else {
    adev->gfx.config.max_cu_per_sh = 3U;
    adev->gfx.config.max_backends_per_se = 1U;
  }
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_texture_channel_caches = 4U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 16U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 302055425U;
  goto ldv_53110;
  case 2U: ;
  case 4U: ;
  default:
  adev->gfx.config.max_shader_engines = 1U;
  adev->gfx.config.max_tile_pipes = 2U;
  adev->gfx.config.max_cu_per_sh = 2U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 1U;
  adev->gfx.config.max_texture_channel_caches = 2U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 16U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 302055425U;
  goto ldv_53110;
  }
  ldv_53110:
  amdgpu_mm_wreg(adev, 8192U, 255U, 0);
  mc_shared_chmap = amdgpu_mm_rreg(adev, 2049U, 0);
  adev->gfx.config.mc_arb_ramcfg = amdgpu_mm_rreg(adev, 2520U, 0);
  mc_arb_ramcfg = adev->gfx.config.mc_arb_ramcfg;
  adev->gfx.config.num_tile_pipes = adev->gfx.config.max_tile_pipes;
  adev->gfx.config.mem_max_burst_length_bytes = 256U;
  if ((adev->flags & 131072UL) != 0UL) {
    tmp = amdgpu_mm_rreg(adev, 2577U, 0);
    dimm00_addr_map = tmp & 15U;
    dimm01_addr_map = (tmp & 240U) >> 4;
    tmp = amdgpu_mm_rreg(adev, 2578U, 0);
    dimm10_addr_map = tmp & 15U;
    dimm11_addr_map = (tmp & 240U) >> 4;
    if (((dimm00_addr_map == 0U || dimm00_addr_map == 3U) || dimm00_addr_map == 4U) || dimm00_addr_map > 12U) {
      dimm00_addr_map = 0U;
    } else {
    }
    if (((dimm01_addr_map == 0U || dimm01_addr_map == 3U) || dimm01_addr_map == 4U) || dimm01_addr_map > 12U) {
      dimm01_addr_map = 0U;
    } else {
    }
    if (((dimm10_addr_map == 0U || dimm10_addr_map == 3U) || dimm10_addr_map == 4U) || dimm10_addr_map > 12U) {
      dimm10_addr_map = 0U;
    } else {
    }
    if (((dimm11_addr_map == 0U || dimm11_addr_map == 3U) || dimm11_addr_map == 4U) || dimm11_addr_map > 12U) {
      dimm11_addr_map = 0U;
    } else {
    }
    if (((dimm00_addr_map == 11U || dimm01_addr_map == 11U) || dimm10_addr_map == 11U) || dimm11_addr_map == 11U) {
      adev->gfx.config.mem_row_size_in_kb = 2U;
    } else {
      adev->gfx.config.mem_row_size_in_kb = 1U;
    }
  } else {
    tmp = (mc_arb_ramcfg & 192U) >> 6;
    adev->gfx.config.mem_row_size_in_kb = (unsigned int )((4 << (int )(tmp + 8U)) / 1024);
    if (adev->gfx.config.mem_row_size_in_kb > 4U) {
      adev->gfx.config.mem_row_size_in_kb = 4U;
    } else {
    }
  }
  adev->gfx.config.shader_engine_tile_size = 32U;
  adev->gfx.config.num_gpus = 1U;
  adev->gfx.config.multi_gpu_tile_size = 64U;
  gb_addr_config = gb_addr_config & 3489660927U;
  switch (adev->gfx.config.mem_row_size_in_kb) {
  case 1U: ;
  default:
  gb_addr_config = gb_addr_config;
  goto ldv_53118;
  case 2U:
  gb_addr_config = gb_addr_config | 268435456U;
  goto ldv_53118;
  case 4U:
  gb_addr_config = gb_addr_config | 536870912U;
  goto ldv_53118;
  }
  ldv_53118:
  adev->gfx.config.gb_addr_config = gb_addr_config;
  amdgpu_mm_wreg(adev, 9790U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 3026U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 768U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 13318U, gb_addr_config & 112U, 0);
  amdgpu_mm_wreg(adev, 13830U, gb_addr_config & 112U, 0);
  amdgpu_mm_wreg(adev, 15315U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 15316U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 15317U, gb_addr_config, 0);
  gfx_v7_0_tiling_mode_table_init(adev);
  gfx_v7_0_setup_rb(adev, adev->gfx.config.max_shader_engines, adev->gfx.config.max_sh_per_se,
                    adev->gfx.config.max_backends_per_se);
  amdgpu_mm_wreg(adev, 8665U, 24624U, 0);
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  sh_mem_cfg = 12U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_53122;
  ldv_53121:
  cik_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  amdgpu_mm_wreg(adev, 8973U, sh_mem_cfg, 0);
  amdgpu_mm_wreg(adev, 8971U, 1U, 0);
  amdgpu_mm_wreg(adev, 8972U, 0U, 0);
  amdgpu_mm_wreg(adev, 8970U, 0U, 0);
  i = i + 1;
  ldv_53122: ;
  if (i <= 15) {
    goto ldv_53121;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  gmc_v7_0_init_compute_vmid(adev);
  amdgpu_mm_wreg(adev, 9240U, 32U, 0);
  amdgpu_mm_wreg(adev, 9538U, 65536U, 0);
  tmp = amdgpu_mm_rreg(adev, 9280U, 0);
  tmp = tmp | 50331648U;
  amdgpu_mm_wreg(adev, 9280U, tmp, 0);
  amdgpu_mm_wreg(adev, 8960U, 1U, 0);
  amdgpu_mm_wreg(adev, 9740U, 0U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 9741U, 0);
  tmp = tmp___0 & 267386880U;
  tmp = tmp | 1024U;
  amdgpu_mm_wreg(adev, 9741U, tmp, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 9742U, 0);
  tmp = tmp___1 & 4294835683U;
  tmp = tmp | 131584U;
  amdgpu_mm_wreg(adev, 9742U, tmp, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 9860U, 0);
  tmp = tmp___2 & 4294901759U;
  tmp = tmp | 98824U;
  amdgpu_mm_wreg(adev, 9860U, tmp, 0);
  amdgpu_mm_wreg(adev, 9295U, 4U, 0);
  amdgpu_mm_wreg(adev, 8947U, ((adev->gfx.config.sc_prim_fifo_size_frontend | (adev->gfx.config.sc_prim_fifo_size_backend << 6)) | (adev->gfx.config.sc_hiz_tile_fifo_size << 15)) | (adev->gfx.config.sc_earlyz_tile_fifo_size << 23),
                 0);
  amdgpu_mm_wreg(adev, 49741U, 1U, 0);
  amdgpu_mm_wreg(adev, 55304U, 0U, 0);
  amdgpu_mm_wreg(adev, 8960U, 0U, 0);
  amdgpu_mm_wreg(adev, 8905U, 16715775U, 0);
  amdgpu_mm_wreg(adev, 8753U, 194U, 0);
  amdgpu_mm_wreg(adev, 8757U, 16U, 0);
  amdgpu_mm_wreg(adev, 49793U, 0U, 0);
  amdgpu_mm_wreg(adev, 8837U, 7U, 0);
  amdgpu_mm_wreg(adev, 8956U, 1U, 0);
  mutex_unlock(& adev->grbm_idx_mutex);
  __const_udelay(214750UL);
  return;
}
}
static void gfx_v7_0_scratch_init(struct amdgpu_device *adev )
{
  int i ;
  {
  adev->gfx.scratch.num_reg = 7U;
  adev->gfx.scratch.reg_base = 49216U;
  i = 0;
  goto ldv_53129;
  ldv_53128:
  adev->gfx.scratch.free[i] = 1;
  adev->gfx.scratch.reg[i] = adev->gfx.scratch.reg_base + (u32 )i;
  i = i + 1;
  ldv_53129: ;
  if ((unsigned int )i < adev->gfx.scratch.num_reg) {
    goto ldv_53128;
  } else {
  }
  return;
}
}
static int gfx_v7_0_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 scratch ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_gfx_scratch_get(adev, & scratch);
  if (r != 0) {
    drm_err("amdgpu: cp failed to get scratch reg (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_mm_wreg(adev, scratch, 3405700781U, 0);
  r = amdgpu_ring_lock(ring, 3U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring %d (%d).\n", ring->idx, r);
    amdgpu_gfx_scratch_free(adev, scratch);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 3221321984U);
  amdgpu_ring_write(ring, scratch - 49152U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_53141;
  ldv_53140:
  tmp = amdgpu_mm_rreg(adev, scratch, 0);
  if (tmp == 3735928559U) {
    goto ldv_53139;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53141: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_53140;
  } else {
  }
  ldv_53139: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (scratch(0x%04X)=0x%08X)\n", ring->idx, scratch,
            tmp);
    r = -22;
  }
  amdgpu_gfx_scratch_free(adev, scratch);
  return (r);
}
}
static void gfx_v7_0_ring_emit_hdp_flush(struct amdgpu_ring *ring )
{
  u32 ref_and_mask ;
  int usepfp ;
  {
  usepfp = (unsigned int )ring->type != 1U;
  if ((unsigned int )ring->type == 1U) {
    switch (ring->me) {
    case 1U:
    ref_and_mask = (u32 )(4 << (int )ring->pipe);
    goto ldv_53148;
    case 2U:
    ref_and_mask = (u32 )(64 << (int )ring->pipe);
    goto ldv_53148;
    default: ;
    return;
    }
    ldv_53148: ;
  } else {
    ref_and_mask = 1U;
  }
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, (u32 )((usepfp << 8) | 67));
  amdgpu_ring_write(ring, 5431U);
  amdgpu_ring_write(ring, 5432U);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, 32U);
  return;
}
}
static void gfx_v7_0_ring_emit_fence_gfx(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                         unsigned int flags )
{
  bool write64bit ;
  bool int_sel ;
  {
  write64bit = (flags & 1U) != 0U;
  int_sel = (flags & 2U) != 0U;
  amdgpu_ring_write(ring, 3221505792U);
  amdgpu_ring_write(ring, 197908U);
  amdgpu_ring_write(ring, (u32 )addr & 4294967292U);
  amdgpu_ring_write(ring, ((unsigned int )(addr >> 32ULL) & 65535U) | 536870912U);
  amdgpu_ring_write(ring, (unsigned int )seq - 1U);
  amdgpu_ring_write(ring, (unsigned int )((seq - 1ULL) >> 32ULL));
  amdgpu_ring_write(ring, 3221505792U);
  amdgpu_ring_write(ring, 197908U);
  amdgpu_ring_write(ring, (u32 )addr & 4294967292U);
  amdgpu_ring_write(ring, (((unsigned int )(addr >> 32ULL) & 65535U) | ((int )write64bit ? 1073741824U : 536870912U)) | ((int )int_sel ? 33554432U : 0U));
  amdgpu_ring_write(ring, (unsigned int )seq);
  amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  return;
}
}
static void gfx_v7_0_ring_emit_fence_compute(struct amdgpu_ring *ring , u64 addr ,
                                             u64 seq , unsigned int flags )
{
  bool write64bit ;
  bool int_sel ;
  {
  write64bit = (flags & 1U) != 0U;
  int_sel = (flags & 2U) != 0U;
  amdgpu_ring_write(ring, 3221571840U);
  amdgpu_ring_write(ring, 197908U);
  amdgpu_ring_write(ring, (u32 )(((int )write64bit ? 1073741824 : 536870912) | ((int )int_sel ? 33554432 : 0)));
  amdgpu_ring_write(ring, (u32 )addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )seq);
  amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  return;
}
}
static bool gfx_v7_0_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  uint64_t addr ;
  unsigned int sel ;
  {
  addr = semaphore->gpu_addr;
  sel = (int )emit_wait ? 3758096384U : 3221225472U;
  amdgpu_ring_write(ring, 3221305600U);
  amdgpu_ring_write(ring, (u32 )addr);
  amdgpu_ring_write(ring, ((unsigned int )(addr >> 32ULL) & 65535U) | sel);
  if ((int )emit_wait && (unsigned int )ring->type == 0U) {
    amdgpu_ring_write(ring, 3221242368U);
    amdgpu_ring_write(ring, 0U);
  } else {
  }
  return (1);
}
}
static void gfx_v7_0_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  bool need_ctx_switch ;
  u32 header ;
  u32 control ;
  u32 next_rptr ;
  {
  need_ctx_switch = (unsigned long )ring->current_ctx != (unsigned long )ib->ctx;
  control = 0U;
  next_rptr = ring->wptr + 5U;
  if (((unsigned int )ring->type == 0U && (ib->flags & 2U) != 0U) && ! need_ctx_switch) {
    return;
  } else {
  }
  if ((unsigned int )ring->type == 1U) {
    control = control | 8388608U;
  } else {
  }
  if ((int )need_ctx_switch && (unsigned int )ring->type == 0U) {
    next_rptr = next_rptr + 2U;
  } else {
  }
  next_rptr = next_rptr + 4U;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 1049856U);
  amdgpu_ring_write(ring, (u32 )ring->next_rptr_gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ring->next_rptr_gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, next_rptr);
  if ((int )need_ctx_switch && (unsigned int )ring->type == 0U) {
    amdgpu_ring_write(ring, 3221261056U);
    amdgpu_ring_write(ring, 0U);
  } else {
  }
  if ((int )ib->flags & 1) {
    header = 3221369600U;
  } else {
    header = 3221372672U;
  }
  control = (ib->length_dw | ((unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0) ? (ib->vm)->ids[ring->idx].id << 24 : 0U)) | control;
  amdgpu_ring_write(ring, header);
  amdgpu_ring_write(ring, (u32 )ib->gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL) & 65535U);
  amdgpu_ring_write(ring, control);
  return;
}
}
static int gfx_v7_0_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ib ib ;
  u32 scratch ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_gfx_scratch_get(adev, & scratch);
  if (r != 0) {
    drm_err("amdgpu: failed to get scratch reg (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_mm_wreg(adev, scratch, 3405700781U, 0);
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 256U, & ib);
  if (r != 0) {
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    amdgpu_gfx_scratch_free(adev, scratch);
    return (r);
  } else {
  }
  *(ib.ptr) = 3221321984U;
  *(ib.ptr + 1UL) = scratch - 49152U;
  *(ib.ptr + 2UL) = 3735928559U;
  ib.length_dw = 3U;
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)0);
  if (r != 0) {
    amdgpu_gfx_scratch_free(adev, scratch);
    amdgpu_ib_free(adev, & ib);
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_fence_wait(ib.fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    amdgpu_gfx_scratch_free(adev, scratch);
    amdgpu_ib_free(adev, & ib);
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_53193;
  ldv_53192:
  tmp = amdgpu_mm_rreg(adev, scratch, 0);
  if (tmp == 3735928559U) {
    goto ldv_53191;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53193: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_53192;
  } else {
  }
  ldv_53191: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ib test on ring %d succeeded in %u usecs\n", ((ib.fence)->ring)->idx,
           i);
  } else {
    drm_err("amdgpu: ib test failed (scratch(0x%04X)=0x%08X)\n", scratch, tmp);
    r = -22;
  }
  amdgpu_gfx_scratch_free(adev, scratch);
  amdgpu_ib_free(adev, & ib);
  return (r);
}
}
static void gfx_v7_0_cp_gfx_enable(struct amdgpu_device *adev , bool enable )
{
  int i ;
  {
  if ((int )enable) {
    amdgpu_mm_wreg(adev, 8630U, 0U, 0);
  } else {
    amdgpu_mm_wreg(adev, 8630U, 352321536U, 0);
    i = 0;
    goto ldv_53200;
    ldv_53199:
    adev->gfx.gfx_ring[i].ready = 0;
    i = i + 1;
    ldv_53200: ;
    if ((unsigned int )i < adev->gfx.num_gfx_rings) {
      goto ldv_53199;
    } else {
    }
  }
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v7_0_cp_gfx_load_microcode(struct amdgpu_device *adev )
{
  struct gfx_firmware_header_v1_0 const *pfp_hdr ;
  struct gfx_firmware_header_v1_0 const *ce_hdr ;
  struct gfx_firmware_header_v1_0 const *me_hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  __le32 const *tmp___1 ;
  __u32 tmp___2 ;
  __le32 const *tmp___3 ;
  __u32 tmp___4 ;
  {
  if (((unsigned long )adev->gfx.me_fw == (unsigned long )((struct firmware const *)0) || (unsigned long )adev->gfx.pfp_fw == (unsigned long )((struct firmware const *)0)) || (unsigned long )adev->gfx.ce_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  pfp_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.pfp_fw)->data;
  ce_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.ce_fw)->data;
  me_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.me_fw)->data;
  amdgpu_ucode_print_gfx_hdr(& pfp_hdr->header);
  amdgpu_ucode_print_gfx_hdr(& ce_hdr->header);
  amdgpu_ucode_print_gfx_hdr(& me_hdr->header);
  adev->gfx.pfp_fw_version = pfp_hdr->header.ucode_version;
  adev->gfx.ce_fw_version = ce_hdr->header.ucode_version;
  adev->gfx.me_fw_version = me_hdr->header.ucode_version;
  adev->gfx.me_feature_version = me_hdr->ucode_feature_version;
  adev->gfx.ce_feature_version = ce_hdr->ucode_feature_version;
  adev->gfx.pfp_feature_version = pfp_hdr->ucode_feature_version;
  gfx_v7_0_cp_gfx_enable(adev, 0);
  fw_data = (__le32 const *)(adev->gfx.pfp_fw)->data + (unsigned long )pfp_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )pfp_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 12372U, 0U, 0);
  i = 0U;
  goto ldv_53212;
  ldv_53211:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, 12373U, tmp___0, 0);
  i = i + 1U;
  ldv_53212: ;
  if (i < fw_size) {
    goto ldv_53211;
  } else {
  }
  amdgpu_mm_wreg(adev, 12372U, adev->gfx.pfp_fw_version, 0);
  fw_data = (__le32 const *)(adev->gfx.ce_fw)->data + (unsigned long )ce_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )ce_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 12378U, 0U, 0);
  i = 0U;
  goto ldv_53215;
  ldv_53214:
  tmp___1 = fw_data;
  fw_data = fw_data + 1;
  tmp___2 = __le32_to_cpup(tmp___1);
  amdgpu_mm_wreg(adev, 12379U, tmp___2, 0);
  i = i + 1U;
  ldv_53215: ;
  if (i < fw_size) {
    goto ldv_53214;
  } else {
  }
  amdgpu_mm_wreg(adev, 12378U, adev->gfx.ce_fw_version, 0);
  fw_data = (__le32 const *)(adev->gfx.me_fw)->data + (unsigned long )me_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )me_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 12375U, 0U, 0);
  i = 0U;
  goto ldv_53218;
  ldv_53217:
  tmp___3 = fw_data;
  fw_data = fw_data + 1;
  tmp___4 = __le32_to_cpup(tmp___3);
  amdgpu_mm_wreg(adev, 12376U, tmp___4, 0);
  i = i + 1U;
  ldv_53218: ;
  if (i < fw_size) {
    goto ldv_53217;
  } else {
  }
  amdgpu_mm_wreg(adev, 12375U, adev->gfx.me_fw_version, 0);
  return (0);
}
}
static int gfx_v7_0_cp_gfx_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  struct cs_section_def const *sect ;
  struct cs_extent_def const *ext ;
  int r ;
  int i ;
  u32 tmp ;
  {
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring);
  sect = (struct cs_section_def const *)0;
  ext = (struct cs_extent_def const *)0;
  amdgpu_mm_wreg(adev, 12462U, adev->gfx.config.max_hw_contexts - 1U, 0);
  amdgpu_mm_wreg(adev, 12368U, 0U, 0);
  amdgpu_mm_wreg(adev, 12363U, 1U, 0);
  gfx_v7_0_cp_gfx_enable(adev, 1);
  tmp = gfx_v7_0_get_csb_size(adev);
  r = amdgpu_ring_lock(ring, tmp + 8U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 3221360896U);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_write(ring, 32768U);
  amdgpu_ring_write(ring, 32768U);
  amdgpu_ring_write(ring, 3221244416U);
  amdgpu_ring_write(ring, 536870912U);
  amdgpu_ring_write(ring, 3221301248U);
  amdgpu_ring_write(ring, 2147483648U);
  amdgpu_ring_write(ring, 2147483648U);
  sect = adev->gfx.rlc.cs_data;
  goto ldv_53235;
  ldv_53234:
  ext = sect->section;
  goto ldv_53232;
  ldv_53231: ;
  if ((unsigned int )sect->id == 1U) {
    amdgpu_ring_write(ring, (((unsigned int )ext->reg_count & 16383U) << 16) | 3221252352U);
    amdgpu_ring_write(ring, (unsigned int )ext->reg_index - 40960U);
    i = 0;
    goto ldv_53229;
    ldv_53228:
    amdgpu_ring_write(ring, *(ext->extent + (unsigned long )i));
    i = i + 1;
    ldv_53229: ;
    if ((unsigned int )i < (unsigned int )ext->reg_count) {
      goto ldv_53228;
    } else {
    }
  } else {
  }
  ext = ext + 1;
  ldv_53232: ;
  if ((unsigned long )ext->extent != (unsigned long )((unsigned int const * )0U)) {
    goto ldv_53231;
  } else {
  }
  sect = sect + 1;
  ldv_53235: ;
  if ((unsigned long )sect->section != (unsigned long )((struct cs_extent_def const * )0)) {
    goto ldv_53234;
  } else {
  }
  amdgpu_ring_write(ring, 3221383424U);
  amdgpu_ring_write(ring, 212U);
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  amdgpu_ring_write(ring, 369098770U);
  amdgpu_ring_write(ring, 0U);
  goto ldv_53238;
  case 1U:
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  goto ldv_53238;
  case 2U: ;
  case 4U:
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  goto ldv_53238;
  case 3U:
  amdgpu_ring_write(ring, 973084186U);
  amdgpu_ring_write(ring, 46U);
  goto ldv_53238;
  default:
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  goto ldv_53238;
  }
  ldv_53238:
  amdgpu_ring_write(ring, 3221244416U);
  amdgpu_ring_write(ring, 805306368U);
  amdgpu_ring_write(ring, 3221230080U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 3221383424U);
  amdgpu_ring_write(ring, 790U);
  amdgpu_ring_write(ring, 14U);
  amdgpu_ring_write(ring, 16U);
  amdgpu_ring_unlock_commit(ring);
  return (0);
}
}
static int gfx_v7_0_cp_gfx_resume(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 tmp ;
  u32 rb_bufsz ;
  u64 rb_addr ;
  u64 rptr_addr ;
  int r ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  unsigned long __ms ;
  unsigned long tmp___2 ;
  {
  amdgpu_mm_wreg(adev, 49263U, 0U, 0);
  if ((unsigned int )adev->asic_type != 3U) {
    amdgpu_mm_wreg(adev, 49266U, 0U, 0);
  } else {
  }
  amdgpu_mm_wreg(adev, 8641U, 0U, 0);
  amdgpu_mm_wreg(adev, 12369U, 0U, 0);
  amdgpu_mm_wreg(adev, 49233U, 0U, 0);
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring);
  tmp___0 = __roundup_pow_of_two((unsigned long )(ring->ring_size / 8U));
  tmp___1 = __ilog2_u64((u64 )tmp___0);
  rb_bufsz = (u32 )tmp___1;
  tmp = rb_bufsz | 2304U;
  amdgpu_mm_wreg(adev, 12353U, tmp, 0);
  amdgpu_mm_wreg(adev, 12353U, tmp | 2147483648U, 0);
  ring->wptr = 0U;
  amdgpu_mm_wreg(adev, 12357U, ring->wptr, 0);
  rptr_addr = adev->wb.gpu_addr + (uint64_t )(ring->rptr_offs * 4U);
  amdgpu_mm_wreg(adev, 12355U, (unsigned int )rptr_addr, 0);
  amdgpu_mm_wreg(adev, 12356U, (unsigned int )(rptr_addr >> 32ULL) & 255U, 0);
  amdgpu_mm_wreg(adev, 49232U, 0U, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_53255;
    ldv_53254:
    __const_udelay(4295000UL);
    ldv_53255:
    tmp___2 = __ms;
    __ms = __ms - 1UL;
    if (tmp___2 != 0UL) {
      goto ldv_53254;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 12353U, tmp, 0);
  rb_addr = ring->gpu_addr >> 8;
  amdgpu_mm_wreg(adev, 12352U, (u32 )rb_addr, 0);
  amdgpu_mm_wreg(adev, 12465U, (unsigned int )(rb_addr >> 32ULL), 0);
  gfx_v7_0_cp_gfx_start(adev);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  return (0);
}
}
static u32 gfx_v7_0_ring_get_rptr_gfx(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs);
  return (rptr);
}
}
static u32 gfx_v7_0_ring_get_wptr_gfx(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 wptr ;
  {
  adev = ring->adev;
  wptr = amdgpu_mm_rreg(adev, 12357U, 0);
  return (wptr);
}
}
static void gfx_v7_0_ring_set_wptr_gfx(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  amdgpu_mm_wreg(adev, 12357U, ring->wptr, 0);
  amdgpu_mm_rreg(adev, 12357U, 0);
  return;
}
}
static u32 gfx_v7_0_ring_get_rptr_compute(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs);
  return (rptr);
}
}
static u32 gfx_v7_0_ring_get_wptr_compute(struct amdgpu_ring *ring )
{
  u32 wptr ;
  {
  wptr = *((ring->adev)->wb.wb + (unsigned long )ring->wptr_offs);
  return (wptr);
}
}
static void gfx_v7_0_ring_set_wptr_compute(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  *(adev->wb.wb + (unsigned long )ring->wptr_offs) = ring->wptr;
  amdgpu_mm_wdoorbell(adev, ring->doorbell_index, ring->wptr);
  return;
}
}
static void gfx_v7_0_cp_compute_enable(struct amdgpu_device *adev , bool enable )
{
  int i ;
  {
  if ((int )enable) {
    amdgpu_mm_wreg(adev, 8333U, 0U, 0);
  } else {
    amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
    i = 0;
    goto ldv_53288;
    ldv_53287:
    adev->gfx.compute_ring[i].ready = 0;
    i = i + 1;
    ldv_53288: ;
    if ((unsigned int )i < adev->gfx.num_compute_rings) {
      goto ldv_53287;
    } else {
    }
  }
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v7_0_cp_compute_load_microcode(struct amdgpu_device *adev )
{
  struct gfx_firmware_header_v1_0 const *mec_hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  struct gfx_firmware_header_v1_0 const *mec2_hdr ;
  __le32 const *tmp___1 ;
  __u32 tmp___2 ;
  {
  if ((unsigned long )adev->gfx.mec_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  mec_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec_fw)->data;
  amdgpu_ucode_print_gfx_hdr(& mec_hdr->header);
  adev->gfx.mec_fw_version = mec_hdr->header.ucode_version;
  gfx_v7_0_cp_compute_enable(adev, 0);
  fw_data = (__le32 const *)(adev->gfx.mec_fw)->data + (unsigned long )mec_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )mec_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 12380U, 0U, 0);
  i = 0U;
  goto ldv_53298;
  ldv_53297:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, 12381U, tmp___0, 0);
  i = i + 1U;
  ldv_53298: ;
  if (i < fw_size) {
    goto ldv_53297;
  } else {
  }
  amdgpu_mm_wreg(adev, 12380U, 0U, 0);
  if ((unsigned int )adev->asic_type == 1U) {
    if ((unsigned long )adev->gfx.mec2_fw == (unsigned long )((struct firmware const *)0)) {
      return (-22);
    } else {
    }
    mec2_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec2_fw)->data;
    amdgpu_ucode_print_gfx_hdr(& mec2_hdr->header);
    adev->gfx.mec2_fw_version = mec2_hdr->header.ucode_version;
    fw_data = (__le32 const *)(adev->gfx.mec2_fw)->data + (unsigned long )mec2_hdr->header.ucode_array_offset_bytes;
    fw_size = (unsigned int )mec2_hdr->header.ucode_size_bytes / 4U;
    amdgpu_mm_wreg(adev, 12382U, 0U, 0);
    i = 0U;
    goto ldv_53302;
    ldv_53301:
    tmp___1 = fw_data;
    fw_data = fw_data + 1;
    tmp___2 = __le32_to_cpup(tmp___1);
    amdgpu_mm_wreg(adev, 12383U, tmp___2, 0);
    i = i + 1U;
    ldv_53302: ;
    if (i < fw_size) {
      goto ldv_53301;
    } else {
    }
    amdgpu_mm_wreg(adev, 12382U, 0U, 0);
  } else {
  }
  return (0);
}
}
static int gfx_v7_0_cp_compute_start(struct amdgpu_device *adev )
{
  {
  gfx_v7_0_cp_compute_enable(adev, 1);
  return (0);
}
}
static void gfx_v7_0_cp_compute_fini(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  struct amdgpu_ring *ring ;
  long tmp ;
  {
  i = 0;
  goto ldv_53314;
  ldv_53313:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if ((unsigned long )ring->mqd_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(ring->mqd_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve MQD bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(ring->mqd_obj);
    amdgpu_bo_unreserve(ring->mqd_obj);
    amdgpu_bo_unref(& ring->mqd_obj);
    ring->mqd_obj = (struct amdgpu_bo *)0;
  } else {
  }
  i = i + 1;
  ldv_53314: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53313;
  } else {
  }
  return;
}
}
static void gfx_v7_0_mec_fini(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->gfx.mec.hpd_eop_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->gfx.mec.hpd_eop_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve HPD EOP bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(adev->gfx.mec.hpd_eop_obj);
    amdgpu_bo_unreserve(adev->gfx.mec.hpd_eop_obj);
    amdgpu_bo_unref(& adev->gfx.mec.hpd_eop_obj);
    adev->gfx.mec.hpd_eop_obj = (struct amdgpu_bo *)0;
  } else {
  }
  return;
}
}
static int gfx_v7_0_mec_init(struct amdgpu_device *adev )
{
  int r ;
  u32 *hpd ;
  long tmp ;
  {
  adev->gfx.mec.num_mec = 1U;
  adev->gfx.mec.num_pipe = 1U;
  adev->gfx.mec.num_queue = (adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 8U;
  if ((unsigned long )adev->gfx.mec.hpd_eop_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, (unsigned long )((adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 4096U),
                         4096, 1, 2U, 0ULL, (struct sg_table *)0, & adev->gfx.mec.hpd_eop_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) create HDP EOP bo failed\n",
               r);
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_reserve(adev->gfx.mec.hpd_eop_obj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    gfx_v7_0_mec_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->gfx.mec.hpd_eop_obj, 2U, & adev->gfx.mec.hpd_eop_gpu_addr);
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) pin HDP EOP bo failed\n", r);
    gfx_v7_0_mec_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->gfx.mec.hpd_eop_obj, (void **)(& hpd));
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) map HDP EOP bo failed\n", r);
    gfx_v7_0_mec_fini(adev);
    return (r);
  } else {
  }
  memset((void *)hpd, 0, (size_t )((adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 4096U));
  amdgpu_bo_kunmap(adev->gfx.mec.hpd_eop_obj);
  amdgpu_bo_unreserve(adev->gfx.mec.hpd_eop_obj);
  return (0);
}
}
static int gfx_v7_0_cp_compute_resume(struct amdgpu_device *adev )
{
  int r ;
  int i ;
  int j ;
  u32 tmp ;
  bool use_doorbell ;
  u64 hqd_gpu_addr ;
  u64 mqd_gpu_addr ;
  u64 eop_gpu_addr ;
  u64 wb_gpu_addr ;
  u32 *buf ;
  struct bonaire_mqd *mqd ;
  int me ;
  int pipe ;
  struct amdgpu_ring *ring ;
  long tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  unsigned long tmp___3 ;
  int tmp___4 ;
  {
  use_doorbell = 1;
  r = gfx_v7_0_cp_compute_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 12416U, 0);
  tmp = tmp | 8388608U;
  amdgpu_mm_wreg(adev, 12416U, tmp, 0);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_53403;
  ldv_53402:
  me = i <= 3 ? 1 : 2;
  pipe = i > 3 ? i + -4 : i;
  eop_gpu_addr = adev->gfx.mec.hpd_eop_gpu_addr + (u64 )(i * 4096);
  cik_srbm_select(adev, (u32 )me, (u32 )pipe, 0U, 0U);
  amdgpu_mm_wreg(adev, 12865U, (u32 )(eop_gpu_addr >> 8), 0);
  amdgpu_mm_wreg(adev, 12866U, (unsigned int )(eop_gpu_addr >> 32ULL) >> 8, 0);
  amdgpu_mm_wreg(adev, 12867U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 12868U, 0);
  tmp = tmp & 4294967232U;
  tmp = tmp | 8U;
  amdgpu_mm_wreg(adev, 12868U, tmp, 0);
  i = i + 1;
  ldv_53403: ;
  if ((u32 )i < adev->gfx.mec.num_pipe * adev->gfx.mec.num_mec) {
    goto ldv_53402;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  i = 0;
  goto ldv_53410;
  ldv_53409:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if ((unsigned long )ring->mqd_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, 604UL, 4096, 1, 2U, 0ULL, (struct sg_table *)0, & ring->mqd_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) create MQD bo failed\n",
               r);
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_reserve(ring->mqd_obj, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    gfx_v7_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(ring->mqd_obj, 2U, & mqd_gpu_addr);
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) pin MQD bo failed\n", r);
    gfx_v7_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(ring->mqd_obj, (void **)(& buf));
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) map MQD bo failed\n", r);
    gfx_v7_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  memset((void *)buf, 0, 604UL);
  mqd = (struct bonaire_mqd *)buf;
  mqd->header = 3224438784U;
  mqd->static_thread_mgmt01[0] = 4294967295U;
  mqd->static_thread_mgmt01[1] = 4294967295U;
  mqd->static_thread_mgmt23[0] = 4294967295U;
  mqd->static_thread_mgmt23[1] = 4294967295U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  cik_srbm_select(adev, ring->me, ring->pipe, ring->queue, 0U);
  tmp = amdgpu_mm_rreg(adev, 12419U, 0);
  tmp = tmp & 2147483647U;
  amdgpu_mm_wreg(adev, 12419U, tmp, 0);
  mqd->queue_state.cp_hqd_pq_doorbell_control = amdgpu_mm_rreg(adev, 12884U, 0);
  if ((int )use_doorbell) {
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control | 1073741824U;
  } else {
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control & 3221225471U;
  }
  amdgpu_mm_wreg(adev, 12884U, mqd->queue_state.cp_hqd_pq_doorbell_control, 0);
  mqd->queue_state.cp_hqd_dequeue_request = 0U;
  mqd->queue_state.cp_hqd_pq_rptr = 0U;
  mqd->queue_state.cp_hqd_pq_wptr = 0U;
  tmp___2 = amdgpu_mm_rreg(adev, 12871U, 0);
  if ((int )tmp___2 & 1) {
    amdgpu_mm_wreg(adev, 12893U, 1U, 0);
    j = 0;
    goto ldv_53408;
    ldv_53407:
    tmp___1 = amdgpu_mm_rreg(adev, 12871U, 0);
    if ((tmp___1 & 1U) == 0U) {
      goto ldv_53406;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_53408: ;
    if (adev->usec_timeout > j) {
      goto ldv_53407;
    } else {
    }
    ldv_53406:
    amdgpu_mm_wreg(adev, 12893U, mqd->queue_state.cp_hqd_dequeue_request, 0);
    amdgpu_mm_wreg(adev, 12879U, mqd->queue_state.cp_hqd_pq_rptr, 0);
    amdgpu_mm_wreg(adev, 12885U, mqd->queue_state.cp_hqd_pq_wptr, 0);
  } else {
  }
  mqd->queue_state.cp_mqd_base_addr = (u32 )mqd_gpu_addr & 4294967292U;
  mqd->queue_state.cp_mqd_base_addr_hi = (unsigned int )(mqd_gpu_addr >> 32ULL);
  amdgpu_mm_wreg(adev, 12869U, mqd->queue_state.cp_mqd_base_addr, 0);
  amdgpu_mm_wreg(adev, 12870U, mqd->queue_state.cp_mqd_base_addr_hi, 0);
  mqd->queue_state.cp_mqd_control = amdgpu_mm_rreg(adev, 12903U, 0);
  mqd->queue_state.cp_mqd_control = mqd->queue_state.cp_mqd_control & 4294967280U;
  amdgpu_mm_wreg(adev, 12903U, mqd->queue_state.cp_mqd_control, 0);
  hqd_gpu_addr = ring->gpu_addr >> 8;
  mqd->queue_state.cp_hqd_pq_base = (u32 )hqd_gpu_addr;
  mqd->queue_state.cp_hqd_pq_base_hi = (unsigned int )(hqd_gpu_addr >> 32ULL);
  amdgpu_mm_wreg(adev, 12877U, mqd->queue_state.cp_hqd_pq_base, 0);
  amdgpu_mm_wreg(adev, 12878U, mqd->queue_state.cp_hqd_pq_base_hi, 0);
  mqd->queue_state.cp_hqd_pq_control = amdgpu_mm_rreg(adev, 12886U, 0);
  mqd->queue_state.cp_hqd_pq_control = mqd->queue_state.cp_hqd_pq_control & 4294951104U;
  tmp___3 = __roundup_pow_of_two((unsigned long )(ring->ring_size / 8U));
  tmp___4 = __ilog2_u64((u64 )tmp___3);
  mqd->queue_state.cp_hqd_pq_control = mqd->queue_state.cp_hqd_pq_control | (u32 )tmp___4;
  mqd->queue_state.cp_hqd_pq_control = mqd->queue_state.cp_hqd_pq_control | 2304U;
  mqd->queue_state.cp_hqd_pq_control = mqd->queue_state.cp_hqd_pq_control & 3422552063U;
  mqd->queue_state.cp_hqd_pq_control = mqd->queue_state.cp_hqd_pq_control | 3221225472U;
  amdgpu_mm_wreg(adev, 12886U, mqd->queue_state.cp_hqd_pq_control, 0);
  wb_gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->wptr_offs * 4U);
  mqd->queue_state.cp_hqd_pq_wptr_poll_addr = (u32 )wb_gpu_addr & 4294967292U;
  mqd->queue_state.cp_hqd_pq_wptr_poll_addr_hi = (unsigned int )(wb_gpu_addr >> 32ULL) & 65535U;
  amdgpu_mm_wreg(adev, 12882U, mqd->queue_state.cp_hqd_pq_wptr_poll_addr, 0);
  amdgpu_mm_wreg(adev, 12883U, mqd->queue_state.cp_hqd_pq_wptr_poll_addr_hi, 0);
  wb_gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->rptr_offs * 4U);
  mqd->queue_state.cp_hqd_pq_rptr_report_addr = (u32 )wb_gpu_addr & 4294967292U;
  mqd->queue_state.cp_hqd_pq_rptr_report_addr_hi = (unsigned int )(wb_gpu_addr >> 32ULL) & 65535U;
  amdgpu_mm_wreg(adev, 12880U, mqd->queue_state.cp_hqd_pq_rptr_report_addr, 0);
  amdgpu_mm_wreg(adev, 12881U, mqd->queue_state.cp_hqd_pq_rptr_report_addr_hi, 0);
  if ((int )use_doorbell) {
    mqd->queue_state.cp_hqd_pq_doorbell_control = amdgpu_mm_rreg(adev, 12884U, 0);
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control & 4286578691U;
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control | (ring->doorbell_index << 2);
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control | 1073741824U;
    mqd->queue_state.cp_hqd_pq_doorbell_control = mqd->queue_state.cp_hqd_pq_doorbell_control & 1879048191U;
  } else {
    mqd->queue_state.cp_hqd_pq_doorbell_control = 0U;
  }
  amdgpu_mm_wreg(adev, 12884U, mqd->queue_state.cp_hqd_pq_doorbell_control, 0);
  ring->wptr = 0U;
  mqd->queue_state.cp_hqd_pq_wptr = ring->wptr;
  amdgpu_mm_wreg(adev, 12885U, mqd->queue_state.cp_hqd_pq_wptr, 0);
  mqd->queue_state.cp_hqd_pq_rptr = amdgpu_mm_rreg(adev, 12879U, 0);
  mqd->queue_state.cp_hqd_vmid = 0U;
  amdgpu_mm_wreg(adev, 12872U, mqd->queue_state.cp_hqd_vmid, 0);
  mqd->queue_state.cp_hqd_active = 1U;
  amdgpu_mm_wreg(adev, 12871U, mqd->queue_state.cp_hqd_active, 0);
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  amdgpu_bo_kunmap(ring->mqd_obj);
  amdgpu_bo_unreserve(ring->mqd_obj);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
  } else {
  }
  i = i + 1;
  ldv_53410: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53409;
  } else {
  }
  return (0);
}
}
static void gfx_v7_0_cp_enable(struct amdgpu_device *adev , bool enable )
{
  {
  gfx_v7_0_cp_gfx_enable(adev, (int )enable);
  gfx_v7_0_cp_compute_enable(adev, (int )enable);
  return;
}
}
static int gfx_v7_0_cp_load_microcode(struct amdgpu_device *adev )
{
  int r ;
  {
  r = gfx_v7_0_cp_gfx_load_microcode(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v7_0_cp_compute_load_microcode(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static void gfx_v7_0_enable_gui_idle_interrupt(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 12394U, 0);
  tmp = tmp___0;
  if ((int )enable) {
    tmp = tmp | 1572864U;
  } else {
    tmp = tmp & 4293394431U;
  }
  amdgpu_mm_wreg(adev, 12394U, tmp, 0);
  return;
}
}
static int gfx_v7_0_cp_resume(struct amdgpu_device *adev )
{
  int r ;
  {
  gfx_v7_0_enable_gui_idle_interrupt(adev, 0);
  r = gfx_v7_0_cp_load_microcode(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v7_0_cp_gfx_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v7_0_cp_compute_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  gfx_v7_0_enable_gui_idle_interrupt(adev, 1);
  return (0);
}
}
static void gfx_v7_0_ce_sync_me(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(adev->gfx.ce_sync_offs * 4U);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 1280U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, 531U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 4294967295U);
  amdgpu_ring_write(ring, 4U);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 2148533504U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static void gfx_v7_0_ring_emit_vm_flush(struct amdgpu_ring *ring , unsigned int vm_id ,
                                        uint64_t pd_addr )
{
  int usepfp ;
  {
  usepfp = (unsigned int )ring->type == 0U;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, (u32 )(usepfp << 30));
  if (vm_id <= 7U) {
    amdgpu_ring_write(ring, vm_id + 1359U);
  } else {
    amdgpu_ring_write(ring, vm_id + 1286U);
  }
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )(pd_addr >> 12));
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )(1 << (int )vm_id));
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 32U);
  if (usepfp != 0) {
    amdgpu_ring_write(ring, 3221242368U);
    amdgpu_ring_write(ring, 0U);
    gfx_v7_0_ce_sync_me(ring);
  } else {
  }
  return;
}
}
static void gfx_v7_0_rlc_fini(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  if ((unsigned long )adev->gfx.rlc.save_restore_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->gfx.rlc.save_restore_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve RLC sr bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(adev->gfx.rlc.save_restore_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.save_restore_obj);
    amdgpu_bo_unref(& adev->gfx.rlc.save_restore_obj);
    adev->gfx.rlc.save_restore_obj = (struct amdgpu_bo *)0;
  } else {
  }
  if ((unsigned long )adev->gfx.rlc.clear_state_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->gfx.rlc.clear_state_obj, 0);
    tmp___0 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___0 != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve RLC c bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(adev->gfx.rlc.clear_state_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.clear_state_obj);
    amdgpu_bo_unref(& adev->gfx.rlc.clear_state_obj);
    adev->gfx.rlc.clear_state_obj = (struct amdgpu_bo *)0;
  } else {
  }
  if ((unsigned long )adev->gfx.rlc.cp_table_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->gfx.rlc.cp_table_obj, 0);
    tmp___1 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___1 != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve RLC cp table bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(adev->gfx.rlc.cp_table_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.cp_table_obj);
    amdgpu_bo_unref(& adev->gfx.rlc.cp_table_obj);
    adev->gfx.rlc.cp_table_obj = (struct amdgpu_bo *)0;
  } else {
  }
  return;
}
}
static int gfx_v7_0_rlc_init(struct amdgpu_device *adev )
{
  u32 const *src_ptr ;
  u32 volatile *dst_ptr ;
  u32 dws ;
  u32 i ;
  struct cs_section_def const *cs_data ;
  int r ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  if ((adev->flags & 131072UL) != 0UL) {
    if ((unsigned int )adev->asic_type == 1U) {
      adev->gfx.rlc.reg_list = (u32 const *)(& spectre_rlc_save_restore_register_list);
      adev->gfx.rlc.reg_list_size = 443U;
    } else {
      adev->gfx.rlc.reg_list = (u32 const *)(& kalindi_rlc_save_restore_register_list);
      adev->gfx.rlc.reg_list_size = 321U;
    }
  } else {
  }
  adev->gfx.rlc.cs_data = (struct cs_section_def const *)(& ci_cs_data);
  adev->gfx.rlc.cp_table_size = 1920U;
  src_ptr = adev->gfx.rlc.reg_list;
  dws = adev->gfx.rlc.reg_list_size;
  dws = dws + 240U;
  cs_data = adev->gfx.rlc.cs_data;
  if ((unsigned long )src_ptr != (unsigned long )((u32 const *)0U)) {
    if ((unsigned long )adev->gfx.rlc.save_restore_obj == (unsigned long )((struct amdgpu_bo *)0)) {
      r = amdgpu_bo_create(adev, (unsigned long )(dws * 4U), 4096, 1, 4U, 0ULL, (struct sg_table *)0,
                           & adev->gfx.rlc.save_restore_obj);
      if (r != 0) {
        dev_warn((struct device const *)adev->dev, "(%d) create RLC sr bo failed\n",
                 r);
        return (r);
      } else {
      }
    } else {
    }
    r = amdgpu_bo_reserve(adev->gfx.rlc.save_restore_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(adev->gfx.rlc.save_restore_obj, 4U, & adev->gfx.rlc.save_restore_gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(adev->gfx.rlc.save_restore_obj);
      dev_warn((struct device const *)adev->dev, "(%d) pin RLC sr bo failed\n",
               r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(adev->gfx.rlc.save_restore_obj, (void **)(& adev->gfx.rlc.sr_ptr));
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) map RLC sr bo failed\n",
               r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    dst_ptr = adev->gfx.rlc.sr_ptr;
    i = 0U;
    goto ldv_53458;
    ldv_53457:
    *(dst_ptr + (unsigned long )i) = *(src_ptr + (unsigned long )i);
    i = i + 1U;
    ldv_53458: ;
    if (adev->gfx.rlc.reg_list_size > i) {
      goto ldv_53457;
    } else {
    }
    amdgpu_bo_kunmap(adev->gfx.rlc.save_restore_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.save_restore_obj);
  } else {
  }
  if ((unsigned long )cs_data != (unsigned long )((struct cs_section_def const *)0)) {
    dws = gfx_v7_0_get_csb_size(adev);
    adev->gfx.rlc.clear_state_size = dws;
    if ((unsigned long )adev->gfx.rlc.clear_state_obj == (unsigned long )((struct amdgpu_bo *)0)) {
      r = amdgpu_bo_create(adev, (unsigned long )(dws * 4U), 4096, 1, 4U, 0ULL, (struct sg_table *)0,
                           & adev->gfx.rlc.clear_state_obj);
      if (r != 0) {
        dev_warn((struct device const *)adev->dev, "(%d) create RLC c bo failed\n",
                 r);
        gfx_v7_0_rlc_fini(adev);
        return (r);
      } else {
      }
    } else {
    }
    r = amdgpu_bo_reserve(adev->gfx.rlc.clear_state_obj, 0);
    tmp___0 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___0 != 0L) {
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(adev->gfx.rlc.clear_state_obj, 4U, & adev->gfx.rlc.clear_state_gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(adev->gfx.rlc.clear_state_obj);
      dev_warn((struct device const *)adev->dev, "(%d) pin RLC c bo failed\n", r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(adev->gfx.rlc.clear_state_obj, (void **)(& adev->gfx.rlc.cs_ptr));
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) map RLC c bo failed\n", r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    dst_ptr = adev->gfx.rlc.cs_ptr;
    gfx_v7_0_get_csb_buffer(adev, dst_ptr);
    amdgpu_bo_kunmap(adev->gfx.rlc.clear_state_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.clear_state_obj);
  } else {
  }
  if (adev->gfx.rlc.cp_table_size != 0U) {
    if ((unsigned long )adev->gfx.rlc.cp_table_obj == (unsigned long )((struct amdgpu_bo *)0)) {
      r = amdgpu_bo_create(adev, (unsigned long )adev->gfx.rlc.cp_table_size, 4096,
                           1, 4U, 0ULL, (struct sg_table *)0, & adev->gfx.rlc.cp_table_obj);
      if (r != 0) {
        dev_warn((struct device const *)adev->dev, "(%d) create RLC cp table bo failed\n",
                 r);
        gfx_v7_0_rlc_fini(adev);
        return (r);
      } else {
      }
    } else {
    }
    r = amdgpu_bo_reserve(adev->gfx.rlc.cp_table_obj, 0);
    tmp___1 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___1 != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve RLC cp table bo failed\n",
               r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(adev->gfx.rlc.cp_table_obj, 4U, & adev->gfx.rlc.cp_table_gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(adev->gfx.rlc.cp_table_obj);
      dev_warn((struct device const *)adev->dev, "(%d) pin RLC cp_table bo failed\n",
               r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(adev->gfx.rlc.cp_table_obj, (void **)(& adev->gfx.rlc.cp_table_ptr));
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) map RLC cp table bo failed\n",
               r);
      gfx_v7_0_rlc_fini(adev);
      return (r);
    } else {
    }
    gfx_v7_0_init_cp_pg_table(adev);
    amdgpu_bo_kunmap(adev->gfx.rlc.cp_table_obj);
    amdgpu_bo_unreserve(adev->gfx.rlc.cp_table_obj);
  } else {
  }
  return (0);
}
}
static void gfx_v7_0_enable_lbpw(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 12505U, 0);
  if ((int )enable) {
    tmp = tmp | 1U;
  } else {
    tmp = tmp & 4294967294U;
  }
  amdgpu_mm_wreg(adev, 12505U, tmp, 0);
  return;
}
}
static void gfx_v7_0_wait_for_rlc_serdes(struct amdgpu_device *adev )
{
  u32 i ;
  u32 j ;
  u32 k ;
  u32 mask ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0U;
  goto ldv_53479;
  ldv_53478:
  j = 0U;
  goto ldv_53476;
  ldv_53475:
  gfx_v7_0_select_se_sh(adev, i, j);
  k = 0U;
  goto ldv_53474;
  ldv_53473:
  tmp = amdgpu_mm_rreg(adev, 12577U, 0);
  if (tmp == 0U) {
    goto ldv_53472;
  } else {
  }
  __const_udelay(4295UL);
  k = k + 1U;
  ldv_53474: ;
  if ((u32 )adev->usec_timeout > k) {
    goto ldv_53473;
  } else {
  }
  ldv_53472:
  j = j + 1U;
  ldv_53476: ;
  if (adev->gfx.config.max_sh_per_se > j) {
    goto ldv_53475;
  } else {
  }
  i = i + 1U;
  ldv_53479: ;
  if (adev->gfx.config.max_shader_engines > i) {
    goto ldv_53478;
  } else {
  }
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  mask = 524287U;
  k = 0U;
  goto ldv_53483;
  ldv_53482:
  tmp___0 = amdgpu_mm_rreg(adev, 12578U, 0);
  if ((tmp___0 & mask) == 0U) {
    goto ldv_53481;
  } else {
  }
  __const_udelay(4295UL);
  k = k + 1U;
  ldv_53483: ;
  if ((u32 )adev->usec_timeout > k) {
    goto ldv_53482;
  } else {
  }
  ldv_53481: ;
  return;
}
}
static void gfx_v7_0_update_rlc(struct amdgpu_device *adev , u32 rlc )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 12480U, 0);
  if (tmp != rlc) {
    amdgpu_mm_wreg(adev, 12480U, rlc, 0);
  } else {
  }
  return;
}
}
static u32 gfx_v7_0_halt_rlc(struct amdgpu_device *adev )
{
  u32 data ;
  u32 orig ;
  u32 i ;
  u32 tmp ;
  {
  data = amdgpu_mm_rreg(adev, 12480U, 0);
  orig = data;
  if ((int )data & 1) {
    data = data & 4294967294U;
    amdgpu_mm_wreg(adev, 12480U, data, 0);
    i = 0U;
    goto ldv_53497;
    ldv_53496:
    tmp = amdgpu_mm_rreg(adev, 12544U, 0);
    if ((tmp & 1U) == 0U) {
      goto ldv_53495;
    } else {
    }
    __const_udelay(4295UL);
    i = i + 1U;
    ldv_53497: ;
    if ((u32 )adev->usec_timeout > i) {
      goto ldv_53496;
    } else {
    }
    ldv_53495:
    gfx_v7_0_wait_for_rlc_serdes(adev);
  } else {
  }
  return (orig);
}
}
void gfx_v7_0_enter_rlc_safe_mode(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 i ;
  u32 mask ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  {
  tmp = 3U;
  amdgpu_mm_wreg(adev, 12602U, tmp, 0);
  mask = 6U;
  i = 0U;
  goto ldv_53506;
  ldv_53505:
  tmp___0 = amdgpu_mm_rreg(adev, 12544U, 0);
  if ((tmp___0 & mask) == mask) {
    goto ldv_53504;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53506: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_53505;
  } else {
  }
  ldv_53504:
  i = 0U;
  goto ldv_53509;
  ldv_53508:
  tmp___1 = amdgpu_mm_rreg(adev, 12602U, 0);
  if ((tmp___1 & 1U) == 0U) {
    goto ldv_53507;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53509: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_53508;
  } else {
  }
  ldv_53507: ;
  return;
}
}
void gfx_v7_0_exit_rlc_safe_mode(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = 1U;
  amdgpu_mm_wreg(adev, 12602U, tmp, 0);
  return;
}
}
void gfx_v7_0_rlc_stop(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 12480U, 0U, 0);
  gfx_v7_0_enable_gui_idle_interrupt(adev, 0);
  gfx_v7_0_wait_for_rlc_serdes(adev);
  return;
}
}
static void gfx_v7_0_rlc_start(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 12480U, 1U, 0);
  gfx_v7_0_enable_gui_idle_interrupt(adev, 1);
  __const_udelay(214750UL);
  return;
}
}
static void gfx_v7_0_rlc_reset(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 8200U, 0);
  tmp = tmp___0;
  tmp = tmp | 4U;
  amdgpu_mm_wreg(adev, 8200U, tmp, 0);
  __const_udelay(214750UL);
  tmp = tmp & 4294967291U;
  amdgpu_mm_wreg(adev, 8200U, tmp, 0);
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v7_0_rlc_resume(struct amdgpu_device *adev )
{
  struct rlc_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  u32 tmp ;
  u32 tmp___0 ;
  __le32 const *tmp___1 ;
  __u32 tmp___2 ;
  {
  if ((unsigned long )adev->gfx.rlc_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct rlc_firmware_header_v1_0 const *)(adev->gfx.rlc_fw)->data;
  amdgpu_ucode_print_rlc_hdr(& hdr->header);
  adev->gfx.rlc_fw_version = hdr->header.ucode_version;
  gfx_v7_0_rlc_stop(adev);
  tmp___0 = amdgpu_mm_rreg(adev, 12553U, 0);
  tmp = tmp___0 & 4294967292U;
  amdgpu_mm_wreg(adev, 12553U, tmp, 0);
  gfx_v7_0_rlc_reset(adev);
  gfx_v7_0_init_pg(adev);
  amdgpu_mm_wreg(adev, 12507U, 0U, 0);
  amdgpu_mm_wreg(adev, 12498U, 32768U, 0);
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  amdgpu_mm_wreg(adev, 12559U, 4294967295U, 0);
  amdgpu_mm_wreg(adev, 12561U, 6292488U, 0);
  amdgpu_mm_wreg(adev, 12505U, 2147483652U, 0);
  mutex_unlock(& adev->grbm_idx_mutex);
  amdgpu_mm_wreg(adev, 12483U, 0U, 0);
  amdgpu_mm_wreg(adev, 12519U, 0U, 0);
  fw_data = (__le32 const *)(adev->gfx.rlc_fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 12514U, 0U, 0);
  i = 0U;
  goto ldv_53533;
  ldv_53532:
  tmp___1 = fw_data;
  fw_data = fw_data + 1;
  tmp___2 = __le32_to_cpup(tmp___1);
  amdgpu_mm_wreg(adev, 12515U, tmp___2, 0);
  i = i + 1U;
  ldv_53533: ;
  if (i < fw_size) {
    goto ldv_53532;
  } else {
  }
  amdgpu_mm_wreg(adev, 12514U, adev->gfx.rlc_fw_version, 0);
  gfx_v7_0_enable_lbpw(adev, 0);
  if ((unsigned int )adev->asic_type == 0U) {
    amdgpu_mm_wreg(adev, 12510U, 0U, 0);
  } else {
  }
  gfx_v7_0_rlc_start(adev);
  return (0);
}
}
static void gfx_v7_0_enable_cgcg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  u32 tmp ;
  u32 tmp2 ;
  {
  data = amdgpu_mm_rreg(adev, 12553U, 0);
  orig = data;
  if ((int )enable && (adev->cg_flags & 4U) != 0U) {
    gfx_v7_0_enable_gui_idle_interrupt(adev, 1);
    tmp = gfx_v7_0_halt_rlc(adev);
    mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
    gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
    amdgpu_mm_wreg(adev, 12573U, 4294967295U, 0);
    amdgpu_mm_wreg(adev, 12574U, 4294967295U, 0);
    tmp2 = 1114367U;
    amdgpu_mm_wreg(adev, 12575U, tmp2, 0);
    mutex_unlock(& adev->grbm_idx_mutex);
    gfx_v7_0_update_rlc(adev, tmp);
    data = data | 3U;
  } else {
    gfx_v7_0_enable_gui_idle_interrupt(adev, 0);
    amdgpu_mm_rreg(adev, 61608U, 0);
    amdgpu_mm_rreg(adev, 61608U, 0);
    amdgpu_mm_rreg(adev, 61608U, 0);
    amdgpu_mm_rreg(adev, 61608U, 0);
    data = data & 4294967292U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12553U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_mgcg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  u32 tmp ;
  {
  tmp = 0U;
  if ((int )enable && (int )adev->cg_flags & 1) {
    if ((adev->cg_flags & 2U) != 0U) {
      if ((adev->cg_flags & 64U) != 0U) {
        data = amdgpu_mm_rreg(adev, 12409U, 0);
        orig = data;
        data = data | 1U;
        if (orig != data) {
          amdgpu_mm_wreg(adev, 12409U, data, 0);
        } else {
        }
      } else {
      }
    } else {
    }
    data = amdgpu_mm_rreg(adev, 12552U, 0);
    orig = data;
    data = data | 1U;
    data = data & 4294967293U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12552U, data, 0);
    } else {
    }
    tmp = gfx_v7_0_halt_rlc(adev);
    mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
    gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
    amdgpu_mm_wreg(adev, 12573U, 4294967295U, 0);
    amdgpu_mm_wreg(adev, 12574U, 4294967295U, 0);
    data = 4194559U;
    amdgpu_mm_wreg(adev, 12575U, data, 0);
    mutex_unlock(& adev->grbm_idx_mutex);
    gfx_v7_0_update_rlc(adev, tmp);
    if ((adev->cg_flags & 16U) != 0U) {
      data = amdgpu_mm_rreg(adev, 61440U, 0);
      orig = data;
      data = data & 4294049791U;
      data = data | 262144U;
      data = data | 1048576U;
      data = data & 4292870143U;
      if ((adev->cg_flags & 2U) != 0U && (adev->cg_flags & 32U) != 0U) {
        data = data & 4290772991U;
      } else {
      }
      data = data & 16777215U;
      data = data | 8388608U;
      data = data | 2516582400U;
      if (orig != data) {
        amdgpu_mm_wreg(adev, 61440U, data, 0);
      } else {
      }
    } else {
    }
  } else {
    data = amdgpu_mm_rreg(adev, 12552U, 0);
    orig = data;
    data = data | 3U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12552U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 12486U, 0);
    if ((int )data & 1) {
      data = data & 4294967294U;
      amdgpu_mm_wreg(adev, 12486U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 12409U, 0);
    if ((int )data & 1) {
      data = data & 4294967294U;
      amdgpu_mm_wreg(adev, 12409U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 61440U, 0);
    orig = data;
    data = data | 6291456U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 61440U, data, 0);
    } else {
    }
    tmp = gfx_v7_0_halt_rlc(adev);
    mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
    gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
    amdgpu_mm_wreg(adev, 12573U, 4294967295U, 0);
    amdgpu_mm_wreg(adev, 12574U, 4294967295U, 0);
    data = 8388863U;
    amdgpu_mm_wreg(adev, 12575U, data, 0);
    mutex_unlock(& adev->grbm_idx_mutex);
    gfx_v7_0_update_rlc(adev, tmp);
  }
  return;
}
}
static void gfx_v7_0_update_cg(struct amdgpu_device *adev , bool enable )
{
  {
  gfx_v7_0_enable_gui_idle_interrupt(adev, 0);
  if ((int )enable) {
    gfx_v7_0_enable_mgcg(adev, 1);
    gfx_v7_0_enable_cgcg(adev, 1);
  } else {
    gfx_v7_0_enable_cgcg(adev, 0);
    gfx_v7_0_enable_mgcg(adev, 0);
  }
  gfx_v7_0_enable_gui_idle_interrupt(adev, 1);
  return;
}
}
static void gfx_v7_0_enable_sclk_slowdown_on_pu(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 128U) != 0U) {
    data = data | 131072U;
  } else {
    data = data & 4294836223U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_sclk_slowdown_on_pd(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 128U) != 0U) {
    data = data | 262144U;
  } else {
    data = data & 4294705151U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_cp_pg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 32U) != 0U) {
    data = data & 4294934527U;
  } else {
    data = data | 32768U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_gds_pg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 64U) != 0U) {
    data = data & 4294959103U;
  } else {
    data = data | 8192U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_init_cp_pg_table(struct amdgpu_device *adev )
{
  __le32 const *fw_data ;
  u32 volatile *dst_ptr ;
  int me ;
  int i ;
  int max_me ;
  u32 bo_offset ;
  u32 table_offset ;
  u32 table_size ;
  struct gfx_firmware_header_v1_0 const *hdr ;
  struct gfx_firmware_header_v1_0 const *hdr___0 ;
  struct gfx_firmware_header_v1_0 const *hdr___1 ;
  struct gfx_firmware_header_v1_0 const *hdr___2 ;
  struct gfx_firmware_header_v1_0 const *hdr___3 ;
  {
  max_me = 4;
  bo_offset = 0U;
  if ((unsigned int )adev->asic_type == 1U) {
    max_me = 5;
  } else {
  }
  if ((unsigned long )adev->gfx.rlc.cp_table_ptr == (unsigned long )((u32 volatile *)0U)) {
    return;
  } else {
  }
  dst_ptr = adev->gfx.rlc.cp_table_ptr;
  me = 0;
  goto ldv_53598;
  ldv_53597: ;
  if (me == 0) {
    hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.ce_fw)->data;
    fw_data = (__le32 const *)(adev->gfx.ce_fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
    table_offset = hdr->jt_offset;
    table_size = hdr->jt_size;
  } else
  if (me == 1) {
    hdr___0 = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.pfp_fw)->data;
    fw_data = (__le32 const *)(adev->gfx.pfp_fw)->data + (unsigned long )hdr___0->header.ucode_array_offset_bytes;
    table_offset = hdr___0->jt_offset;
    table_size = hdr___0->jt_size;
  } else
  if (me == 2) {
    hdr___1 = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.me_fw)->data;
    fw_data = (__le32 const *)(adev->gfx.me_fw)->data + (unsigned long )hdr___1->header.ucode_array_offset_bytes;
    table_offset = hdr___1->jt_offset;
    table_size = hdr___1->jt_size;
  } else
  if (me == 3) {
    hdr___2 = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec_fw)->data;
    fw_data = (__le32 const *)(adev->gfx.mec_fw)->data + (unsigned long )hdr___2->header.ucode_array_offset_bytes;
    table_offset = hdr___2->jt_offset;
    table_size = hdr___2->jt_size;
  } else {
    hdr___3 = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec2_fw)->data;
    fw_data = (__le32 const *)(adev->gfx.mec2_fw)->data + (unsigned long )hdr___3->header.ucode_array_offset_bytes;
    table_offset = hdr___3->jt_offset;
    table_size = hdr___3->jt_size;
  }
  i = 0;
  goto ldv_53595;
  ldv_53594:
  *(dst_ptr + (unsigned long )(bo_offset + (u32 )i)) = *(fw_data + (unsigned long )(table_offset + (u32 )i));
  i = i + 1;
  ldv_53595: ;
  if ((u32 )i < table_size) {
    goto ldv_53594;
  } else {
  }
  bo_offset = bo_offset + table_size;
  me = me + 1;
  ldv_53598: ;
  if (me < max_me) {
    goto ldv_53597;
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_gfx_cgpg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  if ((int )enable && (int )adev->pg_flags & 1) {
    data = amdgpu_mm_rreg(adev, 12547U, 0);
    orig = data;
    data = data | 1U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12547U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 12565U, 0);
    orig = data;
    data = data | 1U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12565U, data, 0);
    } else {
    }
  } else {
    data = amdgpu_mm_rreg(adev, 12547U, 0);
    orig = data;
    data = data & 4294967294U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12547U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 12565U, 0);
    orig = data;
    data = data & 4294967294U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 12565U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 40960U, 0);
  }
  return;
}
}
static u32 gfx_v7_0_get_cu_active_bitmap(struct amdgpu_device *adev , u32 se , u32 sh )
{
  u32 mask ;
  u32 tmp ;
  u32 tmp1 ;
  int i ;
  {
  mask = 0U;
  gfx_v7_0_select_se_sh(adev, se, sh);
  tmp = amdgpu_mm_rreg(adev, 8815U, 0);
  tmp1 = amdgpu_mm_rreg(adev, 8816U, 0);
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  tmp = tmp & 4294901760U;
  tmp = tmp | tmp1;
  tmp = tmp >> 16;
  i = 0;
  goto ldv_53616;
  ldv_53615:
  mask = mask << 1;
  mask = mask | 1U;
  i = i + 1;
  ldv_53616: ;
  if ((unsigned int )i < adev->gfx.config.max_cu_per_sh) {
    goto ldv_53615;
  } else {
  }
  return (~ tmp & mask);
}
}
static void gfx_v7_0_init_ao_cu_mask(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 active_cu_number ;
  struct amdgpu_cu_info cu_info ;
  {
  gfx_v7_0_get_cu_info(adev, & cu_info);
  tmp = cu_info.ao_cu_mask;
  active_cu_number = cu_info.number;
  amdgpu_mm_wreg(adev, 12563U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 12564U, 0);
  tmp = tmp & 4294967040U;
  tmp = tmp | active_cu_number;
  amdgpu_mm_wreg(adev, 12564U, tmp, 0);
  return;
}
}
static void gfx_v7_0_enable_gfx_static_mgpg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 2U) != 0U) {
    data = data | 8U;
  } else {
    data = data & 4294967287U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_enable_gfx_dynamic_mgpg(struct amdgpu_device *adev , bool enable )
{
  u32 data ;
  u32 orig ;
  {
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  if ((int )enable && (adev->pg_flags & 4U) != 0U) {
    data = data | 4U;
  } else {
    data = data & 4294967291U;
  }
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  return;
}
}
static void gfx_v7_0_init_gfx_cgpg(struct amdgpu_device *adev )
{
  u32 data ;
  u32 orig ;
  u32 i ;
  {
  if ((unsigned long )adev->gfx.rlc.cs_data != (unsigned long )((struct cs_section_def const *)0)) {
    amdgpu_mm_wreg(adev, 12588U, 61U, 0);
    amdgpu_mm_wreg(adev, 12589U, (unsigned int )(adev->gfx.rlc.clear_state_gpu_addr >> 32ULL),
                   0);
    amdgpu_mm_wreg(adev, 12589U, (unsigned int )adev->gfx.rlc.clear_state_gpu_addr,
                   0);
    amdgpu_mm_wreg(adev, 12589U, adev->gfx.rlc.clear_state_size, 0);
  } else {
    amdgpu_mm_wreg(adev, 12588U, 61U, 0);
    i = 0U;
    goto ldv_53643;
    ldv_53642:
    amdgpu_mm_wreg(adev, 12589U, 0U, 0);
    i = i + 1U;
    ldv_53643: ;
    if (i <= 2U) {
      goto ldv_53642;
    } else {
    }
  }
  if ((unsigned long )adev->gfx.rlc.reg_list != (unsigned long )((u32 const *)0U)) {
    amdgpu_mm_wreg(adev, 12588U, 144U, 0);
    i = 0U;
    goto ldv_53646;
    ldv_53645:
    amdgpu_mm_wreg(adev, 12589U, *(adev->gfx.rlc.reg_list + (unsigned long )i), 0);
    i = i + 1U;
    ldv_53646: ;
    if (adev->gfx.rlc.reg_list_size > i) {
      goto ldv_53645;
    } else {
    }
  } else {
  }
  data = amdgpu_mm_rreg(adev, 12547U, 0);
  orig = data;
  data = data | 2U;
  if (orig != data) {
    amdgpu_mm_wreg(adev, 12547U, data, 0);
  } else {
  }
  amdgpu_mm_wreg(adev, 12509U, (u32 )(adev->gfx.rlc.save_restore_gpu_addr >> 8), 0);
  amdgpu_mm_wreg(adev, 12510U, (u32 )(adev->gfx.rlc.cp_table_gpu_addr >> 8), 0);
  data = amdgpu_mm_rreg(adev, 8642U, 0);
  data = data & 65535U;
  data = data | 6291456U;
  amdgpu_mm_wreg(adev, 8642U, data, 0);
  data = 269488144U;
  amdgpu_mm_wreg(adev, 12557U, data, 0);
  data = amdgpu_mm_rreg(adev, 12511U, 0);
  data = data & 4294967040U;
  data = data | 3U;
  amdgpu_mm_wreg(adev, 12511U, data, 0);
  data = amdgpu_mm_rreg(adev, 12565U, 0);
  data = data & 4294443015U;
  data = data | 14336U;
  amdgpu_mm_wreg(adev, 12565U, data, 0);
  return;
}
}
static void gfx_v7_0_update_gfx_pg(struct amdgpu_device *adev , bool enable )
{
  {
  gfx_v7_0_enable_gfx_cgpg(adev, (int )enable);
  gfx_v7_0_enable_gfx_static_mgpg(adev, (int )enable);
  gfx_v7_0_enable_gfx_dynamic_mgpg(adev, (int )enable);
  return;
}
}
static u32 gfx_v7_0_get_csb_size(struct amdgpu_device *adev )
{
  u32 count ;
  struct cs_section_def const *sect ;
  struct cs_extent_def const *ext ;
  {
  count = 0U;
  sect = (struct cs_section_def const *)0;
  ext = (struct cs_extent_def const *)0;
  if ((unsigned long )adev->gfx.rlc.cs_data == (unsigned long )((struct cs_section_def const *)0)) {
    return (0U);
  } else {
  }
  count = count + 2U;
  count = count + 3U;
  sect = adev->gfx.rlc.cs_data;
  goto ldv_53662;
  ldv_53661:
  ext = sect->section;
  goto ldv_53659;
  ldv_53658: ;
  if ((unsigned int )sect->id == 1U) {
    count = ((u32 )ext->reg_count + count) + 2U;
  } else {
    return (0U);
  }
  ext = ext + 1;
  ldv_53659: ;
  if ((unsigned long )ext->extent != (unsigned long )((unsigned int const * )0U)) {
    goto ldv_53658;
  } else {
  }
  sect = sect + 1;
  ldv_53662: ;
  if ((unsigned long )sect->section != (unsigned long )((struct cs_extent_def const * )0)) {
    goto ldv_53661;
  } else {
  }
  count = count + 4U;
  count = count + 2U;
  count = count + 2U;
  return (count);
}
}
static void gfx_v7_0_get_csb_buffer(struct amdgpu_device *adev , u32 volatile *buffer )
{
  u32 count ;
  u32 i ;
  struct cs_section_def const *sect ;
  struct cs_extent_def const *ext ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  {
  count = 0U;
  sect = (struct cs_section_def const *)0;
  ext = (struct cs_extent_def const *)0;
  if ((unsigned long )adev->gfx.rlc.cs_data == (unsigned long )((struct cs_section_def const *)0)) {
    return;
  } else {
  }
  if ((unsigned long )buffer == (unsigned long )((u32 volatile *)0U)) {
    return;
  } else {
  }
  tmp = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp) = 3221244416U;
  tmp___0 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___0) = 536870912U;
  tmp___1 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___1) = 3221301248U;
  tmp___2 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___2) = 2147483648U;
  tmp___3 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___3) = 2147483648U;
  sect = adev->gfx.rlc.cs_data;
  goto ldv_53679;
  ldv_53678:
  ext = sect->section;
  goto ldv_53676;
  ldv_53675: ;
  if ((unsigned int )sect->id == 1U) {
    tmp___4 = count;
    count = count + 1U;
    *(buffer + (unsigned long )tmp___4) = (((unsigned int )ext->reg_count & 16383U) << 16) | 3221252352U;
    tmp___5 = count;
    count = count + 1U;
    *(buffer + (unsigned long )tmp___5) = (unsigned int )ext->reg_index - 40960U;
    i = 0U;
    goto ldv_53673;
    ldv_53672:
    tmp___6 = count;
    count = count + 1U;
    *(buffer + (unsigned long )tmp___6) = *(ext->extent + (unsigned long )i);
    i = i + 1U;
    ldv_53673: ;
    if ((u32 )ext->reg_count > i) {
      goto ldv_53672;
    } else {
    }
  } else {
    return;
  }
  ext = ext + 1;
  ldv_53676: ;
  if ((unsigned long )ext->extent != (unsigned long )((unsigned int const * )0U)) {
    goto ldv_53675;
  } else {
  }
  sect = sect + 1;
  ldv_53679: ;
  if ((unsigned long )sect->section != (unsigned long )((struct cs_extent_def const * )0)) {
    goto ldv_53678;
  } else {
  }
  tmp___7 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___7) = 3221383424U;
  tmp___8 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___8) = 212U;
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  tmp___9 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___9) = 369098770U;
  tmp___10 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___10) = 0U;
  goto ldv_53682;
  case 1U:
  tmp___11 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___11) = 0U;
  tmp___12 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___12) = 0U;
  goto ldv_53682;
  case 2U: ;
  case 4U:
  tmp___13 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___13) = 0U;
  tmp___14 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___14) = 0U;
  goto ldv_53682;
  case 3U:
  tmp___15 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___15) = 973084186U;
  tmp___16 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___16) = 46U;
  goto ldv_53682;
  default:
  tmp___17 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___17) = 0U;
  tmp___18 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___18) = 0U;
  goto ldv_53682;
  }
  ldv_53682:
  tmp___19 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___19) = 3221244416U;
  tmp___20 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___20) = 805306368U;
  tmp___21 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___21) = 3221230080U;
  tmp___22 = count;
  count = count + 1U;
  *(buffer + (unsigned long )tmp___22) = 0U;
  return;
}
}
static void gfx_v7_0_init_pg(struct amdgpu_device *adev )
{
  {
  if ((adev->pg_flags & 231U) != 0U) {
    gfx_v7_0_enable_sclk_slowdown_on_pu(adev, 1);
    gfx_v7_0_enable_sclk_slowdown_on_pd(adev, 1);
    if ((int )adev->pg_flags & 1) {
      gfx_v7_0_init_gfx_cgpg(adev);
      gfx_v7_0_enable_cp_pg(adev, 1);
      gfx_v7_0_enable_gds_pg(adev, 1);
    } else {
    }
    gfx_v7_0_init_ao_cu_mask(adev);
    gfx_v7_0_update_gfx_pg(adev, 1);
  } else {
  }
  return;
}
}
static void gfx_v7_0_fini_pg(struct amdgpu_device *adev )
{
  {
  if ((adev->pg_flags & 231U) != 0U) {
    gfx_v7_0_update_gfx_pg(adev, 0);
    if ((int )adev->pg_flags & 1) {
      gfx_v7_0_enable_cp_pg(adev, 0);
      gfx_v7_0_enable_gds_pg(adev, 0);
    } else {
    }
  } else {
  }
  return;
}
}
uint64_t gfx_v7_0_get_gpu_clock_counter(struct amdgpu_device *adev )
{
  uint64_t clock ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  mutex_lock_nested(& adev->gfx.gpu_clock_mutex, 0U);
  amdgpu_mm_wreg(adev, 12518U, 1U, 0);
  tmp = amdgpu_mm_rreg(adev, 12516U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 12517U, 0);
  clock = (unsigned long long )tmp | ((unsigned long long )tmp___0 << 32);
  mutex_unlock(& adev->gfx.gpu_clock_mutex);
  return (clock);
}
}
static void gfx_v7_0_ring_emit_gds_switch(struct amdgpu_ring *ring , u32 vmid , u32 gds_base ,
                                          u32 gds_size , u32 gws_base , u32 gws_size ,
                                          u32 oa_base , u32 oa_size )
{
  {
  gds_base = gds_base >> 2;
  gds_size = gds_size >> 2;
  gws_base = gws_base >> 12;
  gws_size = gws_size >> 12;
  oa_base = oa_base >> 12;
  oa_size = oa_size >> 12;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset[vmid].mem_base);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, gds_base);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset[vmid].mem_size);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, gds_size);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset[vmid].gws);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (gws_size << 16) | gws_base);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset[vmid].oa);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )((1 << (int )(oa_size + oa_base)) - (1 << (int )oa_base)));
  return;
}
}
static int gfx_v7_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->gfx.num_gfx_rings = 1U;
  adev->gfx.num_compute_rings = 8U;
  gfx_v7_0_set_ring_funcs(adev);
  gfx_v7_0_set_irq_funcs(adev);
  gfx_v7_0_set_gds_init(adev);
  return (0);
}
}
static int gfx_v7_0_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  int i ;
  int r ;
  unsigned int irq_type ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 181U, & adev->gfx.eop_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 184U, & adev->gfx.priv_reg_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 185U, & adev->gfx.priv_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  gfx_v7_0_scratch_init(adev);
  r = gfx_v7_0_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load gfx firmware!\n");
    return (r);
  } else {
  }
  r = gfx_v7_0_rlc_init(adev);
  if (r != 0) {
    drm_err("Failed to init rlc BOs!\n");
    return (r);
  } else {
  }
  r = gfx_v7_0_mec_init(adev);
  if (r != 0) {
    drm_err("Failed to init MEC BOs!\n");
    return (r);
  } else {
  }
  r = amdgpu_wb_get(adev, & adev->gfx.ce_sync_offs);
  if (r != 0) {
    drm_err("(%d) gfx.ce_sync_offs wb alloc failed\n", r);
    return (r);
  } else {
  }
  i = 0;
  goto ldv_53720;
  ldv_53719:
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring) + (unsigned long )i;
  ring->ring_obj = (struct amdgpu_bo *)0;
  sprintf((char *)(& ring->name), "gfx");
  r = amdgpu_ring_init(adev, ring, 1048576U, 4294905856U, 15U, & adev->gfx.eop_irq,
                       0U, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_53720: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_53719;
  } else {
  }
  i = 0;
  goto ldv_53725;
  ldv_53724: ;
  if (i > 31 || i > 7) {
    drm_err("Too many (%d) compute rings!\n", i);
    goto ldv_53723;
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 1;
  ring->doorbell_index = (u32 )(i + 16);
  ring->me = 1U;
  ring->pipe = (u32 )(i / 8);
  ring->queue = (u32 )(i % 8);
  sprintf((char *)(& ring->name), "comp %d.%d.%d", ring->me, ring->pipe, ring->queue);
  irq_type = ring->pipe + 1U;
  r = amdgpu_ring_init(adev, ring, 1048576U, 4294905856U, 15U, & adev->gfx.eop_irq,
                       irq_type, 1);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_53725: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53724;
  } else {
  }
  ldv_53723:
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.mem.gfx_partition_size, 4096,
                       1, 8U, 0ULL, (struct sg_table *)0, & adev->gds.gds_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.gws.gfx_partition_size, 4096,
                       1, 16U, 0ULL, (struct sg_table *)0, & adev->gds.gws_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.oa.gfx_partition_size, 4096,
                       1, 32U, 0ULL, (struct sg_table *)0, & adev->gds.oa_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int gfx_v7_0_sw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_bo_unref(& adev->gds.oa_gfx_bo);
  amdgpu_bo_unref(& adev->gds.gws_gfx_bo);
  amdgpu_bo_unref(& adev->gds.gds_gfx_bo);
  i = 0;
  goto ldv_53732;
  ldv_53731:
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->gfx.gfx_ring) + (unsigned long )i);
  i = i + 1;
  ldv_53732: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_53731;
  } else {
  }
  i = 0;
  goto ldv_53735;
  ldv_53734:
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i);
  i = i + 1;
  ldv_53735: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53734;
  } else {
  }
  amdgpu_wb_free(adev, adev->gfx.ce_sync_offs);
  gfx_v7_0_cp_compute_fini(adev);
  gfx_v7_0_rlc_fini(adev);
  gfx_v7_0_mec_fini(adev);
  return (0);
}
}
static int gfx_v7_0_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gfx_v7_0_gpu_init(adev);
  r = gfx_v7_0_rlc_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v7_0_cp_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gfx.ce_ram_size = 32768U;
  return (r);
}
}
static int gfx_v7_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gfx_v7_0_cp_enable(adev, 0);
  gfx_v7_0_rlc_stop(adev);
  gfx_v7_0_fini_pg(adev);
  return (0);
}
}
static int gfx_v7_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = gfx_v7_0_hw_fini((void *)adev);
  return (tmp);
}
}
static int gfx_v7_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = gfx_v7_0_hw_init((void *)adev);
  return (tmp);
}
}
static bool gfx_v7_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((int )tmp < 0) {
    return (0);
  } else {
    return (1);
  }
}
}
static int gfx_v7_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_53765;
  ldv_53764:
  tmp___0 = amdgpu_mm_rreg(adev, 8196U, 0);
  tmp = tmp___0 & 2147483648U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_53765: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_53764;
  } else {
  }
  return (-110);
}
}
static void gfx_v7_0_print_status(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  u32 tmp___46 ;
  u32 tmp___47 ;
  u32 tmp___48 ;
  u32 tmp___49 ;
  u32 tmp___50 ;
  u32 tmp___51 ;
  u32 tmp___52 ;
  u32 tmp___53 ;
  u32 tmp___54 ;
  u32 tmp___55 ;
  u32 tmp___56 ;
  u32 tmp___57 ;
  u32 tmp___58 ;
  u32 tmp___59 ;
  u32 tmp___60 ;
  u32 tmp___61 ;
  u32 tmp___62 ;
  u32 tmp___63 ;
  u32 tmp___64 ;
  int me ;
  int pipe ;
  int queue ;
  u32 tmp___65 ;
  u32 tmp___66 ;
  u32 tmp___67 ;
  u32 tmp___68 ;
  u32 tmp___69 ;
  u32 tmp___70 ;
  u32 tmp___71 ;
  u32 tmp___72 ;
  u32 tmp___73 ;
  u32 tmp___74 ;
  u32 tmp___75 ;
  u32 tmp___76 ;
  u32 tmp___77 ;
  u32 tmp___78 ;
  u32 tmp___79 ;
  u32 tmp___80 ;
  u32 tmp___81 ;
  u32 tmp___82 ;
  u32 tmp___83 ;
  u32 tmp___84 ;
  u32 tmp___85 ;
  u32 tmp___86 ;
  u32 tmp___87 ;
  u32 tmp___88 ;
  u32 tmp___89 ;
  u32 tmp___90 ;
  u32 tmp___91 ;
  u32 tmp___92 ;
  u32 tmp___93 ;
  u32 tmp___94 ;
  u32 tmp___95 ;
  u32 tmp___96 ;
  u32 tmp___97 ;
  u32 tmp___98 ;
  u32 tmp___99 ;
  u32 tmp___100 ;
  u32 tmp___101 ;
  u32 tmp___102 ;
  u32 tmp___103 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "GFX 7.x registers\n");
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 8194U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 8197U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE0=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 8198U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE1=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 8206U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE2=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 8207U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE3=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 8608U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STAT = 0x%08x\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 8605U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT1 = 0x%08x\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 8606U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT2 = 0x%08x\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 8604U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT3 = 0x%08x\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 8328U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_BUSY_STAT = 0x%08x\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 8329U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STALLED_STAT1 = 0x%08x\n",
            tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 8327U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STATUS = 0x%08x\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 8325U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_BUSY_STAT = 0x%08x\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 8326U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STALLED_STAT1 = 0x%08x\n",
            tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 8324U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STATUS = 0x%08x\n", tmp___14);
  i = 0;
  goto ldv_53773;
  ldv_53772:
  tmp___15 = amdgpu_mm_rreg(adev, (u32 )((i + 2449) * 4), 0);
  _dev_info((struct device const *)adev->dev, "  GB_TILE_MODE%d=0x%08X\n", i, tmp___15);
  i = i + 1;
  ldv_53773: ;
  if (i <= 31) {
    goto ldv_53772;
  } else {
  }
  i = 0;
  goto ldv_53776;
  ldv_53775:
  tmp___16 = amdgpu_mm_rreg(adev, (u32 )((i + 2457) * 4), 0);
  _dev_info((struct device const *)adev->dev, "  GB_MACROTILE_MODE%d=0x%08X\n",
            i, tmp___16);
  i = i + 1;
  ldv_53776: ;
  if (i <= 15) {
    goto ldv_53775;
  } else {
  }
  i = 0;
  goto ldv_53779;
  ldv_53778:
  _dev_info((struct device const *)adev->dev, "  se: %d\n", i);
  gfx_v7_0_select_se_sh(adev, (u32 )i, 4294967295U);
  tmp___17 = amdgpu_mm_rreg(adev, 41172U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_RASTER_CONFIG=0x%08X\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 41173U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_RASTER_CONFIG_1=0x%08X\n",
            tmp___18);
  i = i + 1;
  ldv_53779: ;
  if ((unsigned int )i < adev->gfx.config.max_shader_engines) {
    goto ldv_53778;
  } else {
  }
  gfx_v7_0_select_se_sh(adev, 4294967295U, 4294967295U);
  tmp___19 = amdgpu_mm_rreg(adev, 9790U, 0);
  _dev_info((struct device const *)adev->dev, "  GB_ADDR_CONFIG=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 3026U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_ADDR_CONFIG=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 768U, 0);
  _dev_info((struct device const *)adev->dev, "  DMIF_ADDR_CALC=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 13318U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_TILING_CONFIG=0x%08X\n",
            tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 13830U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA1_TILING_CONFIG=0x%08X\n",
            tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 15315U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_ADDR_CONFIG=0x%08X\n",
            tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 15316U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DB_ADDR_CONFIG=0x%08X\n",
            tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 15317U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DBW_ADDR_CONFIG=0x%08X\n",
            tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 8665U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MEQ_THRESHOLDS=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 9240U, 0);
  _dev_info((struct device const *)adev->dev, "  SX_DEBUG_1=0x%08X\n", tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 9538U, 0);
  _dev_info((struct device const *)adev->dev, "  TA_CNTL_AUX=0x%08X\n", tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 9280U, 0);
  _dev_info((struct device const *)adev->dev, "  SPI_CONFIG_CNTL=0x%08X\n", tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 8960U, 0);
  _dev_info((struct device const *)adev->dev, "  SQ_CONFIG=0x%08X\n", tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 9740U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG=0x%08X\n", tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 9741U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG2=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 9742U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG3=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 9860U, 0);
  _dev_info((struct device const *)adev->dev, "  CB_HW_CONTROL=0x%08X\n", tmp___35);
  tmp___36 = amdgpu_mm_rreg(adev, 9295U, 0);
  _dev_info((struct device const *)adev->dev, "  SPI_CONFIG_CNTL_1=0x%08X\n", tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, 8947U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_FIFO_SIZE=0x%08X\n", tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, 49741U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_NUM_INSTANCES=0x%08X\n", tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, 55304U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_PERFMON_CNTL=0x%08X\n", tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, 8905U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_FORCE_EOV_MAX_CNTS=0x%08X\n",
            tmp___40);
  tmp___41 = amdgpu_mm_rreg(adev, 8753U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_CACHE_INVALIDATION=0x%08X\n",
            tmp___41);
  tmp___42 = amdgpu_mm_rreg(adev, 8757U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_GS_VERTEX_REUSE=0x%08X\n",
            tmp___42);
  tmp___43 = amdgpu_mm_rreg(adev, 49793U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_LINE_STIPPLE_STATE=0x%08X\n",
            tmp___43);
  tmp___44 = amdgpu_mm_rreg(adev, 8837U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_CL_ENHANCE=0x%08X\n", tmp___44);
  tmp___45 = amdgpu_mm_rreg(adev, 8956U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_ENHANCE=0x%08X\n", tmp___45);
  tmp___46 = amdgpu_mm_rreg(adev, 8630U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_ME_CNTL=0x%08X\n", tmp___46);
  tmp___47 = amdgpu_mm_rreg(adev, 12462U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MAX_CONTEXT=0x%08X\n", tmp___47);
  tmp___48 = amdgpu_mm_rreg(adev, 12368U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_ENDIAN_SWAP=0x%08X\n", tmp___48);
  tmp___49 = amdgpu_mm_rreg(adev, 12363U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_DEVICE_ID=0x%08X\n", tmp___49);
  tmp___50 = amdgpu_mm_rreg(adev, 49263U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_SEM_WAIT_TIMER=0x%08X\n", tmp___50);
  if ((unsigned int )adev->asic_type != 3U) {
    tmp___51 = amdgpu_mm_rreg(adev, 49266U, 0);
    _dev_info((struct device const *)adev->dev, "  CP_SEM_INCOMPLETE_TIMER_CNTL=0x%08X\n",
              tmp___51);
  } else {
  }
  tmp___52 = amdgpu_mm_rreg(adev, 8641U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB_WPTR_DELAY=0x%08X\n", tmp___52);
  tmp___53 = amdgpu_mm_rreg(adev, 12369U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB_VMID=0x%08X\n", tmp___53);
  tmp___54 = amdgpu_mm_rreg(adev, 12353U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_CNTL=0x%08X\n", tmp___54);
  tmp___55 = amdgpu_mm_rreg(adev, 12357U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_WPTR=0x%08X\n", tmp___55);
  tmp___56 = amdgpu_mm_rreg(adev, 12355U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_RPTR_ADDR=0x%08X\n", tmp___56);
  tmp___57 = amdgpu_mm_rreg(adev, 12356U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_RPTR_ADDR_HI=0x%08X\n",
            tmp___57);
  tmp___58 = amdgpu_mm_rreg(adev, 12353U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_CNTL=0x%08X\n", tmp___58);
  tmp___59 = amdgpu_mm_rreg(adev, 12352U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_BASE=0x%08X\n", tmp___59);
  tmp___60 = amdgpu_mm_rreg(adev, 12465U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_BASE_HI=0x%08X\n", tmp___60);
  tmp___61 = amdgpu_mm_rreg(adev, 8333U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MEC_CNTL=0x%08X\n", tmp___61);
  tmp___62 = amdgpu_mm_rreg(adev, 12416U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_DEBUG=0x%08X\n", tmp___62);
  tmp___63 = amdgpu_mm_rreg(adev, 49233U, 0);
  _dev_info((struct device const *)adev->dev, "  SCRATCH_ADDR=0x%08X\n", tmp___63);
  tmp___64 = amdgpu_mm_rreg(adev, 49232U, 0);
  _dev_info((struct device const *)adev->dev, "  SCRATCH_UMSK=0x%08X\n", tmp___64);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_53788;
  ldv_53787:
  me = i <= 3 ? 1 : 2;
  pipe = i > 3 ? i + -4 : i;
  _dev_info((struct device const *)adev->dev, "  me: %d, pipe: %d\n", me, pipe);
  cik_srbm_select(adev, (u32 )me, (u32 )pipe, 0U, 0U);
  tmp___65 = amdgpu_mm_rreg(adev, 12865U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HPD_EOP_BASE_ADDR=0x%08X\n",
            tmp___65);
  tmp___66 = amdgpu_mm_rreg(adev, 12866U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HPD_EOP_BASE_ADDR_HI=0x%08X\n",
            tmp___66);
  tmp___67 = amdgpu_mm_rreg(adev, 12867U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HPD_EOP_VMID=0x%08X\n", tmp___67);
  tmp___68 = amdgpu_mm_rreg(adev, 12868U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HPD_EOP_CONTROL=0x%08X\n", tmp___68);
  queue = 0;
  goto ldv_53785;
  ldv_53784:
  cik_srbm_select(adev, (u32 )me, (u32 )pipe, (u32 )queue, 0U);
  _dev_info((struct device const *)adev->dev, "  queue: %d\n", queue);
  tmp___69 = amdgpu_mm_rreg(adev, 12419U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_PQ_WPTR_POLL_CNTL=0x%08X\n",
            tmp___69);
  tmp___70 = amdgpu_mm_rreg(adev, 12884U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_DOORBELL_CONTROL=0x%08X\n",
            tmp___70);
  tmp___71 = amdgpu_mm_rreg(adev, 12871U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_ACTIVE=0x%08X\n", tmp___71);
  tmp___72 = amdgpu_mm_rreg(adev, 12893U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_DEQUEUE_REQUEST=0x%08X\n",
            tmp___72);
  tmp___73 = amdgpu_mm_rreg(adev, 12879U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_RPTR=0x%08X\n", tmp___73);
  tmp___74 = amdgpu_mm_rreg(adev, 12885U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_WPTR=0x%08X\n", tmp___74);
  tmp___75 = amdgpu_mm_rreg(adev, 12877U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_BASE=0x%08X\n", tmp___75);
  tmp___76 = amdgpu_mm_rreg(adev, 12878U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_BASE_HI=0x%08X\n", tmp___76);
  tmp___77 = amdgpu_mm_rreg(adev, 12886U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_CONTROL=0x%08X\n", tmp___77);
  tmp___78 = amdgpu_mm_rreg(adev, 12882U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_WPTR_POLL_ADDR=0x%08X\n",
            tmp___78);
  tmp___79 = amdgpu_mm_rreg(adev, 12883U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_WPTR_POLL_ADDR_HI=0x%08X\n",
            tmp___79);
  tmp___80 = amdgpu_mm_rreg(adev, 12880U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_RPTR_REPORT_ADDR=0x%08X\n",
            tmp___80);
  tmp___81 = amdgpu_mm_rreg(adev, 12881U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_RPTR_REPORT_ADDR_HI=0x%08X\n",
            tmp___81);
  tmp___82 = amdgpu_mm_rreg(adev, 12884U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_DOORBELL_CONTROL=0x%08X\n",
            tmp___82);
  tmp___83 = amdgpu_mm_rreg(adev, 12885U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_PQ_WPTR=0x%08X\n", tmp___83);
  tmp___84 = amdgpu_mm_rreg(adev, 12872U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_HQD_VMID=0x%08X\n", tmp___84);
  tmp___85 = amdgpu_mm_rreg(adev, 12869U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MQD_BASE_ADDR=0x%08X\n", tmp___85);
  tmp___86 = amdgpu_mm_rreg(adev, 12870U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MQD_BASE_ADDR_HI=0x%08X\n",
            tmp___86);
  tmp___87 = amdgpu_mm_rreg(adev, 12903U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MQD_CONTROL=0x%08X\n", tmp___87);
  i = i + 1;
  ldv_53785: ;
  if (queue <= 7) {
    goto ldv_53784;
  } else {
  }
  i = i + 1;
  ldv_53788: ;
  if ((u32 )i < adev->gfx.mec.num_pipe * adev->gfx.mec.num_mec) {
    goto ldv_53787;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  tmp___88 = amdgpu_mm_rreg(adev, 12394U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_INT_CNTL_RING0=0x%08X\n", tmp___88);
  tmp___89 = amdgpu_mm_rreg(adev, 12505U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTL=0x%08X\n", tmp___89);
  tmp___90 = amdgpu_mm_rreg(adev, 12480U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_CNTL=0x%08X\n", tmp___90);
  tmp___91 = amdgpu_mm_rreg(adev, 12553U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_CGCG_CGLS_CTRL=0x%08X\n", tmp___91);
  tmp___92 = amdgpu_mm_rreg(adev, 12507U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTR_INIT=0x%08X\n", tmp___92);
  tmp___93 = amdgpu_mm_rreg(adev, 12498U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTR_MAX=0x%08X\n", tmp___93);
  tmp___94 = amdgpu_mm_rreg(adev, 12559U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_INIT_CU_MASK=0x%08X\n",
            tmp___94);
  tmp___95 = amdgpu_mm_rreg(adev, 12561U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_PARAMS=0x%08X\n", tmp___95);
  tmp___96 = amdgpu_mm_rreg(adev, 12505U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTL=0x%08X\n", tmp___96);
  tmp___97 = amdgpu_mm_rreg(adev, 12483U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_MC_CNTL=0x%08X\n", tmp___97);
  tmp___98 = amdgpu_mm_rreg(adev, 12519U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_UCODE_CNTL=0x%08X\n", tmp___98);
  if ((unsigned int )adev->asic_type == 0U) {
    tmp___99 = amdgpu_mm_rreg(adev, 12510U, 0);
    _dev_info((struct device const *)adev->dev, "  RLC_DRIVER_CPDMA_STATUS=0x%08X\n",
              tmp___99);
  } else {
  }
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_53791;
  ldv_53790:
  cik_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  _dev_info((struct device const *)adev->dev, "  VM %d:\n", i);
  tmp___100 = amdgpu_mm_rreg(adev, 8973U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_CONFIG=0x%08X\n", tmp___100);
  tmp___101 = amdgpu_mm_rreg(adev, 8971U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_APE1_BASE=0x%08X\n", tmp___101);
  tmp___102 = amdgpu_mm_rreg(adev, 8972U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_APE1_LIMIT=0x%08X\n", tmp___102);
  tmp___103 = amdgpu_mm_rreg(adev, 8970U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_BASES=0x%08X\n", tmp___103);
  i = i + 1;
  ldv_53791: ;
  if (i <= 15) {
    goto ldv_53790;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  return;
}
}
static int gfx_v7_0_soft_reset(void *handle )
{
  u32 grbm_soft_reset ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  {
  grbm_soft_reset = 0U;
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((tmp & 1205780480U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 65537U;
  } else {
  }
  if ((tmp & 805306368U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 8194U, 0);
  if ((tmp & 16777216U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 4U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 32U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if (grbm_soft_reset != 0U || srbm_soft_reset != 0U) {
    gfx_v7_0_print_status((void *)adev);
    gfx_v7_0_fini_pg(adev);
    gfx_v7_0_update_cg(adev, 0);
    gfx_v7_0_rlc_stop(adev);
    amdgpu_mm_wreg(adev, 8630U, 352321536U, 0);
    amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
    if (grbm_soft_reset != 0U) {
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
      tmp = tmp | grbm_soft_reset;
      _dev_info((struct device const *)adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
      amdgpu_mm_wreg(adev, 8200U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
      __const_udelay(214750UL);
      tmp = ~ grbm_soft_reset & tmp;
      amdgpu_mm_wreg(adev, 8200U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    } else {
    }
    if (srbm_soft_reset != 0U) {
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
      tmp = tmp | srbm_soft_reset;
      _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
      amdgpu_mm_wreg(adev, 920U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
      __const_udelay(214750UL);
      tmp = ~ srbm_soft_reset & tmp;
      amdgpu_mm_wreg(adev, 920U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
    } else {
    }
    __const_udelay(214750UL);
    gfx_v7_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static void gfx_v7_0_set_gfx_eop_interrupt_state(struct amdgpu_device *adev , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4227858431U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53806;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl | 67108864U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53806;
  default: ;
  goto ldv_53806;
  }
  ldv_53806: ;
  return;
}
}
static void gfx_v7_0_set_compute_eop_interrupt_state(struct amdgpu_device *adev ,
                                                     int me , int pipe , enum amdgpu_interrupt_state state )
{
  u32 mec_int_cntl ;
  u32 mec_int_cntl_reg ;
  long tmp ;
  long tmp___0 ;
  {
  if (me == 1) {
    switch (pipe) {
    case 0:
    mec_int_cntl_reg = 12421U;
    goto ldv_53818;
    default:
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("gfx_v7_0_set_compute_eop_interrupt_state", "invalid pipe %d\n",
                          pipe);
    } else {
    }
    return;
    }
    ldv_53818: ;
  } else {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("gfx_v7_0_set_compute_eop_interrupt_state", "invalid me %d\n",
                          me);
    } else {
    }
    return;
  }
  switch ((unsigned int )state) {
  case 0U:
  mec_int_cntl = amdgpu_mm_rreg(adev, mec_int_cntl_reg, 0);
  mec_int_cntl = mec_int_cntl & 4227858431U;
  amdgpu_mm_wreg(adev, mec_int_cntl_reg, mec_int_cntl, 0);
  goto ldv_53822;
  case 1U:
  mec_int_cntl = amdgpu_mm_rreg(adev, mec_int_cntl_reg, 0);
  mec_int_cntl = mec_int_cntl | 67108864U;
  amdgpu_mm_wreg(adev, mec_int_cntl_reg, mec_int_cntl, 0);
  goto ldv_53822;
  default: ;
  goto ldv_53822;
  }
  ldv_53822: ;
  return;
}
}
static int gfx_v7_0_set_priv_reg_fault_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                             unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4286578687U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53833;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl | 8388608U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53833;
  default: ;
  goto ldv_53833;
  }
  ldv_53833: ;
  return (0);
}
}
static int gfx_v7_0_set_priv_inst_fault_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                              unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4290772991U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53844;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl | 4194304U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_53844;
  default: ;
  goto ldv_53844;
  }
  ldv_53844: ;
  return (0);
}
}
static int gfx_v7_0_set_eop_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                            unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  switch (type) {
  case 0U:
  gfx_v7_0_set_gfx_eop_interrupt_state(adev, state);
  goto ldv_53854;
  case 1U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 1, 0, state);
  goto ldv_53854;
  case 2U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 1, 1, state);
  goto ldv_53854;
  case 3U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 1, 2, state);
  goto ldv_53854;
  case 4U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 1, 3, state);
  goto ldv_53854;
  case 5U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 2, 0, state);
  goto ldv_53854;
  case 6U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 2, 1, state);
  goto ldv_53854;
  case 7U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 2, 2, state);
  goto ldv_53854;
  case 8U:
  gfx_v7_0_set_compute_eop_interrupt_state(adev, 2, 3, state);
  goto ldv_53854;
  default: ;
  goto ldv_53854;
  }
  ldv_53854: ;
  return (0);
}
}
static int gfx_v7_0_eop_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                            struct amdgpu_iv_entry *entry )
{
  u8 me_id ;
  u8 pipe_id ;
  struct amdgpu_ring *ring ;
  int i ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gfx_v7_0_eop_irq", "IH: CP EOP\n");
  } else {
  }
  me_id = (u8 )((entry->ring_id & 12U) >> 2);
  pipe_id = (unsigned int )((u8 )entry->ring_id) & 3U;
  switch ((int )me_id) {
  case 0:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->gfx.gfx_ring));
  goto ldv_53875;
  case 1: ;
  case 2:
  i = 0;
  goto ldv_53879;
  ldv_53878:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if (ring->me == (u32 )me_id && ring->pipe == (u32 )pipe_id) {
    amdgpu_fence_process(ring);
  } else {
  }
  i = i + 1;
  ldv_53879: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53878;
  } else {
  }
  goto ldv_53875;
  }
  ldv_53875: ;
  return (0);
}
}
static int gfx_v7_0_priv_reg_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                 struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal register access in command stream\n");
  schedule_work___2(& adev->reset_work);
  return (0);
}
}
static int gfx_v7_0_priv_inst_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                  struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal instruction in command stream\n");
  schedule_work___2(& adev->reset_work);
  return (0);
}
}
static int gfx_v7_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  gfx_v7_0_enable_gui_idle_interrupt(adev, 0);
  if ((int )gate) {
    gfx_v7_0_enable_mgcg(adev, 1);
    gfx_v7_0_enable_cgcg(adev, 1);
  } else {
    gfx_v7_0_enable_cgcg(adev, 0);
    gfx_v7_0_enable_mgcg(adev, 0);
  }
  gfx_v7_0_enable_gui_idle_interrupt(adev, 1);
  return (0);
}
}
static int gfx_v7_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  if ((adev->pg_flags & 231U) != 0U) {
    gfx_v7_0_update_gfx_pg(adev, (int )gate);
    if ((int )adev->pg_flags & 1) {
      gfx_v7_0_enable_cp_pg(adev, (int )gate);
      gfx_v7_0_enable_gds_pg(adev, (int )gate);
    } else {
    }
  } else {
  }
  return (0);
}
}
struct amd_ip_funcs const gfx_v7_0_ip_funcs =
     {& gfx_v7_0_early_init, (int (*)(void * ))0, & gfx_v7_0_sw_init, & gfx_v7_0_sw_fini,
    & gfx_v7_0_hw_init, & gfx_v7_0_hw_fini, & gfx_v7_0_suspend, & gfx_v7_0_resume,
    & gfx_v7_0_is_idle, & gfx_v7_0_wait_for_idle, & gfx_v7_0_soft_reset, & gfx_v7_0_print_status,
    & gfx_v7_0_set_clockgating_state, & gfx_v7_0_set_powergating_state};
static bool gfx_v7_0_ring_is_lockup(struct amdgpu_ring *ring )
{
  bool tmp ;
  bool tmp___0 ;
  {
  tmp = gfx_v7_0_is_idle((void *)ring->adev);
  if ((int )tmp) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___0 = amdgpu_ring_test_lockup(ring);
  return (tmp___0);
}
}
static struct amdgpu_ring_funcs const gfx_v7_0_ring_funcs_gfx =
     {& gfx_v7_0_ring_get_rptr_gfx, & gfx_v7_0_ring_get_wptr_gfx, & gfx_v7_0_ring_set_wptr_gfx,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & gfx_v7_0_ring_emit_ib, & gfx_v7_0_ring_emit_fence_gfx,
    & gfx_v7_0_ring_emit_semaphore, & gfx_v7_0_ring_emit_vm_flush, & gfx_v7_0_ring_emit_hdp_flush,
    & gfx_v7_0_ring_emit_gds_switch, & gfx_v7_0_ring_test_ring, & gfx_v7_0_ring_test_ib,
    & gfx_v7_0_ring_is_lockup};
static struct amdgpu_ring_funcs const gfx_v7_0_ring_funcs_compute =
     {& gfx_v7_0_ring_get_rptr_compute, & gfx_v7_0_ring_get_wptr_compute, & gfx_v7_0_ring_set_wptr_compute,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & gfx_v7_0_ring_emit_ib, & gfx_v7_0_ring_emit_fence_compute,
    & gfx_v7_0_ring_emit_semaphore, & gfx_v7_0_ring_emit_vm_flush, & gfx_v7_0_ring_emit_hdp_flush,
    & gfx_v7_0_ring_emit_gds_switch, & gfx_v7_0_ring_test_ring, & gfx_v7_0_ring_test_ib,
    & gfx_v7_0_ring_is_lockup};
static void gfx_v7_0_set_ring_funcs(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_53914;
  ldv_53913:
  adev->gfx.gfx_ring[i].funcs = & gfx_v7_0_ring_funcs_gfx;
  i = i + 1;
  ldv_53914: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_53913;
  } else {
  }
  i = 0;
  goto ldv_53917;
  ldv_53916:
  adev->gfx.compute_ring[i].funcs = & gfx_v7_0_ring_funcs_compute;
  i = i + 1;
  ldv_53917: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_53916;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const gfx_v7_0_eop_irq_funcs = {& gfx_v7_0_set_eop_interrupt_state, & gfx_v7_0_eop_irq};
static struct amdgpu_irq_src_funcs const gfx_v7_0_priv_reg_irq_funcs = {& gfx_v7_0_set_priv_reg_fault_state, & gfx_v7_0_priv_reg_irq};
static struct amdgpu_irq_src_funcs const gfx_v7_0_priv_inst_irq_funcs = {& gfx_v7_0_set_priv_inst_fault_state, & gfx_v7_0_priv_inst_irq};
static void gfx_v7_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->gfx.eop_irq.num_types = 9U;
  adev->gfx.eop_irq.funcs = & gfx_v7_0_eop_irq_funcs;
  adev->gfx.priv_reg_irq.num_types = 1U;
  adev->gfx.priv_reg_irq.funcs = & gfx_v7_0_priv_reg_irq_funcs;
  adev->gfx.priv_inst_irq.num_types = 1U;
  adev->gfx.priv_inst_irq.funcs = & gfx_v7_0_priv_inst_irq_funcs;
  return;
}
}
static void gfx_v7_0_set_gds_init(struct amdgpu_device *adev )
{
  {
  adev->gds.mem.total_size = amdgpu_mm_rreg(adev, 13057U, 0);
  adev->gds.gws.total_size = 64U;
  adev->gds.oa.total_size = 16U;
  if (adev->gds.mem.total_size == 65536U) {
    adev->gds.mem.gfx_partition_size = 4096U;
    adev->gds.mem.cs_partition_size = 4096U;
    adev->gds.gws.gfx_partition_size = 4U;
    adev->gds.gws.cs_partition_size = 4U;
    adev->gds.oa.gfx_partition_size = 4U;
    adev->gds.oa.cs_partition_size = 1U;
  } else {
    adev->gds.mem.gfx_partition_size = 1024U;
    adev->gds.mem.cs_partition_size = 1024U;
    adev->gds.gws.gfx_partition_size = 16U;
    adev->gds.gws.cs_partition_size = 16U;
    adev->gds.oa.gfx_partition_size = 4U;
    adev->gds.oa.cs_partition_size = 4U;
  }
  return;
}
}
int gfx_v7_0_get_cu_info(struct amdgpu_device *adev , struct amdgpu_cu_info *cu_info )
{
  int i ;
  int j ;
  int k ;
  int counter ;
  int active_cu_number ;
  u32 mask ;
  u32 bitmap ;
  u32 ao_bitmap ;
  u32 ao_cu_mask ;
  {
  active_cu_number = 0;
  ao_cu_mask = 0U;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0) || (unsigned long )cu_info == (unsigned long )((struct amdgpu_cu_info *)0)) {
    return (-22);
  } else {
  }
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_53948;
  ldv_53947:
  j = 0;
  goto ldv_53945;
  ldv_53944:
  mask = 1U;
  ao_bitmap = 0U;
  counter = 0;
  bitmap = gfx_v7_0_get_cu_active_bitmap(adev, (u32 )i, (u32 )j);
  cu_info->bitmap[i][j] = bitmap;
  k = 0;
  goto ldv_53942;
  ldv_53941: ;
  if ((bitmap & mask) != 0U) {
    if (counter <= 1) {
      ao_bitmap = ao_bitmap | mask;
    } else {
    }
    counter = counter + 1;
  } else {
  }
  mask = mask << 1;
  k = k + 1;
  ldv_53942: ;
  if ((unsigned int )k < adev->gfx.config.max_cu_per_sh) {
    goto ldv_53941;
  } else {
  }
  active_cu_number = active_cu_number + counter;
  ao_cu_mask = (ao_bitmap << (i * 2 + j) * 8) | ao_cu_mask;
  j = j + 1;
  ldv_53945: ;
  if ((unsigned int )j < adev->gfx.config.max_sh_per_se) {
    goto ldv_53944;
  } else {
  }
  i = i + 1;
  ldv_53948: ;
  if ((unsigned int )i < adev->gfx.config.max_shader_engines) {
    goto ldv_53947;
  } else {
  }
  cu_info->number = (u32 )active_cu_number;
  cu_info->ao_cu_mask = ao_cu_mask;
  mutex_unlock(& adev->grbm_idx_mutex);
  return (0);
}
}
extern int ldv_probe_97(void) ;
int ldv_retval_65 ;
extern int ldv_release_97(void) ;
int ldv_retval_66 ;
void ldv_initialize_amdgpu_irq_src_funcs_93(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v7_0_priv_reg_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v7_0_priv_reg_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_96(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  gfx_v7_0_ring_funcs_gfx_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_92(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v7_0_priv_inst_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v7_0_priv_inst_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_95(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  gfx_v7_0_ring_funcs_compute_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_94(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v7_0_eop_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v7_0_eop_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_92(void)
{
  unsigned int ldvarg639 ;
  enum amdgpu_interrupt_state ldvarg640 ;
  struct amdgpu_iv_entry *ldvarg638 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg638 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg639), 0, 4UL);
  ldv_memset((void *)(& ldvarg640), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_92 == 1) {
    gfx_v7_0_set_priv_inst_fault_state(gfx_v7_0_priv_inst_irq_funcs_group0, gfx_v7_0_priv_inst_irq_funcs_group1,
                                       ldvarg639, ldvarg640);
    ldv_state_variable_92 = 1;
  } else {
  }
  goto ldv_53978;
  case 1: ;
  if (ldv_state_variable_92 == 1) {
    gfx_v7_0_priv_inst_irq(gfx_v7_0_priv_inst_irq_funcs_group0, gfx_v7_0_priv_inst_irq_funcs_group1,
                           ldvarg638);
    ldv_state_variable_92 = 1;
  } else {
  }
  goto ldv_53978;
  default:
  ldv_stop();
  }
  ldv_53978: ;
  return;
}
}
void ldv_main_exported_93(void)
{
  struct amdgpu_iv_entry *ldvarg139 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg141 ;
  unsigned int ldvarg140 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg139 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg141), 0, 4UL);
  ldv_memset((void *)(& ldvarg140), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_93 == 1) {
    gfx_v7_0_set_priv_reg_fault_state(gfx_v7_0_priv_reg_irq_funcs_group0, gfx_v7_0_priv_reg_irq_funcs_group1,
                                      ldvarg140, ldvarg141);
    ldv_state_variable_93 = 1;
  } else {
  }
  goto ldv_53988;
  case 1: ;
  if (ldv_state_variable_93 == 1) {
    gfx_v7_0_priv_reg_irq(gfx_v7_0_priv_reg_irq_funcs_group0, gfx_v7_0_priv_reg_irq_funcs_group1,
                          ldvarg139);
    ldv_state_variable_93 = 1;
  } else {
  }
  goto ldv_53988;
  default:
  ldv_stop();
  }
  ldv_53988: ;
  return;
}
}
void ldv_main_exported_95(void)
{
  uint64_t ldvarg75 ;
  u32 ldvarg83 ;
  u32 ldvarg82 ;
  unsigned int ldvarg76 ;
  unsigned int ldvarg89 ;
  u32 ldvarg79 ;
  struct amdgpu_ib *ldvarg86 ;
  void *tmp ;
  uint64_t ldvarg87 ;
  struct amdgpu_semaphore *ldvarg78 ;
  void *tmp___0 ;
  u32 ldvarg80 ;
  uint64_t ldvarg88 ;
  u32 ldvarg84 ;
  u32 ldvarg85 ;
  bool ldvarg77 ;
  u32 ldvarg81 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(272UL);
  ldvarg86 = (struct amdgpu_ib *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg78 = (struct amdgpu_semaphore *)tmp___0;
  ldv_memset((void *)(& ldvarg75), 0, 8UL);
  ldv_memset((void *)(& ldvarg83), 0, 4UL);
  ldv_memset((void *)(& ldvarg82), 0, 4UL);
  ldv_memset((void *)(& ldvarg76), 0, 4UL);
  ldv_memset((void *)(& ldvarg89), 0, 4UL);
  ldv_memset((void *)(& ldvarg79), 0, 4UL);
  ldv_memset((void *)(& ldvarg87), 0, 8UL);
  ldv_memset((void *)(& ldvarg80), 0, 4UL);
  ldv_memset((void *)(& ldvarg88), 0, 8UL);
  ldv_memset((void *)(& ldvarg84), 0, 4UL);
  ldv_memset((void *)(& ldvarg85), 0, 4UL);
  ldv_memset((void *)(& ldvarg77), 0, 1UL);
  ldv_memset((void *)(& ldvarg81), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_fence_compute(gfx_v7_0_ring_funcs_compute_group0, ldvarg88,
                                     ldvarg87, ldvarg89);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 1: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_get_rptr_compute(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 2: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_test_ring(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 3: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_hdp_flush(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 4: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_set_wptr_compute(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 5: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_get_wptr_compute(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 6: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_ib(gfx_v7_0_ring_funcs_compute_group0, ldvarg86);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 7: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_gds_switch(gfx_v7_0_ring_funcs_compute_group0, ldvarg82, ldvarg80,
                                  ldvarg84, ldvarg85, ldvarg79, ldvarg81, ldvarg83);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 8: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_semaphore(gfx_v7_0_ring_funcs_compute_group0, ldvarg78, (int )ldvarg77);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 9: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_test_ib(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 10: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_is_lockup(gfx_v7_0_ring_funcs_compute_group0);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  case 11: ;
  if (ldv_state_variable_95 == 1) {
    gfx_v7_0_ring_emit_vm_flush(gfx_v7_0_ring_funcs_compute_group0, ldvarg76, ldvarg75);
    ldv_state_variable_95 = 1;
  } else {
  }
  goto ldv_54010;
  default:
  ldv_stop();
  }
  ldv_54010: ;
  return;
}
}
void ldv_main_exported_97(void)
{
  void *ldvarg790 ;
  void *tmp ;
  void *ldvarg783 ;
  void *tmp___0 ;
  void *ldvarg788 ;
  void *tmp___1 ;
  enum amd_clockgating_state ldvarg780 ;
  void *ldvarg782 ;
  void *tmp___2 ;
  enum amd_powergating_state ldvarg784 ;
  void *ldvarg787 ;
  void *tmp___3 ;
  void *ldvarg778 ;
  void *tmp___4 ;
  void *ldvarg777 ;
  void *tmp___5 ;
  void *ldvarg785 ;
  void *tmp___6 ;
  void *ldvarg776 ;
  void *tmp___7 ;
  void *ldvarg789 ;
  void *tmp___8 ;
  void *ldvarg779 ;
  void *tmp___9 ;
  void *ldvarg786 ;
  void *tmp___10 ;
  void *ldvarg781 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg790 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg783 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg788 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg782 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg787 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg778 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg777 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg785 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg776 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg789 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg779 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg786 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg781 = tmp___11;
  ldv_memset((void *)(& ldvarg780), 0, 4UL);
  ldv_memset((void *)(& ldvarg784), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_hw_fini(ldvarg790);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_hw_fini(ldvarg790);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_hw_fini(ldvarg790);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 1: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_print_status(ldvarg789);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_print_status(ldvarg789);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_print_status(ldvarg789);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 2: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_early_init(ldvarg788);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_early_init(ldvarg788);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_early_init(ldvarg788);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 3: ;
  if (ldv_state_variable_97 == 2) {
    ldv_retval_66 = gfx_v7_0_suspend(ldvarg787);
    if (ldv_retval_66 == 0) {
      ldv_state_variable_97 = 3;
    } else {
    }
  } else {
  }
  goto ldv_54042;
  case 4: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_sw_init(ldvarg786);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_sw_init(ldvarg786);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_sw_init(ldvarg786);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 5: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_set_powergating_state(ldvarg785, ldvarg784);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_set_powergating_state(ldvarg785, ldvarg784);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_set_powergating_state(ldvarg785, ldvarg784);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 6: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_wait_for_idle(ldvarg783);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_wait_for_idle(ldvarg783);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_wait_for_idle(ldvarg783);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 7: ;
  if (ldv_state_variable_97 == 3) {
    ldv_retval_65 = gfx_v7_0_resume(ldvarg782);
    if (ldv_retval_65 == 0) {
      ldv_state_variable_97 = 2;
    } else {
    }
  } else {
  }
  goto ldv_54042;
  case 8: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_set_clockgating_state(ldvarg781, ldvarg780);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_set_clockgating_state(ldvarg781, ldvarg780);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_set_clockgating_state(ldvarg781, ldvarg780);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 9: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_hw_init(ldvarg779);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_hw_init(ldvarg779);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_hw_init(ldvarg779);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 10: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_soft_reset(ldvarg778);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_soft_reset(ldvarg778);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_soft_reset(ldvarg778);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 11: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_sw_fini(ldvarg777);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_sw_fini(ldvarg777);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_sw_fini(ldvarg777);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 12: ;
  if (ldv_state_variable_97 == 2) {
    gfx_v7_0_is_idle(ldvarg776);
    ldv_state_variable_97 = 2;
  } else {
  }
  if (ldv_state_variable_97 == 1) {
    gfx_v7_0_is_idle(ldvarg776);
    ldv_state_variable_97 = 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    gfx_v7_0_is_idle(ldvarg776);
    ldv_state_variable_97 = 3;
  } else {
  }
  goto ldv_54042;
  case 13: ;
  if (ldv_state_variable_97 == 2) {
    ldv_release_97();
    ldv_state_variable_97 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_97 == 3) {
    ldv_release_97();
    ldv_state_variable_97 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_54042;
  case 14: ;
  if (ldv_state_variable_97 == 1) {
    ldv_probe_97();
    ldv_state_variable_97 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_54042;
  default:
  ldv_stop();
  }
  ldv_54042: ;
  return;
}
}
void ldv_main_exported_94(void)
{
  unsigned int ldvarg1061 ;
  enum amdgpu_interrupt_state ldvarg1062 ;
  struct amdgpu_iv_entry *ldvarg1060 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1060 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1061), 0, 4UL);
  ldv_memset((void *)(& ldvarg1062), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_94 == 1) {
    gfx_v7_0_set_eop_interrupt_state(gfx_v7_0_eop_irq_funcs_group0, gfx_v7_0_eop_irq_funcs_group1,
                                     ldvarg1061, ldvarg1062);
    ldv_state_variable_94 = 1;
  } else {
  }
  goto ldv_54065;
  case 1: ;
  if (ldv_state_variable_94 == 1) {
    gfx_v7_0_eop_irq(gfx_v7_0_eop_irq_funcs_group0, gfx_v7_0_eop_irq_funcs_group1,
                     ldvarg1060);
    ldv_state_variable_94 = 1;
  } else {
  }
  goto ldv_54065;
  default:
  ldv_stop();
  }
  ldv_54065: ;
  return;
}
}
void ldv_main_exported_96(void)
{
  u32 ldvarg437 ;
  uint64_t ldvarg442 ;
  u32 ldvarg434 ;
  struct amdgpu_semaphore *ldvarg432 ;
  void *tmp ;
  unsigned int ldvarg443 ;
  u32 ldvarg438 ;
  u32 ldvarg439 ;
  bool ldvarg431 ;
  u32 ldvarg433 ;
  unsigned int ldvarg430 ;
  uint64_t ldvarg429 ;
  u32 ldvarg436 ;
  struct amdgpu_ib *ldvarg440 ;
  void *tmp___0 ;
  u32 ldvarg435 ;
  uint64_t ldvarg441 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg432 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg440 = (struct amdgpu_ib *)tmp___0;
  ldv_memset((void *)(& ldvarg437), 0, 4UL);
  ldv_memset((void *)(& ldvarg442), 0, 8UL);
  ldv_memset((void *)(& ldvarg434), 0, 4UL);
  ldv_memset((void *)(& ldvarg443), 0, 4UL);
  ldv_memset((void *)(& ldvarg438), 0, 4UL);
  ldv_memset((void *)(& ldvarg439), 0, 4UL);
  ldv_memset((void *)(& ldvarg431), 0, 1UL);
  ldv_memset((void *)(& ldvarg433), 0, 4UL);
  ldv_memset((void *)(& ldvarg430), 0, 4UL);
  ldv_memset((void *)(& ldvarg429), 0, 8UL);
  ldv_memset((void *)(& ldvarg436), 0, 4UL);
  ldv_memset((void *)(& ldvarg435), 0, 4UL);
  ldv_memset((void *)(& ldvarg441), 0, 8UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_fence_gfx(gfx_v7_0_ring_funcs_gfx_group0, ldvarg442, ldvarg441,
                                 ldvarg443);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 1: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_get_rptr_gfx(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 2: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_test_ring(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 3: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_hdp_flush(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 4: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_set_wptr_gfx(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 5: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_get_wptr_gfx(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 6: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_ib(gfx_v7_0_ring_funcs_gfx_group0, ldvarg440);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 7: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_gds_switch(gfx_v7_0_ring_funcs_gfx_group0, ldvarg436, ldvarg434,
                                  ldvarg438, ldvarg439, ldvarg433, ldvarg435, ldvarg437);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 8: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_semaphore(gfx_v7_0_ring_funcs_gfx_group0, ldvarg432, (int )ldvarg431);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 9: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_test_ib(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 10: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_is_lockup(gfx_v7_0_ring_funcs_gfx_group0);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  case 11: ;
  if (ldv_state_variable_96 == 1) {
    gfx_v7_0_ring_emit_vm_flush(gfx_v7_0_ring_funcs_gfx_group0, ldvarg430, ldvarg429);
    ldv_state_variable_96 = 1;
  } else {
  }
  goto ldv_54087;
  default:
  ldv_stop();
  }
  ldv_54087: ;
  return;
}
}
bool ldv_queue_work_on_637(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_638(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_639(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_640(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_641(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_651(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_653(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_652(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_655(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_654(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___4(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_651(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___3(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___4(system_wq, work);
  return (tmp);
}
}
static u32 const sdma_offsets[2U] = { 0U, 512U};
static void cik_sdma_set_ring_funcs(struct amdgpu_device *adev ) ;
static void cik_sdma_set_irq_funcs(struct amdgpu_device *adev ) ;
static void cik_sdma_set_buffer_funcs(struct amdgpu_device *adev ) ;
static void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev ) ;
static int cik_sdma_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  int i ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("cik_sdma_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  chip_name = "bonaire";
  goto ldv_49539;
  case 3U:
  chip_name = "hawaii";
  goto ldv_49539;
  case 1U:
  chip_name = "kaveri";
  goto ldv_49539;
  case 2U:
  chip_name = "kabini";
  goto ldv_49539;
  case 4U:
  chip_name = "mullins";
  goto ldv_49539;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/cik_sdma.c"),
                       "i" (119), "i" (12UL));
  ldv_49545: ;
  goto ldv_49545;
  }
  ldv_49539:
  i = 0;
  goto ldv_49548;
  ldv_49547: ;
  if (i == 0) {
    snprintf((char *)(& fw_name), 30UL, "radeon/%s_sdma.bin", chip_name);
  } else {
    snprintf((char *)(& fw_name), 30UL, "radeon/%s_sdma1.bin", chip_name);
  }
  err = request_firmware(& adev->sdma[i].fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->sdma[i].fw);
  i = i + 1;
  ldv_49548: ;
  if (i <= 1) {
    goto ldv_49547;
  } else {
  }
  out: ;
  if (err != 0) {
    printk("\vcik_sdma: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    i = 0;
    goto ldv_49551;
    ldv_49550:
    release_firmware(adev->sdma[i].fw);
    adev->sdma[i].fw = (struct firmware const *)0;
    i = i + 1;
    ldv_49551: ;
    if (i <= 1) {
      goto ldv_49550;
    } else {
    }
  } else {
  }
  return (err);
}
}
static u32 cik_sdma_ring_get_rptr(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs);
  return ((rptr & 262140U) >> 2);
}
}
static u32 cik_sdma_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 me ;
  u32 tmp ;
  {
  adev = ring->adev;
  me = (unsigned long )(& adev->sdma[0].ring) != (unsigned long )ring;
  tmp = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[me] + 13444U, 0);
  return ((tmp & 262140U) >> 2);
}
}
static void cik_sdma_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 me ;
  {
  adev = ring->adev;
  me = (unsigned long )(& adev->sdma[0].ring) != (unsigned long )ring;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[me] + 13444U, (ring->wptr << 2) & 262140U,
                 0);
  return;
}
}
static void cik_sdma_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  u32 extra_bits ;
  u32 next_rptr ;
  {
  extra_bits = (unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0) ? (ib->vm)->ids[ring->idx].id & 15U : 0U;
  next_rptr = ring->wptr + 5U;
  goto ldv_49574;
  ldv_49573:
  next_rptr = next_rptr + 1U;
  ldv_49574: ;
  if ((next_rptr & 7U) != 4U) {
    goto ldv_49573;
  } else {
  }
  next_rptr = next_rptr + 4U;
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (u32 )ring->next_rptr_gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ring->next_rptr_gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, next_rptr);
  goto ldv_49577;
  ldv_49576:
  amdgpu_ring_write(ring, 0U);
  ldv_49577: ;
  if ((ring->wptr & 7U) != 4U) {
    goto ldv_49576;
  } else {
  }
  amdgpu_ring_write(ring, (extra_bits << 16) | 4U);
  amdgpu_ring_write(ring, (u32 )ib->gpu_addr & 4294967264U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, ib->length_dw);
  return;
}
}
static void cik_sdma_ring_emit_hdp_flush(struct amdgpu_ring *ring )
{
  u32 extra_bits ;
  u32 ref_and_mask ;
  {
  extra_bits = 13312U;
  if ((unsigned long )(& (ring->adev)->sdma[0].ring) == (unsigned long )ring) {
    ref_and_mask = 1024U;
  } else {
    ref_and_mask = 2048U;
  }
  amdgpu_ring_write(ring, (extra_bits << 16) | 8U);
  amdgpu_ring_write(ring, 21728U);
  amdgpu_ring_write(ring, 21724U);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static void cik_sdma_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                     unsigned int flags )
{
  bool write64bit ;
  {
  write64bit = (flags & 1U) != 0U;
  amdgpu_ring_write(ring, 5U);
  amdgpu_ring_write(ring, (unsigned int )addr);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )seq);
  if ((int )write64bit) {
    addr = addr + 4ULL;
    amdgpu_ring_write(ring, 5U);
    amdgpu_ring_write(ring, (unsigned int )addr);
    amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
    amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  } else {
  }
  amdgpu_ring_write(ring, 6U);
  return;
}
}
static bool cik_sdma_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  u64 addr ;
  u32 extra_bits ;
  {
  addr = semaphore->gpu_addr;
  extra_bits = (int )emit_wait ? 0U : 16384U;
  amdgpu_ring_write(ring, (extra_bits << 16) | 7U);
  amdgpu_ring_write(ring, (u32 )addr & 4294967288U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  return (1);
}
}
static void cik_sdma_gfx_stop(struct amdgpu_device *adev )
{
  struct amdgpu_ring *sdma0 ;
  struct amdgpu_ring *sdma1 ;
  u32 rb_cntl ;
  int i ;
  {
  sdma0 = & adev->sdma[0].ring;
  sdma1 = & adev->sdma[1].ring;
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma0 || (unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma1) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.visible_vram_size);
  } else {
  }
  i = 0;
  goto ldv_49606;
  ldv_49605:
  rb_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13440U, 0);
  rb_cntl = rb_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13440U, rb_cntl, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13450U, 0U, 0);
  i = i + 1;
  ldv_49606: ;
  if (i <= 1) {
    goto ldv_49605;
  } else {
  }
  sdma0->ready = 0;
  sdma1->ready = 0;
  return;
}
}
static void cik_sdma_rlc_stop(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void cik_sdma_enable(struct amdgpu_device *adev , bool enable )
{
  u32 me_cntl ;
  int i ;
  {
  if (! enable) {
    cik_sdma_gfx_stop(adev);
    cik_sdma_rlc_stop(adev);
  } else {
  }
  i = 0;
  goto ldv_49618;
  ldv_49617:
  me_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13330U, 0);
  if ((int )enable) {
    me_cntl = me_cntl & 4294967294U;
  } else {
    me_cntl = me_cntl | 1U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13330U, me_cntl, 0);
  i = i + 1;
  ldv_49618: ;
  if (i <= 1) {
    goto ldv_49617;
  } else {
  }
  return;
}
}
static int cik_sdma_gfx_resume(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_cntl ;
  u32 ib_cntl ;
  u32 rb_bufsz ;
  u32 wb_offset ;
  int i ;
  int j ;
  int r ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  i = 0;
  goto ldv_49635;
  ldv_49634:
  ring = & adev->sdma[i].ring;
  wb_offset = ring->rptr_offs * 4U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_49632;
  ldv_49631:
  cik_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13479U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13480U, 0U, 0);
  j = j + 1;
  ldv_49632: ;
  if (j <= 15) {
    goto ldv_49631;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13320U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13321U, 0U, 0);
  tmp = __roundup_pow_of_two((unsigned long )(ring->ring_size / 4U));
  tmp___0 = __ilog2_u64((u64 )tmp);
  rb_bufsz = (u32 )tmp___0;
  rb_cntl = rb_bufsz << 1;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13440U, rb_cntl, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13443U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13444U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13448U, (unsigned int )((adev->wb.gpu_addr + (uint64_t )wb_offset) >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13449U, ((u32 )adev->wb.gpu_addr + wb_offset) & 4294967292U,
                 0);
  rb_cntl = rb_cntl | 4096U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13441U, (u32 )(ring->gpu_addr >> 8),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13442U, (u32 )(ring->gpu_addr >> 40),
                 0);
  ring->wptr = 0U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13444U, ring->wptr << 2, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13440U, rb_cntl | 1U, 0);
  ib_cntl = 1U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13450U, ib_cntl, 0);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )ring) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.real_vram_size);
  } else {
  }
  i = i + 1;
  ldv_49635: ;
  if (i <= 1) {
    goto ldv_49634;
  } else {
  }
  return (0);
}
}
static int cik_sdma_rlc_resume(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static int cik_sdma_load_microcode(struct amdgpu_device *adev )
{
  struct sdma_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  u32 fw_size ;
  int i ;
  int j ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  {
  if ((unsigned long )adev->sdma[0].fw == (unsigned long )((struct firmware const *)0) || (unsigned long )adev->sdma[1].fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  cik_sdma_enable(adev, 0);
  i = 0;
  goto ldv_49652;
  ldv_49651:
  hdr = (struct sdma_firmware_header_v1_0 const *)(adev->sdma[i].fw)->data;
  amdgpu_ucode_print_sdma_hdr(& hdr->header);
  fw_size = (unsigned int )hdr->header.ucode_size_bytes / 4U;
  adev->sdma[i].fw_version = hdr->header.ucode_version;
  fw_data = (__le32 const *)(adev->sdma[i].fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13312U, 0U, 0);
  j = 0;
  goto ldv_49649;
  ldv_49648:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13313U, tmp___0, 0);
  j = j + 1;
  ldv_49649: ;
  if ((u32 )j < fw_size) {
    goto ldv_49648;
  } else {
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets[i] + 13312U, adev->sdma[i].fw_version,
                 0);
  i = i + 1;
  ldv_49652: ;
  if (i <= 1) {
    goto ldv_49651;
  } else {
  }
  return (0);
}
}
static int cik_sdma_start(struct amdgpu_device *adev )
{
  int r ;
  {
  r = cik_sdma_load_microcode(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  cik_sdma_enable(adev, 1);
  r = cik_sdma_gfx_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = cik_sdma_rlc_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int cik_sdma_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ring_lock(ring, 5U);
  if (r != 0) {
    drm_err("amdgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);
    amdgpu_wb_free(adev, index);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )gpu_addr);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_49669;
  ldv_49668:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_49667;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49669: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49668;
  } else {
  }
  ldv_49667: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static int cik_sdma_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ib ib ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 256U, & ib);
  if (r != 0) {
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    return (r);
  } else {
  }
  *(ib.ptr) = 2U;
  *(ib.ptr + 1UL) = (unsigned int )gpu_addr;
  *(ib.ptr + 2UL) = (unsigned int )(gpu_addr >> 32ULL);
  *(ib.ptr + 3UL) = 1U;
  *(ib.ptr + 4UL) = 3735928559U;
  ib.length_dw = 5U;
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_fence_wait(ib.fence, 0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_49682;
  ldv_49681:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_49680;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49682: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49681;
  } else {
  }
  ldv_49680: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ib test on ring %d succeeded in %u usecs\n", ((ib.fence)->ring)->idx,
           i);
  } else {
    drm_err("amdgpu: ib test failed (0x%08X)\n", tmp);
    r = -22;
  }
  amdgpu_ib_free(adev, & ib);
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static void cik_sdma_vm_copy_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t src ,
                                 unsigned int count )
{
  unsigned int bytes ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  {
  goto ldv_49691;
  ldv_49690:
  bytes = count * 8U;
  if (bytes > 2097144U) {
    bytes = 2097144U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 1U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = bytes;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = 0U;
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = (unsigned int )src;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (unsigned int )(src >> 32ULL);
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )pe;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(pe >> 32ULL);
  pe = (uint64_t )bytes + pe;
  src = (uint64_t )bytes + src;
  count = count - bytes / 8U;
  ldv_49691: ;
  if (count != 0U) {
    goto ldv_49690;
  } else {
  }
  return;
}
}
static void cik_sdma_vm_write_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                  unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  goto ldv_49707;
  ldv_49706:
  ndw = count * 2U;
  if (ndw > 1048574U) {
    ndw = 1048574U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 2U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = ndw;
  goto ldv_49704;
  ldv_49703: ;
  if ((flags & 2U) != 0U) {
    value = amdgpu_vm_map_gart((ib->ring)->adev, addr);
    value = value & 0xfffffffffffff000ULL;
  } else
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  addr = (uint64_t )incr + addr;
  value = (uint64_t )flags | value;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (u32 )value;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )(value >> 32ULL);
  ndw = ndw - 2U;
  count = count - 1U;
  pe = pe + 8ULL;
  ldv_49704: ;
  if (ndw != 0U) {
    goto ldv_49703;
  } else {
  }
  ldv_49707: ;
  if (count != 0U) {
    goto ldv_49706;
  } else {
  }
  return;
}
}
static void cik_sdma_vm_set_pte_pde(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                    unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  {
  goto ldv_49720;
  ldv_49719:
  ndw = count;
  if (ndw > 524287U) {
    ndw = 524287U;
  } else {
  }
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 12U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = flags;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = 0U;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (u32 )value;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(value >> 32ULL);
  tmp___6 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___6) = incr;
  tmp___7 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___7) = 0U;
  tmp___8 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___8) = ndw;
  pe = (uint64_t )(ndw * 8U) + pe;
  addr = (uint64_t )(ndw * incr) + addr;
  count = count - ndw;
  ldv_49720: ;
  if (count != 0U) {
    goto ldv_49719;
  } else {
  }
  return;
}
}
static void cik_sdma_vm_pad_ib(struct amdgpu_ib *ib )
{
  u32 tmp ;
  {
  goto ldv_49726;
  ldv_49725:
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 0U;
  ldv_49726: ;
  if ((ib->length_dw & 7U) != 0U) {
    goto ldv_49725;
  } else {
  }
  return;
}
}
static void cik_sdma_ring_emit_vm_flush(struct amdgpu_ring *ring , unsigned int vm_id ,
                                        uint64_t pd_addr )
{
  u32 extra_bits ;
  {
  extra_bits = 0U;
  amdgpu_ring_write(ring, 4026531854U);
  if (vm_id <= 7U) {
    amdgpu_ring_write(ring, vm_id + 1359U);
  } else {
    amdgpu_ring_write(ring, vm_id + 1286U);
  }
  amdgpu_ring_write(ring, (u32 )(pd_addr >> 12));
  amdgpu_ring_write(ring, 4026531854U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, (u32 )(1 << (int )vm_id));
  amdgpu_ring_write(ring, (extra_bits << 16) | 8U);
  amdgpu_ring_write(ring, 5240U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static void cik_enable_sdma_mgcg(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  if ((int )enable && (adev->cg_flags & 2048U) != 0U) {
    amdgpu_mm_wreg(adev, 13315U, 256U, 0);
    amdgpu_mm_wreg(adev, 13827U, 256U, 0);
  } else {
    data = amdgpu_mm_rreg(adev, 13315U, 0);
    orig = data;
    data = data | 4278190080U;
    if (data != orig) {
      amdgpu_mm_wreg(adev, 13315U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 13827U, 0);
    orig = data;
    data = data | 4278190080U;
    if (data != orig) {
      amdgpu_mm_wreg(adev, 13827U, data, 0);
    } else {
    }
  }
  return;
}
}
static void cik_enable_sdma_mgls(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  if ((int )enable && (adev->cg_flags & 1024U) != 0U) {
    data = amdgpu_mm_rreg(adev, 13314U, 0);
    orig = data;
    data = data | 256U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 13314U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 13826U, 0);
    orig = data;
    data = data | 256U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 13826U, data, 0);
    } else {
    }
  } else {
    data = amdgpu_mm_rreg(adev, 13314U, 0);
    orig = data;
    data = data & 4294967039U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 13314U, data, 0);
    } else {
    }
    data = amdgpu_mm_rreg(adev, 13826U, 0);
    orig = data;
    data = data & 4294967039U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 13826U, data, 0);
    } else {
    }
  }
  return;
}
}
static int cik_sdma_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cik_sdma_set_ring_funcs(adev);
  cik_sdma_set_irq_funcs(adev);
  cik_sdma_set_buffer_funcs(adev);
  cik_sdma_set_vm_pte_funcs(adev);
  return (0);
}
}
static int cik_sdma_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  r = cik_sdma_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load sdma firmware!\n");
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 224U, & adev->sdma_trap_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 241U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 247U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->sdma[0].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring = & adev->sdma[1].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring = & adev->sdma[0].ring;
  sprintf((char *)(& ring->name), "sdma0");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 0U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->sdma[1].ring;
  sprintf((char *)(& ring->name), "sdma1");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 1U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int cik_sdma_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_ring_fini(& adev->sdma[0].ring);
  amdgpu_ring_fini(& adev->sdma[1].ring);
  return (0);
}
}
static int cik_sdma_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = cik_sdma_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int cik_sdma_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cik_sdma_enable(adev, 0);
  return (0);
}
}
static int cik_sdma_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_sdma_hw_fini((void *)adev);
  return (tmp);
}
}
static int cik_sdma_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cik_sdma_hw_init((void *)adev);
  return (tmp);
}
}
static bool cik_sdma_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 96U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int cik_sdma_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_49789;
  ldv_49788:
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0 & 96U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49789: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49788;
  } else {
  }
  return (-110);
}
}
static void cik_sdma_print_status(void *handle )
{
  int i ;
  int j ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "CIK SDMA registers\n");
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp);
  i = 0;
  goto ldv_49801;
  ldv_49800:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13325U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_STATUS_REG=0x%08X\n", i,
            tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13330U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_ME_CNTL=0x%08X\n", i, tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13316U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_CNTL=0x%08X\n", i, tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13320U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_SEM_INCOMPLETE_TIMER_CNTL=0x%08X\n",
            i, tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13321U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_SEM_WAIT_FAIL_TIMER_CNTL=0x%08X\n",
            i, tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13450U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_IB_CNTL=0x%08X\n", i,
            tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13440U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_CNTL=0x%08X\n", i,
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13443U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR=0x%08X\n", i,
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13444U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_WPTR=0x%08X\n", i,
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13448U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_HI=0x%08X\n",
            i, tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13449U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_LO=0x%08X\n",
            i, tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13441U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE=0x%08X\n", i,
            tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13442U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE_HI=0x%08X\n",
            i, tmp___12);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_49798;
  ldv_49797:
  cik_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  _dev_info((struct device const *)adev->dev, "  VM %d:\n", j);
  tmp___13 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13479U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_GFX_VIRTUAL_ADDR=0x%08X\n",
            tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets[i] + 13480U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_GFX_APE1_CNTL=0x%08X\n",
            tmp___14);
  j = j + 1;
  ldv_49798: ;
  if (j <= 15) {
    goto ldv_49797;
  } else {
  }
  cik_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  i = i + 1;
  ldv_49801: ;
  if (i <= 1) {
    goto ldv_49800;
  } else {
  }
  return;
}
}
static int cik_sdma_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 32U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13330U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13330U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 1048576U;
  } else {
  }
  if ((tmp & 64U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13842U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13842U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 64U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    cik_sdma_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    cik_sdma_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int cik_sdma_set_trap_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                       unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 sdma_cntl ;
  {
  switch (type) {
  case 0U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_49818;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_49818;
  default: ;
  goto ldv_49818;
  }
  ldv_49818: ;
  goto ldv_49821;
  case 1U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_49824;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_49824;
  default: ;
  goto ldv_49824;
  }
  ldv_49824: ;
  goto ldv_49821;
  default: ;
  goto ldv_49821;
  }
  ldv_49821: ;
  return (0);
}
}
static int cik_sdma_process_trap_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                     struct amdgpu_iv_entry *entry )
{
  u8 instance_id ;
  u8 queue_id ;
  long tmp ;
  {
  instance_id = (unsigned int )((u8 )entry->ring_id) & 3U;
  queue_id = (u8 )((entry->ring_id & 12U) >> 2);
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("cik_sdma_process_trap_irq", "IH: SDMA trap\n");
  } else {
  }
  switch ((int )instance_id) {
  case 0: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[0].ring);
  goto ldv_49838;
  case 1: ;
  goto ldv_49838;
  case 2: ;
  goto ldv_49838;
  }
  ldv_49838: ;
  goto ldv_49841;
  case 1: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[1].ring);
  goto ldv_49844;
  case 1: ;
  goto ldv_49844;
  case 2: ;
  goto ldv_49844;
  }
  ldv_49844: ;
  goto ldv_49841;
  }
  ldv_49841: ;
  return (0);
}
}
static int cik_sdma_process_illegal_inst_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                             struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal instruction in SDMA command stream\n");
  schedule_work___3(& adev->reset_work);
  return (0);
}
}
static int cik_sdma_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  cik_enable_sdma_mgcg(adev, (int )gate);
  cik_enable_sdma_mgls(adev, (int )gate);
  return (0);
}
}
static int cik_sdma_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const cik_sdma_ip_funcs =
     {& cik_sdma_early_init, (int (*)(void * ))0, & cik_sdma_sw_init, & cik_sdma_sw_fini,
    & cik_sdma_hw_init, & cik_sdma_hw_fini, & cik_sdma_suspend, & cik_sdma_resume,
    & cik_sdma_is_idle, & cik_sdma_wait_for_idle, & cik_sdma_soft_reset, & cik_sdma_print_status,
    & cik_sdma_set_clockgating_state, & cik_sdma_set_powergating_state};
static bool cik_sdma_ring_is_lockup(struct amdgpu_ring *ring )
{
  bool tmp ;
  bool tmp___0 ;
  {
  tmp = cik_sdma_is_idle((void *)ring->adev);
  if ((int )tmp) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___0 = amdgpu_ring_test_lockup(ring);
  return (tmp___0);
}
}
static struct amdgpu_ring_funcs const cik_sdma_ring_funcs =
     {& cik_sdma_ring_get_rptr, & cik_sdma_ring_get_wptr, & cik_sdma_ring_set_wptr,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & cik_sdma_ring_emit_ib, & cik_sdma_ring_emit_fence,
    & cik_sdma_ring_emit_semaphore, & cik_sdma_ring_emit_vm_flush, & cik_sdma_ring_emit_hdp_flush,
    0, & cik_sdma_ring_test_ring, & cik_sdma_ring_test_ib, & cik_sdma_ring_is_lockup};
static void cik_sdma_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma[0].ring.funcs = & cik_sdma_ring_funcs;
  adev->sdma[1].ring.funcs = & cik_sdma_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const cik_sdma_trap_irq_funcs = {& cik_sdma_set_trap_irq_state, & cik_sdma_process_trap_irq};
static struct amdgpu_irq_src_funcs const cik_sdma_illegal_inst_irq_funcs = {0, & cik_sdma_process_illegal_inst_irq};
static void cik_sdma_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma_trap_irq.num_types = 2U;
  adev->sdma_trap_irq.funcs = & cik_sdma_trap_irq_funcs;
  adev->sdma_illegal_inst_irq.funcs = & cik_sdma_illegal_inst_irq_funcs;
  return;
}
}
static void cik_sdma_emit_copy_buffer(struct amdgpu_ring *ring , uint64_t src_offset ,
                                      uint64_t dst_offset , u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, byte_count);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (unsigned int )src_offset);
  amdgpu_ring_write(ring, (unsigned int )(src_offset >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  return;
}
}
static void cik_sdma_emit_fill_buffer(struct amdgpu_ring *ring , u32 src_data , uint64_t dst_offset ,
                                      u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 11U);
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  amdgpu_ring_write(ring, src_data);
  amdgpu_ring_write(ring, byte_count);
  return;
}
}
static struct amdgpu_buffer_funcs const cik_sdma_buffer_funcs = {2097151U, 7U, & cik_sdma_emit_copy_buffer, 2097151U, 5U, & cik_sdma_emit_fill_buffer};
static void cik_sdma_set_buffer_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mman.buffer_funcs == (unsigned long )((struct amdgpu_buffer_funcs const *)0)) {
    adev->mman.buffer_funcs = & cik_sdma_buffer_funcs;
    adev->mman.buffer_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
static struct amdgpu_vm_pte_funcs const cik_sdma_vm_pte_funcs = {& cik_sdma_vm_copy_pte, & cik_sdma_vm_write_pte, & cik_sdma_vm_set_pte_pde, & cik_sdma_vm_pad_ib};
static void cik_sdma_set_vm_pte_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->vm_manager.vm_pte_funcs == (unsigned long )((struct amdgpu_vm_pte_funcs const *)0)) {
    adev->vm_manager.vm_pte_funcs = & cik_sdma_vm_pte_funcs;
    adev->vm_manager.vm_pte_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
int ldv_retval_54 ;
extern int ldv_probe_86(void) ;
extern int ldv_probe_91(void) ;
extern int ldv_release_86(void) ;
extern int ldv_release_91(void) ;
int ldv_retval_55 ;
void ldv_initialize_amdgpu_irq_src_funcs_89(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  cik_sdma_trap_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  cik_sdma_trap_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_buffer_funcs_87(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  cik_sdma_buffer_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_vm_pte_funcs_86(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(272UL);
  cik_sdma_vm_pte_funcs_group0 = (struct amdgpu_ib *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_90(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  cik_sdma_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_main_exported_87(void)
{
  u32 ldvarg704 ;
  u32 ldvarg703 ;
  uint64_t ldvarg700 ;
  uint64_t ldvarg702 ;
  uint64_t ldvarg699 ;
  u32 ldvarg701 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg704), 0, 4UL);
  ldv_memset((void *)(& ldvarg703), 0, 4UL);
  ldv_memset((void *)(& ldvarg700), 0, 8UL);
  ldv_memset((void *)(& ldvarg702), 0, 8UL);
  ldv_memset((void *)(& ldvarg699), 0, 8UL);
  ldv_memset((void *)(& ldvarg701), 0, 4UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_87 == 1) {
    cik_sdma_emit_fill_buffer(cik_sdma_buffer_funcs_group0, ldvarg703, ldvarg702,
                              ldvarg704);
    ldv_state_variable_87 = 1;
  } else {
  }
  goto ldv_49927;
  case 1: ;
  if (ldv_state_variable_87 == 1) {
    cik_sdma_emit_copy_buffer(cik_sdma_buffer_funcs_group0, ldvarg700, ldvarg699,
                              ldvarg701);
    ldv_state_variable_87 = 1;
  } else {
  }
  goto ldv_49927;
  default:
  ldv_stop();
  }
  ldv_49927: ;
  return;
}
}
void ldv_main_exported_90(void)
{
  uint64_t ldvarg10 ;
  struct amdgpu_semaphore *ldvarg13 ;
  void *tmp ;
  uint64_t ldvarg17 ;
  struct amdgpu_ib *ldvarg14 ;
  void *tmp___0 ;
  unsigned int ldvarg15 ;
  uint64_t ldvarg16 ;
  bool ldvarg12 ;
  unsigned int ldvarg11 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg13 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg14 = (struct amdgpu_ib *)tmp___0;
  ldv_memset((void *)(& ldvarg10), 0, 8UL);
  ldv_memset((void *)(& ldvarg17), 0, 8UL);
  ldv_memset((void *)(& ldvarg15), 0, 4UL);
  ldv_memset((void *)(& ldvarg16), 0, 8UL);
  ldv_memset((void *)(& ldvarg12), 0, 1UL);
  ldv_memset((void *)(& ldvarg11), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_emit_fence(cik_sdma_ring_funcs_group0, ldvarg17, ldvarg16, ldvarg15);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 1: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_get_rptr(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 2: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_test_ring(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 3: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_emit_hdp_flush(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 4: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_set_wptr(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 5: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_get_wptr(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 6: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_emit_ib(cik_sdma_ring_funcs_group0, ldvarg14);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 7: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_emit_semaphore(cik_sdma_ring_funcs_group0, ldvarg13, (int )ldvarg12);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 8: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_test_ib(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 9: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_is_lockup(cik_sdma_ring_funcs_group0);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  case 10: ;
  if (ldv_state_variable_90 == 1) {
    cik_sdma_ring_emit_vm_flush(cik_sdma_ring_funcs_group0, ldvarg11, ldvarg10);
    ldv_state_variable_90 = 1;
  } else {
  }
  goto ldv_49942;
  default:
  ldv_stop();
  }
  ldv_49942: ;
  return;
}
}
void ldv_main_exported_88(void)
{
  struct amdgpu_irq_src *ldvarg868 ;
  void *tmp ;
  struct amdgpu_device *ldvarg869 ;
  void *tmp___0 ;
  struct amdgpu_iv_entry *ldvarg867 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg868 = (struct amdgpu_irq_src *)tmp;
  tmp___0 = ldv_init_zalloc(23352UL);
  ldvarg869 = (struct amdgpu_device *)tmp___0;
  tmp___1 = ldv_init_zalloc(20UL);
  ldvarg867 = (struct amdgpu_iv_entry *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_88 == 1) {
    cik_sdma_process_illegal_inst_irq(ldvarg869, ldvarg868, ldvarg867);
    ldv_state_variable_88 = 1;
  } else {
  }
  goto ldv_49961;
  default:
  ldv_stop();
  }
  ldv_49961: ;
  return;
}
}
void ldv_main_exported_89(void)
{
  struct amdgpu_iv_entry *ldvarg109 ;
  void *tmp ;
  unsigned int ldvarg110 ;
  enum amdgpu_interrupt_state ldvarg111 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg109 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg110), 0, 4UL);
  ldv_memset((void *)(& ldvarg111), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_89 == 1) {
    cik_sdma_set_trap_irq_state(cik_sdma_trap_irq_funcs_group0, cik_sdma_trap_irq_funcs_group1,
                                ldvarg110, ldvarg111);
    ldv_state_variable_89 = 1;
  } else {
  }
  goto ldv_49970;
  case 1: ;
  if (ldv_state_variable_89 == 1) {
    cik_sdma_process_trap_irq(cik_sdma_trap_irq_funcs_group0, cik_sdma_trap_irq_funcs_group1,
                              ldvarg109);
    ldv_state_variable_89 = 1;
  } else {
  }
  goto ldv_49970;
  default:
  ldv_stop();
  }
  ldv_49970: ;
  return;
}
}
void ldv_main_exported_91(void)
{
  void *ldvarg669 ;
  void *tmp ;
  void *ldvarg664 ;
  void *tmp___0 ;
  void *ldvarg659 ;
  void *tmp___1 ;
  void *ldvarg661 ;
  void *tmp___2 ;
  void *ldvarg660 ;
  void *tmp___3 ;
  void *ldvarg663 ;
  void *tmp___4 ;
  void *ldvarg671 ;
  void *tmp___5 ;
  void *ldvarg658 ;
  void *tmp___6 ;
  void *ldvarg665 ;
  void *tmp___7 ;
  void *ldvarg668 ;
  void *tmp___8 ;
  enum amd_powergating_state ldvarg666 ;
  void *ldvarg667 ;
  void *tmp___9 ;
  void *ldvarg670 ;
  void *tmp___10 ;
  void *ldvarg672 ;
  void *tmp___11 ;
  enum amd_clockgating_state ldvarg662 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg669 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg664 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg659 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg661 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg660 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg663 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg671 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg658 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg665 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg668 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg667 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg670 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg672 = tmp___11;
  ldv_memset((void *)(& ldvarg666), 0, 4UL);
  ldv_memset((void *)(& ldvarg662), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_hw_fini(ldvarg672);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_hw_fini(ldvarg672);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_hw_fini(ldvarg672);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 1: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_print_status(ldvarg671);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_print_status(ldvarg671);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_print_status(ldvarg671);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 2: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_early_init(ldvarg670);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_early_init(ldvarg670);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_early_init(ldvarg670);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 3: ;
  if (ldv_state_variable_91 == 2) {
    ldv_retval_55 = cik_sdma_suspend(ldvarg669);
    if (ldv_retval_55 == 0) {
      ldv_state_variable_91 = 3;
    } else {
    }
  } else {
  }
  goto ldv_49992;
  case 4: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_sw_init(ldvarg668);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_sw_init(ldvarg668);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_sw_init(ldvarg668);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 5: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_set_powergating_state(ldvarg667, ldvarg666);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_set_powergating_state(ldvarg667, ldvarg666);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_set_powergating_state(ldvarg667, ldvarg666);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 6: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_wait_for_idle(ldvarg665);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_wait_for_idle(ldvarg665);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_wait_for_idle(ldvarg665);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 7: ;
  if (ldv_state_variable_91 == 3) {
    ldv_retval_54 = cik_sdma_resume(ldvarg664);
    if (ldv_retval_54 == 0) {
      ldv_state_variable_91 = 2;
    } else {
    }
  } else {
  }
  goto ldv_49992;
  case 8: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_set_clockgating_state(ldvarg663, ldvarg662);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_set_clockgating_state(ldvarg663, ldvarg662);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_set_clockgating_state(ldvarg663, ldvarg662);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 9: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_hw_init(ldvarg661);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_hw_init(ldvarg661);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_hw_init(ldvarg661);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 10: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_soft_reset(ldvarg660);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_soft_reset(ldvarg660);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_soft_reset(ldvarg660);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 11: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_sw_fini(ldvarg659);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_sw_fini(ldvarg659);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_sw_fini(ldvarg659);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 12: ;
  if (ldv_state_variable_91 == 2) {
    cik_sdma_is_idle(ldvarg658);
    ldv_state_variable_91 = 2;
  } else {
  }
  if (ldv_state_variable_91 == 1) {
    cik_sdma_is_idle(ldvarg658);
    ldv_state_variable_91 = 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    cik_sdma_is_idle(ldvarg658);
    ldv_state_variable_91 = 3;
  } else {
  }
  goto ldv_49992;
  case 13: ;
  if (ldv_state_variable_91 == 2) {
    ldv_release_91();
    ldv_state_variable_91 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_91 == 3) {
    ldv_release_91();
    ldv_state_variable_91 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_49992;
  case 14: ;
  if (ldv_state_variable_91 == 1) {
    ldv_probe_91();
    ldv_state_variable_91 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_49992;
  default:
  ldv_stop();
  }
  ldv_49992: ;
  return;
}
}
void ldv_main_exported_86(void)
{
  uint64_t ldvarg255 ;
  uint64_t ldvarg260 ;
  unsigned int ldvarg261 ;
  unsigned int ldvarg253 ;
  u32 ldvarg258 ;
  u32 ldvarg254 ;
  u32 ldvarg262 ;
  u32 ldvarg250 ;
  uint64_t ldvarg259 ;
  uint64_t ldvarg251 ;
  unsigned int ldvarg257 ;
  uint64_t ldvarg252 ;
  uint64_t ldvarg256 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg255), 0, 8UL);
  ldv_memset((void *)(& ldvarg260), 0, 8UL);
  ldv_memset((void *)(& ldvarg261), 0, 4UL);
  ldv_memset((void *)(& ldvarg253), 0, 4UL);
  ldv_memset((void *)(& ldvarg258), 0, 4UL);
  ldv_memset((void *)(& ldvarg254), 0, 4UL);
  ldv_memset((void *)(& ldvarg262), 0, 4UL);
  ldv_memset((void *)(& ldvarg250), 0, 4UL);
  ldv_memset((void *)(& ldvarg259), 0, 8UL);
  ldv_memset((void *)(& ldvarg251), 0, 8UL);
  ldv_memset((void *)(& ldvarg257), 0, 4UL);
  ldv_memset((void *)(& ldvarg252), 0, 8UL);
  ldv_memset((void *)(& ldvarg256), 0, 8UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_86 == 2) {
    cik_sdma_vm_write_pte(cik_sdma_vm_pte_funcs_group0, ldvarg260, ldvarg259, ldvarg261,
                          ldvarg262, ldvarg258);
    ldv_state_variable_86 = 2;
  } else {
  }
  goto ldv_50025;
  case 1: ;
  if (ldv_state_variable_86 == 1) {
    cik_sdma_vm_copy_pte(cik_sdma_vm_pte_funcs_group0, ldvarg256, ldvarg255, ldvarg257);
    ldv_state_variable_86 = 1;
  } else {
  }
  if (ldv_state_variable_86 == 2) {
    cik_sdma_vm_copy_pte(cik_sdma_vm_pte_funcs_group0, ldvarg256, ldvarg255, ldvarg257);
    ldv_state_variable_86 = 2;
  } else {
  }
  goto ldv_50025;
  case 2: ;
  if (ldv_state_variable_86 == 1) {
    cik_sdma_vm_pad_ib(cik_sdma_vm_pte_funcs_group0);
    ldv_state_variable_86 = 1;
  } else {
  }
  if (ldv_state_variable_86 == 2) {
    cik_sdma_vm_pad_ib(cik_sdma_vm_pte_funcs_group0);
    ldv_state_variable_86 = 2;
  } else {
  }
  goto ldv_50025;
  case 3: ;
  if (ldv_state_variable_86 == 1) {
    cik_sdma_vm_set_pte_pde(cik_sdma_vm_pte_funcs_group0, ldvarg252, ldvarg251, ldvarg253,
                            ldvarg254, ldvarg250);
    ldv_state_variable_86 = 1;
  } else {
  }
  if (ldv_state_variable_86 == 2) {
    cik_sdma_vm_set_pte_pde(cik_sdma_vm_pte_funcs_group0, ldvarg252, ldvarg251, ldvarg253,
                            ldvarg254, ldvarg250);
    ldv_state_variable_86 = 2;
  } else {
  }
  goto ldv_50025;
  case 4: ;
  if (ldv_state_variable_86 == 2) {
    ldv_release_86();
    ldv_state_variable_86 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50025;
  case 5: ;
  if (ldv_state_variable_86 == 1) {
    ldv_probe_86();
    ldv_state_variable_86 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50025;
  default:
  ldv_stop();
  }
  ldv_50025: ;
  return;
}
}
bool ldv_queue_work_on_651(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_652(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_653(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_654(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_655(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_665(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_667(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_666(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_669(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_668(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_uvd_sw_init(struct amdgpu_device *adev ) ;
int amdgpu_uvd_sw_fini(struct amdgpu_device *adev ) ;
int amdgpu_uvd_suspend(struct amdgpu_device *adev ) ;
int amdgpu_uvd_resume(struct amdgpu_device *adev ) ;
int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser , u32 ib_idx ) ;
static void uvd_v4_2_mc_resume(struct amdgpu_device *adev ) ;
static void uvd_v4_2_init_cg(struct amdgpu_device *adev ) ;
static void uvd_v4_2_set_ring_funcs(struct amdgpu_device *adev ) ;
static void uvd_v4_2_set_irq_funcs(struct amdgpu_device *adev ) ;
static int uvd_v4_2_start(struct amdgpu_device *adev ) ;
static void uvd_v4_2_stop(struct amdgpu_device *adev ) ;
static u32 uvd_v4_2_ring_get_rptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15780U, 0);
  return (tmp);
}
}
static u32 uvd_v4_2_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15781U, 0);
  return (tmp);
}
}
static void uvd_v4_2_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  return;
}
}
static int uvd_v4_2_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v4_2_set_ring_funcs(adev);
  uvd_v4_2_set_irq_funcs(adev);
  return (0);
}
}
static int uvd_v4_2_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 124U, & adev->uvd.irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->uvd.ring;
  sprintf((char *)(& ring->name), "uvd");
  r = amdgpu_ring_init(adev, ring, 4096U, 2147483648U, 15U, & adev->uvd.irq, 0U, 3);
  return (r);
}
}
static int uvd_v4_2_sw_fini(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_fini(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v4_2_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  u32 tmp ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 53300U, 40000U);
  r = uvd_v4_2_start(adev);
  if (r != 0) {
    goto done;
  } else {
  }
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    goto done;
  } else {
  }
  r = amdgpu_ring_lock(ring, 10U);
  if (r != 0) {
    drm_err("amdgpu: ring failed to lock UVD ring (%d).\n", r);
    goto done;
  } else {
  }
  tmp = 15794U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15793U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15795U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  amdgpu_ring_write(ring, 15792U);
  amdgpu_ring_write(ring, 8U);
  amdgpu_ring_write(ring, 15616U);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_unlock_commit(ring);
  done:
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 0U, 0U);
  if (r == 0) {
    printk("\016[drm] UVD initialized successfully.\n");
  } else {
  }
  return (r);
}
}
static int uvd_v4_2_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  uvd_v4_2_stop(adev);
  ring->ready = 0;
  return (0);
}
}
static int uvd_v4_2_suspend(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = uvd_v4_2_hw_fini((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v4_2_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = uvd_v4_2_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v4_2_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_bufsz ;
  int i ;
  int j ;
  int r ;
  u32 lmi_swap_cntl ;
  u32 mp_swap_cntl ;
  u32 tmp_ ;
  u32 tmp ;
  u32 tmp____0 ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  unsigned long __ms___0 ;
  unsigned long tmp___2 ;
  u32 tmp____1 ;
  u32 tmp___3 ;
  unsigned long __ms___1 ;
  unsigned long tmp___4 ;
  unsigned long __ms___2 ;
  unsigned long tmp___5 ;
  u32 tmp____2 ;
  u32 tmp___6 ;
  unsigned long __ms___3 ;
  unsigned long tmp___7 ;
  u32 status ;
  unsigned long __ms___4 ;
  unsigned long tmp___8 ;
  u32 tmp____3 ;
  u32 tmp___9 ;
  unsigned long __ms___5 ;
  unsigned long tmp___10 ;
  u32 tmp____4 ;
  u32 tmp___11 ;
  unsigned long __ms___6 ;
  unsigned long tmp___12 ;
  u32 tmp____5 ;
  u32 tmp___13 ;
  unsigned long tmp___14 ;
  int tmp___15 ;
  u32 tmp____6 ;
  u32 tmp___16 ;
  {
  ring = & adev->uvd.ring;
  lmi_swap_cntl = 0U;
  mp_swap_cntl = 0U;
  uvd_v4_2_mc_resume(adev);
  amdgpu_mm_wreg(adev, 15658U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967293U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 15680U, tmp_, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____0 = tmp___0;
  tmp____0 = tmp____0 & 4294967039U;
  tmp____0 = tmp____0 | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp____0, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43769;
    ldv_43768:
    __const_udelay(4295000UL);
    ldv_43769:
    tmp___1 = __ms;
    __ms = __ms - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_43768;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8431U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43773;
    ldv_43772:
    __const_udelay(4295000UL);
    ldv_43773:
    tmp___2 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___2 != 0UL) {
      goto ldv_43772;
    } else {
    }
  }
  tmp___3 = amdgpu_mm_rreg(adev, 920U, 0);
  tmp____1 = tmp___3;
  tmp____1 = tmp____1 & 4294705151U;
  tmp____1 = tmp____1;
  amdgpu_mm_wreg(adev, 920U, tmp____1, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___1 = 5UL;
    goto ldv_43778;
    ldv_43777:
    __const_udelay(4295000UL);
    ldv_43778:
    tmp___4 = __ms___1;
    __ms___1 = __ms___1 - 1UL;
    if (tmp___4 != 0UL) {
      goto ldv_43777;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15718U, 3154752U, 0);
  amdgpu_mm_wreg(adev, 15725U, lmi_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15727U, mp_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15737U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15738U, 0U, 0);
  amdgpu_mm_wreg(adev, 15739U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15740U, 0U, 0);
  amdgpu_mm_wreg(adev, 15742U, 0U, 0);
  amdgpu_mm_wreg(adev, 15741U, 136U, 0);
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___2 = 5UL;
    goto ldv_43782;
    ldv_43781:
    __const_udelay(4295000UL);
    ldv_43782:
    tmp___5 = __ms___2;
    __ms___2 = __ms___2 - 1UL;
    if (tmp___5 != 0UL) {
      goto ldv_43781;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 512U, 0);
  tmp___6 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____2 = tmp___6;
  tmp____2 = tmp____2 & 4294967039U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 15677U, tmp____2, 0);
  amdgpu_mm_wreg(adev, 15776U, 0U, 0);
  __ms___3 = 10UL;
  goto ldv_43787;
  ldv_43786:
  __const_udelay(4295000UL);
  ldv_43787:
  tmp___7 = __ms___3;
  __ms___3 = __ms___3 - 1UL;
  if (tmp___7 != 0UL) {
    goto ldv_43786;
  } else {
  }
  i = 0;
  goto ldv_43809;
  ldv_43808:
  j = 0;
  goto ldv_43796;
  ldv_43795:
  status = amdgpu_mm_rreg(adev, 15791U, 0);
  if ((status & 2U) != 0U) {
    goto ldv_43790;
  } else {
  }
  __ms___4 = 10UL;
  goto ldv_43793;
  ldv_43792:
  __const_udelay(4295000UL);
  ldv_43793:
  tmp___8 = __ms___4;
  __ms___4 = __ms___4 - 1UL;
  if (tmp___8 != 0UL) {
    goto ldv_43792;
  } else {
  }
  j = j + 1;
  ldv_43796: ;
  if (j <= 99) {
    goto ldv_43795;
  } else {
  }
  ldv_43790:
  r = 0;
  if ((status & 2U) != 0U) {
    goto ldv_43797;
  } else {
  }
  drm_err("UVD not responding, trying to reset the VCPU!!!\n");
  tmp___9 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____3 = tmp___9;
  tmp____3 = tmp____3 & 4294967287U;
  tmp____3 = tmp____3 | 8U;
  amdgpu_mm_wreg(adev, 15776U, tmp____3, 0);
  __ms___5 = 10UL;
  goto ldv_43801;
  ldv_43800:
  __const_udelay(4295000UL);
  ldv_43801:
  tmp___10 = __ms___5;
  __ms___5 = __ms___5 - 1UL;
  if (tmp___10 != 0UL) {
    goto ldv_43800;
  } else {
  }
  tmp___11 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____4 = tmp___11;
  tmp____4 = tmp____4 & 4294967287U;
  tmp____4 = tmp____4;
  amdgpu_mm_wreg(adev, 15776U, tmp____4, 0);
  __ms___6 = 10UL;
  goto ldv_43806;
  ldv_43805:
  __const_udelay(4295000UL);
  ldv_43806:
  tmp___12 = __ms___6;
  __ms___6 = __ms___6 - 1UL;
  if (tmp___12 != 0UL) {
    goto ldv_43805;
  } else {
  }
  r = -1;
  i = i + 1;
  ldv_43809: ;
  if (i <= 9) {
    goto ldv_43808;
  } else {
  }
  ldv_43797: ;
  if (r != 0) {
    drm_err("UVD not responding, giving up!!!\n");
    return (r);
  } else {
  }
  tmp___13 = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp____5 = tmp___13;
  tmp____5 = tmp____5 & 4294967289U;
  tmp____5 = tmp____5 | 6U;
  amdgpu_mm_wreg(adev, 15680U, tmp____5, 0);
  amdgpu_mm_wreg(adev, 15785U, 285278465U, 0);
  amdgpu_mm_wreg(adev, 15782U, 0U, 0);
  amdgpu_mm_wreg(adev, 15654U, (unsigned int )(ring->gpu_addr >> 32ULL) | 2147942400U,
                 0);
  amdgpu_mm_wreg(adev, 15780U, 0U, 0);
  ring->wptr = amdgpu_mm_rreg(adev, 15780U, 0);
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 15779U, (u32 )ring->gpu_addr, 0);
  tmp___14 = __roundup_pow_of_two((unsigned long )ring->ring_size);
  tmp___15 = __ilog2_u64((u64 )tmp___14);
  rb_bufsz = (u32 )tmp___15;
  rb_bufsz = rb_bufsz | 256U;
  tmp___16 = amdgpu_mm_rreg(adev, 15785U, 0);
  tmp____6 = tmp___16;
  tmp____6 = tmp____6 & 4294893792U;
  tmp____6 = (rb_bufsz & 73503U) | tmp____6;
  amdgpu_mm_wreg(adev, 15785U, tmp____6, 0);
  return (0);
}
}
static void uvd_v4_2_stop(struct amdgpu_device *adev )
{
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  unsigned long __ms___0 ;
  unsigned long tmp___1 ;
  u32 tmp____0 ;
  u32 tmp___2 ;
  {
  amdgpu_mm_wreg(adev, 15785U, 285278465U, 0);
  tmp = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967039U;
  tmp_ = tmp_ | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp_, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43818;
    ldv_43817:
    __const_udelay(4295000UL);
    ldv_43818:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43817;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43822;
    ldv_43821:
    __const_udelay(4295000UL);
    ldv_43822:
    tmp___1 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_43821;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 0U, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____0 = tmp___2;
  tmp____0 = tmp____0 & 4294967039U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 15677U, tmp____0, 0);
  return;
}
}
static void uvd_v4_2_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                     unsigned int flags )
{
  int __ret_warn_on ;
  long tmp ;
  {
  __ret_warn_on = (int )flags & 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/uvd_v4_2.c",
                       422);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, (u32 )seq);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, (u32 )addr);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL) & 255U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 2U);
  return;
}
}
static bool uvd_v4_2_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  uint64_t addr ;
  {
  addr = semaphore->gpu_addr;
  amdgpu_ring_write(ring, 15296U);
  amdgpu_ring_write(ring, (u32 )(addr >> 3) & 1048575U);
  amdgpu_ring_write(ring, 15297U);
  amdgpu_ring_write(ring, (u32 )(addr >> 23) & 1048575U);
  amdgpu_ring_write(ring, 15298U);
  amdgpu_ring_write(ring, (u32 )((int )emit_wait | 128));
  return (1);
}
}
static int uvd_v4_2_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  amdgpu_mm_wreg(adev, 15805U, 3405700781U, 0);
  r = amdgpu_ring_lock(ring, 3U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring %d (%d).\n", ring->idx, r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_43848;
  ldv_43847:
  tmp = amdgpu_mm_rreg(adev, 15805U, 0);
  if (tmp == 3735928559U) {
    goto ldv_43846;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43848: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43847;
  } else {
  }
  ldv_43846: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  return (r);
}
}
static void uvd_v4_2_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  {
  amdgpu_ring_write(ring, 15777U);
  amdgpu_ring_write(ring, (u32 )ib->gpu_addr);
  amdgpu_ring_write(ring, 15778U);
  amdgpu_ring_write(ring, ib->length_dw);
  return;
}
}
static int uvd_v4_2_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_fence *fence ;
  int r ;
  {
  adev = ring->adev;
  fence = (struct amdgpu_fence *)0;
  r = (*((adev->asic_funcs)->set_uvd_clocks))(adev, 53300U, 40000U);
  if (r != 0) {
    drm_err("amdgpu: failed to raise UVD clocks (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_uvd_get_create_msg(ring, 1U, (struct amdgpu_fence **)0);
  if (r != 0) {
    drm_err("amdgpu: failed to get create msg (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_uvd_get_destroy_msg(ring, 1U, & fence);
  if (r != 0) {
    drm_err("amdgpu: failed to get destroy ib (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    goto error;
  } else {
  }
  printk("\016[drm] ib test on ring %d succeeded\n", ring->idx);
  error:
  amdgpu_fence_unref(& fence);
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 0U, 0U);
  return (r);
}
}
static void uvd_v4_2_mc_resume(struct amdgpu_device *adev )
{
  uint64_t addr ;
  u32 size ;
  {
  addr = (adev->uvd.gpu_addr + 256ULL) >> 3;
  size = (u32 )((((unsigned long )(adev->uvd.fw)->size + 4099UL) & 0xfffffffffffff000UL) >> 3);
  amdgpu_mm_wreg(adev, 15746U, (u32 )addr, 0);
  amdgpu_mm_wreg(adev, 15747U, size, 0);
  addr = (uint64_t )size + addr;
  size = 131072U;
  amdgpu_mm_wreg(adev, 15748U, (u32 )addr, 0);
  amdgpu_mm_wreg(adev, 15749U, size, 0);
  addr = (uint64_t )size + addr;
  size = 131072U;
  amdgpu_mm_wreg(adev, 15750U, (u32 )addr, 0);
  amdgpu_mm_wreg(adev, 15751U, size, 0);
  addr = (adev->uvd.gpu_addr >> 28) & 15ULL;
  amdgpu_mm_wreg(adev, 15717U, ((u32 )addr << 12U) | (u32 )addr, 0);
  addr = (adev->uvd.gpu_addr >> 32) & 255ULL;
  amdgpu_mm_wreg(adev, 15654U, (u32 )addr | 2148073472U, 0);
  uvd_v4_2_init_cg(adev);
  return;
}
}
static void uvd_v4_2_enable_mgcg(struct amdgpu_device *adev , bool enable )
{
  u32 orig ;
  u32 data ;
  {
  if ((int )enable && (adev->cg_flags & 8192U) != 0U) {
    data = (*(adev->uvd_ctx_rreg))(adev, 192U);
    data = 4095U;
    (*(adev->uvd_ctx_wreg))(adev, 192U, data);
    data = amdgpu_mm_rreg(adev, 15660U, 0);
    orig = data;
    data = data | 1U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 15660U, data, 0);
    } else {
    }
  } else {
    data = (*(adev->uvd_ctx_rreg))(adev, 192U);
    data = data & 4294963200U;
    (*(adev->uvd_ctx_wreg))(adev, 192U, data);
    data = amdgpu_mm_rreg(adev, 15660U, 0);
    orig = data;
    data = data & 4294967294U;
    if (orig != data) {
      amdgpu_mm_wreg(adev, 15660U, data, 0);
    } else {
    }
  }
  return;
}
}
static void uvd_v4_2_set_dcm(struct amdgpu_device *adev , bool sw_mode )
{
  u32 tmp ;
  u32 tmp2 ;
  {
  tmp = amdgpu_mm_rreg(adev, 15660U, 0);
  tmp = tmp & 4294965251U;
  tmp = tmp | 261U;
  if ((int )sw_mode) {
    tmp = tmp & 2147485695U;
    tmp2 = 31U;
  } else {
    tmp = tmp | 2147481600U;
    tmp2 = 0U;
  }
  amdgpu_mm_wreg(adev, 15660U, tmp, 0);
  (*(adev->uvd_ctx_wreg))(adev, 193U, tmp2);
  return;
}
}
static void uvd_v4_2_init_cg(struct amdgpu_device *adev )
{
  bool hw_mode ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  hw_mode = 1;
  if ((int )hw_mode) {
    uvd_v4_2_set_dcm(adev, 0);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 15660U, 0);
    tmp = tmp___0;
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 15660U, tmp, 0);
  }
  return;
}
}
static bool uvd_v4_2_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  return ((tmp & 524288U) == 0U);
}
}
static int uvd_v4_2_wait_for_idle(void *handle )
{
  unsigned int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43892;
  ldv_43891:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 524288U) == 0U) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_43892: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43891;
  } else {
  }
  return (-110);
}
}
static int uvd_v4_2_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v4_2_stop(adev);
  tmp = amdgpu_mm_rreg(adev, 920U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294705151U;
  tmp_ = tmp_ | 262144U;
  amdgpu_mm_wreg(adev, 920U, tmp_, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms = 5UL;
    goto ldv_43901;
    ldv_43900:
    __const_udelay(4295000UL);
    ldv_43901:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43900;
    } else {
    }
  }
  tmp___1 = uvd_v4_2_start(adev);
  return (tmp___1);
}
}
static void uvd_v4_2_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  u32 tmp___46 ;
  u32 tmp___47 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "UVD 4.2 registers\n");
  tmp = amdgpu_mm_rreg(adev, 15296U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_LOW=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 15297U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_HIGH=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 15298U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CMD=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 15299U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_CMD=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 15300U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA0=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 15301U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 15302U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_ENGINE_CNTL=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 15315U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_ADDR_CONFIG=0x%08X\n",
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 15316U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DB_ADDR_CONFIG=0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 15317U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DBW_ADDR_CONFIG=0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 15616U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CNTL=0x%08X\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 15654U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_EXT40_ADDR=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 15656U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_INDEX=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 15657U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_DATA=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 15658U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_GATE=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 15660U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_CTRL=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 15677U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL2=0x%08X\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 15680U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MASTINT_EN=0x%08X\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 15717U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_ADDR_EXT=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 15718U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 15725U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_SWAP_CNTL=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 15727U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MP_SWAP_CNTL=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 15737U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA0=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 15738U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA1=0x%08X\n", tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 15739U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB0=0x%08X\n", tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 15740U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB1=0x%08X\n", tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 15741U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUX=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 15742U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_ALU=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 15746U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET0=0x%08X\n",
            tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 15747U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE0=0x%08X\n",
            tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 15748U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET1=0x%08X\n",
            tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 15749U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE1=0x%08X\n",
            tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 15750U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET2=0x%08X\n",
            tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 15751U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE2=0x%08X\n",
            tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 15768U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CNTL=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 15776U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SOFT_RESET=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 15777U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_IB_BASE=0x%08X\n", tmp___35);
  tmp___36 = amdgpu_mm_rreg(adev, 15778U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_IB_SIZE=0x%08X\n", tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, 15779U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_BASE=0x%08X\n", tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, 15780U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_RPTR=0x%08X\n", tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, 15781U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR=0x%08X\n", tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, 15782U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR_CNTL=0x%08X\n",
            tmp___40);
  tmp___41 = amdgpu_mm_rreg(adev, 15785U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_CNTL=0x%08X\n", tmp___41);
  tmp___42 = amdgpu_mm_rreg(adev, 15791U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_STATUS=0x%08X\n", tmp___42);
  tmp___43 = amdgpu_mm_rreg(adev, 15792U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_TIMEOUT_STATUS=0x%08X\n",
            tmp___43);
  tmp___44 = amdgpu_mm_rreg(adev, 15793U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___44);
  tmp___45 = amdgpu_mm_rreg(adev, 15794U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_FAULT_TIMEOUT_CNTL=0x%08X\n",
            tmp___45);
  tmp___46 = amdgpu_mm_rreg(adev, 15795U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_SIGNAL_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___46);
  tmp___47 = amdgpu_mm_rreg(adev, 15805U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CONTEXT_ID=0x%08X\n", tmp___47);
  return;
}
}
static int uvd_v4_2_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  return (0);
}
}
static int uvd_v4_2_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("uvd_v4_2_process_interrupt", "IH: UVD TRAP\n");
  } else {
  }
  amdgpu_fence_process(& adev->uvd.ring);
  return (0);
}
}
static int uvd_v4_2_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  uvd_v4_2_enable_mgcg(adev, (int )gate);
  return (0);
}
}
static int uvd_v4_2_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    uvd_v4_2_stop(adev);
    return (0);
  } else {
    tmp = uvd_v4_2_start(adev);
    return (tmp);
  }
}
}
struct amd_ip_funcs const uvd_v4_2_ip_funcs =
     {& uvd_v4_2_early_init, (int (*)(void * ))0, & uvd_v4_2_sw_init, & uvd_v4_2_sw_fini,
    & uvd_v4_2_hw_init, & uvd_v4_2_hw_fini, & uvd_v4_2_suspend, & uvd_v4_2_resume,
    & uvd_v4_2_is_idle, & uvd_v4_2_wait_for_idle, & uvd_v4_2_soft_reset, & uvd_v4_2_print_status,
    & uvd_v4_2_set_clockgating_state, & uvd_v4_2_set_powergating_state};
static struct amdgpu_ring_funcs const uvd_v4_2_ring_funcs =
     {& uvd_v4_2_ring_get_rptr, & uvd_v4_2_ring_get_wptr, & uvd_v4_2_ring_set_wptr,
    & amdgpu_uvd_ring_parse_cs, & uvd_v4_2_ring_emit_ib, & uvd_v4_2_ring_emit_fence,
    & uvd_v4_2_ring_emit_semaphore, 0, 0, 0, & uvd_v4_2_ring_test_ring, & uvd_v4_2_ring_test_ib,
    & amdgpu_ring_test_lockup};
static void uvd_v4_2_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.ring.funcs = & uvd_v4_2_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const uvd_v4_2_irq_funcs = {& uvd_v4_2_set_interrupt_state, & uvd_v4_2_process_interrupt};
static void uvd_v4_2_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.irq.num_types = 1U;
  adev->uvd.irq.funcs = & uvd_v4_2_irq_funcs;
  return;
}
}
extern int ldv_release_85(void) ;
int ldv_retval_77 ;
extern int ldv_probe_85(void) ;
int ldv_retval_78 ;
void ldv_initialize_amdgpu_ring_funcs_84(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  uvd_v4_2_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_83(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  uvd_v4_2_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  uvd_v4_2_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_84(void)
{
  bool ldvarg47 ;
  unsigned int ldvarg52 ;
  uint64_t ldvarg53 ;
  uint64_t ldvarg54 ;
  struct amdgpu_semaphore *ldvarg48 ;
  void *tmp ;
  struct amdgpu_cs_parser *ldvarg50 ;
  void *tmp___0 ;
  struct amdgpu_ib *ldvarg49 ;
  void *tmp___1 ;
  u32 ldvarg51 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg48 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(200UL);
  ldvarg50 = (struct amdgpu_cs_parser *)tmp___0;
  tmp___1 = ldv_init_zalloc(272UL);
  ldvarg49 = (struct amdgpu_ib *)tmp___1;
  ldv_memset((void *)(& ldvarg47), 0, 1UL);
  ldv_memset((void *)(& ldvarg52), 0, 4UL);
  ldv_memset((void *)(& ldvarg53), 0, 8UL);
  ldv_memset((void *)(& ldvarg54), 0, 8UL);
  ldv_memset((void *)(& ldvarg51), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_emit_fence(uvd_v4_2_ring_funcs_group0, ldvarg54, ldvarg53, ldvarg52);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 1: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_get_rptr(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 2: ;
  if (ldv_state_variable_84 == 1) {
    amdgpu_uvd_ring_parse_cs(ldvarg50, ldvarg51);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 3: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_test_ring(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 4: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_set_wptr(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 5: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_get_wptr(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 6: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_emit_ib(uvd_v4_2_ring_funcs_group0, ldvarg49);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 7: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_emit_semaphore(uvd_v4_2_ring_funcs_group0, ldvarg48, (int )ldvarg47);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 8: ;
  if (ldv_state_variable_84 == 1) {
    uvd_v4_2_ring_test_ib(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  case 9: ;
  if (ldv_state_variable_84 == 1) {
    amdgpu_ring_test_lockup(uvd_v4_2_ring_funcs_group0);
    ldv_state_variable_84 = 1;
  } else {
  }
  goto ldv_43963;
  default:
  ldv_stop();
  }
  ldv_43963: ;
  return;
}
}
void ldv_main_exported_85(void)
{
  enum amd_clockgating_state ldvarg1046 ;
  void *ldvarg1056 ;
  void *tmp ;
  void *ldvarg1042 ;
  void *tmp___0 ;
  void *ldvarg1052 ;
  void *tmp___1 ;
  void *ldvarg1055 ;
  void *tmp___2 ;
  void *ldvarg1047 ;
  void *tmp___3 ;
  void *ldvarg1044 ;
  void *tmp___4 ;
  void *ldvarg1048 ;
  void *tmp___5 ;
  void *ldvarg1054 ;
  void *tmp___6 ;
  void *ldvarg1049 ;
  void *tmp___7 ;
  void *ldvarg1045 ;
  void *tmp___8 ;
  void *ldvarg1051 ;
  void *tmp___9 ;
  void *ldvarg1053 ;
  void *tmp___10 ;
  void *ldvarg1043 ;
  void *tmp___11 ;
  enum amd_powergating_state ldvarg1050 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg1056 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg1042 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg1052 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg1055 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg1047 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg1044 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg1048 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg1054 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg1049 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg1045 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg1051 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg1053 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg1043 = tmp___11;
  ldv_memset((void *)(& ldvarg1046), 0, 4UL);
  ldv_memset((void *)(& ldvarg1050), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_hw_fini(ldvarg1056);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_hw_fini(ldvarg1056);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_hw_fini(ldvarg1056);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 1: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_print_status(ldvarg1055);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_print_status(ldvarg1055);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_print_status(ldvarg1055);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 2: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_early_init(ldvarg1054);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_early_init(ldvarg1054);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_early_init(ldvarg1054);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 3: ;
  if (ldv_state_variable_85 == 2) {
    ldv_retval_78 = uvd_v4_2_suspend(ldvarg1053);
    if (ldv_retval_78 == 0) {
      ldv_state_variable_85 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43993;
  case 4: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_sw_init(ldvarg1052);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_sw_init(ldvarg1052);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_sw_init(ldvarg1052);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 5: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_set_powergating_state(ldvarg1051, ldvarg1050);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_set_powergating_state(ldvarg1051, ldvarg1050);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_set_powergating_state(ldvarg1051, ldvarg1050);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 6: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_wait_for_idle(ldvarg1049);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_wait_for_idle(ldvarg1049);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_wait_for_idle(ldvarg1049);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 7: ;
  if (ldv_state_variable_85 == 3) {
    ldv_retval_77 = uvd_v4_2_resume(ldvarg1048);
    if (ldv_retval_77 == 0) {
      ldv_state_variable_85 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43993;
  case 8: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_set_clockgating_state(ldvarg1047, ldvarg1046);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_set_clockgating_state(ldvarg1047, ldvarg1046);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_set_clockgating_state(ldvarg1047, ldvarg1046);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 9: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_hw_init(ldvarg1045);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_hw_init(ldvarg1045);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_hw_init(ldvarg1045);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 10: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_soft_reset(ldvarg1044);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_soft_reset(ldvarg1044);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_soft_reset(ldvarg1044);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 11: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_sw_fini(ldvarg1043);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_sw_fini(ldvarg1043);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_sw_fini(ldvarg1043);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 12: ;
  if (ldv_state_variable_85 == 2) {
    uvd_v4_2_is_idle(ldvarg1042);
    ldv_state_variable_85 = 2;
  } else {
  }
  if (ldv_state_variable_85 == 1) {
    uvd_v4_2_is_idle(ldvarg1042);
    ldv_state_variable_85 = 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    uvd_v4_2_is_idle(ldvarg1042);
    ldv_state_variable_85 = 3;
  } else {
  }
  goto ldv_43993;
  case 13: ;
  if (ldv_state_variable_85 == 2) {
    ldv_release_85();
    ldv_state_variable_85 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_85 == 3) {
    ldv_release_85();
    ldv_state_variable_85 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43993;
  case 14: ;
  if (ldv_state_variable_85 == 1) {
    ldv_probe_85();
    ldv_state_variable_85 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43993;
  default:
  ldv_stop();
  }
  ldv_43993: ;
  return;
}
}
void ldv_main_exported_83(void)
{
  unsigned int ldvarg945 ;
  enum amdgpu_interrupt_state ldvarg946 ;
  struct amdgpu_iv_entry *ldvarg944 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg944 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg945), 0, 4UL);
  ldv_memset((void *)(& ldvarg946), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_83 == 1) {
    uvd_v4_2_set_interrupt_state(uvd_v4_2_irq_funcs_group0, uvd_v4_2_irq_funcs_group1,
                                 ldvarg945, ldvarg946);
    ldv_state_variable_83 = 1;
  } else {
  }
  goto ldv_44016;
  case 1: ;
  if (ldv_state_variable_83 == 1) {
    uvd_v4_2_process_interrupt(uvd_v4_2_irq_funcs_group0, uvd_v4_2_irq_funcs_group1,
                               ldvarg944);
    ldv_state_variable_83 = 1;
  } else {
  }
  goto ldv_44016;
  default:
  ldv_stop();
  }
  ldv_44016: ;
  return;
}
}
bool ldv_queue_work_on_665(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_666(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_667(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_668(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_669(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_679(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_681(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_680(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_683(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_682(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_vce_sw_init(struct amdgpu_device *adev , unsigned long size ) ;
int amdgpu_vce_sw_fini(struct amdgpu_device *adev ) ;
int amdgpu_vce_suspend(struct amdgpu_device *adev ) ;
int amdgpu_vce_resume(struct amdgpu_device *adev ) ;
int amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p , u32 ib_idx ) ;
bool amdgpu_vce_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                    bool emit_wait ) ;
void amdgpu_vce_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib ) ;
void amdgpu_vce_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq , unsigned int flags ) ;
int amdgpu_vce_ring_test_ring(struct amdgpu_ring *ring ) ;
int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring ) ;
static void vce_v2_0_mc_resume(struct amdgpu_device *adev ) ;
static void vce_v2_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void vce_v2_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 vce_v2_0_ring_get_rptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    tmp = amdgpu_mm_rreg(adev, 32867U, 0);
    return (tmp);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 32862U, 0);
    return (tmp___0);
  }
}
}
static u32 vce_v2_0_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    tmp = amdgpu_mm_rreg(adev, 32868U, 0);
    return (tmp);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 32863U, 0);
    return (tmp___0);
  }
}
}
static void vce_v2_0_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    amdgpu_mm_wreg(adev, 32868U, ring->wptr, 0);
  } else {
    amdgpu_mm_wreg(adev, 32863U, ring->wptr, 0);
  }
  return;
}
}
static int vce_v2_0_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  int i ;
  int j ;
  int r ;
  u32 tmp_ ;
  u32 tmp ;
  u32 tmp____0 ;
  u32 tmp___0 ;
  u32 tmp____1 ;
  u32 tmp___1 ;
  unsigned long __ms ;
  unsigned long tmp___2 ;
  u32 tmp____2 ;
  u32 tmp___3 ;
  u32 status ;
  unsigned long __ms___0 ;
  unsigned long tmp___4 ;
  u32 tmp____3 ;
  u32 tmp___5 ;
  unsigned long __ms___1 ;
  unsigned long tmp___6 ;
  u32 tmp____4 ;
  u32 tmp___7 ;
  unsigned long __ms___2 ;
  unsigned long tmp___8 ;
  u32 tmp____5 ;
  u32 tmp___9 ;
  {
  vce_v2_0_mc_resume(adev);
  tmp = amdgpu_mm_rreg(adev, 32769U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_ | 1U;
  amdgpu_mm_wreg(adev, 32769U, tmp_, 0);
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  amdgpu_mm_wreg(adev, 32867U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32868U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32864U, (u32 )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 32865U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 32866U, ring->ring_size / 4U, 0);
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  amdgpu_mm_wreg(adev, 32862U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32863U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32859U, (u32 )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 32860U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 32861U, ring->ring_size / 4U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 32773U, 0);
  tmp____0 = tmp___0;
  tmp____0 = tmp____0 & 4294967294U;
  tmp____0 = tmp____0 | 1U;
  amdgpu_mm_wreg(adev, 32773U, tmp____0, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____1 = tmp___1;
  tmp____1 = tmp____1 & 4294967294U;
  tmp____1 = tmp____1 | 1U;
  amdgpu_mm_wreg(adev, 32840U, tmp____1, 0);
  __ms = 100UL;
  goto ldv_43740;
  ldv_43739:
  __const_udelay(4295000UL);
  ldv_43740:
  tmp___2 = __ms;
  __ms = __ms - 1UL;
  if (tmp___2 != 0UL) {
    goto ldv_43739;
  } else {
  }
  tmp___3 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____2 = tmp___3;
  tmp____2 = tmp____2 & 4294967294U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 32840U, tmp____2, 0);
  i = 0;
  goto ldv_43763;
  ldv_43762:
  j = 0;
  goto ldv_43750;
  ldv_43749:
  status = amdgpu_mm_rreg(adev, 32769U, 0);
  if ((status & 2U) != 0U) {
    goto ldv_43744;
  } else {
  }
  __ms___0 = 10UL;
  goto ldv_43747;
  ldv_43746:
  __const_udelay(4295000UL);
  ldv_43747:
  tmp___4 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___4 != 0UL) {
    goto ldv_43746;
  } else {
  }
  j = j + 1;
  ldv_43750: ;
  if (j <= 99) {
    goto ldv_43749;
  } else {
  }
  ldv_43744:
  r = 0;
  if ((status & 2U) != 0U) {
    goto ldv_43751;
  } else {
  }
  drm_err("VCE not responding, trying to reset the ECPU!!!\n");
  tmp___5 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____3 = tmp___5;
  tmp____3 = tmp____3 & 4294967294U;
  tmp____3 = tmp____3 | 1U;
  amdgpu_mm_wreg(adev, 32840U, tmp____3, 0);
  __ms___1 = 10UL;
  goto ldv_43755;
  ldv_43754:
  __const_udelay(4295000UL);
  ldv_43755:
  tmp___6 = __ms___1;
  __ms___1 = __ms___1 - 1UL;
  if (tmp___6 != 0UL) {
    goto ldv_43754;
  } else {
  }
  tmp___7 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____4 = tmp___7;
  tmp____4 = tmp____4 & 4294967294U;
  tmp____4 = tmp____4;
  amdgpu_mm_wreg(adev, 32840U, tmp____4, 0);
  __ms___2 = 10UL;
  goto ldv_43760;
  ldv_43759:
  __const_udelay(4295000UL);
  ldv_43760:
  tmp___8 = __ms___2;
  __ms___2 = __ms___2 - 1UL;
  if (tmp___8 != 0UL) {
    goto ldv_43759;
  } else {
  }
  r = -1;
  i = i + 1;
  ldv_43763: ;
  if (i <= 9) {
    goto ldv_43762;
  } else {
  }
  ldv_43751:
  tmp___9 = amdgpu_mm_rreg(adev, 32769U, 0);
  tmp____5 = tmp___9;
  tmp____5 = tmp____5 & 4294967294U;
  tmp____5 = tmp____5;
  amdgpu_mm_wreg(adev, 32769U, tmp____5, 0);
  if (r != 0) {
    drm_err("VCE not responding, giving up!!!\n");
    return (r);
  } else {
  }
  return (0);
}
}
static int vce_v2_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  vce_v2_0_set_ring_funcs(adev);
  vce_v2_0_set_irq_funcs(adev);
  return (0);
}
}
static int vce_v2_0_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 167U, & adev->vce.irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_sw_init(adev, 704512UL);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  sprintf((char *)(& ring->name), "vce0");
  r = amdgpu_ring_init(adev, ring, 4096U, 0U, 15U, & adev->vce.irq, 0U, 4);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  sprintf((char *)(& ring->name), "vce1");
  r = amdgpu_ring_init(adev, ring, 4096U, 0U, 15U, & adev->vce.irq, 0U, 4);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v2_0_sw_fini(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_vce_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_sw_fini(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v2_0_hw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = vce_v2_0_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  printk("\016[drm] VCE initialized successfully.\n");
  return (0);
}
}
static int vce_v2_0_hw_fini(void *handle )
{
  {
  return (0);
}
}
static int vce_v2_0_suspend(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = vce_v2_0_hw_fini((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v2_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_vce_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = vce_v2_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static void vce_v2_0_set_sw_cg(struct amdgpu_device *adev , bool gated )
{
  u32 tmp ;
  {
  if ((int )gated) {
    tmp = amdgpu_mm_rreg(adev, 32959U, 0);
    tmp = tmp | 15138816U;
    amdgpu_mm_wreg(adev, 32959U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 33263U, 0);
    tmp = tmp | 4278190080U;
    amdgpu_mm_wreg(adev, 33263U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 33264U, 0);
    tmp = tmp & 4294966275U;
    amdgpu_mm_wreg(adev, 33264U, tmp, 0);
    amdgpu_mm_wreg(adev, 33256U, 0U, 0);
  } else {
    tmp = amdgpu_mm_rreg(adev, 32959U, 0);
    tmp = tmp | 231U;
    tmp = tmp & 4279828479U;
    amdgpu_mm_wreg(adev, 32959U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 33263U, 0);
    tmp = tmp | 2088960U;
    tmp = tmp & 16777215U;
    amdgpu_mm_wreg(adev, 33263U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 33264U, 0);
    tmp = tmp | 1020U;
    amdgpu_mm_wreg(adev, 33264U, tmp, 0);
  }
  return;
}
}
static void vce_v2_0_set_dyn_cg(struct amdgpu_device *adev , bool gated )
{
  u32 orig ;
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 32959U, 0);
  tmp = tmp & 4294574073U;
  if ((int )gated) {
    tmp = tmp | 14745600U;
  } else {
    tmp = tmp | 225U;
    tmp = tmp & 4280221695U;
  }
  amdgpu_mm_wreg(adev, 32959U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 33263U, 0);
  orig = tmp;
  tmp = tmp & 4292878335U;
  tmp = tmp & 16777215U;
  if (tmp != orig) {
    amdgpu_mm_wreg(adev, 33263U, tmp, 0);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 33264U, 0);
  orig = tmp;
  tmp = tmp & 4294966275U;
  if (tmp != orig) {
    amdgpu_mm_wreg(adev, 33264U, tmp, 0);
  } else {
  }
  if ((int )gated) {
    amdgpu_mm_wreg(adev, 33256U, 0U, 0);
  } else {
  }
  return;
}
}
static void vce_v2_0_disable_cg(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 33256U, 7U, 0);
  return;
}
}
static void vce_v2_0_enable_mgcg(struct amdgpu_device *adev , bool enable )
{
  bool sw_cg ;
  {
  sw_cg = 0;
  if ((int )enable && (adev->cg_flags & 16384U) != 0U) {
    if ((int )sw_cg) {
      vce_v2_0_set_sw_cg(adev, 1);
    } else {
      vce_v2_0_set_dyn_cg(adev, 1);
    }
  } else {
    vce_v2_0_disable_cg(adev);
    if ((int )sw_cg) {
      vce_v2_0_set_sw_cg(adev, 0);
    } else {
      vce_v2_0_set_dyn_cg(adev, 0);
    }
  }
  return;
}
}
static void vce_v2_0_init_cg(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 32958U, 0);
  tmp = tmp & 4294963200U;
  tmp = tmp | 64U;
  tmp = tmp | 262144U;
  amdgpu_mm_wreg(adev, 32958U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 33263U, 0);
  tmp = tmp & 4294963200U;
  tmp = tmp | 64U;
  amdgpu_mm_wreg(adev, 33263U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 32959U, 0);
  tmp = tmp | 16U;
  tmp = tmp & 4293918719U;
  amdgpu_mm_wreg(adev, 32959U, tmp, 0);
  return;
}
}
static void vce_v2_0_mc_resume(struct amdgpu_device *adev )
{
  uint64_t addr ;
  u32 size ;
  u32 tmp_ ;
  u32 tmp ;
  u32 tmp____0 ;
  u32 tmp___0 ;
  u32 tmp____1 ;
  u32 tmp___1 ;
  u32 tmp____2 ;
  u32 tmp___2 ;
  u32 tmp____3 ;
  u32 tmp___3 ;
  u32 tmp____4 ;
  u32 tmp___4 ;
  {
  addr = adev->vce.gpu_addr;
  tmp = amdgpu_mm_rreg(adev, 32958U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294901759U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 32958U, tmp_, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 33263U, 0);
  tmp____0 = tmp___0;
  tmp____0 = tmp____0 & 6295551U;
  tmp____0 = tmp____0 | 2093056U;
  amdgpu_mm_wreg(adev, 33263U, tmp____0, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 33264U, 0);
  tmp____1 = tmp___1;
  tmp____1 = tmp____1 & 4294967232U;
  tmp____1 = tmp____1 | 63U;
  amdgpu_mm_wreg(adev, 33264U, tmp____1, 0);
  amdgpu_mm_wreg(adev, 32959U, 247U, 0);
  amdgpu_mm_wreg(adev, 34086U, 3768320U, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 34109U, 0);
  tmp____2 = tmp___2;
  tmp____2 = tmp____2 & 4294967294U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 34109U, tmp____2, 0);
  amdgpu_mm_wreg(adev, 34093U, 0U, 0);
  amdgpu_mm_wreg(adev, 34094U, 0U, 0);
  amdgpu_mm_wreg(adev, 34088U, 0U, 0);
  addr = addr + 256ULL;
  size = 262144U;
  amdgpu_mm_wreg(adev, 32777U, (u32 )addr & 2147483647U, 0);
  amdgpu_mm_wreg(adev, 32778U, size, 0);
  addr = (uint64_t )size + addr;
  size = 65536U;
  amdgpu_mm_wreg(adev, 32779U, (u32 )addr & 2147483647U, 0);
  amdgpu_mm_wreg(adev, 32780U, size, 0);
  addr = (uint64_t )size + addr;
  size = 376832U;
  amdgpu_mm_wreg(adev, 32781U, (u32 )addr & 2147483647U, 0);
  amdgpu_mm_wreg(adev, 32782U, size, 0);
  tmp___3 = amdgpu_mm_rreg(adev, 34077U, 0);
  tmp____3 = tmp___3;
  tmp____3 = tmp____3 & 4294967039U;
  tmp____3 = tmp____3;
  amdgpu_mm_wreg(adev, 34077U, tmp____3, 0);
  tmp___4 = amdgpu_mm_rreg(adev, 33984U, 0);
  tmp____4 = tmp___4;
  tmp____4 = tmp____4 & 4294967287U;
  tmp____4 = tmp____4 | 8U;
  amdgpu_mm_wreg(adev, 33984U, tmp____4, 0);
  vce_v2_0_init_cg(adev);
  return;
}
}
static bool vce_v2_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  return ((tmp & 128U) == 0U);
}
}
static int vce_v2_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43843;
  ldv_43842:
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  if ((tmp & 128U) == 0U) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_43843: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43842;
  } else {
  }
  return (-110);
}
}
static int vce_v2_0_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 920U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4278190079U;
  tmp_ = tmp_ | 16777216U;
  amdgpu_mm_wreg(adev, 920U, tmp_, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms = 5UL;
    goto ldv_43852;
    ldv_43851:
    __const_udelay(4295000UL);
    ldv_43852:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43851;
    } else {
    }
  }
  tmp___1 = vce_v2_0_start(adev);
  return (tmp___1);
}
}
static void vce_v2_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "VCE 2.0 registers\n");
  tmp = amdgpu_mm_rreg(adev, 32769U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 32773U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CNTL=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 32777U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET0=0x%08X\n",
            tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 32778U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE0=0x%08X\n",
            tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 32779U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET1=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 32780U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 32781U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET2=0x%08X\n",
            tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 32782U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE2=0x%08X\n",
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 32840U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_SOFT_RESET=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 32859U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_LO2=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 32860U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_HI2=0x%08X\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 32861U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_SIZE2=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 32862U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_RPTR2=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 32863U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_WPTR2=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 32864U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_LO=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 32865U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_HI=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 32866U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_SIZE=0x%08X\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 32867U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_RPTR=0x%08X\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 32868U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_WPTR=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 32958U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_CLOCK_GATING_A=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 32959U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_CLOCK_GATING_B=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 33256U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_CGTT_CLK_OVERRIDE=0x%08X\n",
            tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 33263U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_UENC_CLOCK_GATING=0x%08X\n",
            tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 33264U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_UENC_REG_CLOCK_GATING=0x%08X\n",
            tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 33984U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_SYS_INT_EN=0x%08X\n", tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 34077U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CTRL2=0x%08X\n", tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 34086U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CTRL=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 34088U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_VM_CTRL=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 34093U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_SWAP_CNTL=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 34094U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_SWAP_CNTL1=0x%08X\n", tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 34109U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CACHE_CTRL=0x%08X\n", tmp___29);
  return;
}
}
static int vce_v2_0_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 val ;
  u32 tmp_ ;
  u32 tmp ;
  {
  val = 0U;
  if ((unsigned int )state == 1U) {
    val = val | 8U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 33984U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967287U;
  tmp_ = (val & 8U) | tmp_;
  amdgpu_mm_wreg(adev, 33984U, tmp_, 0);
  return (0);
}
}
static int vce_v2_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("vce_v2_0_process_interrupt", "IH: VCE\n");
  } else {
  }
  switch (entry->src_data) {
  case 0U:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->vce.ring));
  goto ldv_43873;
  case 1U:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->vce.ring) + 1UL);
  goto ldv_43873;
  default:
  drm_err("Unhandled interrupt: %d %d\n", entry->src_id, entry->src_data);
  goto ldv_43873;
  }
  ldv_43873: ;
  return (0);
}
}
static int vce_v2_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  bool gate ;
  struct amdgpu_device *adev ;
  {
  gate = 0;
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    gate = 1;
  } else {
  }
  vce_v2_0_enable_mgcg(adev, (int )gate);
  return (0);
}
}
static int vce_v2_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    return (0);
  } else {
    tmp = vce_v2_0_start(adev);
    return (tmp);
  }
}
}
struct amd_ip_funcs const vce_v2_0_ip_funcs =
     {& vce_v2_0_early_init, (int (*)(void * ))0, & vce_v2_0_sw_init, & vce_v2_0_sw_fini,
    & vce_v2_0_hw_init, & vce_v2_0_hw_fini, & vce_v2_0_suspend, & vce_v2_0_resume,
    & vce_v2_0_is_idle, & vce_v2_0_wait_for_idle, & vce_v2_0_soft_reset, & vce_v2_0_print_status,
    & vce_v2_0_set_clockgating_state, & vce_v2_0_set_powergating_state};
static struct amdgpu_ring_funcs const vce_v2_0_ring_funcs =
     {& vce_v2_0_ring_get_rptr, & vce_v2_0_ring_get_wptr, & vce_v2_0_ring_set_wptr,
    & amdgpu_vce_ring_parse_cs, & amdgpu_vce_ring_emit_ib, & amdgpu_vce_ring_emit_fence,
    & amdgpu_vce_ring_emit_semaphore, 0, 0, 0, & amdgpu_vce_ring_test_ring, & amdgpu_vce_ring_test_ib,
    & amdgpu_ring_test_lockup};
static void vce_v2_0_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->vce.ring[0].funcs = & vce_v2_0_ring_funcs;
  adev->vce.ring[1].funcs = & vce_v2_0_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const vce_v2_0_irq_funcs = {& vce_v2_0_set_interrupt_state, & vce_v2_0_process_interrupt};
static void vce_v2_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->vce.irq.num_types = 1U;
  adev->vce.irq.funcs = & vce_v2_0_irq_funcs;
  return;
}
}
int ldv_retval_18 ;
int ldv_retval_17 ;
extern int ldv_probe_82(void) ;
extern int ldv_release_82(void) ;
void ldv_initialize_amdgpu_ring_funcs_81(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  vce_v2_0_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_80(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  vce_v2_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  vce_v2_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_81(void)
{
  uint64_t ldvarg216 ;
  bool ldvarg210 ;
  struct amdgpu_semaphore *ldvarg211 ;
  void *tmp ;
  uint64_t ldvarg215 ;
  struct amdgpu_ib *ldvarg212 ;
  void *tmp___0 ;
  struct amdgpu_cs_parser *ldvarg214 ;
  void *tmp___1 ;
  unsigned int ldvarg217 ;
  u32 ldvarg213 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg211 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg212 = (struct amdgpu_ib *)tmp___0;
  tmp___1 = ldv_init_zalloc(200UL);
  ldvarg214 = (struct amdgpu_cs_parser *)tmp___1;
  ldv_memset((void *)(& ldvarg216), 0, 8UL);
  ldv_memset((void *)(& ldvarg210), 0, 1UL);
  ldv_memset((void *)(& ldvarg215), 0, 8UL);
  ldv_memset((void *)(& ldvarg217), 0, 4UL);
  ldv_memset((void *)(& ldvarg213), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_emit_fence(vce_v2_0_ring_funcs_group0, ldvarg216, ldvarg215, ldvarg217);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 1: ;
  if (ldv_state_variable_81 == 1) {
    vce_v2_0_ring_get_rptr(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 2: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_parse_cs(ldvarg214, ldvarg213);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 3: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_test_ring(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 4: ;
  if (ldv_state_variable_81 == 1) {
    vce_v2_0_ring_set_wptr(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 5: ;
  if (ldv_state_variable_81 == 1) {
    vce_v2_0_ring_get_wptr(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 6: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_emit_ib(vce_v2_0_ring_funcs_group0, ldvarg212);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 7: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_emit_semaphore(vce_v2_0_ring_funcs_group0, ldvarg211, (int )ldvarg210);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 8: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_vce_ring_test_ib(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  case 9: ;
  if (ldv_state_variable_81 == 1) {
    amdgpu_ring_test_lockup(vce_v2_0_ring_funcs_group0);
    ldv_state_variable_81 = 1;
  } else {
  }
  goto ldv_43920;
  default:
  ldv_stop();
  }
  ldv_43920: ;
  return;
}
}
void ldv_main_exported_80(void)
{
  unsigned int ldvarg546 ;
  enum amdgpu_interrupt_state ldvarg547 ;
  struct amdgpu_iv_entry *ldvarg545 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg545 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg546), 0, 4UL);
  ldv_memset((void *)(& ldvarg547), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_80 == 1) {
    vce_v2_0_set_interrupt_state(vce_v2_0_irq_funcs_group0, vce_v2_0_irq_funcs_group1,
                                 ldvarg546, ldvarg547);
    ldv_state_variable_80 = 1;
  } else {
  }
  goto ldv_43938;
  case 1: ;
  if (ldv_state_variable_80 == 1) {
    vce_v2_0_process_interrupt(vce_v2_0_irq_funcs_group0, vce_v2_0_irq_funcs_group1,
                               ldvarg545);
    ldv_state_variable_80 = 1;
  } else {
  }
  goto ldv_43938;
  default:
  ldv_stop();
  }
  ldv_43938: ;
  return;
}
}
void ldv_main_exported_82(void)
{
  void *ldvarg324 ;
  void *tmp ;
  void *ldvarg321 ;
  void *tmp___0 ;
  void *ldvarg328 ;
  void *tmp___1 ;
  void *ldvarg330 ;
  void *tmp___2 ;
  void *ldvarg325 ;
  void *tmp___3 ;
  void *ldvarg320 ;
  void *tmp___4 ;
  void *ldvarg331 ;
  void *tmp___5 ;
  void *ldvarg326 ;
  void *tmp___6 ;
  enum amd_powergating_state ldvarg327 ;
  void *ldvarg332 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg323 ;
  void *ldvarg329 ;
  void *tmp___8 ;
  void *ldvarg333 ;
  void *tmp___9 ;
  void *ldvarg319 ;
  void *tmp___10 ;
  void *ldvarg322 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg324 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg321 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg328 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg330 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg325 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg320 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg331 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg326 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg332 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg329 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg333 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg319 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg322 = tmp___11;
  ldv_memset((void *)(& ldvarg327), 0, 4UL);
  ldv_memset((void *)(& ldvarg323), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_hw_fini(ldvarg333);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_hw_fini(ldvarg333);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_hw_fini(ldvarg333);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 1: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_print_status(ldvarg332);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_print_status(ldvarg332);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_print_status(ldvarg332);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 2: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_early_init(ldvarg331);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_early_init(ldvarg331);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_early_init(ldvarg331);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 3: ;
  if (ldv_state_variable_82 == 2) {
    ldv_retval_18 = vce_v2_0_suspend(ldvarg330);
    if (ldv_retval_18 == 0) {
      ldv_state_variable_82 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43960;
  case 4: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_sw_init(ldvarg329);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_sw_init(ldvarg329);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_sw_init(ldvarg329);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 5: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_set_powergating_state(ldvarg328, ldvarg327);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_set_powergating_state(ldvarg328, ldvarg327);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_set_powergating_state(ldvarg328, ldvarg327);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 6: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_wait_for_idle(ldvarg326);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_wait_for_idle(ldvarg326);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_wait_for_idle(ldvarg326);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 7: ;
  if (ldv_state_variable_82 == 3) {
    ldv_retval_17 = vce_v2_0_resume(ldvarg325);
    if (ldv_retval_17 == 0) {
      ldv_state_variable_82 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43960;
  case 8: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_set_clockgating_state(ldvarg324, ldvarg323);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_set_clockgating_state(ldvarg324, ldvarg323);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_set_clockgating_state(ldvarg324, ldvarg323);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 9: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_hw_init(ldvarg322);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_hw_init(ldvarg322);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_hw_init(ldvarg322);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 10: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_soft_reset(ldvarg321);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_soft_reset(ldvarg321);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_soft_reset(ldvarg321);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 11: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_sw_fini(ldvarg320);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_sw_fini(ldvarg320);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_sw_fini(ldvarg320);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 12: ;
  if (ldv_state_variable_82 == 1) {
    vce_v2_0_is_idle(ldvarg319);
    ldv_state_variable_82 = 1;
  } else {
  }
  if (ldv_state_variable_82 == 3) {
    vce_v2_0_is_idle(ldvarg319);
    ldv_state_variable_82 = 3;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    vce_v2_0_is_idle(ldvarg319);
    ldv_state_variable_82 = 2;
  } else {
  }
  goto ldv_43960;
  case 13: ;
  if (ldv_state_variable_82 == 3) {
    ldv_release_82();
    ldv_state_variable_82 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_82 == 2) {
    ldv_release_82();
    ldv_state_variable_82 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43960;
  case 14: ;
  if (ldv_state_variable_82 == 1) {
    ldv_probe_82();
    ldv_state_variable_82 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43960;
  default:
  ldv_stop();
  }
  ldv_43960: ;
  return;
}
}
bool ldv_queue_work_on_679(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_680(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_681(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_682(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_683(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_693(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_695(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_694(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_697(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_696(struct workqueue_struct *ldv_func_arg1 ) ;
struct amd_ip_funcs const vi_common_ip_funcs ;
void vi_srbm_select(struct amdgpu_device *adev , u32 me , u32 pipe , u32 queue , u32 vmid ) ;
struct amd_ip_funcs const cz_dpm_ip_funcs ;
struct amd_ip_funcs const tonga_dpm_ip_funcs ;
struct amd_ip_funcs const iceland_dpm_ip_funcs ;
struct amd_ip_funcs const gmc_v8_0_ip_funcs ;
void gmc_v8_0_mc_stop(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save ) ;
void gmc_v8_0_mc_resume(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save ) ;
int gmc_v8_0_mc_wait_for_idle(struct amdgpu_device *adev ) ;
struct amd_ip_funcs const gfx_v8_0_ip_funcs ;
uint64_t gfx_v8_0_get_gpu_clock_counter(struct amdgpu_device *adev ) ;
void gfx_v8_0_select_se_sh(struct amdgpu_device *adev , u32 se_num , u32 sh_num ) ;
int gfx_v8_0_get_cu_info(struct amdgpu_device *adev , struct amdgpu_cu_info *cu_info ) ;
struct amd_ip_funcs const sdma_v2_4_ip_funcs ;
struct amd_ip_funcs const sdma_v3_0_ip_funcs ;
struct amd_ip_funcs const dce_v10_0_ip_funcs ;
struct amd_ip_funcs const dce_v11_0_ip_funcs ;
struct amd_ip_funcs const iceland_ih_ip_funcs ;
struct amd_ip_funcs const tonga_ih_ip_funcs ;
struct amd_ip_funcs const cz_ih_ip_funcs ;
struct amd_ip_funcs const uvd_v5_0_ip_funcs ;
struct amd_ip_funcs const uvd_v6_0_ip_funcs ;
struct amd_ip_funcs const vce_v3_0_ip_funcs ;
static u32 vi_pcie_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->pcie_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 14U, reg, 0);
  amdgpu_mm_rreg(adev, 14U, 0);
  r = amdgpu_mm_rreg(adev, 15U, 0);
  spin_unlock_irqrestore(& adev->pcie_idx_lock, flags);
  return (r);
}
}
static void vi_pcie_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->pcie_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 14U, reg, 0);
  amdgpu_mm_rreg(adev, 14U, 0);
  amdgpu_mm_wreg(adev, 15U, v, 0);
  amdgpu_mm_rreg(adev, 15U, 0);
  spin_unlock_irqrestore(& adev->pcie_idx_lock, flags);
  return;
}
}
static u32 vi_smc_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, reg, 0);
  r = amdgpu_mm_rreg(adev, 129U, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (r);
}
}
static void vi_smc_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, reg, 0);
  amdgpu_mm_wreg(adev, 129U, v, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return;
}
}
static u32 vi_uvd_ctx_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->uvd_ctx_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 15656U, reg & 511U, 0);
  r = amdgpu_mm_rreg(adev, 15657U, 0);
  spin_unlock_irqrestore(& adev->uvd_ctx_idx_lock, flags);
  return (r);
}
}
static void vi_uvd_ctx_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->uvd_ctx_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 15656U, reg & 511U, 0);
  amdgpu_mm_wreg(adev, 15657U, v, 0);
  spin_unlock_irqrestore(& adev->uvd_ctx_idx_lock, flags);
  return;
}
}
static u32 vi_didt_rreg(struct amdgpu_device *adev , u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->didt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 12928U, reg, 0);
  r = amdgpu_mm_rreg(adev, 12929U, 0);
  spin_unlock_irqrestore(& adev->didt_idx_lock, flags);
  return (r);
}
}
static void vi_didt_wreg(struct amdgpu_device *adev , u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->didt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 12928U, reg, 0);
  amdgpu_mm_wreg(adev, 12929U, v, 0);
  spin_unlock_irqrestore(& adev->didt_idx_lock, flags);
  return;
}
}
static u32 const tonga_mgcg_cgcg_init[21U] =
  { 5497U, 4294967295U, 6291712U, 14U,
        4294967295U, 20971548U, 15U, 983040U,
        0U, 136U, 4294967295U, 3227516940U,
        137U, 3221229567U, 256U, 5497U,
        4278194175U, 256U, 3123U, 3221229567U,
        260U};
static u32 const iceland_mgcg_cgcg_init[15U] =
  { 14U, 4294967295U, 20971548U, 15U,
        983040U, 0U, 136U, 4294967295U,
        3227516940U, 137U, 3221229567U, 256U,
        3123U, 3221229567U, 260U};
static u32 const cz_mgcg_cgcg_init[15U] =
  { 5497U, 4294967295U, 6291712U, 14U,
        4294967295U, 20971548U, 15U, 983040U,
        0U, 5497U, 4278194175U, 256U,
        3123U, 3221229567U, 260U};
static void vi_init_golden_registers(struct amdgpu_device *adev )
{
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& iceland_mgcg_cgcg_init),
                                   15U);
  goto ldv_48114;
  case 6U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_mgcg_cgcg_init),
                                   21U);
  goto ldv_48114;
  case 7U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_mgcg_cgcg_init), 15U);
  goto ldv_48114;
  default: ;
  goto ldv_48114;
  }
  ldv_48114:
  mutex_unlock(& adev->grbm_idx_mutex);
  return;
}
}
static u32 vi_get_xclk(struct amdgpu_device *adev )
{
  u32 reference_clock ;
  u32 tmp ;
  {
  reference_clock = adev->clock.spll.reference_freq;
  if ((adev->flags & 131072UL) != 0UL) {
    return (reference_clock);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3226468772U);
  if ((tmp & 256U) >> 8 != 0U) {
    return (1000U);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, 3226468768U);
  if ((tmp & 2U) >> 1 != 0U) {
    return (reference_clock / 4U);
  } else {
  }
  return (reference_clock);
}
}
void vi_srbm_select(struct amdgpu_device *adev , u32 me , u32 pipe , u32 queue , u32 vmid )
{
  u32 srbm_gfx_cntl ;
  {
  srbm_gfx_cntl = 0U;
  srbm_gfx_cntl = (srbm_gfx_cntl & 4294967292U) | (pipe & 3U);
  srbm_gfx_cntl = (srbm_gfx_cntl & 4294967283U) | ((me << 2) & 12U);
  srbm_gfx_cntl = (srbm_gfx_cntl & 4294967055U) | ((vmid << 4) & 255U);
  srbm_gfx_cntl = (srbm_gfx_cntl & 4294965503U) | ((queue << 8) & 1792U);
  amdgpu_mm_wreg(adev, 913U, srbm_gfx_cntl, 0);
  return;
}
}
static void vi_vga_set_state(struct amdgpu_device *adev , bool state )
{
  {
  return;
}
}
static bool vi_read_disabled_bios(struct amdgpu_device *adev )
{
  u32 bus_cntl ;
  u32 d1vga_control ;
  u32 d2vga_control ;
  u32 vga_render_control ;
  u32 rom_cntl ;
  bool r ;
  {
  d1vga_control = 0U;
  d2vga_control = 0U;
  vga_render_control = 0U;
  bus_cntl = amdgpu_mm_rreg(adev, 5384U, 0);
  if (adev->mode_info.num_crtc != 0) {
    d1vga_control = amdgpu_mm_rreg(adev, 204U, 0);
    d2vga_control = amdgpu_mm_rreg(adev, 206U, 0);
    vga_render_control = amdgpu_mm_rreg(adev, 192U, 0);
  } else {
  }
  rom_cntl = (*(adev->smc_rreg))(adev, 3227516928U);
  amdgpu_mm_wreg(adev, 5384U, bus_cntl & 4294967293U, 0);
  if (adev->mode_info.num_crtc != 0) {
    amdgpu_mm_wreg(adev, 204U, d1vga_control & 4294967038U, 0);
    amdgpu_mm_wreg(adev, 206U, d2vga_control & 4294967038U, 0);
    amdgpu_mm_wreg(adev, 192U, vga_render_control & 4294770687U, 0);
  } else {
  }
  (*(adev->smc_wreg))(adev, 3227516928U, rom_cntl | 2U);
  r = amdgpu_read_bios(adev);
  amdgpu_mm_wreg(adev, 5384U, bus_cntl, 0);
  if (adev->mode_info.num_crtc != 0) {
    amdgpu_mm_wreg(adev, 204U, d1vga_control, 0);
    amdgpu_mm_wreg(adev, 206U, d2vga_control, 0);
    amdgpu_mm_wreg(adev, 192U, vga_render_control, 0);
  } else {
  }
  (*(adev->smc_wreg))(adev, 3227516928U, rom_cntl);
  return (r);
}
}
static struct amdgpu_allowed_register_entry tonga_allowed_read_registers[1U] = { {9835U, 1, (_Bool)0}};
static struct amdgpu_allowed_register_entry cz_allowed_read_registers[5U] = { {9803U, 1, (_Bool)0},
        {9808U, 1, (_Bool)0},
        {9813U, 1, (_Bool)0},
        {9819U, 1, (_Bool)0},
        {9835U, 1, (_Bool)0}};
static struct amdgpu_allowed_register_entry vi_allowed_read_registers[56U] =
  { {8196U, 0, (_Bool)0},
        {9790U, 0, (_Bool)0},
        {2520U, 0, (_Bool)0},
        {9796U, 0, (_Bool)0},
        {9797U, 0, (_Bool)0},
        {9798U, 0, (_Bool)0},
        {9799U, 0, (_Bool)0},
        {9800U, 0, (_Bool)0},
        {9801U, 0, (_Bool)0},
        {9802U, 0, (_Bool)0},
        {9803U, 0, (_Bool)0},
        {9804U, 0, (_Bool)0},
        {9805U, 0, (_Bool)0},
        {9806U, 0, (_Bool)0},
        {9807U, 0, (_Bool)0},
        {9808U, 0, (_Bool)0},
        {9809U, 0, (_Bool)0},
        {9810U, 0, (_Bool)0},
        {9811U, 0, (_Bool)0},
        {9812U, 0, (_Bool)0},
        {9813U, 0, (_Bool)0},
        {9814U, 0, (_Bool)0},
        {9815U, 0, (_Bool)0},
        {9816U, 0, (_Bool)0},
        {9817U, 0, (_Bool)0},
        {9818U, 0, (_Bool)0},
        {9819U, 0, (_Bool)0},
        {9820U, 0, (_Bool)0},
        {9821U, 0, (_Bool)0},
        {9822U, 0, (_Bool)0},
        {9823U, 0, (_Bool)0},
        {9824U, 0, (_Bool)0},
        {9825U, 0, (_Bool)0},
        {9826U, 0, (_Bool)0},
        {9827U, 0, (_Bool)0},
        {9828U, 0, (_Bool)0},
        {9829U, 0, (_Bool)0},
        {9830U, 0, (_Bool)0},
        {9831U, 0, (_Bool)0},
        {9832U, 0, (_Bool)0},
        {9833U, 0, (_Bool)0},
        {9834U, 0, (_Bool)0},
        {9835U, 0, (_Bool)0},
        {9836U, 0, (_Bool)0},
        {9837U, 0, (_Bool)0},
        {9838U, 0, (_Bool)0},
        {9839U, 0, (_Bool)0},
        {9840U, 0, (_Bool)0},
        {9841U, 0, (_Bool)0},
        {9842U, 0, (_Bool)0},
        {9843U, 0, (_Bool)0},
        {9789U, 0, 1},
        {9951U, 0, 1},
        {9791U, 0, 0},
        {41172U, 0, 1},
        {41173U, 0, 1}};
static u32 vi_read_indexed_register(struct amdgpu_device *adev , u32 se_num , u32 sh_num ,
                                    u32 reg_offset )
{
  u32 val ;
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  if (se_num != 4294967295U || sh_num != 4294967295U) {
    gfx_v8_0_select_se_sh(adev, se_num, sh_num);
  } else {
  }
  val = amdgpu_mm_rreg(adev, reg_offset, 0);
  if (se_num != 4294967295U || sh_num != 4294967295U) {
    gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  } else {
  }
  mutex_unlock(& adev->grbm_idx_mutex);
  return (val);
}
}
static int vi_read_register(struct amdgpu_device *adev , u32 se_num , u32 sh_num ,
                            u32 reg_offset , u32 *value )
{
  struct amdgpu_allowed_register_entry *asic_register_table ;
  struct amdgpu_allowed_register_entry *asic_register_entry ;
  u32 size ;
  u32 i ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  {
  asic_register_table = (struct amdgpu_allowed_register_entry *)0;
  *value = 0U;
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  asic_register_table = (struct amdgpu_allowed_register_entry *)(& tonga_allowed_read_registers);
  size = 1U;
  goto ldv_48172;
  case 6U: ;
  case 7U:
  asic_register_table = (struct amdgpu_allowed_register_entry *)(& cz_allowed_read_registers);
  size = 5U;
  goto ldv_48172;
  default: ;
  return (-22);
  }
  ldv_48172: ;
  if ((unsigned long )asic_register_table != (unsigned long )((struct amdgpu_allowed_register_entry *)0)) {
    i = 0U;
    goto ldv_48180;
    ldv_48179:
    asic_register_entry = asic_register_table + (unsigned long )i;
    if (asic_register_entry->reg_offset != reg_offset) {
      goto ldv_48178;
    } else {
    }
    if (! asic_register_entry->untouched) {
      if ((int )asic_register_entry->grbm_indexed) {
        tmp = vi_read_indexed_register(adev, se_num, sh_num, reg_offset);
        *value = tmp;
      } else {
        tmp___0 = amdgpu_mm_rreg(adev, reg_offset, 0);
        *value = tmp___0;
      }
    } else {
    }
    return (0);
    ldv_48178:
    i = i + 1U;
    ldv_48180: ;
    if (i < size) {
      goto ldv_48179;
    } else {
    }
  } else {
  }
  i = 0U;
  goto ldv_48186;
  ldv_48185: ;
  if (vi_allowed_read_registers[i].reg_offset != reg_offset) {
    goto ldv_48184;
  } else {
  }
  if (! vi_allowed_read_registers[i].untouched) {
    if ((int )vi_allowed_read_registers[i].grbm_indexed) {
      tmp___1 = vi_read_indexed_register(adev, se_num, sh_num, reg_offset);
      *value = tmp___1;
    } else {
      tmp___2 = amdgpu_mm_rreg(adev, reg_offset, 0);
      *value = tmp___2;
    }
  } else {
  }
  return (0);
  ldv_48184:
  i = i + 1U;
  ldv_48186: ;
  if (i <= 55U) {
    goto ldv_48185;
  } else {
  }
  return (-22);
}
}
static void vi_print_gpu_status_regs(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  {
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 8194U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 8197U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE0=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 8198U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE1=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 8206U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE2=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 8207U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE3=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 13325U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_STATUS_REG   = 0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 13837U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA1_STATUS_REG   = 0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 8608U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STAT = 0x%08x\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 8605U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT1 = 0x%08x\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 8606U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT2 = 0x%08x\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 8604U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT3 = 0x%08x\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 8328U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_BUSY_STAT = 0x%08x\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 8329U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STALLED_STAT1 = 0x%08x\n",
            tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 8327U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STATUS = 0x%08x\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 8325U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_BUSY_STAT = 0x%08x\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 8326U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STALLED_STAT1 = 0x%08x\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 8324U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STATUS = 0x%08x\n", tmp___18);
  return;
}
}
u32 vi_gpu_check_soft_reset(struct amdgpu_device *adev )
{
  u32 reset_mask ;
  u32 tmp ;
  long tmp___0 ;
  {
  reset_mask = 0U;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((tmp & 1205780480U) != 0U) {
    reset_mask = reset_mask | 1U;
  } else {
  }
  if ((tmp & 805306368U) != 0U) {
    reset_mask = reset_mask | 8U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 8194U, 0);
  if ((tmp & 16777216U) != 0U) {
    reset_mask = reset_mask | 64U;
  } else {
  }
  if ((tmp & 1879048192U) != 0U) {
    reset_mask = reset_mask | 8U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  if ((tmp & 32U) != 0U) {
    reset_mask = reset_mask | 4U;
  } else {
  }
  if ((tmp & 64U) != 0U) {
    reset_mask = reset_mask | 32U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 131072U) != 0U) {
    reset_mask = reset_mask | 256U;
  } else {
  }
  if ((tmp & 16384U) != 0U) {
    reset_mask = reset_mask | 128U;
  } else {
  }
  if ((tmp & 32U) != 0U) {
    reset_mask = reset_mask | 16U;
  } else {
  }
  if ((unsigned int )adev->asic_type != 5U) {
    if ((tmp & 524290U) != 0U) {
      reset_mask = reset_mask | 4096U;
    } else {
    }
  } else {
  }
  if ((tmp & 256U) != 0U) {
    reset_mask = reset_mask | 512U;
  } else {
  }
  if ((tmp & 7680U) != 0U) {
    reset_mask = reset_mask | 1024U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 13325U, 0);
  if ((tmp & 1U) == 0U) {
    reset_mask = reset_mask | 4U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 13837U, 0);
  if ((tmp & 1U) == 0U) {
    reset_mask = reset_mask | 32U;
  } else {
  }
  if ((reset_mask & 1024U) != 0U) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("vi_gpu_check_soft_reset", "MC busy: 0x%08X, clearing.\n",
                          reset_mask);
    } else {
    }
    reset_mask = reset_mask & 4294966271U;
  } else {
  }
  return (reset_mask);
}
}
static void vi_gpu_soft_reset(struct amdgpu_device *adev , u32 reset_mask )
{
  struct amdgpu_mode_mc_save save ;
  u32 grbm_soft_reset ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  int tmp___2 ;
  {
  grbm_soft_reset = 0U;
  srbm_soft_reset = 0U;
  if (reset_mask == 0U) {
    return;
  } else {
  }
  _dev_info((struct device const *)adev->dev, "GPU softreset: 0x%08X\n", reset_mask);
  vi_print_gpu_status_regs(adev);
  tmp___0 = amdgpu_mm_rreg(adev, 1343U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
            tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 1335U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
            tmp___1);
  tmp = amdgpu_mm_rreg(adev, 8630U, 0);
  tmp = tmp | 268435456U;
  tmp = tmp | 67108864U;
  tmp = tmp | 16777216U;
  amdgpu_mm_wreg(adev, 8630U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 8333U, 0);
  tmp = tmp | 1073741824U;
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, 8333U, tmp, 0);
  if ((reset_mask & 4U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13330U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13330U, tmp, 0);
  } else {
  }
  if ((reset_mask & 32U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13842U, 0);
    tmp = tmp | 1U;
    amdgpu_mm_wreg(adev, 13842U, tmp, 0);
  } else {
  }
  gmc_v8_0_mc_stop(adev, & save);
  tmp___2 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___2 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  if ((reset_mask & 11U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    grbm_soft_reset = grbm_soft_reset | 65536U;
  } else {
  }
  if ((reset_mask & 8U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if ((reset_mask & 4U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1048576U;
  } else {
  }
  if ((reset_mask & 32U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 64U;
  } else {
  }
  if ((reset_mask & 2048U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 32U;
  } else {
  }
  if ((reset_mask & 64U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 4U;
  } else {
  }
  if ((reset_mask & 128U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 32768U;
  } else {
  }
  if ((reset_mask & 256U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if ((reset_mask & 16U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if ((reset_mask & 512U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 131072U;
  } else {
  }
  if ((reset_mask & 4096U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 262144U;
  } else {
  }
  if ((reset_mask & 8192U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 16777216U;
  } else {
  }
  if ((reset_mask & 8192U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 2147483648U;
  } else {
  }
  if ((adev->flags & 131072UL) == 0UL) {
    if ((reset_mask & 1024U) != 0U) {
      srbm_soft_reset = srbm_soft_reset | 2048U;
    } else {
    }
  } else {
  }
  if (grbm_soft_reset != 0U) {
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    tmp = tmp | grbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 8200U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    __const_udelay(214750UL);
    tmp = ~ grbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 8200U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 8200U, 0);
  } else {
  }
  if (srbm_soft_reset != 0U) {
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
  } else {
  }
  __const_udelay(214750UL);
  gmc_v8_0_mc_resume(adev, & save);
  __const_udelay(214750UL);
  vi_print_gpu_status_regs(adev);
  return;
}
}
static void vi_gpu_pci_config_reset(struct amdgpu_device *adev )
{
  struct amdgpu_mode_mc_save save ;
  u32 tmp ;
  u32 i ;
  int tmp___0 ;
  u32 tmp___1 ;
  {
  _dev_info((struct device const *)adev->dev, "GPU pci config reset\n");
  tmp = amdgpu_mm_rreg(adev, 8630U, 0);
  tmp = tmp | 268435456U;
  tmp = tmp | 67108864U;
  tmp = tmp | 16777216U;
  amdgpu_mm_wreg(adev, 8630U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 8333U, 0);
  tmp = tmp | 1073741824U;
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, 8333U, tmp, 0);
  amdgpu_mm_wreg(adev, 8630U, 352321536U, 0);
  amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
  tmp = amdgpu_mm_rreg(adev, 13330U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 13330U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 13842U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 13842U, tmp, 0);
  __const_udelay(214750UL);
  gmc_v8_0_mc_stop(adev, & save);
  tmp___0 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___0 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timed out !\n");
  } else {
  }
  pci_clear_master(adev->pdev);
  amdgpu_pci_config_reset(adev);
  __const_udelay(429500UL);
  i = 0U;
  goto ldv_48213;
  ldv_48212:
  tmp___1 = amdgpu_mm_rreg(adev, 5386U, 0);
  if (tmp___1 != 4294967295U) {
    goto ldv_48211;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_48213: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_48212;
  } else {
  }
  ldv_48211: ;
  return;
}
}
static void vi_set_bios_scratch_engine_hung(struct amdgpu_device *adev , bool hung )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 1484U, 0);
  tmp = tmp___0;
  if ((int )hung) {
    tmp = tmp | 536870912U;
  } else {
    tmp = tmp & 3758096383U;
  }
  amdgpu_mm_wreg(adev, 1484U, tmp, 0);
  return;
}
}
static int vi_asic_reset(struct amdgpu_device *adev )
{
  u32 reset_mask ;
  {
  reset_mask = vi_gpu_check_soft_reset(adev);
  if (reset_mask != 0U) {
    vi_set_bios_scratch_engine_hung(adev, 1);
  } else {
  }
  vi_gpu_soft_reset(adev, reset_mask);
  reset_mask = vi_gpu_check_soft_reset(adev);
  if (reset_mask != 0U && amdgpu_hard_reset != 0) {
    vi_gpu_pci_config_reset(adev);
  } else {
  }
  reset_mask = vi_gpu_check_soft_reset(adev);
  if (reset_mask == 0U) {
    vi_set_bios_scratch_engine_hung(adev, 0);
  } else {
  }
  return (0);
}
}
static int vi_set_uvd_clock(struct amdgpu_device *adev , u32 clock , u32 cntl_reg ,
                            u32 status_reg )
{
  int r ;
  int i ;
  struct atom_clock_dividers dividers ;
  u32 tmp ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  {
  r = amdgpu_atombios_get_clock_dividers(adev, 0, clock, 0, & dividers);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp = (*(adev->smc_rreg))(adev, cntl_reg);
  tmp = tmp & 4294966912U;
  tmp = dividers.post_divider | tmp;
  (*(adev->smc_wreg))(adev, cntl_reg, tmp);
  i = 0;
  goto ldv_48239;
  ldv_48238:
  tmp___0 = (*(adev->smc_rreg))(adev, status_reg);
  if ((int )tmp___0 & 1) {
    goto ldv_48233;
  } else {
  }
  __ms = 10UL;
  goto ldv_48236;
  ldv_48235:
  __const_udelay(4295000UL);
  ldv_48236:
  tmp___1 = __ms;
  __ms = __ms - 1UL;
  if (tmp___1 != 0UL) {
    goto ldv_48235;
  } else {
  }
  i = i + 1;
  ldv_48239: ;
  if (i <= 99) {
    goto ldv_48238;
  } else {
  }
  ldv_48233: ;
  if (i == 100) {
    return (-110);
  } else {
  }
  return (0);
}
}
static int vi_set_uvd_clocks(struct amdgpu_device *adev , u32 vclk , u32 dclk )
{
  int r ;
  {
  r = vi_set_uvd_clock(adev, vclk, 3226468516U, 3226468520U);
  if (r != 0) {
    return (r);
  } else {
  }
  r = vi_set_uvd_clock(adev, dclk, 3226468508U, 3226468512U);
  return (0);
}
}
static int vi_set_vce_clocks(struct amdgpu_device *adev , u32 evclk , u32 ecclk )
{
  {
  return (0);
}
}
static void vi_pcie_gen3_enable(struct amdgpu_device *adev )
{
  u32 mask ;
  int ret ;
  {
  if (amdgpu_pcie_gen2 == 0) {
    return;
  } else {
  }
  if ((adev->flags & 131072UL) != 0UL) {
    return;
  } else {
  }
  ret = drm_pcie_get_speed_cap_mask(adev->ddev, & mask);
  if (ret != 0) {
    return;
  } else {
  }
  if ((mask & 6U) == 0U) {
    return;
  } else {
  }
  return;
}
}
static void vi_program_aspm(struct amdgpu_device *adev )
{
  {
  if (amdgpu_aspm == 0) {
    return;
  } else {
  }
  return;
}
}
static void vi_enable_doorbell_aperture(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  {
  if ((adev->flags & 131072UL) != 0UL) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 5377U, 0);
  if ((int )enable) {
    tmp = tmp | 1U;
  } else {
    tmp = tmp & 4294967294U;
  }
  amdgpu_mm_wreg(adev, 5377U, tmp, 0);
  return;
}
}
static struct amdgpu_ip_block_version const topaz_ip_blocks[6U] = { {0, 2U, 0U, 0U, & vi_common_ip_funcs},
        {1, 8U, 0U, 0U, & gmc_v8_0_ip_funcs},
        {2, 2U, 4U, 0U, & iceland_ih_ip_funcs},
        {3, 7U, 1U, 0U, & iceland_dpm_ip_funcs},
        {5, 8U, 0U, 0U, & gfx_v8_0_ip_funcs},
        {6, 2U, 4U, 0U, & sdma_v2_4_ip_funcs}};
static struct amdgpu_ip_block_version const tonga_ip_blocks[9U] =
  { {0, 2U, 0U, 0U, & vi_common_ip_funcs},
        {1, 8U, 0U, 0U, & gmc_v8_0_ip_funcs},
        {2, 3U, 0U, 0U, & tonga_ih_ip_funcs},
        {3, 7U, 1U, 0U, & tonga_dpm_ip_funcs},
        {4, 10U, 0U, 0U, & dce_v10_0_ip_funcs},
        {5, 8U, 0U, 0U, & gfx_v8_0_ip_funcs},
        {6, 3U, 0U, 0U, & sdma_v3_0_ip_funcs},
        {7, 5U, 0U, 0U, & uvd_v5_0_ip_funcs},
        {8, 3U, 0U, 0U, & vce_v3_0_ip_funcs}};
static struct amdgpu_ip_block_version const cz_ip_blocks[9U] =
  { {0, 2U, 0U, 0U, & vi_common_ip_funcs},
        {1, 8U, 0U, 0U, & gmc_v8_0_ip_funcs},
        {2, 3U, 0U, 0U, & cz_ih_ip_funcs},
        {3, 8U, 0U, 0U, & cz_dpm_ip_funcs},
        {4, 11U, 0U, 0U, & dce_v11_0_ip_funcs},
        {5, 8U, 0U, 0U, & gfx_v8_0_ip_funcs},
        {6, 3U, 0U, 0U, & sdma_v3_0_ip_funcs},
        {7, 6U, 0U, 0U, & uvd_v6_0_ip_funcs},
        {8, 3U, 0U, 0U, & vce_v3_0_ip_funcs}};
int vi_set_ip_blocks(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& topaz_ip_blocks);
  adev->num_ip_blocks = 6;
  goto ldv_48273;
  case 6U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& tonga_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_48273;
  case 7U:
  adev->ip_blocks = (struct amdgpu_ip_block_version const *)(& cz_ip_blocks);
  adev->num_ip_blocks = 9;
  goto ldv_48273;
  default: ;
  return (-22);
  }
  ldv_48273: ;
  return (0);
}
}
static u32 vi_get_rev_id(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  if ((unsigned int )adev->asic_type == 5U) {
    tmp = amdgpu_mm_rreg(adev, 4035U, 0);
    return (tmp >> 28);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 5465U, 0);
    return (tmp___0 >> 28);
  }
}
}
static struct amdgpu_asic_funcs const vi_asic_funcs =
     {& vi_read_disabled_bios, & vi_read_register, & vi_vga_set_state, & vi_asic_reset,
    & gmc_v8_0_mc_wait_for_idle, & vi_get_xclk, & gfx_v8_0_get_gpu_clock_counter,
    & gfx_v8_0_get_cu_info, & vi_set_uvd_clocks, & vi_set_vce_clocks};
static int vi_common_early_init(void *handle )
{
  bool smc_enabled ;
  struct amdgpu_device *adev ;
  struct amdgpu_ip_block_version const *tmp ;
  {
  smc_enabled = 0;
  adev = (struct amdgpu_device *)handle;
  adev->smc_rreg = & vi_smc_rreg;
  adev->smc_wreg = & vi_smc_wreg;
  adev->pcie_rreg = & vi_pcie_rreg;
  adev->pcie_wreg = & vi_pcie_wreg;
  adev->uvd_ctx_rreg = & vi_uvd_ctx_rreg;
  adev->uvd_ctx_wreg = & vi_uvd_ctx_wreg;
  adev->didt_rreg = & vi_didt_rreg;
  adev->didt_wreg = & vi_didt_wreg;
  adev->asic_funcs = & vi_asic_funcs;
  tmp = amdgpu_get_ip_block(adev, 3);
  if ((unsigned long )tmp != (unsigned long )((struct amdgpu_ip_block_version const *)0) && (amdgpu_ip_block_mask & 8U) != 0U) {
    smc_enabled = 1;
  } else {
  }
  adev->rev_id = vi_get_rev_id(adev);
  adev->external_rev_id = 255U;
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  adev->has_uvd = 0;
  adev->cg_flags = 0U;
  adev->pg_flags = 0U;
  adev->external_rev_id = 1U;
  if (amdgpu_smc_load_fw != 0 && (int )smc_enabled) {
    adev->firmware.smu_load = 1;
  } else {
  }
  goto ldv_48291;
  case 6U:
  adev->has_uvd = 1;
  adev->cg_flags = 0U;
  adev->pg_flags = 0U;
  adev->external_rev_id = adev->rev_id + 20U;
  if (amdgpu_smc_load_fw != 0 && (int )smc_enabled) {
    adev->firmware.smu_load = 1;
  } else {
  }
  goto ldv_48291;
  case 7U:
  adev->has_uvd = 1;
  adev->cg_flags = 0U;
  adev->pg_flags = 24U;
  adev->external_rev_id = adev->rev_id + 1U;
  if (amdgpu_smc_load_fw != 0 && (int )smc_enabled) {
    adev->firmware.smu_load = 1;
  } else {
  }
  goto ldv_48291;
  default: ;
  return (-22);
  }
  ldv_48291: ;
  return (0);
}
}
static int vi_common_sw_init(void *handle )
{
  {
  return (0);
}
}
static int vi_common_sw_fini(void *handle )
{
  {
  return (0);
}
}
static int vi_common_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  vi_init_golden_registers(adev);
  vi_pcie_gen3_enable(adev);
  vi_program_aspm(adev);
  vi_enable_doorbell_aperture(adev, 1);
  return (0);
}
}
static int vi_common_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  vi_enable_doorbell_aperture(adev, 0);
  return (0);
}
}
static int vi_common_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = vi_common_hw_fini((void *)adev);
  return (tmp);
}
}
static int vi_common_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = vi_common_hw_init((void *)adev);
  return (tmp);
}
}
static bool vi_common_is_idle(void *handle )
{
  {
  return (1);
}
}
static int vi_common_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void vi_common_print_status(void *handle )
{
  {
  return;
}
}
static int vi_common_soft_reset(void *handle )
{
  {
  return (0);
}
}
static int vi_common_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int vi_common_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const vi_common_ip_funcs =
     {& vi_common_early_init, (int (*)(void * ))0, & vi_common_sw_init, & vi_common_sw_fini,
    & vi_common_hw_init, & vi_common_hw_fini, & vi_common_suspend, & vi_common_resume,
    & vi_common_is_idle, & vi_common_wait_for_idle, & vi_common_soft_reset, & vi_common_print_status,
    & vi_common_set_clockgating_state, & vi_common_set_powergating_state};
int ldv_retval_5 ;
extern int ldv_probe_78(void) ;
extern int ldv_probe_79(void) ;
int ldv_retval_6 ;
extern int ldv_release_79(void) ;
extern int ldv_release_78(void) ;
void ldv_initialize_amdgpu_asic_funcs_79(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  vi_asic_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_78(void)
{
  void *ldvarg130 ;
  void *tmp ;
  void *ldvarg125 ;
  void *tmp___0 ;
  void *ldvarg135 ;
  void *tmp___1 ;
  void *ldvarg129 ;
  void *tmp___2 ;
  void *ldvarg138 ;
  void *tmp___3 ;
  void *ldvarg136 ;
  void *tmp___4 ;
  void *ldvarg126 ;
  void *tmp___5 ;
  enum amd_powergating_state ldvarg132 ;
  enum amd_clockgating_state ldvarg128 ;
  void *ldvarg133 ;
  void *tmp___6 ;
  void *ldvarg124 ;
  void *tmp___7 ;
  void *ldvarg134 ;
  void *tmp___8 ;
  void *ldvarg137 ;
  void *tmp___9 ;
  void *ldvarg127 ;
  void *tmp___10 ;
  void *ldvarg131 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg130 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg125 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg135 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg129 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg138 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg136 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg126 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg133 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg124 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg134 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg137 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg127 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg131 = tmp___11;
  ldv_memset((void *)(& ldvarg132), 0, 4UL);
  ldv_memset((void *)(& ldvarg128), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_hw_fini(ldvarg138);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_hw_fini(ldvarg138);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_hw_fini(ldvarg138);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 1: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_print_status(ldvarg137);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_print_status(ldvarg137);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_print_status(ldvarg137);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 2: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_early_init(ldvarg136);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_early_init(ldvarg136);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_early_init(ldvarg136);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 3: ;
  if (ldv_state_variable_78 == 2) {
    ldv_retval_6 = vi_common_suspend(ldvarg135);
    if (ldv_retval_6 == 0) {
      ldv_state_variable_78 = 3;
    } else {
    }
  } else {
  }
  goto ldv_48370;
  case 4: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_sw_init(ldvarg134);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_sw_init(ldvarg134);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_sw_init(ldvarg134);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 5: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_set_powergating_state(ldvarg133, ldvarg132);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_set_powergating_state(ldvarg133, ldvarg132);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_set_powergating_state(ldvarg133, ldvarg132);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 6: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_wait_for_idle(ldvarg131);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_wait_for_idle(ldvarg131);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_wait_for_idle(ldvarg131);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 7: ;
  if (ldv_state_variable_78 == 3) {
    ldv_retval_5 = vi_common_resume(ldvarg130);
    if (ldv_retval_5 == 0) {
      ldv_state_variable_78 = 2;
    } else {
    }
  } else {
  }
  goto ldv_48370;
  case 8: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_set_clockgating_state(ldvarg129, ldvarg128);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_set_clockgating_state(ldvarg129, ldvarg128);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_set_clockgating_state(ldvarg129, ldvarg128);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 9: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_hw_init(ldvarg127);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_hw_init(ldvarg127);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_hw_init(ldvarg127);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 10: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_soft_reset(ldvarg126);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_soft_reset(ldvarg126);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_soft_reset(ldvarg126);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 11: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_sw_fini(ldvarg125);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_sw_fini(ldvarg125);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_sw_fini(ldvarg125);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 12: ;
  if (ldv_state_variable_78 == 1) {
    vi_common_is_idle(ldvarg124);
    ldv_state_variable_78 = 1;
  } else {
  }
  if (ldv_state_variable_78 == 3) {
    vi_common_is_idle(ldvarg124);
    ldv_state_variable_78 = 3;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    vi_common_is_idle(ldvarg124);
    ldv_state_variable_78 = 2;
  } else {
  }
  goto ldv_48370;
  case 13: ;
  if (ldv_state_variable_78 == 3) {
    ldv_release_78();
    ldv_state_variable_78 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_78 == 2) {
    ldv_release_78();
    ldv_state_variable_78 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48370;
  case 14: ;
  if (ldv_state_variable_78 == 1) {
    ldv_probe_78();
    ldv_state_variable_78 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48370;
  default:
  ldv_stop();
  }
  ldv_48370: ;
  return;
}
}
void ldv_main_exported_79(void)
{
  u32 ldvarg415 ;
  u32 ldvarg414 ;
  bool ldvarg411 ;
  u32 ldvarg419 ;
  struct amdgpu_cu_info *ldvarg410 ;
  void *tmp ;
  u32 ldvarg413 ;
  u32 ldvarg416 ;
  u32 *ldvarg417 ;
  void *tmp___0 ;
  u32 ldvarg412 ;
  u32 ldvarg418 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(72UL);
  ldvarg410 = (struct amdgpu_cu_info *)tmp;
  tmp___0 = ldv_init_zalloc(4UL);
  ldvarg417 = (u32 *)tmp___0;
  ldv_memset((void *)(& ldvarg415), 0, 4UL);
  ldv_memset((void *)(& ldvarg414), 0, 4UL);
  ldv_memset((void *)(& ldvarg411), 0, 1UL);
  ldv_memset((void *)(& ldvarg419), 0, 4UL);
  ldv_memset((void *)(& ldvarg413), 0, 4UL);
  ldv_memset((void *)(& ldvarg416), 0, 4UL);
  ldv_memset((void *)(& ldvarg412), 0, 4UL);
  ldv_memset((void *)(& ldvarg418), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_79 == 1) {
    gfx_v8_0_get_gpu_clock_counter(vi_asic_funcs_group0);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    gfx_v8_0_get_gpu_clock_counter(vi_asic_funcs_group0);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 1: ;
  if (ldv_state_variable_79 == 1) {
    vi_asic_reset(vi_asic_funcs_group0);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_asic_reset(vi_asic_funcs_group0);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 2: ;
  if (ldv_state_variable_79 == 2) {
    vi_read_disabled_bios(vi_asic_funcs_group0);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 3: ;
  if (ldv_state_variable_79 == 1) {
    gmc_v8_0_mc_wait_for_idle(vi_asic_funcs_group0);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    gmc_v8_0_mc_wait_for_idle(vi_asic_funcs_group0);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 4: ;
  if (ldv_state_variable_79 == 1) {
    vi_set_uvd_clocks(vi_asic_funcs_group0, ldvarg419, ldvarg418);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_set_uvd_clocks(vi_asic_funcs_group0, ldvarg419, ldvarg418);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 5: ;
  if (ldv_state_variable_79 == 1) {
    vi_read_register(vi_asic_funcs_group0, ldvarg415, ldvarg414, ldvarg416, ldvarg417);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_read_register(vi_asic_funcs_group0, ldvarg415, ldvarg414, ldvarg416, ldvarg417);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 6: ;
  if (ldv_state_variable_79 == 1) {
    vi_set_vce_clocks(vi_asic_funcs_group0, ldvarg413, ldvarg412);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_set_vce_clocks(vi_asic_funcs_group0, ldvarg413, ldvarg412);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 7: ;
  if (ldv_state_variable_79 == 1) {
    vi_vga_set_state(vi_asic_funcs_group0, (int )ldvarg411);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_vga_set_state(vi_asic_funcs_group0, (int )ldvarg411);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 8: ;
  if (ldv_state_variable_79 == 1) {
    gfx_v8_0_get_cu_info(vi_asic_funcs_group0, ldvarg410);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    gfx_v8_0_get_cu_info(vi_asic_funcs_group0, ldvarg410);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 9: ;
  if (ldv_state_variable_79 == 1) {
    vi_get_xclk(vi_asic_funcs_group0);
    ldv_state_variable_79 = 1;
  } else {
  }
  if (ldv_state_variable_79 == 2) {
    vi_get_xclk(vi_asic_funcs_group0);
    ldv_state_variable_79 = 2;
  } else {
  }
  goto ldv_48400;
  case 10: ;
  if (ldv_state_variable_79 == 2) {
    ldv_release_79();
    ldv_state_variable_79 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48400;
  case 11: ;
  if (ldv_state_variable_79 == 1) {
    ldv_probe_79();
    ldv_state_variable_79 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48400;
  default:
  ldv_stop();
  }
  ldv_48400: ;
  return;
}
}
bool ldv_queue_work_on_693(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_694(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_695(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_696(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_697(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_707(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_709(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_708(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_711(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_710(struct workqueue_struct *ldv_func_arg1 ) ;
static void gmc_v8_0_set_gart_funcs(struct amdgpu_device *adev ) ;
static void gmc_v8_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const golden_settings_tonga_a11[21U] =
  { 2529U, 3U, 0U, 2120U,
        127U, 40U, 2165U, 32694U,
        2449U, 1324U, 268435455U, 268435455U,
        1325U, 268435455U, 268435455U, 1326U,
        268435455U, 268435455U, 1327U, 268435455U,
        268435455U};
static u32 const tonga_mgcg_cgcg_init___0[3U] = { 2090U, 4294967295U, 260U};
static u32 const golden_settings_iceland_a11[12U] =
  { 1324U, 268435455U, 268435455U, 1325U,
        268435455U, 268435455U, 1326U, 268435455U,
        268435455U, 1327U, 268435455U, 268435455U};
static u32 const iceland_mgcg_cgcg_init___0[3U] = { 2090U, 4294967295U, 260U};
static u32 const cz_mgcg_cgcg_init___0[3U] = { 2090U, 4294967295U, 260U};
static void gmc_v8_0_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& iceland_mgcg_cgcg_init___0),
                                   3U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_iceland_a11),
                                   12U);
  goto ldv_43705;
  case 6U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_mgcg_cgcg_init___0),
                                   3U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_tonga_a11),
                                   21U);
  goto ldv_43705;
  case 7U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_mgcg_cgcg_init___0),
                                   3U);
  goto ldv_43705;
  default: ;
  goto ldv_43705;
  }
  ldv_43705: ;
  return;
}
}
int gmc_v8_0_mc_wait_for_idle(struct amdgpu_device *adev )
{
  unsigned int i ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  i = 0U;
  goto ldv_43721;
  ldv_43720:
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0 & 16128U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43721: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43720;
  } else {
  }
  return (-1);
}
}
void gmc_v8_0_mc_stop(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 blackout ;
  {
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->stop_mc_access))(adev, save);
  } else {
  }
  (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  blackout = amdgpu_mm_rreg(adev, 2091U, 0);
  if ((blackout & 7U) != 1U) {
    amdgpu_mm_wreg(adev, 5412U, 0U, 0);
    blackout = (blackout & 4294967288U) | 1U;
    amdgpu_mm_wreg(adev, 2091U, blackout, 0);
  } else {
  }
  __const_udelay(429500UL);
  return;
}
}
void gmc_v8_0_mc_resume(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 2091U, 0);
  tmp = tmp & 4294967288U;
  amdgpu_mm_wreg(adev, 2091U, tmp, 0);
  tmp = 1U;
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, 5412U, tmp, 0);
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->resume_mc_access))(adev, save);
  } else {
  }
  return;
}
}
static int gmc_v8_0_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gmc_v8_0_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  chip_name = "topaz";
  goto ldv_43741;
  case 6U:
  chip_name = "tonga";
  goto ldv_43741;
  case 7U: ;
  return (0);
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c"),
                       "i" (207), "i" (12UL));
  ldv_43745: ;
  goto ldv_43745;
  }
  ldv_43741:
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_mc.bin", chip_name);
  err = request_firmware(& adev->mc.fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->mc.fw);
  out: ;
  if (err != 0) {
    printk("\vmc: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    release_firmware(adev->mc.fw);
    adev->mc.fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static int gmc_v8_0_mc_load_microcode(struct amdgpu_device *adev )
{
  struct mc_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  __le32 const *io_mc_regs ;
  u32 running ;
  u32 blackout ;
  int i ;
  int ucode_size ;
  int regs_size ;
  u32 tmp ;
  __le32 const *tmp___0 ;
  __u32 tmp___1 ;
  __le32 const *tmp___2 ;
  __u32 tmp___3 ;
  __le32 const *tmp___4 ;
  __u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  {
  fw_data = (__le32 const *)0U;
  io_mc_regs = (__le32 const *)0U;
  blackout = 0U;
  if ((unsigned long )adev->mc.fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct mc_firmware_header_v1_0 const *)(adev->mc.fw)->data;
  amdgpu_ucode_print_mc_hdr(& hdr->header);
  adev->mc.fw_version = hdr->header.ucode_version;
  regs_size = (int )((unsigned int )hdr->io_debug_size_bytes / 8U);
  io_mc_regs = (__le32 const *)(adev->mc.fw)->data + (unsigned long )hdr->io_debug_array_offset_bytes;
  ucode_size = (int )((unsigned int )hdr->header.ucode_size_bytes / 4U);
  fw_data = (__le32 const *)(adev->mc.fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  tmp = amdgpu_mm_rreg(adev, 2610U, 0);
  running = tmp & 1U;
  if (running == 0U) {
    if (running != 0U) {
      blackout = amdgpu_mm_rreg(adev, 2091U, 0);
      amdgpu_mm_wreg(adev, 2091U, blackout | 1U, 0);
    } else {
    }
    amdgpu_mm_wreg(adev, 2610U, 8U, 0);
    amdgpu_mm_wreg(adev, 2610U, 16U, 0);
    i = 0;
    goto ldv_43759;
    ldv_43758:
    tmp___0 = io_mc_regs;
    io_mc_regs = io_mc_regs + 1;
    tmp___1 = __le32_to_cpup(tmp___0);
    amdgpu_mm_wreg(adev, 2705U, tmp___1, 0);
    tmp___2 = io_mc_regs;
    io_mc_regs = io_mc_regs + 1;
    tmp___3 = __le32_to_cpup(tmp___2);
    amdgpu_mm_wreg(adev, 2706U, tmp___3, 0);
    i = i + 1;
    ldv_43759: ;
    if (i < regs_size) {
      goto ldv_43758;
    } else {
    }
    i = 0;
    goto ldv_43762;
    ldv_43761:
    tmp___4 = fw_data;
    fw_data = fw_data + 1;
    tmp___5 = __le32_to_cpup(tmp___4);
    amdgpu_mm_wreg(adev, 2611U, tmp___5, 0);
    i = i + 1;
    ldv_43762: ;
    if (i < ucode_size) {
      goto ldv_43761;
    } else {
    }
    amdgpu_mm_wreg(adev, 2610U, 8U, 0);
    amdgpu_mm_wreg(adev, 2610U, 4U, 0);
    amdgpu_mm_wreg(adev, 2610U, 1U, 0);
    i = 0;
    goto ldv_43766;
    ldv_43765:
    tmp___6 = amdgpu_mm_rreg(adev, 2618U, 0);
    if ((tmp___6 & 1073741824U) >> 30 != 0U) {
      goto ldv_43764;
    } else {
    }
    __const_udelay(4295UL);
    i = i + 1;
    ldv_43766: ;
    if (adev->usec_timeout > i) {
      goto ldv_43765;
    } else {
    }
    ldv_43764:
    i = 0;
    goto ldv_43769;
    ldv_43768:
    tmp___7 = amdgpu_mm_rreg(adev, 2618U, 0);
    if ((int )tmp___7 < 0) {
      goto ldv_43767;
    } else {
    }
    __const_udelay(4295UL);
    i = i + 1;
    ldv_43769: ;
    if (adev->usec_timeout > i) {
      goto ldv_43768;
    } else {
    }
    ldv_43767: ;
    if (running != 0U) {
      amdgpu_mm_wreg(adev, 2091U, blackout, 0);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void gmc_v8_0_vram_gtt_location(struct amdgpu_device *adev , struct amdgpu_mc *mc )
{
  {
  if (mc->mc_vram_size > 1098437885952ULL) {
    dev_warn((struct device const *)adev->dev, "limiting VRAM\n");
    mc->real_vram_size = 1098437885952ULL;
    mc->mc_vram_size = 1098437885952ULL;
  } else {
  }
  amdgpu_vram_location(adev, & adev->mc, 0ULL);
  adev->mc.gtt_base_align = 0ULL;
  amdgpu_gtt_location(adev, mc);
  return;
}
}
static void gmc_v8_0_mc_program(struct amdgpu_device *adev )
{
  struct amdgpu_mode_mc_save save ;
  u32 tmp ;
  int i ;
  int j ;
  int tmp___0 ;
  int tmp___1 ;
  {
  i = 0;
  j = 0;
  goto ldv_43782;
  ldv_43781:
  amdgpu_mm_wreg(adev, (u32 )(j + 2821), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2822), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2823), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2824), 0U, 0);
  amdgpu_mm_wreg(adev, (u32 )(j + 2825), 0U, 0);
  i = i + 1;
  j = j + 6;
  ldv_43782: ;
  if (i <= 31) {
    goto ldv_43781;
  } else {
  }
  amdgpu_mm_wreg(adev, 5416U, 0U, 0);
  if (adev->mode_info.num_crtc != 0) {
    (*((adev->mode_info.funcs)->set_vga_render_state))(adev, 0);
  } else {
  }
  gmc_v8_0_mc_stop(adev, & save);
  tmp___0 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___0 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  amdgpu_mm_wreg(adev, 2061U, (u32 )(adev->mc.vram_start >> 12), 0);
  amdgpu_mm_wreg(adev, 2062U, (u32 )(adev->mc.vram_end >> 12), 0);
  amdgpu_mm_wreg(adev, 2063U, (u32 )(adev->vram_scratch.gpu_addr >> 12), 0);
  tmp = (u32 )(adev->mc.vram_end >> 24) << 16U;
  tmp = ((u32 )(adev->mc.vram_start >> 24) & 65535U) | tmp;
  amdgpu_mm_wreg(adev, 2057U, tmp, 0);
  amdgpu_mm_wreg(adev, 2817U, (u32 )(adev->mc.vram_start >> 8), 0);
  amdgpu_mm_wreg(adev, 2818U, 1073742080U, 0);
  amdgpu_mm_wreg(adev, 2819U, 1073741823U, 0);
  amdgpu_mm_wreg(adev, 2060U, 0U, 0);
  amdgpu_mm_wreg(adev, 2058U, 268435455U, 0);
  amdgpu_mm_wreg(adev, 2059U, 268435455U, 0);
  tmp___1 = (*((adev->asic_funcs)->wait_for_mc_idle))(adev);
  if (tmp___1 != 0) {
    dev_warn((struct device const *)adev->dev, "Wait for MC idle timedout !\n");
  } else {
  }
  gmc_v8_0_mc_resume(adev, & save);
  amdgpu_mm_wreg(adev, 5412U, 3U, 0);
  tmp = amdgpu_mm_rreg(adev, 3027U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 3027U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 2816U, 0);
  amdgpu_mm_wreg(adev, 2816U, tmp, 0);
  return;
}
}
static int gmc_v8_0_mc_init(struct amdgpu_device *adev )
{
  u32 tmp ;
  int chansize ;
  int numchan ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  unsigned long long _max1 ;
  u64 _max2 ;
  {
  tmp = amdgpu_mm_rreg(adev, 2520U, 0);
  if ((tmp & 256U) >> 8 != 0U) {
    chansize = 64;
  } else {
    chansize = 32;
  }
  tmp = amdgpu_mm_rreg(adev, 2049U, 0);
  switch ((tmp & 61440U) >> 12) {
  case 0U: ;
  default:
  numchan = 1;
  goto ldv_43792;
  case 1U:
  numchan = 2;
  goto ldv_43792;
  case 2U:
  numchan = 4;
  goto ldv_43792;
  case 3U:
  numchan = 8;
  goto ldv_43792;
  case 4U:
  numchan = 3;
  goto ldv_43792;
  case 5U:
  numchan = 6;
  goto ldv_43792;
  case 6U:
  numchan = 10;
  goto ldv_43792;
  case 7U:
  numchan = 12;
  goto ldv_43792;
  case 8U:
  numchan = 16;
  goto ldv_43792;
  }
  ldv_43792:
  adev->mc.vram_width = (unsigned int )(numchan * chansize);
  adev->mc.aper_base = (adev->pdev)->resource[0].start;
  adev->mc.aper_size = (adev->pdev)->resource[0].start != 0ULL || (adev->pdev)->resource[0].end != (adev->pdev)->resource[0].start ? ((adev->pdev)->resource[0].end - (adev->pdev)->resource[0].start) + 1ULL : 0ULL;
  tmp___0 = amdgpu_mm_rreg(adev, 5386U, 0);
  adev->mc.mc_vram_size = (unsigned long long )tmp___0 * 1048576ULL;
  tmp___1 = amdgpu_mm_rreg(adev, 5386U, 0);
  adev->mc.real_vram_size = (unsigned long long )tmp___1 * 1048576ULL;
  adev->mc.visible_vram_size = adev->mc.aper_size;
  if (amdgpu_gart_size == -1) {
    _max1 = 1073741824ULL;
    _max2 = adev->mc.mc_vram_size;
    adev->mc.gtt_size = _max1 > _max2 ? _max1 : _max2;
  } else {
    adev->mc.gtt_size = (unsigned long long )amdgpu_gart_size << 20;
  }
  gmc_v8_0_vram_gtt_location(adev, & adev->mc);
  return (0);
}
}
static void gmc_v8_0_gart_flush_gpu_tlb(struct amdgpu_device *adev , u32 vmid )
{
  {
  amdgpu_mm_wreg(adev, 5408U, 0U, 0);
  amdgpu_mm_wreg(adev, 1310U, (u32 )(1 << (int )vmid), 0);
  return;
}
}
static int gmc_v8_0_gart_set_pte_pde(struct amdgpu_device *adev , void *cpu_pt_addr ,
                                     u32 gpu_page_idx , uint64_t addr , u32 flags )
{
  void *ptr ;
  uint64_t value ;
  {
  ptr = cpu_pt_addr;
  value = addr & 1099511623680ULL;
  value = (uint64_t )flags | value;
  writeq((unsigned long )value, (void volatile *)ptr + (unsigned long )(gpu_page_idx * 8U));
  return (0);
}
}
static int gmc_v8_0_gart_enable(struct amdgpu_device *adev )
{
  int r ;
  int i ;
  u32 tmp ;
  {
  if ((unsigned long )adev->gart.robj == (unsigned long )((struct amdgpu_bo *)0)) {
    dev_err((struct device const *)adev->dev, "No VRAM object for PCIE GART.\n");
    return (-22);
  } else {
  }
  r = amdgpu_gart_table_vram_pin(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 2073U, 0);
  tmp = tmp | 1U;
  tmp = tmp | 2U;
  tmp = tmp | 24U;
  tmp = tmp | 64U;
  tmp = tmp & 4294967263U;
  amdgpu_mm_wreg(adev, 2073U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1280U, 0);
  tmp = tmp | 1U;
  tmp = tmp | 2U;
  tmp = tmp | 512U;
  tmp = tmp | 1024U;
  tmp = tmp | 229376U;
  tmp = (tmp & 4293394431U) | 524288U;
  amdgpu_mm_wreg(adev, 1280U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1281U, 0);
  tmp = tmp | 1U;
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, 1281U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1282U, 0);
  tmp = tmp | 1048576U;
  tmp = (tmp & 4294967232U) | 4U;
  tmp = (tmp & 4293951487U) | 131072U;
  amdgpu_mm_wreg(adev, 1282U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1400U, 0);
  tmp = tmp & 4294967231U;
  tmp = tmp & 4294967167U;
  tmp = tmp & 4294967039U;
  tmp = tmp & 4294966783U;
  tmp = tmp & 4294966271U;
  tmp = tmp & 4294965247U;
  tmp = tmp & 4294963199U;
  tmp = tmp & 4294959103U;
  tmp = tmp & 4294950911U;
  tmp = tmp & 4294934527U;
  tmp = tmp & 4294901759U;
  tmp = tmp & 4294836223U;
  amdgpu_mm_wreg(adev, 1400U, tmp, 0);
  amdgpu_mm_wreg(adev, 1367U, (u32 )(adev->mc.gtt_start >> 12), 0);
  amdgpu_mm_wreg(adev, 1375U, (u32 )(adev->mc.gtt_end >> 12) - 1U, 0);
  amdgpu_mm_wreg(adev, 1359U, (u32 )(adev->gart.table_addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1350U, (unsigned int )(adev->dummy_page.addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1292U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = tmp | 1U;
  tmp = tmp & 4294967289U;
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  amdgpu_mm_wreg(adev, 1397U, 0U, 0);
  amdgpu_mm_wreg(adev, 1398U, 0U, 0);
  amdgpu_mm_wreg(adev, 1399U, 0U, 0);
  amdgpu_mm_wreg(adev, 1368U, 0U, 0);
  amdgpu_mm_wreg(adev, 1376U, adev->vm_manager.max_pfn - 1U, 0);
  i = 1;
  goto ldv_43824;
  ldv_43823: ;
  if (i <= 7) {
    amdgpu_mm_wreg(adev, (u32 )(i + 1359), (u32 )(adev->gart.table_addr >> 12), 0);
  } else {
    amdgpu_mm_wreg(adev, (u32 )(i + 1286), (u32 )(adev->gart.table_addr >> 12), 0);
  }
  i = i + 1;
  ldv_43824: ;
  if (i <= 15) {
    goto ldv_43823;
  } else {
  }
  amdgpu_mm_wreg(adev, 1351U, (unsigned int )(adev->dummy_page.addr >> 12), 0);
  amdgpu_mm_wreg(adev, 1293U, 4U, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = tmp | 1U;
  tmp = (tmp & 4294967289U) | 2U;
  tmp = tmp | 8U;
  tmp = tmp | 16U;
  tmp = tmp | 64U;
  tmp = tmp | 128U;
  tmp = tmp | 512U;
  tmp = tmp | 1024U;
  tmp = tmp | 4096U;
  tmp = tmp | 8192U;
  tmp = tmp | 32768U;
  tmp = tmp | 65536U;
  tmp = tmp | 262144U;
  tmp = tmp | 524288U;
  tmp = tmp | 2097152U;
  tmp = tmp | 4194304U;
  tmp = (tmp & 4043309055U) | ((u32 )((amdgpu_vm_block_size + -9) << 24) & 251658240U);
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  gmc_v8_0_gart_flush_gpu_tlb(adev, 0U);
  printk("\016[drm] PCIE GART of %uM enabled (table at 0x%016llX).\n", (unsigned int )(adev->mc.gtt_size >> 20),
         adev->gart.table_addr);
  adev->gart.ready = 1;
  return (0);
}
}
static int gmc_v8_0_gart_init(struct amdgpu_device *adev )
{
  int r ;
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )adev->gart.robj != (unsigned long )((struct amdgpu_bo *)0)) {
    __ret_warn_on = 1;
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_fmt("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gmc_v8_0.c",
                        662, "R600 PCIE GART already initialized\n");
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
    return (0);
  } else {
  }
  r = amdgpu_gart_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gart.table_size = adev->gart.num_gpu_pages * 8U;
  tmp___0 = amdgpu_gart_table_vram_alloc(adev);
  return (tmp___0);
}
}
static void gmc_v8_0_gart_disable(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  amdgpu_mm_wreg(adev, 1284U, 0U, 0);
  amdgpu_mm_wreg(adev, 1285U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 2073U, 0);
  tmp = tmp & 4294967294U;
  tmp = tmp & 4294967293U;
  tmp = tmp & 4294967231U;
  amdgpu_mm_wreg(adev, 2073U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1280U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, 1280U, tmp, 0);
  amdgpu_mm_wreg(adev, 1281U, 0U, 0);
  amdgpu_gart_table_vram_unpin(adev);
  return;
}
}
static void gmc_v8_0_gart_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_gart_table_vram_free(adev);
  amdgpu_gart_fini(adev);
  return;
}
}
static int gmc_v8_0_vm_init(struct amdgpu_device *adev )
{
  u64 tmp ;
  u32 tmp___0 ;
  {
  adev->vm_manager.nvm = 8U;
  if ((adev->flags & 131072UL) != 0UL) {
    tmp___0 = amdgpu_mm_rreg(adev, 2074U, 0);
    tmp = (u64 )tmp___0;
    tmp = tmp << 22;
    adev->vm_manager.vram_base_offset = tmp;
  } else {
    adev->vm_manager.vram_base_offset = 0ULL;
  }
  return (0);
}
}
static void gmc_v8_0_vm_fini(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void gmc_v8_0_vm_decode_fault(struct amdgpu_device *adev , u32 status , u32 addr ,
                                     u32 mc_client )
{
  u32 mc_id ;
  u32 vmid ;
  u32 protections ;
  char block[5U] ;
  {
  vmid = (status & 503316480U) >> 25;
  protections = status & 255U;
  block[0] = (char )(mc_client >> 24);
  block[1] = (char )(mc_client >> 16);
  block[2] = (char )(mc_client >> 8);
  block[3] = (char )mc_client;
  block[4] = 0;
  mc_id = (status & 2093056U) >> 12;
  printk("VM fault (0x%02x, vmid %d) at page %u, %s from \'%s\' (0x%08x) (%d)\n",
         protections, vmid, addr, (status & 16777216U) >> 24 != 0U ? (char *)"write" : (char *)"read",
         (char *)(& block), mc_client, mc_id);
  return;
}
}
static int gmc_v8_0_convert_vram_type(int mc_seq_vram_type )
{
  {
  switch (mc_seq_vram_type) {
  case 268435456: ;
  return (1);
  case 536870912: ;
  return (2);
  case 805306368: ;
  return (3);
  case 1073741824: ;
  return (4);
  case 1342177280: ;
  return (5);
  case 1610612736: ;
  return (6);
  case -1342177280: ;
  return (7);
  default: ;
  return (0);
  }
}
}
static int gmc_v8_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v8_0_set_gart_funcs(adev);
  gmc_v8_0_set_irq_funcs(adev);
  if ((adev->flags & 131072UL) != 0UL) {
    adev->mc.vram_type = 0U;
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 2688U, 0);
    tmp = tmp___0;
    tmp = tmp & 4026531840U;
    tmp___1 = gmc_v8_0_convert_vram_type((int )tmp);
    adev->mc.vram_type = (u32 )tmp___1;
  }
  return (0);
}
}
static int gmc_v8_0_sw_init(void *handle )
{
  int r ;
  int dma_bits ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_gem_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 146U, & adev->mc.vm_fault);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 147U, & adev->mc.vm_fault);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->vm_manager.max_pfn = (u32 )(amdgpu_vm_size << 18);
  adev->mc.mc_mask = 1099511627775ULL;
  adev->need_dma32 = 0;
  dma_bits = (int )adev->need_dma32 ? 32 : 40;
  r = pci_set_dma_mask(adev->pdev, dma_bits != 64 ? (1ULL << dma_bits) - 1ULL : 0xffffffffffffffffULL);
  if (r != 0) {
    adev->need_dma32 = 1;
    dma_bits = 32;
    printk("\famdgpu: No suitable DMA available.\n");
  } else {
  }
  r = pci_set_consistent_dma_mask(adev->pdev, dma_bits != 64 ? (1ULL << dma_bits) - 1ULL : 0xffffffffffffffffULL);
  if (r != 0) {
    pci_set_consistent_dma_mask(adev->pdev, 4294967295ULL);
    printk("\famdgpu: No coherent DMA available.\n");
  } else {
  }
  r = gmc_v8_0_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load mc firmware!\n");
    return (r);
  } else {
  }
  r = gmc_v8_0_mc_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gmc_v8_0_gart_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! adev->vm_manager.enabled) {
    r = gmc_v8_0_vm_init(adev);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "vm manager initialization failed (%d).\n",
              r);
      return (r);
    } else {
    }
    adev->vm_manager.enabled = 1;
  } else {
  }
  return (r);
}
}
static int gmc_v8_0_sw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->vm_manager.enabled) {
    i = 0;
    goto ldv_43884;
    ldv_43883:
    amdgpu_fence_unref((struct amdgpu_fence **)(& adev->vm_manager.active) + (unsigned long )i);
    i = i + 1;
    ldv_43884: ;
    if (i <= 15) {
      goto ldv_43883;
    } else {
    }
    gmc_v8_0_vm_fini(adev);
    adev->vm_manager.enabled = 0;
  } else {
  }
  gmc_v8_0_gart_fini(adev);
  amdgpu_gem_fini(adev);
  amdgpu_bo_fini(adev);
  return (0);
}
}
static int gmc_v8_0_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v8_0_init_golden_registers(adev);
  gmc_v8_0_mc_program(adev);
  if ((adev->flags & 131072UL) == 0UL) {
    r = gmc_v8_0_mc_load_microcode(adev);
    if (r != 0) {
      drm_err("Failed to load MC firmware!\n");
      return (r);
    } else {
    }
  } else {
  }
  r = gmc_v8_0_gart_enable(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int gmc_v8_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gmc_v8_0_gart_disable(adev);
  return (0);
}
}
static int gmc_v8_0_suspend(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->vm_manager.enabled) {
    i = 0;
    goto ldv_43901;
    ldv_43900:
    amdgpu_fence_unref((struct amdgpu_fence **)(& adev->vm_manager.active) + (unsigned long )i);
    i = i + 1;
    ldv_43901: ;
    if (i <= 15) {
      goto ldv_43900;
    } else {
    }
    gmc_v8_0_vm_fini(adev);
    adev->vm_manager.enabled = 0;
  } else {
  }
  gmc_v8_0_hw_fini((void *)adev);
  return (0);
}
}
static int gmc_v8_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = gmc_v8_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! adev->vm_manager.enabled) {
    r = gmc_v8_0_vm_init(adev);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "vm manager initialization failed (%d).\n",
              r);
      return (r);
    } else {
    }
    adev->vm_manager.enabled = 1;
  } else {
  }
  return (r);
}
}
static bool gmc_v8_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 7936U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int gmc_v8_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43920;
  ldv_43919:
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0 & 16128U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43920: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43919;
  } else {
  }
  return (-110);
}
}
static void gmc_v8_0_print_status(void *handle )
{
  int i ;
  int j ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "GMC 8.x registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 1343U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
            tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 1335U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
            tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 2073U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_MX_L1_TLB_CNTL=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 1280U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 1281U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL2=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 1282U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL3=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 1400U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CNTL4=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 1367U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PAGE_TABLE_START_ADDR=0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 1375U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PAGE_TABLE_END_ADDR=0x%08X\n",
            tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 1350U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_PROTECTION_FAULT_DEFAULT_ADDR=0x%08X\n",
            tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 1292U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_CNTL2=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 1284U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT0_CNTL=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 1397U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CONTEXT1_IDENTITY_APERTURE_LOW_ADDR=0x%08X\n",
            tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 1398U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_L2_CONTEXT1_IDENTITY_APERTURE_HIGH_ADDR=0x%08X\n",
            tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 1399U, 0);
  _dev_info((struct device const *)adev->dev, "  mmVM_L2_CONTEXT_IDENTITY_PHYSICAL_OFFSET=0x%08X\n",
            tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 1368U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PAGE_TABLE_START_ADDR=0x%08X\n",
            tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 1376U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PAGE_TABLE_END_ADDR=0x%08X\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 1351U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_DEFAULT_ADDR=0x%08X\n",
            tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 1293U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_CNTL2=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 1285U, 0);
  _dev_info((struct device const *)adev->dev, "  VM_CONTEXT1_CNTL=0x%08X\n", tmp___20);
  i = 0;
  goto ldv_43929;
  ldv_43928: ;
  if (i <= 7) {
    tmp___21 = amdgpu_mm_rreg(adev, (u32 )(i + 1359), 0);
    _dev_info((struct device const *)adev->dev, "  VM_CONTEXT%d_PAGE_TABLE_BASE_ADDR=0x%08X\n",
              i, tmp___21);
  } else {
    tmp___22 = amdgpu_mm_rreg(adev, (u32 )(i + 1286), 0);
    _dev_info((struct device const *)adev->dev, "  VM_CONTEXT%d_PAGE_TABLE_BASE_ADDR=0x%08X\n",
              i, tmp___22);
  }
  i = i + 1;
  ldv_43929: ;
  if (i <= 15) {
    goto ldv_43928;
  } else {
  }
  tmp___23 = amdgpu_mm_rreg(adev, 2061U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_LOW_ADDR=0x%08X\n",
            tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 2062U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_HIGH_ADDR=0x%08X\n",
            tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 2063U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_SYSTEM_APERTURE_DEFAULT_ADDR=0x%08X\n",
            tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 2057U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_FB_LOCATION=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 2060U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_BASE=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 2058U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_TOP=0x%08X\n", tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 2059U, 0);
  _dev_info((struct device const *)adev->dev, "  MC_VM_AGP_BOT=0x%08X\n", tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 5416U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_REG_COHERENCY_FLUSH_CNTL=0x%08X\n",
            tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 2817U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_BASE=0x%08X\n",
            tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 2818U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_INFO=0x%08X\n",
            tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 2819U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_NONSURFACE_SIZE=0x%08X\n",
            tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 3027U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_MISC_CNTL=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 2816U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_HOST_PATH_CNTL=0x%08X\n", tmp___35);
  i = 0;
  j = 0;
  goto ldv_43932;
  ldv_43931:
  _dev_info((struct device const *)adev->dev, "  %d:\n", i);
  tmp___36 = amdgpu_mm_rreg(adev, (u32 )(j + 2821), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2821, tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, (u32 )(j + 2822), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2822, tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, (u32 )(j + 2823), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2823, tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, (u32 )(j + 2824), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2824, tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, (u32 )(j + 2825), 0);
  _dev_info((struct device const *)adev->dev, "  0x%04X=0x%08X\n", j + 2825, tmp___40);
  i = i + 1;
  j = j + 6;
  ldv_43932: ;
  if (i <= 31) {
    goto ldv_43931;
  } else {
  }
  tmp___41 = amdgpu_mm_rreg(adev, 5412U, 0);
  _dev_info((struct device const *)adev->dev, "  BIF_FB_EN=0x%08X\n", tmp___41);
  return;
}
}
static int gmc_v8_0_soft_reset(void *handle )
{
  struct amdgpu_mode_mc_save save ;
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 256U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 131072U;
  } else {
  }
  if ((tmp & 7680U) != 0U) {
    if ((adev->flags & 131072UL) == 0UL) {
      srbm_soft_reset = srbm_soft_reset | 2048U;
    } else {
    }
  } else {
  }
  if (srbm_soft_reset != 0U) {
    gmc_v8_0_print_status((void *)adev);
    gmc_v8_0_mc_stop(adev, & save);
    tmp___1 = gmc_v8_0_wait_for_idle((void *)adev);
    if (tmp___1 != 0) {
      dev_warn((struct device const *)adev->dev, "Wait for GMC idle timed out !\n");
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    gmc_v8_0_mc_resume(adev, & save);
    __const_udelay(214750UL);
    gmc_v8_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int gmc_v8_0_vm_fault_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                             unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 tmp ;
  u32 bits ;
  {
  bits = 2396744U;
  switch ((unsigned int )state) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = ~ bits & tmp;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = ~ bits & tmp;
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  goto ldv_43950;
  case 1U:
  tmp = amdgpu_mm_rreg(adev, 1284U, 0);
  tmp = tmp | bits;
  amdgpu_mm_wreg(adev, 1284U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 1285U, 0);
  tmp = tmp | bits;
  amdgpu_mm_wreg(adev, 1285U, tmp, 0);
  goto ldv_43950;
  default: ;
  goto ldv_43950;
  }
  ldv_43950: ;
  return (0);
}
}
static int gmc_v8_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  u32 addr ;
  u32 status ;
  u32 mc_client ;
  u32 tmp_ ;
  u32 tmp ;
  {
  addr = amdgpu_mm_rreg(adev, 1343U, 0);
  status = amdgpu_mm_rreg(adev, 1335U, 0);
  mc_client = amdgpu_mm_rreg(adev, 1337U, 0);
  dev_err((struct device const *)adev->dev, "GPU fault detected: %d 0x%08x\n", entry->src_id,
          entry->src_data);
  dev_err((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_ADDR   0x%08X\n",
          addr);
  dev_err((struct device const *)adev->dev, "  VM_CONTEXT1_PROTECTION_FAULT_STATUS 0x%08X\n",
          status);
  gmc_v8_0_vm_decode_fault(adev, status, addr, mc_client);
  tmp = amdgpu_mm_rreg(adev, 1293U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967294U;
  tmp_ = tmp_ | 1U;
  amdgpu_mm_wreg(adev, 1293U, tmp_, 0);
  return (0);
}
}
static int gmc_v8_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int gmc_v8_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const gmc_v8_0_ip_funcs =
     {& gmc_v8_0_early_init, (int (*)(void * ))0, & gmc_v8_0_sw_init, & gmc_v8_0_sw_fini,
    & gmc_v8_0_hw_init, & gmc_v8_0_hw_fini, & gmc_v8_0_suspend, & gmc_v8_0_resume,
    & gmc_v8_0_is_idle, & gmc_v8_0_wait_for_idle, & gmc_v8_0_soft_reset, & gmc_v8_0_print_status,
    & gmc_v8_0_set_clockgating_state, & gmc_v8_0_set_powergating_state};
static struct amdgpu_gart_funcs const gmc_v8_0_gart_funcs = {& gmc_v8_0_gart_flush_gpu_tlb, & gmc_v8_0_gart_set_pte_pde};
static struct amdgpu_irq_src_funcs const gmc_v8_0_irq_funcs = {& gmc_v8_0_vm_fault_interrupt_state, & gmc_v8_0_process_interrupt};
static void gmc_v8_0_set_gart_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->gart.gart_funcs == (unsigned long )((struct amdgpu_gart_funcs const *)0)) {
    adev->gart.gart_funcs = & gmc_v8_0_gart_funcs;
  } else {
  }
  return;
}
}
static void gmc_v8_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->mc.vm_fault.num_types = 1U;
  adev->mc.vm_fault.funcs = & gmc_v8_0_irq_funcs;
  return;
}
}
int ldv_retval_60 ;
int ldv_retval_59 ;
extern int ldv_release_77(void) ;
extern int ldv_probe_77(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_75(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gmc_v8_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gmc_v8_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_gart_funcs_76(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gmc_v8_0_gart_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_77(void)
{
  void *ldvarg718 ;
  void *tmp ;
  void *ldvarg726 ;
  void *tmp___0 ;
  enum amd_powergating_state ldvarg721 ;
  void *ldvarg722 ;
  void *tmp___1 ;
  void *ldvarg723 ;
  void *tmp___2 ;
  void *ldvarg725 ;
  void *tmp___3 ;
  void *ldvarg715 ;
  void *tmp___4 ;
  void *ldvarg716 ;
  void *tmp___5 ;
  void *ldvarg727 ;
  void *tmp___6 ;
  void *ldvarg714 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg717 ;
  void *ldvarg720 ;
  void *tmp___8 ;
  void *ldvarg719 ;
  void *tmp___9 ;
  void *ldvarg724 ;
  void *tmp___10 ;
  void *ldvarg713 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg718 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg726 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg722 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg723 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg725 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg715 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg716 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg727 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg714 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg720 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg719 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg724 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg713 = tmp___11;
  ldv_memset((void *)(& ldvarg721), 0, 4UL);
  ldv_memset((void *)(& ldvarg717), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_hw_fini(ldvarg727);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_hw_fini(ldvarg727);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_hw_fini(ldvarg727);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 1: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_print_status(ldvarg726);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_print_status(ldvarg726);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_print_status(ldvarg726);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 2: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_early_init(ldvarg725);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_early_init(ldvarg725);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_early_init(ldvarg725);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 3: ;
  if (ldv_state_variable_77 == 2) {
    ldv_retval_60 = gmc_v8_0_suspend(ldvarg724);
    if (ldv_retval_60 == 0) {
      ldv_state_variable_77 = 3;
    } else {
    }
  } else {
  }
  goto ldv_44010;
  case 4: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_sw_init(ldvarg723);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_sw_init(ldvarg723);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_sw_init(ldvarg723);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 5: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_set_powergating_state(ldvarg722, ldvarg721);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_set_powergating_state(ldvarg722, ldvarg721);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_set_powergating_state(ldvarg722, ldvarg721);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 6: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_wait_for_idle(ldvarg720);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_wait_for_idle(ldvarg720);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_wait_for_idle(ldvarg720);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 7: ;
  if (ldv_state_variable_77 == 3) {
    ldv_retval_59 = gmc_v8_0_resume(ldvarg719);
    if (ldv_retval_59 == 0) {
      ldv_state_variable_77 = 2;
    } else {
    }
  } else {
  }
  goto ldv_44010;
  case 8: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_set_clockgating_state(ldvarg718, ldvarg717);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_set_clockgating_state(ldvarg718, ldvarg717);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_set_clockgating_state(ldvarg718, ldvarg717);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 9: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_hw_init(ldvarg716);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_hw_init(ldvarg716);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_hw_init(ldvarg716);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 10: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_soft_reset(ldvarg715);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_soft_reset(ldvarg715);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_soft_reset(ldvarg715);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 11: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_sw_fini(ldvarg714);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_sw_fini(ldvarg714);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_sw_fini(ldvarg714);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 12: ;
  if (ldv_state_variable_77 == 2) {
    gmc_v8_0_is_idle(ldvarg713);
    ldv_state_variable_77 = 2;
  } else {
  }
  if (ldv_state_variable_77 == 1) {
    gmc_v8_0_is_idle(ldvarg713);
    ldv_state_variable_77 = 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    gmc_v8_0_is_idle(ldvarg713);
    ldv_state_variable_77 = 3;
  } else {
  }
  goto ldv_44010;
  case 13: ;
  if (ldv_state_variable_77 == 2) {
    ldv_release_77();
    ldv_state_variable_77 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_77 == 3) {
    ldv_release_77();
    ldv_state_variable_77 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_44010;
  case 14: ;
  if (ldv_state_variable_77 == 1) {
    ldv_probe_77();
    ldv_state_variable_77 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_44010;
  default:
  ldv_stop();
  }
  ldv_44010: ;
  return;
}
}
void ldv_main_exported_75(void)
{
  enum amdgpu_interrupt_state ldvarg943 ;
  struct amdgpu_iv_entry *ldvarg941 ;
  void *tmp ;
  unsigned int ldvarg942 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg941 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg943), 0, 4UL);
  ldv_memset((void *)(& ldvarg942), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_75 == 1) {
    gmc_v8_0_vm_fault_interrupt_state(gmc_v8_0_irq_funcs_group0, gmc_v8_0_irq_funcs_group1,
                                      ldvarg942, ldvarg943);
    ldv_state_variable_75 = 1;
  } else {
  }
  goto ldv_44033;
  case 1: ;
  if (ldv_state_variable_75 == 1) {
    gmc_v8_0_process_interrupt(gmc_v8_0_irq_funcs_group0, gmc_v8_0_irq_funcs_group1,
                               ldvarg941);
    ldv_state_variable_75 = 1;
  } else {
  }
  goto ldv_44033;
  default:
  ldv_stop();
  }
  ldv_44033: ;
  return;
}
}
void ldv_main_exported_76(void)
{
  uint64_t ldvarg265 ;
  u32 ldvarg267 ;
  u32 ldvarg263 ;
  void *ldvarg264 ;
  void *tmp ;
  u32 ldvarg266 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg264 = tmp;
  ldv_memset((void *)(& ldvarg265), 0, 8UL);
  ldv_memset((void *)(& ldvarg267), 0, 4UL);
  ldv_memset((void *)(& ldvarg263), 0, 4UL);
  ldv_memset((void *)(& ldvarg266), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_76 == 1) {
    gmc_v8_0_gart_flush_gpu_tlb(gmc_v8_0_gart_funcs_group0, ldvarg267);
    ldv_state_variable_76 = 1;
  } else {
  }
  goto ldv_44045;
  case 1: ;
  if (ldv_state_variable_76 == 1) {
    gmc_v8_0_gart_set_pte_pde(gmc_v8_0_gart_funcs_group0, ldvarg264, ldvarg263, ldvarg265,
                              ldvarg266);
    ldv_state_variable_76 = 1;
  } else {
  }
  goto ldv_44045;
  default:
  ldv_stop();
  }
  ldv_44045: ;
  return;
}
}
bool ldv_queue_work_on_707(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_708(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_709(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_710(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_711(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_721(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_723(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_722(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_725(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_724(struct workqueue_struct *ldv_func_arg1 ) ;
extern bool flush_work(struct work_struct * ) ;
bool ldv_flush_work_726(struct work_struct *ldv_func_arg1 ) ;
bool ldv_flush_work_727(struct work_struct *ldv_func_arg1 ) ;
void activate_work_6(struct work_struct *work , int state ) ;
void invoke_work_6(void) ;
void call_and_disable_all_6(int state ) ;
void activate_work_7(struct work_struct *work , int state ) ;
void disable_work_7(struct work_struct *work ) ;
void call_and_disable_work_7(struct work_struct *work ) ;
void call_and_disable_all_7(int state ) ;
void invoke_work_7(void) ;
void disable_work_6(struct work_struct *work ) ;
void call_and_disable_work_6(struct work_struct *work ) ;
extern void pci_disable_msi(struct pci_dev * ) ;
extern int pci_enable_msi_range(struct pci_dev * , int , int ) ;
__inline static int pci_enable_msi_exact(struct pci_dev *dev , int nvec )
{
  int rc ;
  int tmp ;
  {
  tmp = pci_enable_msi_range(dev, nvec, nvec);
  rc = tmp;
  if (rc < 0) {
    return (rc);
  } else {
  }
  return (0);
}
}
extern int drm_irq_install(struct drm_device * , int ) ;
extern int drm_irq_uninstall(struct drm_device * ) ;
extern int drm_vblank_init(struct drm_device * , int ) ;
extern void drm_vblank_cleanup(struct drm_device * ) ;
extern bool drm_helper_hpd_irq_event(struct drm_device * ) ;
int amdgpu_ih_process(struct amdgpu_device *adev ) ;
void amdgpu_irq_dispatch(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry ) ;
static void amdgpu_hotplug_work_func(struct work_struct *work )
{
  struct amdgpu_device *adev ;
  struct work_struct const *__mptr ;
  struct drm_device *dev ;
  struct drm_mode_config *mode_config ;
  struct drm_connector *connector ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  __mptr = (struct work_struct const *)work;
  adev = (struct amdgpu_device *)__mptr + 0xffffffffffffe5c8UL;
  dev = adev->ddev;
  mode_config = & dev->mode_config;
  mutex_lock_nested(& mode_config->mutex, 0U);
  if (mode_config->num_connector != 0) {
    __mptr___0 = (struct list_head const *)mode_config->connector_list.next;
    connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
    goto ldv_47948;
    ldv_47947:
    amdgpu_connector_hotplug(connector);
    __mptr___1 = (struct list_head const *)connector->head.next;
    connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
    ldv_47948: ;
    if ((unsigned long )(& connector->head) != (unsigned long )(& mode_config->connector_list)) {
      goto ldv_47947;
    } else {
    }
  } else {
  }
  mutex_unlock(& mode_config->mutex);
  drm_helper_hpd_irq_event(dev);
  return;
}
}
static void amdgpu_irq_reset_work_func(struct work_struct *work )
{
  struct amdgpu_device *adev ;
  struct work_struct const *__mptr ;
  {
  __mptr = (struct work_struct const *)work;
  adev = (struct amdgpu_device *)__mptr + 0xffffffffffffff18UL;
  amdgpu_gpu_reset(adev);
  return;
}
}
static void amdgpu_irq_disable_all(struct amdgpu_device *adev )
{
  unsigned long irqflags ;
  unsigned int i ;
  unsigned int j ;
  int r ;
  raw_spinlock_t *tmp ;
  struct amdgpu_irq_src *src ;
  {
  tmp = spinlock_check(& adev->irq.lock);
  irqflags = _raw_spin_lock_irqsave(tmp);
  i = 0U;
  goto ldv_47972;
  ldv_47971:
  src = adev->irq.sources[i];
  if (((unsigned long )src == (unsigned long )((struct amdgpu_irq_src *)0) || (unsigned long )(src->funcs)->set == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                                                                         struct amdgpu_irq_src * ,
                                                                                                                                                         unsigned int ,
                                                                                                                                                         enum amdgpu_interrupt_state ))0)) || src->num_types == 0U) {
    goto ldv_47967;
  } else {
  }
  j = 0U;
  goto ldv_47969;
  ldv_47968:
  atomic_set(src->enabled_types + (unsigned long )j, 0);
  r = (*((src->funcs)->set))(adev, src, j, 0);
  if (r != 0) {
    drm_err("error disabling interrupt (%d)\n", r);
  } else {
  }
  j = j + 1U;
  ldv_47969: ;
  if (src->num_types > j) {
    goto ldv_47968;
  } else {
  }
  ldv_47967:
  i = i + 1U;
  ldv_47972: ;
  if (i <= 255U) {
    goto ldv_47971;
  } else {
  }
  spin_unlock_irqrestore(& adev->irq.lock, irqflags);
  return;
}
}
void amdgpu_irq_preinstall(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_irq_disable_all(adev);
  amdgpu_ih_process(adev);
  return;
}
}
int amdgpu_irq_postinstall(struct drm_device *dev )
{
  {
  dev->max_vblank_count = 2097151U;
  return (0);
}
}
void amdgpu_irq_uninstall(struct drm_device *dev )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0)) {
    return;
  } else {
  }
  amdgpu_irq_disable_all(adev);
  return;
}
}
irqreturn_t amdgpu_irq_handler(int irq , void *arg )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  irqreturn_t ret ;
  int tmp ;
  {
  dev = (struct drm_device *)arg;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_ih_process(adev);
  ret = (irqreturn_t )tmp;
  if ((unsigned int )ret == 1U) {
    pm_runtime_mark_last_busy(dev->dev);
  } else {
  }
  return (ret);
}
}
static bool amdgpu_msi_ok(struct amdgpu_device *adev )
{
  {
  if (amdgpu_msi == 1) {
    return (1);
  } else
  if (amdgpu_msi == 0) {
    return (0);
  } else {
  }
  return (1);
}
}
int amdgpu_irq_init(struct amdgpu_device *adev )
{
  int r ;
  struct lock_class_key __key ;
  int ret ;
  int tmp ;
  bool tmp___0 ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___1 ;
  atomic_long_t __constr_expr_1 ;
  {
  r = 0;
  spinlock_check(& adev->irq.lock);
  __raw_spin_lock_init(& adev->irq.lock.__annonCompField18.rlock, "&(&adev->irq.lock)->rlock",
                       & __key);
  r = drm_vblank_init(adev->ddev, adev->mode_info.num_crtc);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->irq.msi_enabled = 0;
  tmp___0 = amdgpu_msi_ok(adev);
  if ((int )tmp___0) {
    tmp = pci_enable_msi_exact(adev->pdev, 1);
    ret = tmp;
    if (ret == 0) {
      adev->irq.msi_enabled = 1;
      _dev_info((struct device const *)adev->dev, "amdgpu: using MSI.\n");
    } else {
    }
  } else {
  }
  __init_work(& adev->hotplug_work, 0);
  __constr_expr_0.counter = 137438953408L;
  adev->hotplug_work.data = __constr_expr_0;
  lockdep_init_map(& adev->hotplug_work.lockdep_map, "(&adev->hotplug_work)", & __key___0,
                   0);
  INIT_LIST_HEAD(& adev->hotplug_work.entry);
  adev->hotplug_work.func = & amdgpu_hotplug_work_func;
  __init_work(& adev->reset_work, 0);
  __constr_expr_1.counter = 137438953408L;
  adev->reset_work.data = __constr_expr_1;
  lockdep_init_map(& adev->reset_work.lockdep_map, "(&adev->reset_work)", & __key___1,
                   0);
  INIT_LIST_HEAD(& adev->reset_work.entry);
  adev->reset_work.func = & amdgpu_irq_reset_work_func;
  adev->irq.installed = 1;
  r = drm_irq_install(adev->ddev, (int )((adev->ddev)->pdev)->irq);
  if (r != 0) {
    adev->irq.installed = 0;
    ldv_flush_work_726(& adev->hotplug_work);
    return (r);
  } else {
  }
  printk("\016[drm] amdgpu: irq initialized.\n");
  return (0);
}
}
void amdgpu_irq_fini(struct amdgpu_device *adev )
{
  unsigned int i ;
  struct amdgpu_irq_src *src ;
  {
  drm_vblank_cleanup(adev->ddev);
  if ((int )adev->irq.installed) {
    drm_irq_uninstall(adev->ddev);
    adev->irq.installed = 0;
    if ((int )adev->irq.msi_enabled) {
      pci_disable_msi(adev->pdev);
    } else {
    }
    ldv_flush_work_727(& adev->hotplug_work);
  } else {
  }
  i = 0U;
  goto ldv_48012;
  ldv_48011:
  src = adev->irq.sources[i];
  if ((unsigned long )src == (unsigned long )((struct amdgpu_irq_src *)0)) {
    goto ldv_48010;
  } else {
  }
  kfree((void const *)src->enabled_types);
  src->enabled_types = (atomic_t *)0;
  ldv_48010:
  i = i + 1U;
  ldv_48012: ;
  if (i <= 255U) {
    goto ldv_48011;
  } else {
  }
  return;
}
}
int amdgpu_irq_add_id(struct amdgpu_device *adev , unsigned int src_id , struct amdgpu_irq_src *source )
{
  atomic_t *types ;
  void *tmp ;
  {
  if (src_id > 255U) {
    return (-22);
  } else {
  }
  if ((unsigned long )adev->irq.sources[src_id] != (unsigned long )((struct amdgpu_irq_src *)0)) {
    return (-22);
  } else {
  }
  if ((unsigned long )source->funcs == (unsigned long )((struct amdgpu_irq_src_funcs const *)0)) {
    return (-22);
  } else {
  }
  if (source->num_types != 0U && (unsigned long )source->enabled_types == (unsigned long )((atomic_t *)0)) {
    tmp = kcalloc((size_t )source->num_types, 4UL, 208U);
    types = (atomic_t *)tmp;
    if ((unsigned long )types == (unsigned long )((atomic_t *)0)) {
      return (-12);
    } else {
    }
    source->enabled_types = types;
  } else {
  }
  adev->irq.sources[src_id] = source;
  return (0);
}
}
void amdgpu_irq_dispatch(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry )
{
  unsigned int src_id ;
  struct amdgpu_irq_src *src ;
  int r ;
  long tmp ;
  long tmp___0 ;
  {
  src_id = entry->src_id;
  if (src_id > 255U) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("amdgpu_irq_dispatch", "Invalid src_id in IV: %d\n", src_id);
    } else {
    }
    return;
  } else {
  }
  src = adev->irq.sources[src_id];
  if ((unsigned long )src == (unsigned long )((struct amdgpu_irq_src *)0)) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_irq_dispatch", "Unhandled interrupt src_id: %d\n",
                          src_id);
    } else {
    }
    return;
  } else {
  }
  r = (*((src->funcs)->process))(adev, src, entry);
  if (r != 0) {
    drm_err("error processing interrupt (%d)\n", r);
  } else {
  }
  return;
}
}
int amdgpu_irq_update(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type )
{
  unsigned long irqflags ;
  enum amdgpu_interrupt_state state ;
  int r ;
  raw_spinlock_t *tmp ;
  bool tmp___0 ;
  {
  tmp = spinlock_check(& adev->irq.lock);
  irqflags = _raw_spin_lock_irqsave(tmp);
  tmp___0 = amdgpu_irq_enabled(adev, src, type);
  if ((int )tmp___0) {
    state = 1;
  } else {
    state = 0;
  }
  r = (*((src->funcs)->set))(adev, src, type, state);
  spin_unlock_irqrestore(& adev->irq.lock, irqflags);
  return (r);
}
}
int amdgpu_irq_get(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type )
{
  int tmp ;
  int tmp___0 ;
  {
  if (! (adev->ddev)->irq_enabled) {
    return (-2);
  } else {
  }
  if (src->num_types <= type) {
    return (-22);
  } else {
  }
  if ((unsigned long )src->enabled_types == (unsigned long )((atomic_t *)0) || (unsigned long )(src->funcs)->set == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                                                                          struct amdgpu_irq_src * ,
                                                                                                                                                          unsigned int ,
                                                                                                                                                          enum amdgpu_interrupt_state ))0)) {
    return (-22);
  } else {
  }
  tmp___0 = atomic_add_return(1, src->enabled_types + (unsigned long )type);
  if (tmp___0 == 1) {
    tmp = amdgpu_irq_update(adev, src, type);
    return (tmp);
  } else {
  }
  return (0);
}
}
bool amdgpu_irq_get_delayed(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                            unsigned int type )
{
  int tmp ;
  {
  if (src->num_types <= type || (unsigned long )src->enabled_types == (unsigned long )((atomic_t *)0)) {
    return (0);
  } else {
  }
  tmp = atomic_add_return(1, src->enabled_types + (unsigned long )type);
  return (tmp == 1);
}
}
int amdgpu_irq_put(struct amdgpu_device *adev , struct amdgpu_irq_src *src , unsigned int type )
{
  int tmp ;
  int tmp___0 ;
  {
  if (! (adev->ddev)->irq_enabled) {
    return (-2);
  } else {
  }
  if (src->num_types <= type) {
    return (-22);
  } else {
  }
  if ((unsigned long )src->enabled_types == (unsigned long )((atomic_t *)0) || (unsigned long )(src->funcs)->set == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                                                                          struct amdgpu_irq_src * ,
                                                                                                                                                          unsigned int ,
                                                                                                                                                          enum amdgpu_interrupt_state ))0)) {
    return (-22);
  } else {
  }
  tmp___0 = atomic_dec_and_test(src->enabled_types + (unsigned long )type);
  if (tmp___0 != 0) {
    tmp = amdgpu_irq_update(adev, src, type);
    return (tmp);
  } else {
  }
  return (0);
}
}
bool amdgpu_irq_enabled(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                        unsigned int type )
{
  int tmp ;
  {
  if (! (adev->ddev)->irq_enabled) {
    return (0);
  } else {
  }
  if (src->num_types <= type) {
    return (0);
  } else {
  }
  if ((unsigned long )src->enabled_types == (unsigned long )((atomic_t *)0) || (unsigned long )(src->funcs)->set == (unsigned long )((int (* )(struct amdgpu_device * ,
                                                                                                                                                          struct amdgpu_irq_src * ,
                                                                                                                                                          unsigned int ,
                                                                                                                                                          enum amdgpu_interrupt_state ))0)) {
    return (0);
  } else {
  }
  tmp = atomic_read((atomic_t const *)src->enabled_types + (unsigned long )type);
  return (tmp != 0);
}
}
void activate_work_6(struct work_struct *work , int state )
{
  {
  if (ldv_work_6_0 == 0) {
    ldv_work_struct_6_0 = work;
    ldv_work_6_0 = state;
    return;
  } else {
  }
  if (ldv_work_6_1 == 0) {
    ldv_work_struct_6_1 = work;
    ldv_work_6_1 = state;
    return;
  } else {
  }
  if (ldv_work_6_2 == 0) {
    ldv_work_struct_6_2 = work;
    ldv_work_6_2 = state;
    return;
  } else {
  }
  if (ldv_work_6_3 == 0) {
    ldv_work_struct_6_3 = work;
    ldv_work_6_3 = state;
    return;
  } else {
  }
  return;
}
}
void invoke_work_6(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_6_0 == 2 || ldv_work_6_0 == 3) {
    ldv_work_6_0 = 4;
    amdgpu_hotplug_work_func(ldv_work_struct_6_0);
    ldv_work_6_0 = 1;
  } else {
  }
  goto ldv_48069;
  case 1: ;
  if (ldv_work_6_1 == 2 || ldv_work_6_1 == 3) {
    ldv_work_6_1 = 4;
    amdgpu_hotplug_work_func(ldv_work_struct_6_0);
    ldv_work_6_1 = 1;
  } else {
  }
  goto ldv_48069;
  case 2: ;
  if (ldv_work_6_2 == 2 || ldv_work_6_2 == 3) {
    ldv_work_6_2 = 4;
    amdgpu_hotplug_work_func(ldv_work_struct_6_0);
    ldv_work_6_2 = 1;
  } else {
  }
  goto ldv_48069;
  case 3: ;
  if (ldv_work_6_3 == 2 || ldv_work_6_3 == 3) {
    ldv_work_6_3 = 4;
    amdgpu_hotplug_work_func(ldv_work_struct_6_0);
    ldv_work_6_3 = 1;
  } else {
  }
  goto ldv_48069;
  default:
  ldv_stop();
  }
  ldv_48069: ;
  return;
}
}
void call_and_disable_all_6(int state )
{
  {
  if (ldv_work_6_0 == state) {
    call_and_disable_work_6(ldv_work_struct_6_0);
  } else {
  }
  if (ldv_work_6_1 == state) {
    call_and_disable_work_6(ldv_work_struct_6_1);
  } else {
  }
  if (ldv_work_6_2 == state) {
    call_and_disable_work_6(ldv_work_struct_6_2);
  } else {
  }
  if (ldv_work_6_3 == state) {
    call_and_disable_work_6(ldv_work_struct_6_3);
  } else {
  }
  return;
}
}
void activate_work_7(struct work_struct *work , int state )
{
  {
  if (ldv_work_7_0 == 0) {
    ldv_work_struct_7_0 = work;
    ldv_work_7_0 = state;
    return;
  } else {
  }
  if (ldv_work_7_1 == 0) {
    ldv_work_struct_7_1 = work;
    ldv_work_7_1 = state;
    return;
  } else {
  }
  if (ldv_work_7_2 == 0) {
    ldv_work_struct_7_2 = work;
    ldv_work_7_2 = state;
    return;
  } else {
  }
  if (ldv_work_7_3 == 0) {
    ldv_work_struct_7_3 = work;
    ldv_work_7_3 = state;
    return;
  } else {
  }
  return;
}
}
void disable_work_7(struct work_struct *work )
{
  {
  if ((ldv_work_7_0 == 3 || ldv_work_7_0 == 2) && (unsigned long )ldv_work_struct_7_0 == (unsigned long )work) {
    ldv_work_7_0 = 1;
  } else {
  }
  if ((ldv_work_7_1 == 3 || ldv_work_7_1 == 2) && (unsigned long )ldv_work_struct_7_1 == (unsigned long )work) {
    ldv_work_7_1 = 1;
  } else {
  }
  if ((ldv_work_7_2 == 3 || ldv_work_7_2 == 2) && (unsigned long )ldv_work_struct_7_2 == (unsigned long )work) {
    ldv_work_7_2 = 1;
  } else {
  }
  if ((ldv_work_7_3 == 3 || ldv_work_7_3 == 2) && (unsigned long )ldv_work_struct_7_3 == (unsigned long )work) {
    ldv_work_7_3 = 1;
  } else {
  }
  return;
}
}
void call_and_disable_work_7(struct work_struct *work )
{
  {
  if ((ldv_work_7_0 == 2 || ldv_work_7_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_7_0) {
    amdgpu_irq_reset_work_func(work);
    ldv_work_7_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_7_1 == 2 || ldv_work_7_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_7_1) {
    amdgpu_irq_reset_work_func(work);
    ldv_work_7_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_7_2 == 2 || ldv_work_7_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_7_2) {
    amdgpu_irq_reset_work_func(work);
    ldv_work_7_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_7_3 == 2 || ldv_work_7_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_7_3) {
    amdgpu_irq_reset_work_func(work);
    ldv_work_7_3 = 1;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_7(int state )
{
  {
  if (ldv_work_7_0 == state) {
    call_and_disable_work_7(ldv_work_struct_7_0);
  } else {
  }
  if (ldv_work_7_1 == state) {
    call_and_disable_work_7(ldv_work_struct_7_1);
  } else {
  }
  if (ldv_work_7_2 == state) {
    call_and_disable_work_7(ldv_work_struct_7_2);
  } else {
  }
  if (ldv_work_7_3 == state) {
    call_and_disable_work_7(ldv_work_struct_7_3);
  } else {
  }
  return;
}
}
void work_init_7(void)
{
  {
  ldv_work_7_0 = 0;
  ldv_work_7_1 = 0;
  ldv_work_7_2 = 0;
  ldv_work_7_3 = 0;
  return;
}
}
void invoke_work_7(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_7_0 == 2 || ldv_work_7_0 == 3) {
    ldv_work_7_0 = 4;
    amdgpu_irq_reset_work_func(ldv_work_struct_7_0);
    ldv_work_7_0 = 1;
  } else {
  }
  goto ldv_48101;
  case 1: ;
  if (ldv_work_7_1 == 2 || ldv_work_7_1 == 3) {
    ldv_work_7_1 = 4;
    amdgpu_irq_reset_work_func(ldv_work_struct_7_0);
    ldv_work_7_1 = 1;
  } else {
  }
  goto ldv_48101;
  case 2: ;
  if (ldv_work_7_2 == 2 || ldv_work_7_2 == 3) {
    ldv_work_7_2 = 4;
    amdgpu_irq_reset_work_func(ldv_work_struct_7_0);
    ldv_work_7_2 = 1;
  } else {
  }
  goto ldv_48101;
  case 3: ;
  if (ldv_work_7_3 == 2 || ldv_work_7_3 == 3) {
    ldv_work_7_3 = 4;
    amdgpu_irq_reset_work_func(ldv_work_struct_7_0);
    ldv_work_7_3 = 1;
  } else {
  }
  goto ldv_48101;
  default:
  ldv_stop();
  }
  ldv_48101: ;
  return;
}
}
void work_init_6(void)
{
  {
  ldv_work_6_0 = 0;
  ldv_work_6_1 = 0;
  ldv_work_6_2 = 0;
  ldv_work_6_3 = 0;
  return;
}
}
void disable_work_6(struct work_struct *work )
{
  {
  if ((ldv_work_6_0 == 3 || ldv_work_6_0 == 2) && (unsigned long )ldv_work_struct_6_0 == (unsigned long )work) {
    ldv_work_6_0 = 1;
  } else {
  }
  if ((ldv_work_6_1 == 3 || ldv_work_6_1 == 2) && (unsigned long )ldv_work_struct_6_1 == (unsigned long )work) {
    ldv_work_6_1 = 1;
  } else {
  }
  if ((ldv_work_6_2 == 3 || ldv_work_6_2 == 2) && (unsigned long )ldv_work_struct_6_2 == (unsigned long )work) {
    ldv_work_6_2 = 1;
  } else {
  }
  if ((ldv_work_6_3 == 3 || ldv_work_6_3 == 2) && (unsigned long )ldv_work_struct_6_3 == (unsigned long )work) {
    ldv_work_6_3 = 1;
  } else {
  }
  return;
}
}
void call_and_disable_work_6(struct work_struct *work )
{
  {
  if ((ldv_work_6_0 == 2 || ldv_work_6_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_6_0) {
    amdgpu_hotplug_work_func(work);
    ldv_work_6_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_6_1 == 2 || ldv_work_6_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_6_1) {
    amdgpu_hotplug_work_func(work);
    ldv_work_6_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_6_2 == 2 || ldv_work_6_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_6_2) {
    amdgpu_hotplug_work_func(work);
    ldv_work_6_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_6_3 == 2 || ldv_work_6_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_6_3) {
    amdgpu_hotplug_work_func(work);
    ldv_work_6_3 = 1;
    return;
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_721(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_722(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_723(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_724(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_725(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_flush_work_726(struct work_struct *ldv_func_arg1 )
{
  ldv_func_ret_type___3 ldv_func_res ;
  bool tmp ;
  {
  tmp = flush_work(ldv_func_arg1);
  ldv_func_res = tmp;
  call_and_disable_work_2(ldv_func_arg1);
  return (ldv_func_res);
}
}
bool ldv_flush_work_727(struct work_struct *ldv_func_arg1 )
{
  ldv_func_ret_type___4 ldv_func_res ;
  bool tmp ;
  {
  tmp = flush_work(ldv_func_arg1);
  ldv_func_res = tmp;
  call_and_disable_work_2(ldv_func_arg1);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern unsigned long __phys_addr(unsigned long ) ;
__inline static int atomic_xchg(atomic_t *v , int new )
{
  int __ret ;
  {
  __ret = new;
  switch (4UL) {
  case 1UL:
  __asm__ volatile ("xchgb %b0, %1\n": "+q" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5694;
  case 2UL:
  __asm__ volatile ("xchgw %w0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5694;
  case 4UL:
  __asm__ volatile ("xchgl %0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5694;
  case 8UL:
  __asm__ volatile ("xchgq %q0, %1\n": "+r" (__ret), "+m" (v->counter): : "memory",
                       "cc");
  goto ldv_5694;
  default:
  __xchg_wrong_size();
  }
  ldv_5694: ;
  return (__ret);
}
}
bool ldv_queue_work_on_739(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_741(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_740(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_743(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_742(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static dma_addr_t dma_map_single_attrs(struct device *dev , void *ptr , size_t size ,
                                                enum dma_data_direction dir , struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  int tmp___0 ;
  long tmp___1 ;
  unsigned long tmp___2 ;
  unsigned long tmp___3 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  kmemcheck_mark_initialized(ptr, (unsigned int )size);
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (19), "i" (12UL));
    ldv_25611: ;
    goto ldv_25611;
  } else {
  }
  tmp___2 = __phys_addr((unsigned long )ptr);
  addr = (*(ops->map_page))(dev, (struct page *)-24189255811072L + (tmp___2 >> 12),
                            (unsigned long )ptr & 4095UL, size, dir, attrs);
  tmp___3 = __phys_addr((unsigned long )ptr);
  debug_dma_map_page(dev, (struct page *)-24189255811072L + (tmp___3 >> 12), (unsigned long )ptr & 4095UL,
                     size, (int )dir, addr, 1);
  return (addr);
}
}
__inline static void dma_unmap_single_attrs(struct device *dev , dma_addr_t addr ,
                                            size_t size , enum dma_data_direction dir ,
                                            struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (36), "i" (12UL));
    ldv_25620: ;
    goto ldv_25620;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, attrs);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 1);
  return;
}
}
__inline static dma_addr_t pci_map_single(struct pci_dev *hwdev , void *ptr , size_t size ,
                                          int direction )
{
  dma_addr_t tmp ;
  {
  tmp = dma_map_single_attrs((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                             ptr, size, (enum dma_data_direction )direction, (struct dma_attrs *)0);
  return (tmp);
}
}
__inline static void pci_unmap_single(struct pci_dev *hwdev , dma_addr_t dma_addr ,
                                      size_t size , int direction )
{
  {
  dma_unmap_single_attrs((unsigned long )hwdev != (unsigned long )((struct pci_dev *)0) ? & hwdev->dev : (struct device *)0,
                         dma_addr, size, (enum dma_data_direction )direction, (struct dma_attrs *)0);
  return;
}
}
static int amdgpu_ih_ring_alloc(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->irq.ih.ring_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, (unsigned long )adev->irq.ih.ring_size, 4096, 1, 2U,
                         0ULL, (struct sg_table *)0, & adev->irq.ih.ring_obj);
    if (r != 0) {
      drm_err("amdgpu: failed to create ih ring buffer (%d).\n", r);
      return (r);
    } else {
    }
    r = amdgpu_bo_reserve(adev->irq.ih.ring_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      return (r);
    } else {
    }
    r = amdgpu_bo_pin(adev->irq.ih.ring_obj, 2U, & adev->irq.ih.gpu_addr);
    if (r != 0) {
      amdgpu_bo_unreserve(adev->irq.ih.ring_obj);
      drm_err("amdgpu: failed to pin ih ring buffer (%d).\n", r);
      return (r);
    } else {
    }
    r = amdgpu_bo_kmap(adev->irq.ih.ring_obj, (void **)(& adev->irq.ih.ring));
    amdgpu_bo_unreserve(adev->irq.ih.ring_obj);
    if (r != 0) {
      drm_err("amdgpu: failed to map ih ring buffer (%d).\n", r);
      return (r);
    } else {
    }
  } else {
  }
  return (0);
}
}
int amdgpu_ih_ring_init(struct amdgpu_device *adev , unsigned int ring_size , bool use_bus_addr )
{
  u32 rb_bufsz ;
  int r ;
  unsigned long tmp ;
  int tmp___0 ;
  void *tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp = __roundup_pow_of_two((unsigned long )(ring_size / 4U));
  tmp___0 = __ilog2_u64((u64 )tmp);
  rb_bufsz = (u32 )tmp___0;
  ring_size = (unsigned int )(4 << (int )rb_bufsz);
  adev->irq.ih.ring_size = ring_size;
  adev->irq.ih.ptr_mask = adev->irq.ih.ring_size - 1U;
  adev->irq.ih.rptr = 0U;
  adev->irq.ih.use_bus_addr = use_bus_addr;
  if ((int )adev->irq.ih.use_bus_addr) {
    if ((unsigned long )adev->irq.ih.ring == (unsigned long )((u32 volatile *)0U)) {
      tmp___1 = kzalloc((size_t )(adev->irq.ih.ring_size + 8U), 208U);
      adev->irq.ih.ring = (u32 volatile *)tmp___1;
      if ((unsigned long )adev->irq.ih.ring == (unsigned long )((u32 volatile *)0U)) {
        return (-12);
      } else {
      }
      adev->irq.ih.rb_dma_addr = pci_map_single(adev->pdev, (void *)adev->irq.ih.ring,
                                                (size_t )adev->irq.ih.ring_size, 0);
      tmp___2 = pci_dma_mapping_error(adev->pdev, adev->irq.ih.rb_dma_addr);
      if (tmp___2 != 0) {
        dev_err((struct device const *)(& (adev->pdev)->dev), "Failed to DMA MAP the IH RB page\n");
        kfree((void const *)adev->irq.ih.ring);
        return (-12);
      } else {
      }
      adev->irq.ih.wptr_offs = adev->irq.ih.ring_size / 4U;
      adev->irq.ih.rptr_offs = adev->irq.ih.ring_size / 4U + 1U;
    } else {
    }
    return (0);
  } else {
    r = amdgpu_wb_get(adev, & adev->irq.ih.wptr_offs);
    if (r != 0) {
      dev_err((struct device const *)adev->dev, "(%d) ih wptr_offs wb alloc failed\n",
              r);
      return (r);
    } else {
    }
    r = amdgpu_wb_get(adev, & adev->irq.ih.rptr_offs);
    if (r != 0) {
      amdgpu_wb_free(adev, adev->irq.ih.wptr_offs);
      dev_err((struct device const *)adev->dev, "(%d) ih rptr_offs wb alloc failed\n",
              r);
      return (r);
    } else {
    }
    tmp___3 = amdgpu_ih_ring_alloc(adev);
    return (tmp___3);
  }
}
}
void amdgpu_ih_ring_fini(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((int )adev->irq.ih.use_bus_addr) {
    if ((unsigned long )adev->irq.ih.ring != (unsigned long )((u32 volatile *)0U)) {
      pci_unmap_single(adev->pdev, adev->irq.ih.rb_dma_addr, (size_t )(adev->irq.ih.ring_size + 8U),
                       0);
      kfree((void const *)adev->irq.ih.ring);
      adev->irq.ih.ring = (u32 volatile *)0U;
    } else {
    }
  } else {
    if ((unsigned long )adev->irq.ih.ring_obj != (unsigned long )((struct amdgpu_bo *)0)) {
      r = amdgpu_bo_reserve(adev->irq.ih.ring_obj, 0);
      tmp = ldv__builtin_expect(r == 0, 1L);
      if (tmp != 0L) {
        amdgpu_bo_kunmap(adev->irq.ih.ring_obj);
        amdgpu_bo_unpin(adev->irq.ih.ring_obj);
        amdgpu_bo_unreserve(adev->irq.ih.ring_obj);
      } else {
      }
      amdgpu_bo_unref(& adev->irq.ih.ring_obj);
      adev->irq.ih.ring = (u32 volatile *)0U;
      adev->irq.ih.ring_obj = (struct amdgpu_bo *)0;
    } else {
    }
    amdgpu_wb_free(adev, adev->irq.ih.wptr_offs);
    amdgpu_wb_free(adev, adev->irq.ih.rptr_offs);
  }
  return;
}
}
int amdgpu_ih_process(struct amdgpu_device *adev )
{
  struct amdgpu_iv_entry entry ;
  u32 wptr ;
  int tmp ;
  long tmp___0 ;
  {
  if (! adev->irq.ih.enabled || (int )adev->shutdown) {
    return (0);
  } else {
  }
  wptr = (*((adev->irq.ih_funcs)->get_wptr))(adev);
  restart_ih:
  tmp = atomic_xchg(& adev->irq.ih.lock, 1);
  if (tmp != 0) {
    return (0);
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("amdgpu_ih_process", "%s: rptr %d, wptr %d\n", "amdgpu_ih_process",
                        adev->irq.ih.rptr, wptr);
  } else {
  }
  __asm__ volatile ("lfence": : : "memory");
  goto ldv_43663;
  ldv_43662:
  (*((adev->irq.ih_funcs)->decode_iv))(adev, & entry);
  adev->irq.ih.rptr = adev->irq.ih.rptr & adev->irq.ih.ptr_mask;
  amdgpu_irq_dispatch(adev, & entry);
  ldv_43663: ;
  if (adev->irq.ih.rptr != wptr) {
    goto ldv_43662;
  } else {
  }
  (*((adev->irq.ih_funcs)->set_rptr))(adev);
  atomic_set(& adev->irq.ih.lock, 0);
  wptr = (*((adev->irq.ih_funcs)->get_wptr))(adev);
  if (adev->irq.ih.rptr != wptr) {
    goto restart_ih;
  } else {
  }
  return (1);
}
}
bool ldv_queue_work_on_739(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_740(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_741(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_742(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_743(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_753(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_755(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_754(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_757(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_756(struct workqueue_struct *ldv_func_arg1 ) ;
static void iceland_ih_set_interrupt_funcs(struct amdgpu_device *adev ) ;
static void iceland_ih_enable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_cntl ;
  u32 tmp ;
  u32 ih_rb_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp___0;
  ih_cntl = ih_cntl | 1U;
  ih_rb_cntl = ih_rb_cntl | 1U;
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  adev->irq.ih.enabled = 1;
  return;
}
}
static void iceland_ih_disable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_rb_cntl ;
  u32 tmp ;
  u32 ih_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = tmp___0;
  ih_rb_cntl = ih_rb_cntl & 4294967294U;
  ih_cntl = ih_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  adev->irq.ih.enabled = 0;
  adev->irq.ih.rptr = 0U;
  return;
}
}
static int iceland_ih_irq_init(struct amdgpu_device *adev )
{
  int ret ;
  int rb_bufsz ;
  u32 interrupt_cntl ;
  u32 ih_cntl ;
  u32 ih_rb_cntl ;
  u64 wptr_off ;
  unsigned long tmp ;
  {
  ret = 0;
  iceland_ih_disable_interrupts(adev);
  amdgpu_mm_wreg(adev, 5403U, (u32 )(adev->dummy_page.addr >> 8), 0);
  interrupt_cntl = amdgpu_mm_rreg(adev, 5402U, 0);
  interrupt_cntl = interrupt_cntl & 4294967294U;
  interrupt_cntl = interrupt_cntl & 4294967287U;
  amdgpu_mm_wreg(adev, 5402U, interrupt_cntl, 0);
  amdgpu_mm_wreg(adev, 3633U, (u32 )(adev->irq.ih.gpu_addr >> 8), 0);
  tmp = __roundup_pow_of_two((unsigned long )(adev->irq.ih.ring_size / 4U));
  rb_bufsz = __ilog2_u64((u64 )tmp);
  ih_rb_cntl = 65536U;
  ih_rb_cntl = ih_rb_cntl | 2147483648U;
  ih_rb_cntl = (ih_rb_cntl & 4294967233U) | ((u32 )(rb_bufsz << 1) & 62U);
  ih_rb_cntl = ih_rb_cntl | 256U;
  wptr_off = adev->wb.gpu_addr + (uint64_t )(adev->irq.ih.wptr_offs * 4U);
  amdgpu_mm_wreg(adev, 3637U, (unsigned int )wptr_off, 0);
  amdgpu_mm_wreg(adev, 3636U, (unsigned int )(wptr_off >> 32ULL) & 255U, 0);
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  ih_cntl = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = ih_cntl & 3791650815U;
  if ((int )adev->irq.msi_enabled) {
    ih_cntl = ih_cntl | 16U;
  } else {
  }
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  pci_set_master(adev->pdev);
  iceland_ih_enable_interrupts(adev);
  return (ret);
}
}
static void iceland_ih_irq_disable(struct amdgpu_device *adev )
{
  unsigned long __ms ;
  unsigned long tmp ;
  {
  iceland_ih_disable_interrupts(adev);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43666;
    ldv_43665:
    __const_udelay(4295000UL);
    ldv_43666:
    tmp = __ms;
    __ms = __ms - 1UL;
    if (tmp != 0UL) {
      goto ldv_43665;
    } else {
    }
  }
  return;
}
}
static u32 iceland_ih_get_wptr(struct amdgpu_device *adev )
{
  u32 wptr ;
  u32 tmp ;
  {
  wptr = *(adev->wb.wb + (unsigned long )adev->irq.ih.wptr_offs);
  if ((int )wptr & 1) {
    wptr = wptr & 4294967294U;
    dev_warn((struct device const *)adev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",
             wptr, adev->irq.ih.rptr, (wptr + 16U) & adev->irq.ih.ptr_mask);
    adev->irq.ih.rptr = (wptr + 16U) & adev->irq.ih.ptr_mask;
    tmp = amdgpu_mm_rreg(adev, 3632U, 0);
    tmp = tmp | 2147483648U;
    amdgpu_mm_wreg(adev, 3632U, tmp, 0);
  } else {
  }
  return (adev->irq.ih.ptr_mask & wptr);
}
}
static void iceland_ih_decode_iv(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry )
{
  u32 ring_index ;
  u32 dw[4U] ;
  {
  ring_index = adev->irq.ih.rptr >> 2;
  dw[0] = *(adev->irq.ih.ring + (unsigned long )ring_index);
  dw[1] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 1U));
  dw[2] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 2U));
  dw[3] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 3U));
  entry->src_id = dw[0] & 255U;
  entry->src_data = dw[1] & 268435455U;
  entry->ring_id = dw[2] & 255U;
  entry->vm_id = (dw[2] >> 8) & 255U;
  entry->pas_id = dw[2] >> 16;
  adev->irq.ih.rptr = adev->irq.ih.rptr + 16U;
  return;
}
}
static void iceland_ih_set_rptr(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 3634U, adev->irq.ih.rptr, 0);
  return;
}
}
static int iceland_ih_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  iceland_ih_set_interrupt_funcs(adev);
  return (0);
}
}
static int iceland_ih_sw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_ih_ring_init(adev, 65536U, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_init(adev);
  return (r);
}
}
static int iceland_ih_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_irq_fini(adev);
  amdgpu_ih_ring_fini(adev);
  return (0);
}
}
static int iceland_ih_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = iceland_ih_irq_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int iceland_ih_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  iceland_ih_irq_disable(adev);
  return (0);
}
}
static int iceland_ih_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = iceland_ih_hw_fini((void *)adev);
  return (tmp);
}
}
static int iceland_ih_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = iceland_ih_hw_init((void *)adev);
  return (tmp);
}
}
static bool iceland_ih_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) >> 17 != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int iceland_ih_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43724;
  ldv_43723:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 131072U) >> 17 == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43724: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43723;
  } else {
  }
  return (-110);
}
}
static void iceland_ih_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "ICELAND IH registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 5402U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 5403U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL2=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 3638U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_CNTL=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 3632U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 3633U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_BASE=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 3637U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_LO=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 3636U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_HI=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 3634U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_RPTR=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 3635U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR=0x%08X\n", tmp___9);
  return;
}
}
static int iceland_ih_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    iceland_ih_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    iceland_ih_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int iceland_ih_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int iceland_ih_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const iceland_ih_ip_funcs =
     {& iceland_ih_early_init, (int (*)(void * ))0, & iceland_ih_sw_init, & iceland_ih_sw_fini,
    & iceland_ih_hw_init, & iceland_ih_hw_fini, & iceland_ih_suspend, & iceland_ih_resume,
    & iceland_ih_is_idle, & iceland_ih_wait_for_idle, & iceland_ih_soft_reset, & iceland_ih_print_status,
    & iceland_ih_set_clockgating_state, & iceland_ih_set_powergating_state};
static struct amdgpu_ih_funcs const iceland_ih_funcs = {& iceland_ih_get_wptr, & iceland_ih_decode_iv, & iceland_ih_set_rptr};
static void iceland_ih_set_interrupt_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->irq.ih_funcs == (unsigned long )((struct amdgpu_ih_funcs const *)0)) {
    adev->irq.ih_funcs = & iceland_ih_funcs;
  } else {
  }
  return;
}
}
int ldv_retval_53 ;
extern int ldv_release_74(void) ;
int ldv_retval_52 ;
extern int ldv_probe_74(void) ;
void ldv_initialize_amdgpu_ih_funcs_73(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  iceland_ih_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_74(void)
{
  void *ldvarg597 ;
  void *tmp ;
  void *ldvarg598 ;
  void *tmp___0 ;
  void *ldvarg603 ;
  void *tmp___1 ;
  void *ldvarg604 ;
  void *tmp___2 ;
  enum amd_powergating_state ldvarg600 ;
  void *ldvarg594 ;
  void *tmp___3 ;
  void *ldvarg593 ;
  void *tmp___4 ;
  void *ldvarg601 ;
  void *tmp___5 ;
  void *ldvarg606 ;
  void *tmp___6 ;
  void *ldvarg602 ;
  void *tmp___7 ;
  void *ldvarg592 ;
  void *tmp___8 ;
  void *ldvarg599 ;
  void *tmp___9 ;
  void *ldvarg605 ;
  void *tmp___10 ;
  void *ldvarg595 ;
  void *tmp___11 ;
  enum amd_clockgating_state ldvarg596 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg597 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg598 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg603 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg604 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg594 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg593 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg601 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg606 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg602 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg592 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg599 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg605 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg595 = tmp___11;
  ldv_memset((void *)(& ldvarg600), 0, 4UL);
  ldv_memset((void *)(& ldvarg596), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_hw_fini(ldvarg606);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_hw_fini(ldvarg606);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_hw_fini(ldvarg606);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 1: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_print_status(ldvarg605);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_print_status(ldvarg605);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_print_status(ldvarg605);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 2: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_early_init(ldvarg604);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_early_init(ldvarg604);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_early_init(ldvarg604);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 3: ;
  if (ldv_state_variable_74 == 2) {
    ldv_retval_53 = iceland_ih_suspend(ldvarg603);
    if (ldv_retval_53 == 0) {
      ldv_state_variable_74 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43777;
  case 4: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_sw_init(ldvarg602);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_sw_init(ldvarg602);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_sw_init(ldvarg602);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 5: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_set_powergating_state(ldvarg601, ldvarg600);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_set_powergating_state(ldvarg601, ldvarg600);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_set_powergating_state(ldvarg601, ldvarg600);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 6: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_wait_for_idle(ldvarg599);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_wait_for_idle(ldvarg599);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_wait_for_idle(ldvarg599);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 7: ;
  if (ldv_state_variable_74 == 3) {
    ldv_retval_52 = iceland_ih_resume(ldvarg598);
    if (ldv_retval_52 == 0) {
      ldv_state_variable_74 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43777;
  case 8: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_set_clockgating_state(ldvarg597, ldvarg596);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_set_clockgating_state(ldvarg597, ldvarg596);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_set_clockgating_state(ldvarg597, ldvarg596);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 9: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_hw_init(ldvarg595);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_hw_init(ldvarg595);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_hw_init(ldvarg595);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 10: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_soft_reset(ldvarg594);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_soft_reset(ldvarg594);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_soft_reset(ldvarg594);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 11: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_sw_fini(ldvarg593);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_sw_fini(ldvarg593);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_sw_fini(ldvarg593);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 12: ;
  if (ldv_state_variable_74 == 2) {
    iceland_ih_is_idle(ldvarg592);
    ldv_state_variable_74 = 2;
  } else {
  }
  if (ldv_state_variable_74 == 1) {
    iceland_ih_is_idle(ldvarg592);
    ldv_state_variable_74 = 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    iceland_ih_is_idle(ldvarg592);
    ldv_state_variable_74 = 3;
  } else {
  }
  goto ldv_43777;
  case 13: ;
  if (ldv_state_variable_74 == 2) {
    ldv_release_74();
    ldv_state_variable_74 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_74 == 3) {
    ldv_release_74();
    ldv_state_variable_74 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43777;
  case 14: ;
  if (ldv_state_variable_74 == 1) {
    ldv_probe_74();
    ldv_state_variable_74 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43777;
  default:
  ldv_stop();
  }
  ldv_43777: ;
  return;
}
}
void ldv_main_exported_73(void)
{
  struct amdgpu_iv_entry *ldvarg249 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg249 = (struct amdgpu_iv_entry *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_73 == 1) {
    iceland_ih_decode_iv(iceland_ih_funcs_group0, ldvarg249);
    ldv_state_variable_73 = 1;
  } else {
  }
  goto ldv_43798;
  case 1: ;
  if (ldv_state_variable_73 == 1) {
    iceland_ih_get_wptr(iceland_ih_funcs_group0);
    ldv_state_variable_73 = 1;
  } else {
  }
  goto ldv_43798;
  case 2: ;
  if (ldv_state_variable_73 == 1) {
    iceland_ih_set_rptr(iceland_ih_funcs_group0);
    ldv_state_variable_73 = 1;
  } else {
  }
  goto ldv_43798;
  default:
  ldv_stop();
  }
  ldv_43798: ;
  return;
}
}
bool ldv_queue_work_on_753(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_754(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_755(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_756(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_757(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_767(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_769(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_768(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_771(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_770(struct workqueue_struct *ldv_func_arg1 ) ;
static void tonga_ih_set_interrupt_funcs(struct amdgpu_device *adev ) ;
static void tonga_ih_enable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_rb_cntl ;
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp;
  ih_rb_cntl = ih_rb_cntl | 1U;
  ih_rb_cntl = ih_rb_cntl | 131072U;
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  adev->irq.ih.enabled = 1;
  return;
}
}
static void tonga_ih_disable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_rb_cntl ;
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp;
  ih_rb_cntl = ih_rb_cntl & 4294967294U;
  ih_rb_cntl = ih_rb_cntl & 4294836223U;
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  adev->irq.ih.enabled = 0;
  adev->irq.ih.rptr = 0U;
  return;
}
}
static int tonga_ih_irq_init(struct amdgpu_device *adev )
{
  int ret ;
  int rb_bufsz ;
  u32 interrupt_cntl ;
  u32 ih_rb_cntl ;
  u32 ih_doorbell_rtpr ;
  u64 wptr_off ;
  unsigned long tmp ;
  {
  ret = 0;
  tonga_ih_disable_interrupts(adev);
  amdgpu_mm_wreg(adev, 5403U, (u32 )(adev->dummy_page.addr >> 8), 0);
  interrupt_cntl = amdgpu_mm_rreg(adev, 5402U, 0);
  interrupt_cntl = interrupt_cntl & 4294967294U;
  interrupt_cntl = interrupt_cntl & 4294967287U;
  amdgpu_mm_wreg(adev, 5402U, interrupt_cntl, 0);
  if ((int )adev->irq.ih.use_bus_addr) {
    amdgpu_mm_wreg(adev, 3633U, (u32 )(adev->irq.ih.rb_dma_addr >> 8), 0);
  } else {
    amdgpu_mm_wreg(adev, 3633U, (u32 )(adev->irq.ih.gpu_addr >> 8), 0);
  }
  tmp = __roundup_pow_of_two((unsigned long )(adev->irq.ih.ring_size / 4U));
  rb_bufsz = __ilog2_u64((u64 )tmp);
  ih_rb_cntl = 2147483648U;
  ih_rb_cntl = (ih_rb_cntl & 4294967233U) | ((u32 )(rb_bufsz << 1) & 62U);
  ih_rb_cntl = ih_rb_cntl | 256U;
  ih_rb_cntl = ih_rb_cntl & 4043309055U;
  if ((int )adev->irq.msi_enabled) {
    ih_rb_cntl = ih_rb_cntl | 2097152U;
  } else {
  }
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  if ((int )adev->irq.ih.use_bus_addr) {
    wptr_off = adev->irq.ih.rb_dma_addr + (dma_addr_t )(adev->irq.ih.wptr_offs * 4U);
  } else {
    wptr_off = adev->wb.gpu_addr + (uint64_t )(adev->irq.ih.wptr_offs * 4U);
  }
  amdgpu_mm_wreg(adev, 3637U, (unsigned int )wptr_off, 0);
  amdgpu_mm_wreg(adev, 3636U, (unsigned int )(wptr_off >> 32ULL) & 255U, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  ih_doorbell_rtpr = amdgpu_mm_rreg(adev, 3650U, 0);
  if ((int )adev->irq.ih.use_doorbell) {
    ih_doorbell_rtpr = (ih_doorbell_rtpr & 4292870144U) | (adev->irq.ih.doorbell_index & 2097151U);
    ih_doorbell_rtpr = ih_doorbell_rtpr | 268435456U;
  } else {
    ih_doorbell_rtpr = ih_doorbell_rtpr & 4026531839U;
  }
  amdgpu_mm_wreg(adev, 3650U, ih_doorbell_rtpr, 0);
  pci_set_master(adev->pdev);
  tonga_ih_enable_interrupts(adev);
  return (ret);
}
}
static void tonga_ih_irq_disable(struct amdgpu_device *adev )
{
  unsigned long __ms ;
  unsigned long tmp ;
  {
  tonga_ih_disable_interrupts(adev);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43664;
    ldv_43663:
    __const_udelay(4295000UL);
    ldv_43664:
    tmp = __ms;
    __ms = __ms - 1UL;
    if (tmp != 0UL) {
      goto ldv_43663;
    } else {
    }
  }
  return;
}
}
static u32 tonga_ih_get_wptr(struct amdgpu_device *adev )
{
  u32 wptr ;
  u32 tmp ;
  {
  if ((int )adev->irq.ih.use_bus_addr) {
    wptr = *(adev->irq.ih.ring + (unsigned long )adev->irq.ih.wptr_offs);
  } else {
    wptr = *(adev->wb.wb + (unsigned long )adev->irq.ih.wptr_offs);
  }
  if ((int )wptr & 1) {
    wptr = wptr & 4294967294U;
    dev_warn((struct device const *)adev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",
             wptr, adev->irq.ih.rptr, (wptr + 16U) & adev->irq.ih.ptr_mask);
    adev->irq.ih.rptr = (wptr + 16U) & adev->irq.ih.ptr_mask;
    tmp = amdgpu_mm_rreg(adev, 3632U, 0);
    tmp = tmp | 2147483648U;
    amdgpu_mm_wreg(adev, 3632U, tmp, 0);
  } else {
  }
  return (adev->irq.ih.ptr_mask & wptr);
}
}
static void tonga_ih_decode_iv(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry )
{
  u32 ring_index ;
  u32 dw[4U] ;
  {
  ring_index = adev->irq.ih.rptr >> 2;
  dw[0] = *(adev->irq.ih.ring + (unsigned long )ring_index);
  dw[1] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 1U));
  dw[2] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 2U));
  dw[3] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 3U));
  entry->src_id = dw[0] & 255U;
  entry->src_data = dw[1] & 268435455U;
  entry->ring_id = dw[2] & 255U;
  entry->vm_id = (dw[2] >> 8) & 255U;
  entry->pas_id = dw[2] >> 16;
  adev->irq.ih.rptr = adev->irq.ih.rptr + 16U;
  return;
}
}
static void tonga_ih_set_rptr(struct amdgpu_device *adev )
{
  {
  if ((int )adev->irq.ih.use_doorbell) {
    if ((int )adev->irq.ih.use_bus_addr) {
      *(adev->irq.ih.ring + (unsigned long )adev->irq.ih.rptr_offs) = adev->irq.ih.rptr;
    } else {
      *(adev->wb.wb + (unsigned long )adev->irq.ih.rptr_offs) = adev->irq.ih.rptr;
    }
    amdgpu_mm_wdoorbell(adev, adev->irq.ih.doorbell_index, adev->irq.ih.rptr);
  } else {
    amdgpu_mm_wreg(adev, 3634U, adev->irq.ih.rptr, 0);
  }
  return;
}
}
static int tonga_ih_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  tonga_ih_set_interrupt_funcs(adev);
  return (0);
}
}
static int tonga_ih_sw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_ih_ring_init(adev, 4096U, 1);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->irq.ih.use_doorbell = 1;
  adev->irq.ih.doorbell_index = 488U;
  r = amdgpu_irq_init(adev);
  return (r);
}
}
static int tonga_ih_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_irq_fini(adev);
  amdgpu_ih_ring_fini(adev);
  return (0);
}
}
static int tonga_ih_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = tonga_ih_irq_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int tonga_ih_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  tonga_ih_irq_disable(adev);
  return (0);
}
}
static int tonga_ih_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = tonga_ih_hw_fini((void *)adev);
  return (tmp);
}
}
static int tonga_ih_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = tonga_ih_hw_init((void *)adev);
  return (tmp);
}
}
static bool tonga_ih_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) >> 17 != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int tonga_ih_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43722;
  ldv_43721:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 131072U) >> 17 == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43722: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43721;
  } else {
  }
  return (-110);
}
}
static void tonga_ih_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "TONGA IH registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 5402U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 5403U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL2=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 3638U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_CNTL=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 3632U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 3633U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_BASE=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 3637U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_LO=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 3636U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_HI=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 3634U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_RPTR=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 3635U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR=0x%08X\n", tmp___9);
  return;
}
}
static int tonga_ih_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    tonga_ih_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tonga_ih_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int tonga_ih_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int tonga_ih_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const tonga_ih_ip_funcs =
     {& tonga_ih_early_init, (int (*)(void * ))0, & tonga_ih_sw_init, & tonga_ih_sw_fini,
    & tonga_ih_hw_init, & tonga_ih_hw_fini, & tonga_ih_suspend, & tonga_ih_resume,
    & tonga_ih_is_idle, & tonga_ih_wait_for_idle, & tonga_ih_soft_reset, & tonga_ih_print_status,
    & tonga_ih_set_clockgating_state, & tonga_ih_set_powergating_state};
static struct amdgpu_ih_funcs const tonga_ih_funcs = {& tonga_ih_get_wptr, & tonga_ih_decode_iv, & tonga_ih_set_rptr};
static void tonga_ih_set_interrupt_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->irq.ih_funcs == (unsigned long )((struct amdgpu_ih_funcs const *)0)) {
    adev->irq.ih_funcs = & tonga_ih_funcs;
  } else {
  }
  return;
}
}
int ldv_retval_50 ;
extern int ldv_probe_72(void) ;
extern int ldv_release_72(void) ;
int ldv_retval_51 ;
void ldv_initialize_amdgpu_ih_funcs_71(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  tonga_ih_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_72(void)
{
  enum amd_clockgating_state ldvarg581 ;
  void *ldvarg583 ;
  void *tmp ;
  void *ldvarg586 ;
  void *tmp___0 ;
  void *ldvarg591 ;
  void *tmp___1 ;
  void *ldvarg589 ;
  void *tmp___2 ;
  void *ldvarg580 ;
  void *tmp___3 ;
  void *ldvarg579 ;
  void *tmp___4 ;
  void *ldvarg590 ;
  void *tmp___5 ;
  void *ldvarg584 ;
  void *tmp___6 ;
  void *ldvarg588 ;
  void *tmp___7 ;
  void *ldvarg578 ;
  void *tmp___8 ;
  void *ldvarg577 ;
  void *tmp___9 ;
  void *ldvarg582 ;
  void *tmp___10 ;
  enum amd_powergating_state ldvarg585 ;
  void *ldvarg587 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg583 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg586 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg591 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg589 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg580 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg579 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg590 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg584 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg588 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg578 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg577 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg582 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg587 = tmp___11;
  ldv_memset((void *)(& ldvarg581), 0, 4UL);
  ldv_memset((void *)(& ldvarg585), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_hw_fini(ldvarg591);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_hw_fini(ldvarg591);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_hw_fini(ldvarg591);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 1: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_print_status(ldvarg590);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_print_status(ldvarg590);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_print_status(ldvarg590);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 2: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_early_init(ldvarg589);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_early_init(ldvarg589);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_early_init(ldvarg589);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 3: ;
  if (ldv_state_variable_72 == 2) {
    ldv_retval_51 = tonga_ih_suspend(ldvarg588);
    if (ldv_retval_51 == 0) {
      ldv_state_variable_72 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43775;
  case 4: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_sw_init(ldvarg587);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_sw_init(ldvarg587);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_sw_init(ldvarg587);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 5: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_set_powergating_state(ldvarg586, ldvarg585);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_set_powergating_state(ldvarg586, ldvarg585);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_set_powergating_state(ldvarg586, ldvarg585);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 6: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_wait_for_idle(ldvarg584);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_wait_for_idle(ldvarg584);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_wait_for_idle(ldvarg584);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 7: ;
  if (ldv_state_variable_72 == 3) {
    ldv_retval_50 = tonga_ih_resume(ldvarg583);
    if (ldv_retval_50 == 0) {
      ldv_state_variable_72 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43775;
  case 8: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_set_clockgating_state(ldvarg582, ldvarg581);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_set_clockgating_state(ldvarg582, ldvarg581);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_set_clockgating_state(ldvarg582, ldvarg581);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 9: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_hw_init(ldvarg580);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_hw_init(ldvarg580);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_hw_init(ldvarg580);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 10: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_soft_reset(ldvarg579);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_soft_reset(ldvarg579);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_soft_reset(ldvarg579);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 11: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_sw_fini(ldvarg578);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_sw_fini(ldvarg578);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_sw_fini(ldvarg578);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 12: ;
  if (ldv_state_variable_72 == 2) {
    tonga_ih_is_idle(ldvarg577);
    ldv_state_variable_72 = 2;
  } else {
  }
  if (ldv_state_variable_72 == 1) {
    tonga_ih_is_idle(ldvarg577);
    ldv_state_variable_72 = 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    tonga_ih_is_idle(ldvarg577);
    ldv_state_variable_72 = 3;
  } else {
  }
  goto ldv_43775;
  case 13: ;
  if (ldv_state_variable_72 == 2) {
    ldv_release_72();
    ldv_state_variable_72 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_72 == 3) {
    ldv_release_72();
    ldv_state_variable_72 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43775;
  case 14: ;
  if (ldv_state_variable_72 == 1) {
    ldv_probe_72();
    ldv_state_variable_72 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43775;
  default:
  ldv_stop();
  }
  ldv_43775: ;
  return;
}
}
void ldv_main_exported_71(void)
{
  struct amdgpu_iv_entry *ldvarg23 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg23 = (struct amdgpu_iv_entry *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_71 == 1) {
    tonga_ih_decode_iv(tonga_ih_funcs_group0, ldvarg23);
    ldv_state_variable_71 = 1;
  } else {
  }
  goto ldv_43796;
  case 1: ;
  if (ldv_state_variable_71 == 1) {
    tonga_ih_get_wptr(tonga_ih_funcs_group0);
    ldv_state_variable_71 = 1;
  } else {
  }
  goto ldv_43796;
  case 2: ;
  if (ldv_state_variable_71 == 1) {
    tonga_ih_set_rptr(tonga_ih_funcs_group0);
    ldv_state_variable_71 = 1;
  } else {
  }
  goto ldv_43796;
  default:
  ldv_stop();
  }
  ldv_43796: ;
  return;
}
}
bool ldv_queue_work_on_767(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_768(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_769(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_770(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_771(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_781(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_783(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_782(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_785(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_784(struct workqueue_struct *ldv_func_arg1 ) ;
static void cz_ih_set_interrupt_funcs(struct amdgpu_device *adev ) ;
static void cz_ih_enable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_cntl ;
  u32 tmp ;
  u32 ih_rb_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp___0;
  ih_cntl = ih_cntl | 1U;
  ih_rb_cntl = ih_rb_cntl | 1U;
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  adev->irq.ih.enabled = 1;
  return;
}
}
static void cz_ih_disable_interrupts(struct amdgpu_device *adev )
{
  u32 ih_rb_cntl ;
  u32 tmp ;
  u32 ih_cntl ;
  u32 tmp___0 ;
  {
  tmp = amdgpu_mm_rreg(adev, 3632U, 0);
  ih_rb_cntl = tmp;
  tmp___0 = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = tmp___0;
  ih_rb_cntl = ih_rb_cntl & 4294967294U;
  ih_cntl = ih_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  adev->irq.ih.enabled = 0;
  adev->irq.ih.rptr = 0U;
  return;
}
}
static int cz_ih_irq_init(struct amdgpu_device *adev )
{
  int ret ;
  int rb_bufsz ;
  u32 interrupt_cntl ;
  u32 ih_cntl ;
  u32 ih_rb_cntl ;
  u64 wptr_off ;
  unsigned long tmp ;
  {
  ret = 0;
  cz_ih_disable_interrupts(adev);
  amdgpu_mm_wreg(adev, 5403U, (u32 )(adev->dummy_page.addr >> 8), 0);
  interrupt_cntl = amdgpu_mm_rreg(adev, 5402U, 0);
  interrupt_cntl = interrupt_cntl & 4294967294U;
  interrupt_cntl = interrupt_cntl & 4294967287U;
  amdgpu_mm_wreg(adev, 5402U, interrupt_cntl, 0);
  amdgpu_mm_wreg(adev, 3633U, (u32 )(adev->irq.ih.gpu_addr >> 8), 0);
  tmp = __roundup_pow_of_two((unsigned long )(adev->irq.ih.ring_size / 4U));
  rb_bufsz = __ilog2_u64((u64 )tmp);
  ih_rb_cntl = 65536U;
  ih_rb_cntl = ih_rb_cntl | 2147483648U;
  ih_rb_cntl = (ih_rb_cntl & 4294967233U) | ((u32 )(rb_bufsz << 1) & 62U);
  ih_rb_cntl = ih_rb_cntl | 256U;
  wptr_off = adev->wb.gpu_addr + (uint64_t )(adev->irq.ih.wptr_offs * 4U);
  amdgpu_mm_wreg(adev, 3637U, (unsigned int )wptr_off, 0);
  amdgpu_mm_wreg(adev, 3636U, (unsigned int )(wptr_off >> 32ULL) & 255U, 0);
  amdgpu_mm_wreg(adev, 3632U, ih_rb_cntl, 0);
  amdgpu_mm_wreg(adev, 3634U, 0U, 0);
  amdgpu_mm_wreg(adev, 3635U, 0U, 0);
  ih_cntl = amdgpu_mm_rreg(adev, 3638U, 0);
  ih_cntl = ih_cntl & 3791650815U;
  if ((int )adev->irq.msi_enabled) {
    ih_cntl = ih_cntl | 16U;
  } else {
  }
  amdgpu_mm_wreg(adev, 3638U, ih_cntl, 0);
  pci_set_master(adev->pdev);
  cz_ih_enable_interrupts(adev);
  return (ret);
}
}
static void cz_ih_irq_disable(struct amdgpu_device *adev )
{
  unsigned long __ms ;
  unsigned long tmp ;
  {
  cz_ih_disable_interrupts(adev);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43666;
    ldv_43665:
    __const_udelay(4295000UL);
    ldv_43666:
    tmp = __ms;
    __ms = __ms - 1UL;
    if (tmp != 0UL) {
      goto ldv_43665;
    } else {
    }
  }
  return;
}
}
static u32 cz_ih_get_wptr(struct amdgpu_device *adev )
{
  u32 wptr ;
  u32 tmp ;
  {
  wptr = *(adev->wb.wb + (unsigned long )adev->irq.ih.wptr_offs);
  if ((int )wptr & 1) {
    wptr = wptr & 4294967294U;
    dev_warn((struct device const *)adev->dev, "IH ring buffer overflow (0x%08X, 0x%08X, 0x%08X)\n",
             wptr, adev->irq.ih.rptr, (wptr + 16U) & adev->irq.ih.ptr_mask);
    adev->irq.ih.rptr = (wptr + 16U) & adev->irq.ih.ptr_mask;
    tmp = amdgpu_mm_rreg(adev, 3632U, 0);
    tmp = tmp | 2147483648U;
    amdgpu_mm_wreg(adev, 3632U, tmp, 0);
  } else {
  }
  return (adev->irq.ih.ptr_mask & wptr);
}
}
static void cz_ih_decode_iv(struct amdgpu_device *adev , struct amdgpu_iv_entry *entry )
{
  u32 ring_index ;
  u32 dw[4U] ;
  {
  ring_index = adev->irq.ih.rptr >> 2;
  dw[0] = *(adev->irq.ih.ring + (unsigned long )ring_index);
  dw[1] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 1U));
  dw[2] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 2U));
  dw[3] = *(adev->irq.ih.ring + (unsigned long )(ring_index + 3U));
  entry->src_id = dw[0] & 255U;
  entry->src_data = dw[1] & 268435455U;
  entry->ring_id = dw[2] & 255U;
  entry->vm_id = (dw[2] >> 8) & 255U;
  entry->pas_id = dw[2] >> 16;
  adev->irq.ih.rptr = adev->irq.ih.rptr + 16U;
  return;
}
}
static void cz_ih_set_rptr(struct amdgpu_device *adev )
{
  {
  amdgpu_mm_wreg(adev, 3634U, adev->irq.ih.rptr, 0);
  return;
}
}
static int cz_ih_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cz_ih_set_interrupt_funcs(adev);
  return (0);
}
}
static int cz_ih_sw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_ih_ring_init(adev, 65536U, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_init(adev);
  return (r);
}
}
static int cz_ih_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_irq_fini(adev);
  amdgpu_ih_ring_fini(adev);
  return (0);
}
}
static int cz_ih_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = cz_ih_irq_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int cz_ih_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cz_ih_irq_disable(adev);
  return (0);
}
}
static int cz_ih_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cz_ih_hw_fini((void *)adev);
  return (tmp);
}
}
static int cz_ih_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = cz_ih_hw_init((void *)adev);
  return (tmp);
}
}
static bool cz_ih_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) >> 17 != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int cz_ih_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43724;
  ldv_43723:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 131072U) >> 17 == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43724: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43723;
  } else {
  }
  return (-110);
}
}
static void cz_ih_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "CZ IH registers\n");
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 5402U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 5403U, 0);
  _dev_info((struct device const *)adev->dev, "  INTERRUPT_CNTL2=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 3638U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_CNTL=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 3632U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_CNTL=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 3633U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_BASE=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 3637U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_LO=0x%08X\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 3636U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR_ADDR_HI=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 3634U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_RPTR=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 3635U, 0);
  _dev_info((struct device const *)adev->dev, "  IH_RB_WPTR=0x%08X\n", tmp___9);
  return;
}
}
static int cz_ih_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 916U, 0);
  tmp = tmp___0;
  if ((tmp & 131072U) != 0U) {
    srbm_soft_reset = srbm_soft_reset | 1024U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    cz_ih_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    cz_ih_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int cz_ih_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int cz_ih_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const cz_ih_ip_funcs =
     {& cz_ih_early_init, (int (*)(void * ))0, & cz_ih_sw_init, & cz_ih_sw_fini, & cz_ih_hw_init,
    & cz_ih_hw_fini, & cz_ih_suspend, & cz_ih_resume, & cz_ih_is_idle, & cz_ih_wait_for_idle,
    & cz_ih_soft_reset, & cz_ih_print_status, & cz_ih_set_clockgating_state, & cz_ih_set_powergating_state};
static struct amdgpu_ih_funcs const cz_ih_funcs = {& cz_ih_get_wptr, & cz_ih_decode_iv, & cz_ih_set_rptr};
static void cz_ih_set_interrupt_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->irq.ih_funcs == (unsigned long )((struct amdgpu_ih_funcs const *)0)) {
    adev->irq.ih_funcs = & cz_ih_funcs;
  } else {
  }
  return;
}
}
extern int ldv_release_70(void) ;
int ldv_retval_68 ;
extern int ldv_probe_70(void) ;
int ldv_retval_69 ;
void ldv_initialize_amdgpu_ih_funcs_69(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  cz_ih_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_69(void)
{
  struct amdgpu_iv_entry *ldvarg357 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg357 = (struct amdgpu_iv_entry *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_69 == 1) {
    cz_ih_decode_iv(cz_ih_funcs_group0, ldvarg357);
    ldv_state_variable_69 = 1;
  } else {
  }
  goto ldv_43763;
  case 1: ;
  if (ldv_state_variable_69 == 1) {
    cz_ih_get_wptr(cz_ih_funcs_group0);
    ldv_state_variable_69 = 1;
  } else {
  }
  goto ldv_43763;
  case 2: ;
  if (ldv_state_variable_69 == 1) {
    cz_ih_set_rptr(cz_ih_funcs_group0);
    ldv_state_variable_69 = 1;
  } else {
  }
  goto ldv_43763;
  default:
  ldv_stop();
  }
  ldv_43763: ;
  return;
}
}
void ldv_main_exported_70(void)
{
  void *ldvarg854 ;
  void *tmp ;
  void *ldvarg846 ;
  void *tmp___0 ;
  void *ldvarg841 ;
  void *tmp___1 ;
  void *ldvarg850 ;
  void *tmp___2 ;
  void *ldvarg842 ;
  void *tmp___3 ;
  void *ldvarg855 ;
  void *tmp___4 ;
  enum amd_clockgating_state ldvarg845 ;
  void *ldvarg853 ;
  void *tmp___5 ;
  void *ldvarg844 ;
  void *tmp___6 ;
  void *ldvarg847 ;
  void *tmp___7 ;
  void *ldvarg852 ;
  void *tmp___8 ;
  void *ldvarg848 ;
  void *tmp___9 ;
  void *ldvarg843 ;
  void *tmp___10 ;
  enum amd_powergating_state ldvarg849 ;
  void *ldvarg851 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg854 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg846 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg841 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg850 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg842 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg855 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg853 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg844 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg847 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg852 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg848 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg843 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg851 = tmp___11;
  ldv_memset((void *)(& ldvarg845), 0, 4UL);
  ldv_memset((void *)(& ldvarg849), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_hw_fini(ldvarg855);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_hw_fini(ldvarg855);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_hw_fini(ldvarg855);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 1: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_print_status(ldvarg854);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_print_status(ldvarg854);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_print_status(ldvarg854);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 2: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_early_init(ldvarg853);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_early_init(ldvarg853);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_early_init(ldvarg853);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 3: ;
  if (ldv_state_variable_70 == 2) {
    ldv_retval_69 = cz_ih_suspend(ldvarg852);
    if (ldv_retval_69 == 0) {
      ldv_state_variable_70 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43786;
  case 4: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_sw_init(ldvarg851);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_sw_init(ldvarg851);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_sw_init(ldvarg851);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 5: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_set_powergating_state(ldvarg850, ldvarg849);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_set_powergating_state(ldvarg850, ldvarg849);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_set_powergating_state(ldvarg850, ldvarg849);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 6: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_wait_for_idle(ldvarg848);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_wait_for_idle(ldvarg848);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_wait_for_idle(ldvarg848);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 7: ;
  if (ldv_state_variable_70 == 3) {
    ldv_retval_68 = cz_ih_resume(ldvarg847);
    if (ldv_retval_68 == 0) {
      ldv_state_variable_70 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43786;
  case 8: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_set_clockgating_state(ldvarg846, ldvarg845);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_set_clockgating_state(ldvarg846, ldvarg845);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_set_clockgating_state(ldvarg846, ldvarg845);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 9: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_hw_init(ldvarg844);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_hw_init(ldvarg844);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_hw_init(ldvarg844);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 10: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_soft_reset(ldvarg843);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_soft_reset(ldvarg843);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_soft_reset(ldvarg843);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 11: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_sw_fini(ldvarg842);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_sw_fini(ldvarg842);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_sw_fini(ldvarg842);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 12: ;
  if (ldv_state_variable_70 == 2) {
    cz_ih_is_idle(ldvarg841);
    ldv_state_variable_70 = 2;
  } else {
  }
  if (ldv_state_variable_70 == 1) {
    cz_ih_is_idle(ldvarg841);
    ldv_state_variable_70 = 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    cz_ih_is_idle(ldvarg841);
    ldv_state_variable_70 = 3;
  } else {
  }
  goto ldv_43786;
  case 13: ;
  if (ldv_state_variable_70 == 2) {
    ldv_release_70();
    ldv_state_variable_70 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_70 == 3) {
    ldv_release_70();
    ldv_state_variable_70 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43786;
  case 14: ;
  if (ldv_state_variable_70 == 1) {
    ldv_probe_70();
    ldv_state_variable_70 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43786;
  default:
  ldv_stop();
  }
  ldv_43786: ;
  return;
}
}
bool ldv_queue_work_on_781(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_782(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_783(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_784(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_785(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
extern size_t strlcpy(char * , char const * , size_t ) ;
bool ldv_queue_work_on_795(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_797(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_796(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_799(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_798(struct workqueue_struct *ldv_func_arg1 ) ;
extern struct i2c_client *i2c_new_device(struct i2c_adapter * , struct i2c_board_info const * ) ;
bool amdgpu_is_uvd_state(u32 class , u32 class2 ) ;
void amdgpu_calculate_u_and_p(u32 i , u32 r_c , u32 p_b , u32 *p , u32 *u ) ;
int amdgpu_calculate_at(u32 t , u32 h , u32 fh , u32 fl , u32 *tl , u32 *th ) ;
void amdgpu_dpm_print_class_info(u32 class , u32 class2 )
{
  {
  printk("\tui class: ");
  switch (class & 7U) {
  case 0U: ;
  default:
  printk("none\n");
  goto ldv_47985;
  case 1U:
  printk("battery\n");
  goto ldv_47985;
  case 3U:
  printk("balanced\n");
  goto ldv_47985;
  case 5U:
  printk("performance\n");
  goto ldv_47985;
  }
  ldv_47985:
  printk("\tinternal class: ");
  if ((class & 4294967288U) == 0U && class2 == 0U) {
    printk("none");
  } else {
    if ((class & 8U) != 0U) {
      printk("boot ");
    } else {
    }
    if ((class & 16U) != 0U) {
      printk("thermal ");
    } else {
    }
    if ((class & 32U) != 0U) {
      printk("limited_pwr ");
    } else {
    }
    if ((class & 64U) != 0U) {
      printk("rest ");
    } else {
    }
    if ((class & 128U) != 0U) {
      printk("forced ");
    } else {
    }
    if ((class & 256U) != 0U) {
      printk("3d_perf ");
    } else {
    }
    if ((class & 512U) != 0U) {
      printk("ovrdrv ");
    } else {
    }
    if ((class & 1024U) != 0U) {
      printk("uvd ");
    } else {
    }
    if ((class & 2048U) != 0U) {
      printk("3d_low ");
    } else {
    }
    if ((class & 4096U) != 0U) {
      printk("acpi ");
    } else {
    }
    if ((class & 8192U) != 0U) {
      printk("uvd_hd2 ");
    } else {
    }
    if ((class & 16384U) != 0U) {
      printk("uvd_hd ");
    } else {
    }
    if ((class & 32768U) != 0U) {
      printk("uvd_sd ");
    } else {
    }
    if ((int )class2 & 1) {
      printk("limited_pwr2 ");
    } else {
    }
    if ((class2 & 2U) != 0U) {
      printk("ulv ");
    } else {
    }
    if ((class2 & 4U) != 0U) {
      printk("uvd_mvc ");
    } else {
    }
  }
  printk("\n");
  return;
}
}
void amdgpu_dpm_print_cap_info(u32 caps )
{
  {
  printk("\tcaps: ");
  if ((int )caps & 1) {
    printk("single_disp ");
  } else {
  }
  if ((caps & 2U) != 0U) {
    printk("video ");
  } else {
  }
  if ((caps & 16384U) != 0U) {
    printk("no_dc ");
  } else {
  }
  printk("\n");
  return;
}
}
void amdgpu_dpm_print_ps_status(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  {
  printk("\tstatus: ");
  if ((unsigned long )adev->pm.dpm.current_ps == (unsigned long )rps) {
    printk("c ");
  } else {
  }
  if ((unsigned long )adev->pm.dpm.requested_ps == (unsigned long )rps) {
    printk("r ");
  } else {
  }
  if ((unsigned long )adev->pm.dpm.boot_ps == (unsigned long )rps) {
    printk("b ");
  } else {
  }
  printk("\n");
  return;
}
}
u32 amdgpu_dpm_get_vblank_time(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_crtc *crtc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  u32 line_time_us ;
  u32 vblank_lines ;
  u32 vblank_time_us ;
  struct list_head const *__mptr ;
  struct drm_crtc const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  vblank_time_us = 4294967295U;
  if (adev->mode_info.num_crtc != 0 && (int )adev->mode_info.mode_config_initialized) {
    __mptr = (struct list_head const *)dev->mode_config.crtc_list.next;
    crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
    goto ldv_48013;
    ldv_48012:
    __mptr___0 = (struct drm_crtc const *)crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    if (((int )crtc->enabled && (int )amdgpu_crtc->enabled) && amdgpu_crtc->hw_mode.clock != 0) {
      line_time_us = (u32 )((amdgpu_crtc->hw_mode.crtc_htotal * 1000) / amdgpu_crtc->hw_mode.clock);
      vblank_lines = (u32 )((amdgpu_crtc->hw_mode.crtc_vblank_end - amdgpu_crtc->hw_mode.crtc_vdisplay) + (int )amdgpu_crtc->v_border * 2);
      vblank_time_us = vblank_lines * line_time_us;
      goto ldv_48011;
    } else {
    }
    __mptr___1 = (struct list_head const *)crtc->head.next;
    crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
    ldv_48013: ;
    if ((unsigned long )(& crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
      goto ldv_48012;
    } else {
    }
    ldv_48011: ;
  } else {
  }
  return (vblank_time_us);
}
}
u32 amdgpu_dpm_get_vrefresh(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_crtc *crtc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  u32 vrefresh ;
  struct list_head const *__mptr ;
  struct drm_crtc const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  vrefresh = 0U;
  if (adev->mode_info.num_crtc != 0 && (int )adev->mode_info.mode_config_initialized) {
    __mptr = (struct list_head const *)dev->mode_config.crtc_list.next;
    crtc = (struct drm_crtc *)__mptr + 0xfffffffffffffff0UL;
    goto ldv_48029;
    ldv_48028:
    __mptr___0 = (struct drm_crtc const *)crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    if (((int )crtc->enabled && (int )amdgpu_crtc->enabled) && amdgpu_crtc->hw_mode.clock != 0) {
      vrefresh = (u32 )amdgpu_crtc->hw_mode.vrefresh;
      goto ldv_48027;
    } else {
    }
    __mptr___1 = (struct list_head const *)crtc->head.next;
    crtc = (struct drm_crtc *)__mptr___1 + 0xfffffffffffffff0UL;
    ldv_48029: ;
    if ((unsigned long )(& crtc->head) != (unsigned long )(& dev->mode_config.crtc_list)) {
      goto ldv_48028;
    } else {
    }
    ldv_48027: ;
  } else {
  }
  return (vrefresh);
}
}
void amdgpu_calculate_u_and_p(u32 i , u32 r_c , u32 p_b , u32 *p , u32 *u )
{
  u32 b_c ;
  u32 i_c ;
  u32 tmp ;
  {
  b_c = 0U;
  i_c = (i * r_c) / 100U;
  tmp = i_c >> (int )p_b;
  goto ldv_48041;
  ldv_48040:
  b_c = b_c + 1U;
  tmp = tmp >> 1;
  ldv_48041: ;
  if (tmp != 0U) {
    goto ldv_48040;
  } else {
  }
  *u = (b_c + 1U) / 2U;
  *p = i_c >> (int )(*u * 2U);
  return;
}
}
int amdgpu_calculate_at(u32 t , u32 h , u32 fh , u32 fl , u32 *tl , u32 *th )
{
  u32 k ;
  u32 a ;
  u32 ah ;
  u32 al ;
  u32 t1 ;
  {
  if ((fl == 0U || fh == 0U) || fl > fh) {
    return (-22);
  } else {
  }
  k = (fh * 100U) / fl;
  t1 = (k - 100U) * t;
  a = ((h * 100U + t1) * 1000U) / (t1 / 100U + 10000U);
  a = (a + 5U) / 10U;
  ah = (a * t + 5000U) / 10000U;
  al = a - ah;
  *th = t - ah;
  *tl = t + al;
  return (0);
}
}
bool amdgpu_is_uvd_state(u32 class , u32 class2 )
{
  {
  if ((class & 1024U) != 0U) {
    return (1);
  } else {
  }
  if ((class & 8192U) != 0U) {
    return (1);
  } else {
  }
  if ((class & 16384U) != 0U) {
    return (1);
  } else {
  }
  if ((class & 32768U) != 0U) {
    return (1);
  } else {
  }
  if ((class2 & 4U) != 0U) {
    return (1);
  } else {
  }
  return (0);
}
}
bool amdgpu_is_internal_thermal_sensor(enum amdgpu_int_thermal_type sensor )
{
  {
  switch ((unsigned int )sensor) {
  case 3U: ;
  case 4U: ;
  case 6U: ;
  case 7U: ;
  case 8U: ;
  case 9U: ;
  case 11U: ;
  case 12U: ;
  return (1);
  case 5U: ;
  case 10U: ;
  return (0);
  case 0U: ;
  case 1U: ;
  case 2U: ;
  default: ;
  return (0);
  }
}
}
static int amdgpu_parse_clk_voltage_dep_table(struct amdgpu_clock_voltage_dependency_table *amdgpu_table ,
                                              ATOM_PPLIB_Clock_Voltage_Dependency_Table *atom_table )
{
  u32 size ;
  int i ;
  ATOM_PPLIB_Clock_Voltage_Dependency_Record *entry ;
  void *tmp ;
  {
  size = (u32 )atom_table->ucNumEntries * 8U;
  tmp = kzalloc((size_t )size, 208U);
  amdgpu_table->entries = (struct amdgpu_clock_voltage_dependency_entry *)tmp;
  if ((unsigned long )amdgpu_table->entries == (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
    return (-12);
  } else {
  }
  entry = (ATOM_PPLIB_Clock_Voltage_Dependency_Record *)(& atom_table->entries);
  i = 0;
  goto ldv_48098;
  ldv_48097:
  (amdgpu_table->entries + (unsigned long )i)->clk = (u32 )((int )entry->usClockLow | ((int )entry->ucClockHigh << 16));
  (amdgpu_table->entries + (unsigned long )i)->v = entry->usVoltage;
  entry = entry + 5U;
  i = i + 1;
  ldv_48098: ;
  if ((int )atom_table->ucNumEntries > i) {
    goto ldv_48097;
  } else {
  }
  amdgpu_table->count = (u32 )atom_table->ucNumEntries;
  return (0);
}
}
int amdgpu_get_platform_caps(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  union power_info___0 *power_info ;
  int index ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  bool tmp ;
  int tmp___0 ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  power_info = (union power_info___0 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  adev->pm.dpm.platform_caps = power_info->pplib.ulPlatformCaps;
  adev->pm.dpm.backbias_response_time = (u32 )power_info->pplib.usBackbiasTime;
  adev->pm.dpm.voltage_response_time = (u32 )power_info->pplib.usVoltageTime;
  return (0);
}
}
int amdgpu_parse_extended_power_table(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  union power_info___0 *power_info ;
  union fan_info *fan_info ;
  ATOM_PPLIB_Clock_Voltage_Dependency_Table *dep_table ;
  int index ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  int ret ;
  int i ;
  bool tmp ;
  int tmp___0 ;
  ATOM_PPLIB_Clock_Voltage_Limit_Table *clk_v ;
  ATOM_PPLIB_PhaseSheddingLimits_Table *psl ;
  ATOM_PPLIB_PhaseSheddingLimits_Record *entry ;
  void *tmp___1 ;
  ATOM_PPLIB_CAC_Leakage_Table *cac_table ;
  ATOM_PPLIB_CAC_Leakage_Record *entry___0 ;
  u32 size ;
  void *tmp___2 ;
  ATOM_PPLIB_EXTENDEDHEADER *ext_hdr ;
  VCEClockInfoArray *array ;
  ATOM_PPLIB_VCE_Clock_Voltage_Limit_Table *limits ;
  ATOM_PPLIB_VCE_State_Table *states ;
  ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record *entry___1 ;
  ATOM_PPLIB_VCE_State_Record *state_entry ;
  VCEClockInfo *vce_clk ;
  u32 size___0 ;
  void *tmp___3 ;
  UVDClockInfoArray *array___0 ;
  ATOM_PPLIB_UVD_Clock_Voltage_Limit_Table *limits___0 ;
  ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record *entry___2 ;
  u32 size___1 ;
  void *tmp___4 ;
  UVDClockInfo *uvd_clk ;
  ATOM_PPLIB_SAMClk_Voltage_Limit_Table *limits___1 ;
  ATOM_PPLIB_SAMClk_Voltage_Limit_Record *entry___3 ;
  u32 size___2 ;
  void *tmp___5 ;
  ATOM_PPLIB_PPM_Table *ppm ;
  void *tmp___6 ;
  ATOM_PPLIB_ACPClk_Voltage_Limit_Table *limits___2 ;
  ATOM_PPLIB_ACPClk_Voltage_Limit_Record *entry___4 ;
  u32 size___3 ;
  void *tmp___7 ;
  u8 rev ;
  ATOM_PowerTune_Table *pt ;
  void *tmp___8 ;
  ATOM_PPLIB_POWERTUNE_Table_V1 *ppt ;
  ATOM_PPLIB_POWERTUNE_Table *ppt___0 ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  power_info = (union power_info___0 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  if ((unsigned int )power_info->pplib.usTableSize > 45U) {
    if ((unsigned int )power_info->pplib3.usFanTableOffset != 0U) {
      fan_info = (union fan_info *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib3.usFanTableOffset));
      adev->pm.dpm.fan.t_hyst = fan_info->fan.ucTHyst;
      adev->pm.dpm.fan.t_min = fan_info->fan.usTMin;
      adev->pm.dpm.fan.t_med = fan_info->fan.usTMed;
      adev->pm.dpm.fan.t_high = fan_info->fan.usTHigh;
      adev->pm.dpm.fan.pwm_min = fan_info->fan.usPWMMin;
      adev->pm.dpm.fan.pwm_med = fan_info->fan.usPWMMed;
      adev->pm.dpm.fan.pwm_high = fan_info->fan.usPWMHigh;
      if ((unsigned int )fan_info->fan.ucFanTableFormat > 1U) {
        adev->pm.dpm.fan.t_max = fan_info->fan2.usTMax;
      } else {
        adev->pm.dpm.fan.t_max = 10900U;
      }
      adev->pm.dpm.fan.cycle_delay = 100000U;
      if ((unsigned int )fan_info->fan.ucFanTableFormat > 2U) {
        adev->pm.dpm.fan.control_mode = fan_info->fan3.ucFanControlMode;
        adev->pm.dpm.fan.default_max_fan_pwm = fan_info->fan3.usFanPWMMax;
        adev->pm.dpm.fan.default_fan_output_sensitivity = 4836U;
        adev->pm.dpm.fan.fan_output_sensitivity = fan_info->fan3.usFanOutputSensitivity;
      } else {
      }
      adev->pm.dpm.fan.ucode_fan_control = 1;
    } else {
    }
  } else {
  }
  if ((unsigned int )power_info->pplib.usTableSize > 65U) {
    if ((unsigned int )power_info->pplib4.usVddcDependencyOnSCLKOffset != 0U) {
      dep_table = (ATOM_PPLIB_Clock_Voltage_Dependency_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usVddcDependencyOnSCLKOffset));
      ret = amdgpu_parse_clk_voltage_dep_table(& adev->pm.dpm.dyn_state.vddc_dependency_on_sclk,
                                               dep_table);
      if (ret != 0) {
        amdgpu_free_extended_power_table(adev);
        return (ret);
      } else {
      }
    } else {
    }
    if ((unsigned int )power_info->pplib4.usVddciDependencyOnMCLKOffset != 0U) {
      dep_table = (ATOM_PPLIB_Clock_Voltage_Dependency_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usVddciDependencyOnMCLKOffset));
      ret = amdgpu_parse_clk_voltage_dep_table(& adev->pm.dpm.dyn_state.vddci_dependency_on_mclk,
                                               dep_table);
      if (ret != 0) {
        amdgpu_free_extended_power_table(adev);
        return (ret);
      } else {
      }
    } else {
    }
    if ((unsigned int )power_info->pplib4.usVddcDependencyOnMCLKOffset != 0U) {
      dep_table = (ATOM_PPLIB_Clock_Voltage_Dependency_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usVddcDependencyOnMCLKOffset));
      ret = amdgpu_parse_clk_voltage_dep_table(& adev->pm.dpm.dyn_state.vddc_dependency_on_mclk,
                                               dep_table);
      if (ret != 0) {
        amdgpu_free_extended_power_table(adev);
        return (ret);
      } else {
      }
    } else {
    }
    if ((unsigned int )power_info->pplib4.usMvddDependencyOnMCLKOffset != 0U) {
      dep_table = (ATOM_PPLIB_Clock_Voltage_Dependency_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usMvddDependencyOnMCLKOffset));
      ret = amdgpu_parse_clk_voltage_dep_table(& adev->pm.dpm.dyn_state.mvdd_dependency_on_mclk,
                                               dep_table);
      if (ret != 0) {
        amdgpu_free_extended_power_table(adev);
        return (ret);
      } else {
      }
    } else {
    }
    if ((unsigned int )power_info->pplib4.usMaxClockVoltageOnDCOffset != 0U) {
      clk_v = (ATOM_PPLIB_Clock_Voltage_Limit_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usMaxClockVoltageOnDCOffset));
      if ((unsigned int )clk_v->ucNumEntries != 0U) {
        adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.sclk = (u32 )((int )clk_v->entries[0].usSclkLow | ((int )clk_v->entries[0].ucSclkHigh << 16));
        adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.mclk = (u32 )((int )clk_v->entries[0].usMclkLow | ((int )clk_v->entries[0].ucMclkHigh << 16));
        adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.vddc = clk_v->entries[0].usVddc;
        adev->pm.dpm.dyn_state.max_clock_voltage_on_dc.vddci = clk_v->entries[0].usVddci;
      } else {
      }
    } else {
    }
    if ((unsigned int )power_info->pplib4.usVddcPhaseShedLimitsTableOffset != 0U) {
      psl = (ATOM_PPLIB_PhaseSheddingLimits_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib4.usVddcPhaseShedLimitsTableOffset));
      tmp___1 = kzalloc((unsigned long )psl->ucNumEntries * 12UL, 208U);
      adev->pm.dpm.dyn_state.phase_shedding_limits_table.entries = (struct amdgpu_phase_shedding_limits_entry *)tmp___1;
      if ((unsigned long )adev->pm.dpm.dyn_state.phase_shedding_limits_table.entries == (unsigned long )((struct amdgpu_phase_shedding_limits_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      entry = (ATOM_PPLIB_PhaseSheddingLimits_Record *)(& psl->entries);
      i = 0;
      goto ldv_48126;
      ldv_48125:
      (adev->pm.dpm.dyn_state.phase_shedding_limits_table.entries + (unsigned long )i)->sclk = (u32 )((int )entry->usSclkLow | ((int )entry->ucSclkHigh << 16));
      (adev->pm.dpm.dyn_state.phase_shedding_limits_table.entries + (unsigned long )i)->mclk = (u32 )((int )entry->usMclkLow | ((int )entry->ucMclkHigh << 16));
      (adev->pm.dpm.dyn_state.phase_shedding_limits_table.entries + (unsigned long )i)->voltage = entry->usVoltage;
      entry = entry + 8U;
      i = i + 1;
      ldv_48126: ;
      if ((int )psl->ucNumEntries > i) {
        goto ldv_48125;
      } else {
      }
      adev->pm.dpm.dyn_state.phase_shedding_limits_table.count = (u32 )psl->ucNumEntries;
    } else {
    }
  } else {
  }
  if ((unsigned int )power_info->pplib.usTableSize > 87U) {
    adev->pm.dpm.tdp_limit = power_info->pplib5.ulTDPLimit;
    adev->pm.dpm.near_tdp_limit = power_info->pplib5.ulNearTDPLimit;
    adev->pm.dpm.near_tdp_limit_adjusted = adev->pm.dpm.near_tdp_limit;
    adev->pm.dpm.tdp_od_limit = power_info->pplib5.usTDPODLimit;
    if ((unsigned int )adev->pm.dpm.tdp_od_limit != 0U) {
      adev->pm.dpm.power_control = 1;
    } else {
      adev->pm.dpm.power_control = 0;
    }
    adev->pm.dpm.tdp_adjustment = 0U;
    adev->pm.dpm.sq_ramping_threshold = power_info->pplib5.ulSQRampingThreshold;
    adev->pm.dpm.cac_leakage = power_info->pplib5.ulCACLeakage;
    adev->pm.dpm.load_line_slope = power_info->pplib5.usLoadLineSlope;
    if ((unsigned int )power_info->pplib5.usCACLeakageTableOffset != 0U) {
      cac_table = (ATOM_PPLIB_CAC_Leakage_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib5.usCACLeakageTableOffset));
      size = (u32 )cac_table->ucNumEntries * 16U;
      tmp___2 = kzalloc((size_t )size, 208U);
      adev->pm.dpm.dyn_state.cac_leakage_table.entries = (union amdgpu_cac_leakage_entry *)tmp___2;
      if ((unsigned long )adev->pm.dpm.dyn_state.cac_leakage_table.entries == (unsigned long )((union amdgpu_cac_leakage_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      entry___0 = (ATOM_PPLIB_CAC_Leakage_Record *)(& cac_table->entries);
      i = 0;
      goto ldv_48132;
      ldv_48131: ;
      if ((adev->pm.dpm.platform_caps & 8388608U) != 0U) {
        (adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc1 = entry___0->__annonCompField125.usVddc1;
        (adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc2 = entry___0->__annonCompField125.usVddc2;
        (adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField80.vddc3 = entry___0->__annonCompField125.usVddc3;
      } else {
        (adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField79.vddc = entry___0->__annonCompField124.usVddc;
        (adev->pm.dpm.dyn_state.cac_leakage_table.entries + (unsigned long )i)->__annonCompField79.leakage = entry___0->__annonCompField124.ulLeakageValue;
      }
      entry___0 = entry___0 + 6U;
      i = i + 1;
      ldv_48132: ;
      if ((int )cac_table->ucNumEntries > i) {
        goto ldv_48131;
      } else {
      }
      adev->pm.dpm.dyn_state.cac_leakage_table.count = (u32 )cac_table->ucNumEntries;
    } else {
    }
  } else {
  }
  if ((unsigned int )power_info->pplib.usTableSize > 45U) {
    ext_hdr = (ATOM_PPLIB_EXTENDEDHEADER *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib3.usExtendendedHeaderOffset));
    if ((unsigned int )ext_hdr->usSize > 11U && (unsigned int )ext_hdr->usVCETableOffset != 0U) {
      array = (VCEClockInfoArray *)((mode_info->atom_context)->bios + (((unsigned long )data_offset + (unsigned long )ext_hdr->usVCETableOffset) + 1UL));
      limits = (ATOM_PPLIB_VCE_Clock_Voltage_Limit_Table *)((mode_info->atom_context)->bios + ((((unsigned long )data_offset + (unsigned long )ext_hdr->usVCETableOffset) + (unsigned long )array->ucNumEntries * 6UL) + 2UL));
      states = (ATOM_PPLIB_VCE_State_Table *)((mode_info->atom_context)->bios + ((((unsigned long )data_offset + (unsigned long )ext_hdr->usVCETableOffset) + ((unsigned long )limits->numEntries * 3UL + (unsigned long )array->ucNumEntries * 6UL)) + 3UL));
      size___0 = (u32 )limits->numEntries * 12U;
      tmp___3 = kzalloc((size_t )size___0, 208U);
      adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries = (struct amdgpu_vce_clock_voltage_dependency_entry *)tmp___3;
      if ((unsigned long )adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries == (unsigned long )((struct amdgpu_vce_clock_voltage_dependency_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.count = limits->numEntries;
      entry___1 = (ATOM_PPLIB_VCE_Clock_Voltage_Limit_Record *)(& limits->entries);
      state_entry = (ATOM_PPLIB_VCE_State_Record *)(& states->entries);
      i = 0;
      goto ldv_48143;
      ldv_48142:
      vce_clk = (VCEClockInfo *)(& array->entries) + (unsigned long )entry___1->ucVCEClockInfoIndex * 6UL;
      (adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )i)->evclk = (u32 )((int )vce_clk->usEVClkLow | ((int )vce_clk->ucEVClkHigh << 16));
      (adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )i)->ecclk = (u32 )((int )vce_clk->usECClkLow | ((int )vce_clk->ucECClkHigh << 16));
      (adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table.entries + (unsigned long )i)->v = entry___1->usVoltage;
      entry___1 = entry___1 + 3U;
      i = i + 1;
      ldv_48143: ;
      if ((int )limits->numEntries > i) {
        goto ldv_48142;
      } else {
      }
      i = 0;
      goto ldv_48147;
      ldv_48146: ;
      if (i > 5) {
        goto ldv_48145;
      } else {
      }
      vce_clk = (VCEClockInfo *)(& array->entries) + (unsigned long )state_entry->ucVCEClockInfoIndex * 6UL;
      adev->pm.dpm.vce_states[i].evclk = (u32 )((int )vce_clk->usEVClkLow | ((int )vce_clk->ucEVClkHigh << 16));
      adev->pm.dpm.vce_states[i].ecclk = (u32 )((int )vce_clk->usECClkLow | ((int )vce_clk->ucECClkHigh << 16));
      adev->pm.dpm.vce_states[i].clk_idx = (unsigned int )state_entry->ucClockInfoIndex & 63U;
      adev->pm.dpm.vce_states[i].pstate = (u8 )((int )state_entry->ucClockInfoIndex >> 6);
      state_entry = state_entry + 2U;
      i = i + 1;
      ldv_48147: ;
      if ((int )states->numEntries > i) {
        goto ldv_48146;
      } else {
      }
      ldv_48145: ;
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 13U && (unsigned int )ext_hdr->usUVDTableOffset != 0U) {
      array___0 = (UVDClockInfoArray *)((mode_info->atom_context)->bios + (((unsigned long )data_offset + (unsigned long )ext_hdr->usUVDTableOffset) + 1UL));
      limits___0 = (ATOM_PPLIB_UVD_Clock_Voltage_Limit_Table *)((mode_info->atom_context)->bios + ((((unsigned long )data_offset + (unsigned long )ext_hdr->usUVDTableOffset) + (unsigned long )array___0->ucNumEntries * 6UL) + 2UL));
      size___1 = (u32 )limits___0->numEntries * 12U;
      tmp___4 = kzalloc((size_t )size___1, 208U);
      adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries = (struct amdgpu_uvd_clock_voltage_dependency_entry *)tmp___4;
      if ((unsigned long )adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries == (unsigned long )((struct amdgpu_uvd_clock_voltage_dependency_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.count = limits___0->numEntries;
      entry___2 = (ATOM_PPLIB_UVD_Clock_Voltage_Limit_Record *)(& limits___0->entries);
      i = 0;
      goto ldv_48154;
      ldv_48153:
      uvd_clk = (UVDClockInfo *)(& array___0->entries) + (unsigned long )entry___2->ucUVDClockInfoIndex * 6UL;
      (adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )i)->vclk = (u32 )((int )uvd_clk->usVClkLow | ((int )uvd_clk->ucVClkHigh << 16));
      (adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )i)->dclk = (u32 )((int )uvd_clk->usDClkLow | ((int )uvd_clk->ucDClkHigh << 16));
      (adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table.entries + (unsigned long )i)->v = entry___2->usVoltage;
      entry___2 = entry___2 + 3U;
      i = i + 1;
      ldv_48154: ;
      if ((int )limits___0->numEntries > i) {
        goto ldv_48153;
      } else {
      }
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 15U && (unsigned int )ext_hdr->usSAMUTableOffset != 0U) {
      limits___1 = (ATOM_PPLIB_SAMClk_Voltage_Limit_Table *)((mode_info->atom_context)->bios + (((unsigned long )data_offset + (unsigned long )ext_hdr->usSAMUTableOffset) + 1UL));
      size___2 = (u32 )limits___1->numEntries * 8U;
      tmp___5 = kzalloc((size_t )size___2, 208U);
      adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries = (struct amdgpu_clock_voltage_dependency_entry *)tmp___5;
      if ((unsigned long )adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries == (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.count = (u32 )limits___1->numEntries;
      entry___3 = (ATOM_PPLIB_SAMClk_Voltage_Limit_Record *)(& limits___1->entries);
      i = 0;
      goto ldv_48160;
      ldv_48159:
      (adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries + (unsigned long )i)->clk = (u32 )((int )entry___3->usSAMClockLow | ((int )entry___3->ucSAMClockHigh << 16));
      (adev->pm.dpm.dyn_state.samu_clock_voltage_dependency_table.entries + (unsigned long )i)->v = entry___3->usVoltage;
      entry___3 = entry___3 + 5U;
      i = i + 1;
      ldv_48160: ;
      if ((int )limits___1->numEntries > i) {
        goto ldv_48159;
      } else {
      }
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 17U && (unsigned int )ext_hdr->usPPMTableOffset != 0U) {
      ppm = (ATOM_PPLIB_PPM_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )ext_hdr->usPPMTableOffset));
      tmp___6 = kzalloc(36UL, 208U);
      adev->pm.dpm.dyn_state.ppm_table = (struct amdgpu_ppm_table *)tmp___6;
      if ((unsigned long )adev->pm.dpm.dyn_state.ppm_table == (unsigned long )((struct amdgpu_ppm_table *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      (adev->pm.dpm.dyn_state.ppm_table)->ppm_design = ppm->ucPpmDesign;
      (adev->pm.dpm.dyn_state.ppm_table)->cpu_core_number = ppm->usCpuCoreNumber;
      (adev->pm.dpm.dyn_state.ppm_table)->platform_tdp = ppm->ulPlatformTDP;
      (adev->pm.dpm.dyn_state.ppm_table)->small_ac_platform_tdp = ppm->ulSmallACPlatformTDP;
      (adev->pm.dpm.dyn_state.ppm_table)->platform_tdc = ppm->ulPlatformTDC;
      (adev->pm.dpm.dyn_state.ppm_table)->small_ac_platform_tdc = ppm->ulSmallACPlatformTDC;
      (adev->pm.dpm.dyn_state.ppm_table)->apu_tdp = ppm->ulApuTDP;
      (adev->pm.dpm.dyn_state.ppm_table)->dgpu_tdp = ppm->ulDGpuTDP;
      (adev->pm.dpm.dyn_state.ppm_table)->dgpu_ulv_power = ppm->ulDGpuUlvPower;
      (adev->pm.dpm.dyn_state.ppm_table)->tj_max = ppm->ulTjmax;
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 19U && (unsigned int )ext_hdr->usACPTableOffset != 0U) {
      limits___2 = (ATOM_PPLIB_ACPClk_Voltage_Limit_Table *)((mode_info->atom_context)->bios + (((unsigned long )data_offset + (unsigned long )ext_hdr->usACPTableOffset) + 1UL));
      size___3 = (u32 )limits___2->numEntries * 8U;
      tmp___7 = kzalloc((size_t )size___3, 208U);
      adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries = (struct amdgpu_clock_voltage_dependency_entry *)tmp___7;
      if ((unsigned long )adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries == (unsigned long )((struct amdgpu_clock_voltage_dependency_entry *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.count = (u32 )limits___2->numEntries;
      entry___4 = (ATOM_PPLIB_ACPClk_Voltage_Limit_Record *)(& limits___2->entries);
      i = 0;
      goto ldv_48167;
      ldv_48166:
      (adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries + (unsigned long )i)->clk = (u32 )((int )entry___4->usACPClockLow | ((int )entry___4->ucACPClockHigh << 16));
      (adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table.entries + (unsigned long )i)->v = entry___4->usVoltage;
      entry___4 = entry___4 + 5U;
      i = i + 1;
      ldv_48167: ;
      if ((int )limits___2->numEntries > i) {
        goto ldv_48166;
      } else {
      }
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 21U && (unsigned int )ext_hdr->usPowerTuneTableOffset != 0U) {
      rev = *((u8 *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )ext_hdr->usPowerTuneTableOffset)));
      tmp___8 = kzalloc(16UL, 208U);
      adev->pm.dpm.dyn_state.cac_tdp_table = (struct amdgpu_cac_tdp_table *)tmp___8;
      if ((unsigned long )adev->pm.dpm.dyn_state.cac_tdp_table == (unsigned long )((struct amdgpu_cac_tdp_table *)0)) {
        amdgpu_free_extended_power_table(adev);
        return (-12);
      } else {
      }
      if ((unsigned int )rev != 0U) {
        ppt = (ATOM_PPLIB_POWERTUNE_Table_V1 *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )ext_hdr->usPowerTuneTableOffset));
        (adev->pm.dpm.dyn_state.cac_tdp_table)->maximum_power_delivery_limit = ppt->usMaximumPowerDeliveryLimit;
        pt = & ppt->power_tune_table;
      } else {
        ppt___0 = (ATOM_PPLIB_POWERTUNE_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )ext_hdr->usPowerTuneTableOffset));
        (adev->pm.dpm.dyn_state.cac_tdp_table)->maximum_power_delivery_limit = 255U;
        pt = & ppt___0->power_tune_table;
      }
      (adev->pm.dpm.dyn_state.cac_tdp_table)->tdp = pt->usTDP;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->configurable_tdp = pt->usConfigurableTDP;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->tdc = pt->usTDC;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->battery_power_limit = pt->usBatteryPowerLimit;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->small_power_limit = pt->usSmallPowerLimit;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->low_cac_leakage = pt->usLowCACLeakage;
      (adev->pm.dpm.dyn_state.cac_tdp_table)->high_cac_leakage = pt->usHighCACLeakage;
    } else {
    }
    if ((unsigned int )ext_hdr->usSize > 23U && (unsigned int )ext_hdr->usSclkVddgfxTableOffset != 0U) {
      dep_table = (ATOM_PPLIB_Clock_Voltage_Dependency_Table *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )ext_hdr->usSclkVddgfxTableOffset));
      ret = amdgpu_parse_clk_voltage_dep_table(& adev->pm.dpm.dyn_state.vddgfx_dependency_on_sclk,
                                               dep_table);
      if (ret != 0) {
        kfree((void const *)adev->pm.dpm.dyn_state.vddgfx_dependency_on_sclk.entries);
        return (ret);
      } else {
      }
    } else {
    }
  } else {
  }
  return (0);
}
}
void amdgpu_free_extended_power_table(struct amdgpu_device *adev )
{
  struct amdgpu_dpm_dynamic_state *dyn_state ;
  {
  dyn_state = & adev->pm.dpm.dyn_state;
  kfree((void const *)dyn_state->vddc_dependency_on_sclk.entries);
  kfree((void const *)dyn_state->vddci_dependency_on_mclk.entries);
  kfree((void const *)dyn_state->vddc_dependency_on_mclk.entries);
  kfree((void const *)dyn_state->mvdd_dependency_on_mclk.entries);
  kfree((void const *)dyn_state->cac_leakage_table.entries);
  kfree((void const *)dyn_state->phase_shedding_limits_table.entries);
  kfree((void const *)dyn_state->ppm_table);
  kfree((void const *)dyn_state->cac_tdp_table);
  kfree((void const *)dyn_state->vce_clock_voltage_dependency_table.entries);
  kfree((void const *)dyn_state->uvd_clock_voltage_dependency_table.entries);
  kfree((void const *)dyn_state->samu_clock_voltage_dependency_table.entries);
  kfree((void const *)dyn_state->acp_clock_voltage_dependency_table.entries);
  kfree((void const *)dyn_state->vddgfx_dependency_on_sclk.entries);
  return;
}
}
static char const *pp_lib_thermal_controller_names[20U] =
  { "NONE", "lm63", "adm1032", "adm1030",
        "max6649", "lm64", "f75375", "RV6xx",
        "RV770", "adt7473", "NONE", "External GPIO",
        "Evergreen", "emc2103", "Sumo", "Northern Islands",
        "Southern Islands", "lm96163", "Sea Islands", "Kaveri/Kabini"};
void amdgpu_add_thermal_controller(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  ATOM_PPLIB_POWERPLAYTABLE *power_table ;
  int index ;
  ATOM_PPLIB_THERMALCONTROLLER *controller ;
  struct amdgpu_i2c_bus_rec i2c_bus ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  bool tmp ;
  int tmp___0 ;
  struct i2c_board_info info ;
  char const *name ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  power_table = (ATOM_PPLIB_POWERPLAYTABLE *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  controller = & power_table->sThermalController;
  if ((unsigned int )controller->ucType != 0U) {
    if ((int )((signed char )controller->ucFanParameters) < 0) {
      adev->pm.no_fan = 1;
    } else {
    }
    adev->pm.fan_pulses_per_revolution = (unsigned int )controller->ucFanParameters & 15U;
    if ((unsigned int )adev->pm.fan_pulses_per_revolution != 0U) {
      adev->pm.fan_min_rpm = controller->ucFanMinRPM;
      adev->pm.fan_max_rpm = controller->ucFanMaxRPM;
    } else {
    }
    if ((unsigned int )controller->ucType == 7U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 3;
    } else
    if ((unsigned int )controller->ucType == 8U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 4;
    } else
    if ((unsigned int )controller->ucType == 12U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 6;
    } else
    if ((unsigned int )controller->ucType == 14U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 7;
    } else
    if ((unsigned int )controller->ucType == 15U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 8;
    } else
    if ((unsigned int )controller->ucType == 16U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 9;
    } else
    if ((unsigned int )controller->ucType == 18U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 11;
    } else
    if ((unsigned int )controller->ucType == 19U) {
      printk("\016[drm] Internal thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 12;
    } else
    if ((unsigned int )controller->ucType == 11U) {
      printk("\016[drm] External GPIO thermal controller %s fan control\n", (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 2;
    } else
    if ((unsigned int )controller->ucType == 137U) {
      printk("\016[drm] ADT7473 with internal thermal controller %s fan control\n",
             (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 5;
    } else
    if ((unsigned int )controller->ucType == 141U) {
      printk("\016[drm] EMC2103 with internal thermal controller %s fan control\n",
             (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 10;
    } else
    if ((unsigned int )controller->ucType <= 19U) {
      printk("\016[drm] Possible %s thermal controller at 0x%02x %s fan control\n",
             pp_lib_thermal_controller_names[(int )controller->ucType], (int )controller->ucI2cAddress >> 1,
             (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
      adev->pm.int_thermal_type = 1;
      i2c_bus = amdgpu_atombios_lookup_i2c_gpio(adev, (int )controller->ucI2cLine);
      adev->pm.i2c_bus = amdgpu_i2c_lookup(adev, & i2c_bus);
      if ((unsigned long )adev->pm.i2c_bus != (unsigned long )((struct amdgpu_i2c_chan *)0)) {
        info.type[0] = (char)0;
        info.type[1] = (char)0;
        info.type[2] = (char)0;
        info.type[3] = (char)0;
        info.type[4] = (char)0;
        info.type[5] = (char)0;
        info.type[6] = (char)0;
        info.type[7] = (char)0;
        info.type[8] = (char)0;
        info.type[9] = (char)0;
        info.type[10] = (char)0;
        info.type[11] = (char)0;
        info.type[12] = (char)0;
        info.type[13] = (char)0;
        info.type[14] = (char)0;
        info.type[15] = (char)0;
        info.type[16] = (char)0;
        info.type[17] = (char)0;
        info.type[18] = (char)0;
        info.type[19] = (char)0;
        info.flags = (unsigned short)0;
        info.addr = (unsigned short)0;
        info.platform_data = 0;
        info.archdata = 0;
        info.of_node = 0;
        info.fwnode = 0;
        info.irq = 0;
        name = pp_lib_thermal_controller_names[(int )controller->ucType];
        info.addr = (unsigned short )((int )controller->ucI2cAddress >> 1);
        strlcpy((char *)(& info.type), name, 20UL);
        i2c_new_device(& (adev->pm.i2c_bus)->adapter, (struct i2c_board_info const *)(& info));
      } else {
      }
    } else {
      printk("\016[drm] Unknown thermal controller type %d at 0x%02x %s fan control\n",
             (int )controller->ucType, (int )controller->ucI2cAddress >> 1, (int )((signed char )controller->ucFanParameters) < 0 ? (char *)"without" : (char *)"with");
    }
  } else {
  }
  return;
}
}
enum amdgpu_pcie_gen amdgpu_get_pcie_gen_support(struct amdgpu_device *adev , u32 sys_mask ,
                                                 enum amdgpu_pcie_gen asic_gen , enum amdgpu_pcie_gen default_gen )
{
  {
  switch ((unsigned int )asic_gen) {
  case 0U: ;
  return (0);
  case 1U: ;
  return (1);
  case 2U: ;
  return (2);
  default: ;
  if ((sys_mask & 4U) != 0U && (unsigned int )default_gen == 2U) {
    return (2);
  } else
  if ((sys_mask & 2U) != 0U && (unsigned int )default_gen == 1U) {
    return (1);
  } else {
    return (0);
  }
  }
  return (0);
}
}
u16 amdgpu_get_pcie_lane_support(struct amdgpu_device *adev , u16 asic_lanes , u16 default_lanes )
{
  {
  switch ((int )asic_lanes) {
  case 0: ;
  default: ;
  return (default_lanes);
  case 1: ;
  return (1U);
  case 2: ;
  return (2U);
  case 4: ;
  return (4U);
  case 8: ;
  return (8U);
  case 12: ;
  return (12U);
  case 16: ;
  return (16U);
  }
}
}
u8 amdgpu_encode_pci_lane_width(u32 lanes )
{
  u8 encoded_lanes[17U] ;
  {
  encoded_lanes[0] = 0U;
  encoded_lanes[1] = 1U;
  encoded_lanes[2] = 2U;
  encoded_lanes[3] = 0U;
  encoded_lanes[4] = 3U;
  encoded_lanes[5] = 0U;
  encoded_lanes[6] = 0U;
  encoded_lanes[7] = 0U;
  encoded_lanes[8] = 4U;
  encoded_lanes[9] = 0U;
  encoded_lanes[10] = 0U;
  encoded_lanes[11] = 0U;
  encoded_lanes[12] = 5U;
  encoded_lanes[13] = 0U;
  encoded_lanes[14] = 0U;
  encoded_lanes[15] = 0U;
  encoded_lanes[16] = 6U;
  if (lanes > 16U) {
    return (0U);
  } else {
  }
  return (encoded_lanes[lanes]);
}
}
bool ldv_queue_work_on_795(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_796(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_797(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_798(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_799(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_809(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_811(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_810(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_813(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_812(struct workqueue_struct *ldv_func_arg1 ) ;
u32 cz_get_argument(struct amdgpu_device *adev )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 480U, 0);
  return (tmp);
}
}
static struct cz_smu_private_data *cz_smu_get_priv(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  {
  priv = (struct cz_smu_private_data *)adev->smu.priv;
  return (priv);
}
}
int cz_send_msg_to_smc_async(struct amdgpu_device *adev , u16 msg )
{
  int i ;
  u32 content ;
  u32 tmp ;
  {
  content = 0U;
  i = 0;
  goto ldv_43862;
  ldv_43861:
  tmp = amdgpu_mm_rreg(adev, 464U, 0);
  if (content != tmp) {
    goto ldv_43860;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43862: ;
  if (adev->usec_timeout > i) {
    goto ldv_43861;
  } else {
  }
  ldv_43860: ;
  if (adev->usec_timeout == i) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 464U, 0U, 0);
  amdgpu_mm_wreg(adev, 448U, (u32 )msg, 0);
  return (0);
}
}
int cz_send_msg_to_smc(struct amdgpu_device *adev , u16 msg )
{
  int i ;
  u32 content ;
  u32 tmp ;
  int tmp___0 ;
  {
  content = 0U;
  tmp = 0U;
  tmp___0 = cz_send_msg_to_smc_async(adev, (int )msg);
  if (tmp___0 != 0) {
    return (-22);
  } else {
  }
  i = 0;
  goto ldv_43872;
  ldv_43871:
  tmp = amdgpu_mm_rreg(adev, 464U, 0);
  if (content != tmp) {
    goto ldv_43870;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43872: ;
  if (adev->usec_timeout > i) {
    goto ldv_43871;
  } else {
  }
  ldv_43870: ;
  if (adev->usec_timeout == i) {
    return (-22);
  } else {
  }
  if (tmp != 1U) {
    dev_err((struct device const *)adev->dev, "SMC Failed to send Message.\n");
    return (-22);
  } else {
  }
  return (0);
}
}
int cz_send_msg_to_smc_with_parameter_async(struct amdgpu_device *adev , u16 msg ,
                                            u32 parameter )
{
  int tmp ;
  {
  amdgpu_mm_wreg(adev, 480U, parameter, 0);
  tmp = cz_send_msg_to_smc_async(adev, (int )msg);
  return (tmp);
}
}
int cz_send_msg_to_smc_with_parameter(struct amdgpu_device *adev , u16 msg , u32 parameter )
{
  int tmp ;
  {
  amdgpu_mm_wreg(adev, 480U, parameter, 0);
  tmp = cz_send_msg_to_smc(adev, (int )msg);
  return (tmp);
}
}
static int cz_set_smc_sram_address(struct amdgpu_device *adev , u32 smc_address ,
                                   u32 limit )
{
  {
  if ((smc_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_address + 3U > limit) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 384U, smc_address + 268435456U, 0);
  return (0);
}
}
int cz_read_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 *value ,
                           u32 limit )
{
  int ret ;
  {
  ret = cz_set_smc_sram_address(adev, smc_address, limit);
  if (ret != 0) {
    return (ret);
  } else {
  }
  *value = amdgpu_mm_rreg(adev, 385U, 0);
  return (0);
}
}
int cz_write_smc_sram_dword(struct amdgpu_device *adev , u32 smc_address , u32 value ,
                            u32 limit )
{
  int ret ;
  {
  ret = cz_set_smc_sram_address(adev, smc_address, limit);
  if (ret != 0) {
    return (ret);
  } else {
  }
  amdgpu_mm_wreg(adev, 385U, value, 0);
  return (0);
}
}
static int cz_smu_request_load_fw(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  u32 smc_addr ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  smc_addr = 130968U;
  cz_write_smc_sram_dword(adev, smc_addr, 0U, smc_addr + 4U);
  cz_send_msg_to_smc_with_parameter(adev, 51, priv->toc_buffer.mc_addr_high);
  cz_send_msg_to_smc_with_parameter(adev, 52, priv->toc_buffer.mc_addr_low);
  cz_send_msg_to_smc(adev, 594);
  cz_send_msg_to_smc_with_parameter(adev, 596, (u32 )priv->toc_entry_aram);
  cz_send_msg_to_smc_with_parameter(adev, 596, (u32 )priv->toc_entry_power_profiling_index);
  cz_send_msg_to_smc_with_parameter(adev, 596, (u32 )priv->toc_entry_initialize_index);
  return (0);
}
}
static int cz_smu_check_fw_load_finish(struct amdgpu_device *adev , u32 fw_mask )
{
  int i ;
  u32 index ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  index = 268566424U;
  amdgpu_mm_wreg(adev, 384U, index, 0);
  i = 0;
  goto ldv_43915;
  ldv_43914:
  tmp = amdgpu_mm_rreg(adev, 385U, 0);
  if ((tmp & fw_mask) == fw_mask) {
    goto ldv_43913;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43915: ;
  if (adev->usec_timeout > i) {
    goto ldv_43914;
  } else {
  }
  ldv_43913: ;
  if (adev->usec_timeout <= i) {
    tmp___0 = amdgpu_mm_rreg(adev, 385U, 0);
    dev_err((struct device const *)adev->dev, "SMU check loaded firmware failed, expecting 0x%x, getting 0x%x",
            fw_mask, tmp___0);
    return (-22);
  } else {
  }
  return (0);
}
}
static int cz_smu_check_finished(struct amdgpu_device *adev , enum AMDGPU_UCODE_ID id )
{
  {
  switch ((unsigned int )id) {
  case 0U: ;
  if ((int )adev->smu.fw_flags & 1) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 1U: ;
  if ((adev->smu.fw_flags & 2U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 2U: ;
  if ((adev->smu.fw_flags & 4U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 3U: ;
  if ((adev->smu.fw_flags & 8U) != 0U) {
    return (0);
  } else {
  }
  case 4U: ;
  if ((adev->smu.fw_flags & 16U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 5U: ;
  if ((adev->smu.fw_flags & 32U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 6U: ;
  if ((adev->smu.fw_flags & 64U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 7U: ;
  if ((adev->smu.fw_flags & 256U) != 0U) {
    return (0);
  } else {
  }
  goto ldv_43921;
  case 8U: ;
  default: ;
  goto ldv_43921;
  }
  ldv_43921: ;
  return (1);
}
}
static int cz_load_mec_firmware(struct amdgpu_device *adev )
{
  struct amdgpu_firmware_info *ucode ;
  u32 reg_data ;
  u32 tmp ;
  {
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 5UL;
  if ((unsigned long )ucode->fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 8333U, 0);
  tmp = tmp | 1073741824U;
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, 8333U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 12475U, 0);
  tmp = tmp & 4294967280U;
  tmp = tmp & 4286578687U;
  tmp = tmp & 4278190079U;
  tmp = (tmp & 3892314111U) | 134217728U;
  amdgpu_mm_wreg(adev, 12475U, tmp, 0);
  reg_data = (unsigned int )ucode->mc_addr & 4294963200U;
  amdgpu_mm_wreg(adev, 12473U, reg_data, 0);
  reg_data = (unsigned int )(ucode->mc_addr >> 32ULL) & 65535U;
  amdgpu_mm_wreg(adev, 12474U, reg_data, 0);
  return (0);
}
}
int cz_smu_start(struct amdgpu_device *adev )
{
  int ret ;
  u32 fw_to_check ;
  {
  ret = 0;
  fw_to_check = 383U;
  cz_smu_request_load_fw(adev);
  ret = cz_smu_check_fw_load_finish(adev, fw_to_check);
  if (ret != 0) {
    return (ret);
  } else {
  }
  if ((unsigned int )adev->asic_type == 7U) {
    ret = cz_load_mec_firmware(adev);
    if (ret != 0) {
      dev_err((struct device const *)adev->dev, "(%d) Mec Firmware load failed\n",
              ret);
      return (ret);
    } else {
    }
  } else {
  }
  adev->smu.fw_flags = 383U;
  return (ret);
}
}
static u32 cz_convert_fw_type(u32 fw_type )
{
  enum AMDGPU_UCODE_ID result ;
  {
  result = 8;
  switch (fw_type) {
  case 0U:
  result = 0;
  goto ldv_43947;
  case 1U:
  result = 1;
  goto ldv_43947;
  case 2U:
  result = 2;
  goto ldv_43947;
  case 3U:
  result = 3;
  goto ldv_43947;
  case 4U:
  result = 4;
  goto ldv_43947;
  case 5U: ;
  case 6U:
  result = 5;
  goto ldv_43947;
  case 8U:
  result = 7;
  goto ldv_43947;
  default:
  drm_err("UCode type is out of range!");
  }
  ldv_43947: ;
  return ((u32 )result);
}
}
static uint8_t cz_smu_translate_firmware_enum_to_arg(enum cz_scratch_entry firmware_enum )
{
  uint8_t ret ;
  {
  ret = 0U;
  switch ((unsigned int )firmware_enum) {
  case 0U:
  ret = 0U;
  goto ldv_43961;
  case 1U:
  ret = 1U;
  goto ldv_43961;
  case 2U:
  ret = 2U;
  goto ldv_43961;
  case 3U:
  ret = 3U;
  goto ldv_43961;
  case 4U:
  ret = 4U;
  goto ldv_43961;
  case 5U:
  ret = 5U;
  goto ldv_43961;
  case 6U:
  ret = 6U;
  goto ldv_43961;
  case 7U:
  ret = 7U;
  goto ldv_43961;
  case 8U:
  ret = 8U;
  goto ldv_43961;
  case 9U:
  ret = 9U;
  goto ldv_43961;
  case 10U:
  ret = 10U;
  goto ldv_43961;
  case 11U:
  ret = 11U;
  goto ldv_43961;
  case 12U:
  ret = 12U;
  goto ldv_43961;
  case 13U:
  ret = 13U;
  goto ldv_43961;
  case 14U:
  ret = 0U;
  goto ldv_43961;
  case 15U: ;
  case 16U: ;
  case 17U: ;
  case 18U: ;
  case 19U: ;
  case 20U:
  ret = 1U;
  goto ldv_43961;
  case 21U:
  ret = 1U;
  goto ldv_43961;
  }
  ldv_43961: ;
  return (ret);
}
}
static int cz_smu_populate_single_firmware_entry(struct amdgpu_device *adev , enum cz_scratch_entry firmware_enum ,
                                                 struct cz_buffer_entry *entry )
{
  uint64_t gpu_addr ;
  u32 data_size ;
  uint8_t ucode_id ;
  uint8_t tmp ;
  enum AMDGPU_UCODE_ID id ;
  u32 tmp___0 ;
  struct amdgpu_firmware_info *ucode ;
  struct gfx_firmware_header_v1_0 const *header ;
  {
  tmp = cz_smu_translate_firmware_enum_to_arg(firmware_enum);
  ucode_id = tmp;
  tmp___0 = cz_convert_fw_type((u32 )ucode_id);
  id = tmp___0;
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )id;
  if ((unsigned long )ucode->fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  gpu_addr = ucode->mc_addr;
  header = (struct gfx_firmware_header_v1_0 const *)(ucode->fw)->data;
  data_size = header->header.ucode_size_bytes;
  if ((unsigned int )firmware_enum == 5U || (unsigned int )firmware_enum == 6U) {
    gpu_addr = (uint64_t )(header->jt_offset << 2) + gpu_addr;
    data_size = header->jt_size << 2;
  } else {
  }
  entry->mc_addr_low = (unsigned int )gpu_addr;
  entry->mc_addr_high = (unsigned int )(gpu_addr >> 32ULL);
  entry->data_size = data_size;
  entry->firmware_ID = firmware_enum;
  return (0);
}
}
static int cz_smu_populate_single_scratch_entry(struct amdgpu_device *adev , enum cz_scratch_entry scratch_type ,
                                                u32 size_in_byte , struct cz_buffer_entry *entry )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  uint64_t mc_addr ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  mc_addr = ((unsigned long long )priv->smu_buffer.mc_addr_high << 32) | (unsigned long long )priv->smu_buffer.mc_addr_low;
  mc_addr = (uint64_t )size_in_byte + mc_addr;
  priv->smu_buffer_used_bytes = (int )priv->smu_buffer_used_bytes + (int )((u16 )size_in_byte);
  entry->data_size = size_in_byte;
  entry->kaddr = priv->smu_buffer.kaddr + (unsigned long )priv->smu_buffer_used_bytes;
  entry->mc_addr_low = (unsigned int )mc_addr;
  entry->mc_addr_high = (unsigned int )(mc_addr >> 32ULL);
  entry->firmware_ID = scratch_type;
  return (0);
}
}
static int cz_smu_populate_single_ucode_load_task(struct amdgpu_device *adev , enum cz_scratch_entry firmware_enum ,
                                                  bool is_last )
{
  uint8_t i ;
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  struct TOC *toc ;
  struct SMU_Task *task ;
  u16 tmp___0 ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  toc = (struct TOC *)priv->toc_buffer.kaddr;
  tmp___0 = priv->toc_entry_used_count;
  priv->toc_entry_used_count = (u16 )((int )priv->toc_entry_used_count + 1);
  task = (struct SMU_Task *)(& toc->tasks) + (unsigned long )tmp___0;
  task->type = 1U;
  task->arg = cz_smu_translate_firmware_enum_to_arg(firmware_enum);
  task->next = (int )is_last ? 65535U : priv->toc_entry_used_count;
  i = 0U;
  goto ldv_44013;
  ldv_44012: ;
  if ((unsigned int )priv->driver_buffer[(int )i].firmware_ID == (unsigned int )firmware_enum) {
    goto ldv_44011;
  } else {
  }
  i = (uint8_t )((int )i + 1);
  ldv_44013: ;
  if ((int )priv->driver_buffer_length > (int )i) {
    goto ldv_44012;
  } else {
  }
  ldv_44011: ;
  if ((int )priv->driver_buffer_length <= (int )i) {
    dev_err((struct device const *)adev->dev, "Invalid Firmware Type\n");
    return (-22);
  } else {
  }
  task->addr.low = priv->driver_buffer[(int )i].mc_addr_low;
  task->addr.high = priv->driver_buffer[(int )i].mc_addr_high;
  task->size_bytes = priv->driver_buffer[(int )i].data_size;
  return (0);
}
}
static int cz_smu_populate_single_scratch_task(struct amdgpu_device *adev , enum cz_scratch_entry firmware_enum ,
                                               uint8_t type , bool is_last )
{
  uint8_t i ;
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  struct TOC *toc ;
  struct SMU_Task *task ;
  u16 tmp___0 ;
  struct cz_ih_meta_data *pIHReg_restore ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  toc = (struct TOC *)priv->toc_buffer.kaddr;
  tmp___0 = priv->toc_entry_used_count;
  priv->toc_entry_used_count = (u16 )((int )priv->toc_entry_used_count + 1);
  task = (struct SMU_Task *)(& toc->tasks) + (unsigned long )tmp___0;
  task->type = type;
  task->arg = cz_smu_translate_firmware_enum_to_arg(firmware_enum);
  task->next = (int )is_last ? 65535U : priv->toc_entry_used_count;
  i = 0U;
  goto ldv_44026;
  ldv_44025: ;
  if ((unsigned int )priv->scratch_buffer[(int )i].firmware_ID == (unsigned int )firmware_enum) {
    goto ldv_44024;
  } else {
  }
  i = (uint8_t )((int )i + 1);
  ldv_44026: ;
  if ((int )priv->scratch_buffer_length > (int )i) {
    goto ldv_44025;
  } else {
  }
  ldv_44024: ;
  if ((int )priv->scratch_buffer_length <= (int )i) {
    dev_err((struct device const *)adev->dev, "Invalid Firmware Type\n");
    return (-22);
  } else {
  }
  task->addr.low = priv->scratch_buffer[(int )i].mc_addr_low;
  task->addr.high = priv->scratch_buffer[(int )i].mc_addr_high;
  task->size_bytes = priv->scratch_buffer[(int )i].data_size;
  if ((unsigned int )firmware_enum == 20U) {
    pIHReg_restore = (struct cz_ih_meta_data *)priv->scratch_buffer[(int )i].kaddr;
    pIHReg_restore->command = 8451U;
  } else {
  }
  return (0);
}
}
static int cz_smu_construct_toc_for_rlc_aram_save(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  priv->toc_entry_aram = priv->toc_entry_used_count;
  cz_smu_populate_single_scratch_task(adev, 10, 2, 1);
  return (0);
}
}
static int cz_smu_construct_toc_for_vddgfx_enter(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  struct TOC *toc ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  toc = (struct TOC *)priv->toc_buffer.kaddr;
  toc->JobList[0] = (unsigned char )priv->toc_entry_used_count;
  cz_smu_populate_single_scratch_task(adev, 9, 2, 0);
  cz_smu_populate_single_scratch_task(adev, 11, 2, 1);
  return (0);
}
}
static int cz_smu_construct_toc_for_vddgfx_exit(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  struct TOC *toc ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  toc = (struct TOC *)priv->toc_buffer.kaddr;
  toc->JobList[1] = (unsigned char )priv->toc_entry_used_count;
  if ((int )adev->firmware.smu_load) {
    cz_smu_populate_single_ucode_load_task(adev, 2, 0);
    cz_smu_populate_single_ucode_load_task(adev, 3, 0);
    cz_smu_populate_single_ucode_load_task(adev, 4, 0);
    cz_smu_populate_single_ucode_load_task(adev, 5, 0);
    cz_smu_populate_single_ucode_load_task(adev, 6, 0);
    cz_smu_populate_single_ucode_load_task(adev, 8, 0);
  } else {
  }
  cz_smu_populate_single_scratch_task(adev, 9, 1, 0);
  cz_smu_populate_single_scratch_task(adev, 10, 1, 0);
  cz_smu_populate_single_scratch_task(adev, 11, 1, 1);
  return (0);
}
}
static int cz_smu_construct_toc_for_power_profiling(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  priv->toc_entry_power_profiling_index = priv->toc_entry_used_count;
  cz_smu_populate_single_scratch_task(adev, 14, 5, 1);
  return (0);
}
}
static int cz_smu_construct_toc_for_bootup(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  priv->toc_entry_initialize_index = priv->toc_entry_used_count;
  if ((int )adev->firmware.smu_load) {
    cz_smu_populate_single_ucode_load_task(adev, 0, 0);
    cz_smu_populate_single_ucode_load_task(adev, 1, 0);
    cz_smu_populate_single_ucode_load_task(adev, 2, 0);
    cz_smu_populate_single_ucode_load_task(adev, 3, 0);
    cz_smu_populate_single_ucode_load_task(adev, 4, 0);
    cz_smu_populate_single_ucode_load_task(adev, 5, 0);
    cz_smu_populate_single_ucode_load_task(adev, 6, 0);
    cz_smu_populate_single_ucode_load_task(adev, 8, 1);
  } else {
  }
  return (0);
}
}
static int cz_smu_construct_toc_for_clock_table(struct amdgpu_device *adev )
{
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  priv->toc_entry_clock_table = priv->toc_entry_used_count;
  cz_smu_populate_single_scratch_task(adev, 21, 5, 1);
  return (0);
}
}
static int cz_smu_initialize_toc_empty_job_list(struct amdgpu_device *adev )
{
  int i ;
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  struct TOC *toc ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  toc = (struct TOC *)priv->toc_buffer.kaddr;
  i = 0;
  goto ldv_44061;
  ldv_44060:
  toc->JobList[i] = 255U;
  i = i + 1;
  ldv_44061: ;
  if (i <= 31) {
    goto ldv_44060;
  } else {
  }
  return (0);
}
}
int cz_smu_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_bo_unref(& adev->smu.toc_buf);
  amdgpu_bo_unref(& adev->smu.smu_buf);
  kfree((void const *)adev->smu.priv);
  adev->smu.priv = (void *)0;
  if ((int )adev->firmware.smu_load) {
    amdgpu_ucode_fini_bo(adev);
  } else {
  }
  return (0);
}
}
int cz_smu_download_pptable(struct amdgpu_device *adev , void **table )
{
  uint8_t i ;
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  i = 0U;
  goto ldv_44074;
  ldv_44073: ;
  if ((unsigned int )priv->scratch_buffer[(int )i].firmware_ID == 21U) {
    goto ldv_44072;
  } else {
  }
  i = (uint8_t )((int )i + 1);
  ldv_44074: ;
  if ((int )priv->scratch_buffer_length > (int )i) {
    goto ldv_44073;
  } else {
  }
  ldv_44072: ;
  if ((int )priv->scratch_buffer_length <= (int )i) {
    dev_err((struct device const *)adev->dev, "Invalid Scratch Type\n");
    return (-22);
  } else {
  }
  *table = priv->scratch_buffer[(int )i].kaddr;
  cz_send_msg_to_smc_with_parameter(adev, 73, priv->scratch_buffer[(int )i].mc_addr_high);
  cz_send_msg_to_smc_with_parameter(adev, 74, priv->scratch_buffer[(int )i].mc_addr_low);
  cz_send_msg_to_smc_with_parameter(adev, 596, (u32 )priv->toc_entry_clock_table);
  cz_send_msg_to_smc(adev, 615);
  return (0);
}
}
int cz_smu_upload_pptable(struct amdgpu_device *adev )
{
  uint8_t i ;
  struct cz_smu_private_data *priv ;
  struct cz_smu_private_data *tmp ;
  {
  tmp = cz_smu_get_priv(adev);
  priv = tmp;
  i = 0U;
  goto ldv_44082;
  ldv_44081: ;
  if ((unsigned int )priv->scratch_buffer[(int )i].firmware_ID == 21U) {
    goto ldv_44080;
  } else {
  }
  i = (uint8_t )((int )i + 1);
  ldv_44082: ;
  if ((int )priv->scratch_buffer_length > (int )i) {
    goto ldv_44081;
  } else {
  }
  ldv_44080: ;
  if ((int )priv->scratch_buffer_length <= (int )i) {
    dev_err((struct device const *)adev->dev, "Invalid Scratch Type\n");
    return (-22);
  } else {
  }
  cz_send_msg_to_smc_with_parameter(adev, 73, priv->scratch_buffer[(int )i].mc_addr_high);
  cz_send_msg_to_smc_with_parameter(adev, 74, priv->scratch_buffer[(int )i].mc_addr_low);
  cz_send_msg_to_smc_with_parameter(adev, 596, (u32 )priv->toc_entry_clock_table);
  cz_send_msg_to_smc(adev, 616);
  return (0);
}
}
static struct amdgpu_smumgr_funcs const cz_smumgr_funcs = {(int (*)(struct amdgpu_device * , u32 ))(& cz_smu_check_finished), (int (*)(struct amdgpu_device * ))0,
    (int (*)(struct amdgpu_device * , u32 ))0};
int cz_smu_init(struct amdgpu_device *adev )
{
  int ret ;
  uint64_t mc_addr ;
  struct amdgpu_bo **toc_buf ;
  struct amdgpu_bo **smu_buf ;
  void *toc_buf_ptr ;
  void *smu_buf_ptr ;
  struct cz_smu_private_data *priv ;
  void *tmp ;
  uint8_t tmp___0 ;
  int tmp___1 ;
  uint8_t tmp___2 ;
  int tmp___3 ;
  uint8_t tmp___4 ;
  int tmp___5 ;
  uint8_t tmp___6 ;
  int tmp___7 ;
  uint8_t tmp___8 ;
  int tmp___9 ;
  uint8_t tmp___10 ;
  int tmp___11 ;
  uint8_t tmp___12 ;
  int tmp___13 ;
  uint8_t tmp___14 ;
  int tmp___15 ;
  uint8_t tmp___16 ;
  int tmp___17 ;
  uint8_t tmp___18 ;
  int tmp___19 ;
  uint8_t tmp___20 ;
  int tmp___21 ;
  uint8_t tmp___22 ;
  int tmp___23 ;
  uint8_t tmp___24 ;
  int tmp___25 ;
  {
  ret = -22;
  mc_addr = 0ULL;
  toc_buf = & adev->smu.toc_buf;
  smu_buf = & adev->smu.smu_buf;
  toc_buf_ptr = (void *)0;
  smu_buf_ptr = (void *)0;
  tmp = kzalloc(696UL, 208U);
  priv = (struct cz_smu_private_data *)tmp;
  if ((unsigned long )priv == (unsigned long )((struct cz_smu_private_data *)0)) {
    return (-12);
  } else {
  }
  if ((int )adev->firmware.smu_load) {
    amdgpu_ucode_init_bo(adev);
  } else {
  }
  adev->smu.priv = (void *)priv;
  adev->smu.fw_flags = 0U;
  priv->toc_buffer.data_size = 4096U;
  priv->smu_buffer.data_size = 13280U;
  ret = amdgpu_bo_create(adev, (unsigned long )priv->toc_buffer.data_size, 4096, 1,
                         2U, 0ULL, (struct sg_table *)0, toc_buf);
  if (ret != 0) {
    dev_err((struct device const *)adev->dev, "(%d) SMC TOC buffer allocation failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_create(adev, (unsigned long )priv->smu_buffer.data_size, 4096, 1,
                         2U, 0ULL, (struct sg_table *)0, smu_buf);
  if (ret != 0) {
    dev_err((struct device const *)adev->dev, "(%d) SMC Internal buffer allocation failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_reserve(adev->smu.toc_buf, 0);
  if (ret != 0) {
    amdgpu_bo_unref(& adev->smu.toc_buf);
    dev_err((struct device const *)adev->dev, "(%d) SMC TOC buffer reserve failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_pin(adev->smu.toc_buf, 2U, & mc_addr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.toc_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    dev_err((struct device const *)adev->dev, "(%d) SMC TOC buffer pin failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_kmap(*toc_buf, & toc_buf_ptr);
  if (ret != 0) {
    goto smu_init_failed;
  } else {
  }
  amdgpu_bo_unreserve(adev->smu.toc_buf);
  priv->toc_buffer.mc_addr_low = (unsigned int )mc_addr;
  priv->toc_buffer.mc_addr_high = (unsigned int )(mc_addr >> 32ULL);
  priv->toc_buffer.kaddr = toc_buf_ptr;
  ret = amdgpu_bo_reserve(adev->smu.smu_buf, 0);
  if (ret != 0) {
    amdgpu_bo_unref(& adev->smu.smu_buf);
    dev_err((struct device const *)adev->dev, "(%d) SMC Internal buffer reserve failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_pin(adev->smu.smu_buf, 2U, & mc_addr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.smu_buf);
    dev_err((struct device const *)adev->dev, "(%d) SMC Internal buffer pin failed\n",
            ret);
    return (ret);
  } else {
  }
  ret = amdgpu_bo_kmap(*smu_buf, & smu_buf_ptr);
  if (ret != 0) {
    goto smu_init_failed;
  } else {
  }
  amdgpu_bo_unreserve(adev->smu.smu_buf);
  priv->smu_buffer.mc_addr_low = (unsigned int )mc_addr;
  priv->smu_buffer.mc_addr_high = (unsigned int )(mc_addr >> 32ULL);
  priv->smu_buffer.kaddr = smu_buf_ptr;
  if ((int )adev->firmware.smu_load) {
    tmp___0 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___1 = cz_smu_populate_single_firmware_entry(adev, 0, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___0);
    if (tmp___1 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___2 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___3 = cz_smu_populate_single_firmware_entry(adev, 1, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___2);
    if (tmp___3 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___4 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___5 = cz_smu_populate_single_firmware_entry(adev, 2, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___4);
    if (tmp___5 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___6 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___7 = cz_smu_populate_single_firmware_entry(adev, 3, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___6);
    if (tmp___7 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___8 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___9 = cz_smu_populate_single_firmware_entry(adev, 4, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___8);
    if (tmp___9 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___10 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___11 = cz_smu_populate_single_firmware_entry(adev, 5, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___10);
    if (tmp___11 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___12 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___13 = cz_smu_populate_single_firmware_entry(adev, 6, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___12);
    if (tmp___13 != 0) {
      goto smu_init_failed;
    } else {
    }
    tmp___14 = priv->driver_buffer_length;
    priv->driver_buffer_length = (uint8_t )((int )priv->driver_buffer_length + 1);
    tmp___15 = cz_smu_populate_single_firmware_entry(adev, 8, (struct cz_buffer_entry *)(& priv->driver_buffer) + (unsigned long )tmp___14);
    if (tmp___15 != 0) {
      goto smu_init_failed;
    } else {
    }
  } else {
  }
  tmp___16 = priv->scratch_buffer_length;
  priv->scratch_buffer_length = (uint8_t )((int )priv->scratch_buffer_length + 1);
  tmp___17 = cz_smu_populate_single_scratch_entry(adev, 9, 132U, (struct cz_buffer_entry *)(& priv->scratch_buffer) + (unsigned long )tmp___16);
  if (tmp___17 != 0) {
    goto smu_init_failed;
  } else {
  }
  tmp___18 = priv->scratch_buffer_length;
  priv->scratch_buffer_length = (uint8_t )((int )priv->scratch_buffer_length + 1);
  tmp___19 = cz_smu_populate_single_scratch_entry(adev, 10, 8192U, (struct cz_buffer_entry *)(& priv->scratch_buffer) + (unsigned long )tmp___18);
  if (tmp___19 != 0) {
    goto smu_init_failed;
  } else {
  }
  tmp___20 = priv->scratch_buffer_length;
  priv->scratch_buffer_length = (uint8_t )((int )priv->scratch_buffer_length + 1);
  tmp___21 = cz_smu_populate_single_scratch_entry(adev, 11, 4096U, (struct cz_buffer_entry *)(& priv->scratch_buffer) + (unsigned long )tmp___20);
  if (tmp___21 != 0) {
    goto smu_init_failed;
  } else {
  }
  tmp___22 = priv->scratch_buffer_length;
  priv->scratch_buffer_length = (uint8_t )((int )priv->scratch_buffer_length + 1);
  tmp___23 = cz_smu_populate_single_scratch_entry(adev, 14, 48U, (struct cz_buffer_entry *)(& priv->scratch_buffer) + (unsigned long )tmp___22);
  if (tmp___23 != 0) {
    goto smu_init_failed;
  } else {
  }
  tmp___24 = priv->scratch_buffer_length;
  priv->scratch_buffer_length = (uint8_t )((int )priv->scratch_buffer_length + 1);
  tmp___25 = cz_smu_populate_single_scratch_entry(adev, 21, 744U, (struct cz_buffer_entry *)(& priv->scratch_buffer) + (unsigned long )tmp___24);
  if (tmp___25 != 0) {
    goto smu_init_failed;
  } else {
  }
  cz_smu_initialize_toc_empty_job_list(adev);
  cz_smu_construct_toc_for_rlc_aram_save(adev);
  cz_smu_construct_toc_for_vddgfx_enter(adev);
  cz_smu_construct_toc_for_vddgfx_exit(adev);
  cz_smu_construct_toc_for_power_profiling(adev);
  cz_smu_construct_toc_for_bootup(adev);
  cz_smu_construct_toc_for_clock_table(adev);
  adev->smu.smumgr_funcs = & cz_smumgr_funcs;
  return (0);
  smu_init_failed:
  amdgpu_bo_unref(toc_buf);
  amdgpu_bo_unref(smu_buf);
  return (ret);
}
}
void ldv_main_exported_68(void)
{
  struct amdgpu_device *ldvarg857 ;
  void *tmp ;
  u32 ldvarg856 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  ldvarg857 = (struct amdgpu_device *)tmp;
  ldv_memset((void *)(& ldvarg856), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_68 == 1) {
    cz_smu_check_finished(ldvarg857, (enum AMDGPU_UCODE_ID )ldvarg856);
    ldv_state_variable_68 = 1;
  } else {
  }
  goto ldv_44101;
  default:
  ldv_stop();
  }
  ldv_44101: ;
  return;
}
}
bool ldv_queue_work_on_809(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_810(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_811(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_812(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_813(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_823(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_825(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_824(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_827(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_826(struct workqueue_struct *ldv_func_arg1 ) ;
static void cz_dpm_powergate_uvd(struct amdgpu_device *adev , bool gate ) ;
static void cz_dpm_powergate_vce(struct amdgpu_device *adev , bool gate ) ;
static struct cz_ps *cz_get_ps(struct amdgpu_ps *rps )
{
  struct cz_ps *ps ;
  {
  ps = (struct cz_ps *)rps->ps_priv;
  return (ps);
}
}
static struct cz_power_info *cz_get_pi(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  {
  pi = (struct cz_power_info *)adev->pm.dpm.priv;
  return (pi);
}
}
static u16 cz_convert_8bit_index_to_voltage(struct amdgpu_device *adev , u16 voltage )
{
  u16 tmp ;
  {
  tmp = (unsigned int )voltage * 65511U + 6200U;
  return (tmp);
}
}
static void cz_construct_max_power_limits_table(struct amdgpu_device *adev , struct amdgpu_clock_and_voltage_limits *table )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *dep_table ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  dep_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  if (dep_table->count != 0U) {
    table->sclk = (dep_table->entries + (unsigned long )(dep_table->count - 1U))->clk;
    table->vddc = cz_convert_8bit_index_to_voltage(adev, (int )(dep_table->entries + (unsigned long )(dep_table->count - 1U))->v);
  } else {
  }
  table->mclk = pi->sys_info.nbp_memory_clock[0];
  return;
}
}
static int cz_parse_sys_info_table(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_mode_info *mode_info ;
  int index ;
  union igp_info___1 *igp_info ;
  u8 frev ;
  u8 crev ;
  u16 data_offset ;
  int i ;
  bool tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  mode_info = & adev->mode_info;
  index = 30;
  i = 0;
  tmp___0 = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U,
                                          & frev, & crev, & data_offset);
  if ((int )tmp___0) {
    igp_info = (union igp_info___1 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
    if ((unsigned int )crev != 9U) {
      drm_err("Unsupported IGP table: %d %d\n", (int )frev, (int )crev);
      return (-22);
    } else {
    }
    pi->sys_info.bootup_sclk = igp_info->info_9.ulBootUpEngineClock;
    pi->sys_info.bootup_uma_clk = igp_info->info_9.ulBootUpUMAClock;
    pi->sys_info.dentist_vco_freq = igp_info->info_9.ulDentistVCOFreq;
    pi->sys_info.bootup_nb_voltage_index = igp_info->info_9.usBootUpNBVoltage;
    if ((unsigned int )igp_info->info_9.ucHtcTmpLmt == 0U) {
      pi->sys_info.htc_tmp_lmt = 203U;
    } else {
      pi->sys_info.htc_tmp_lmt = igp_info->info_9.ucHtcTmpLmt;
    }
    if ((unsigned int )igp_info->info_9.ucHtcHystLmt == 0U) {
      pi->sys_info.htc_hyst_lmt = 5U;
    } else {
      pi->sys_info.htc_hyst_lmt = igp_info->info_9.ucHtcHystLmt;
    }
    if ((int )pi->sys_info.htc_tmp_lmt <= (int )pi->sys_info.htc_hyst_lmt) {
      drm_err("The htcTmpLmt should be larger than htcHystLmt.\n");
      return (-22);
    } else {
    }
    if ((igp_info->info_9.ulSystemConfig & 8U) != 0U && (int )pi->enable_nb_ps_policy) {
      pi->sys_info.nb_dpm_enable = 1U;
    } else {
      pi->sys_info.nb_dpm_enable = 0U;
    }
    i = 0;
    goto ldv_48316;
    ldv_48315: ;
    if (i <= 1) {
      pi->sys_info.nbp_memory_clock[i] = igp_info->info_9.ulNbpStateMemclkFreq[i];
    } else {
    }
    pi->sys_info.nbp_n_clock[i] = igp_info->info_9.ulNbpStateNClkFreq[i];
    i = i + 1;
    ldv_48316: ;
    if (i <= 3) {
      goto ldv_48315;
    } else {
    }
    i = 0;
    goto ldv_48319;
    ldv_48318:
    pi->sys_info.display_clock[i] = igp_info->info_9.sDispClkVoltageMapping[i].ulMaximumSupportedCLK;
    i = i + 1;
    ldv_48319: ;
    if (i <= 7) {
      goto ldv_48318;
    } else {
    }
    i = 0;
    goto ldv_48322;
    ldv_48321:
    pi->sys_info.nbp_voltage_index[i] = (uint8_t )igp_info->info_9.usNBPStateVoltage[i];
    i = i + 1;
    ldv_48322: ;
    if (i <= 3) {
      goto ldv_48321;
    } else {
    }
    if ((igp_info->info_9.ulGPUCapInfo & 16U) != 0U) {
      pi->caps_enable_dfs_bypass = 1;
    } else {
    }
    pi->sys_info.uma_channel_number = (u32 )igp_info->info_9.ucUMAChannelNumber;
    cz_construct_max_power_limits_table(adev, & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac);
  } else {
  }
  return (0);
}
}
static void cz_patch_voltage_values(struct amdgpu_device *adev )
{
  int i ;
  struct amdgpu_uvd_clock_voltage_dependency_table *uvd_table ;
  struct amdgpu_vce_clock_voltage_dependency_table *vce_table ;
  struct amdgpu_clock_voltage_dependency_table *acp_table ;
  {
  uvd_table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  vce_table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  acp_table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  if ((unsigned int )uvd_table->count != 0U) {
    i = 0;
    goto ldv_48332;
    ldv_48331:
    (uvd_table->entries + (unsigned long )i)->v = cz_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(uvd_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48332: ;
    if ((int )uvd_table->count > i) {
      goto ldv_48331;
    } else {
    }
  } else {
  }
  if ((unsigned int )vce_table->count != 0U) {
    i = 0;
    goto ldv_48335;
    ldv_48334:
    (vce_table->entries + (unsigned long )i)->v = cz_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(vce_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48335: ;
    if ((int )vce_table->count > i) {
      goto ldv_48334;
    } else {
    }
  } else {
  }
  if (acp_table->count != 0U) {
    i = 0;
    goto ldv_48338;
    ldv_48337:
    (acp_table->entries + (unsigned long )i)->v = cz_convert_8bit_index_to_voltage(adev,
                                                                                   (int )(acp_table->entries + (unsigned long )i)->v);
    i = i + 1;
    ldv_48338: ;
    if ((u32 )i < acp_table->count) {
      goto ldv_48337;
    } else {
    }
  } else {
  }
  return;
}
}
static void cz_construct_boot_state(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  pi->boot_pl.sclk = pi->sys_info.bootup_sclk;
  pi->boot_pl.vddc_index = (uint8_t )pi->sys_info.bootup_nb_voltage_index;
  pi->boot_pl.ds_divider_index = 0U;
  pi->boot_pl.ss_divider_index = 0U;
  pi->boot_pl.allow_gnb_slow = 1U;
  pi->boot_pl.force_nbp_state = 0U;
  pi->boot_pl.display_wm = 0U;
  pi->boot_pl.vce_wm = 0U;
  return;
}
}
static void cz_patch_boot_state(struct amdgpu_device *adev , struct cz_ps *ps )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ps->num_levels = 1U;
  ps->levels[0] = pi->boot_pl;
  return;
}
}
static void cz_parse_pplib_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                      int index , union pplib_clock_info___1 *clock_info )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct cz_ps *ps ;
  struct cz_ps *tmp___0 ;
  struct cz_pl *pl ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  tmp___0 = cz_get_ps(rps);
  ps = tmp___0;
  pl = (struct cz_pl *)(& ps->levels) + (unsigned long )index;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  pl->sclk = (table->entries + (unsigned long )clock_info->carrizo.index)->clk;
  pl->vddc_index = (uint8_t )(table->entries + (unsigned long )clock_info->carrizo.index)->v;
  ps->num_levels = (u32 )(index + 1);
  if ((int )pi->caps_sclk_ds) {
    pl->ds_divider_index = 5U;
    pl->ss_divider_index = 5U;
  } else {
  }
  return;
}
}
static void cz_parse_pplib_non_clock_info(struct amdgpu_device *adev , struct amdgpu_ps *rps ,
                                          struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ,
                                          u8 table_rev )
{
  struct cz_ps *ps ;
  struct cz_ps *tmp ;
  {
  tmp = cz_get_ps(rps);
  ps = tmp;
  rps->caps = non_clock_info->ulCapsAndSettings;
  rps->class = (u32 )non_clock_info->usClassification;
  rps->class2 = (u32 )non_clock_info->usClassification2;
  if ((unsigned int )table_rev > 12U) {
    rps->vclk = non_clock_info->ulVCLK;
    rps->dclk = non_clock_info->ulDCLK;
  } else {
    rps->vclk = 0U;
    rps->dclk = 0U;
  }
  if ((rps->class & 8U) != 0U) {
    adev->pm.dpm.boot_ps = rps;
    cz_patch_boot_state(adev, ps);
  } else {
  }
  if ((rps->class & 1024U) != 0U) {
    adev->pm.dpm.uvd_ps = rps;
  } else {
  }
  return;
}
}
static int cz_parse_power_table(struct amdgpu_device *adev )
{
  struct amdgpu_mode_info *mode_info ;
  struct _ATOM_PPLIB_NONCLOCK_INFO *non_clock_info ;
  union pplib_power_state *power_state ;
  int i ;
  int j ;
  int k ;
  int non_clock_array_index ;
  int clock_array_index ;
  union pplib_clock_info___1 *clock_info ;
  struct _StateArray *state_array ;
  struct _ClockInfoArray *clock_info_array ;
  struct _NonClockInfoArray *non_clock_info_array ;
  union power_info___1 *power_info ;
  int index ;
  u16 data_offset ;
  u8 frev ;
  u8 crev ;
  u8 *power_state_offset ;
  struct cz_ps *ps ;
  bool tmp ;
  int tmp___0 ;
  void *tmp___1 ;
  void *tmp___2 ;
  {
  mode_info = & adev->mode_info;
  index = 15;
  tmp = amdgpu_atom_parse_data_header(mode_info->atom_context, index, (u16 *)0U, & frev,
                                      & crev, & data_offset);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  power_info = (union power_info___1 *)(mode_info->atom_context)->bios + (unsigned long )data_offset;
  state_array = (struct _StateArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usStateArrayOffset));
  clock_info_array = (struct _ClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usClockInfoArrayOffset));
  non_clock_info_array = (struct _NonClockInfoArray *)((mode_info->atom_context)->bios + ((unsigned long )data_offset + (unsigned long )power_info->pplib.usNonClockInfoArrayOffset));
  tmp___1 = kzalloc((unsigned long )state_array->ucNumEntries * 48UL, 208U);
  adev->pm.dpm.ps = (struct amdgpu_ps *)tmp___1;
  if ((unsigned long )adev->pm.dpm.ps == (unsigned long )((struct amdgpu_ps *)0)) {
    return (-12);
  } else {
  }
  power_state_offset = (u8 *)(& state_array->states);
  adev->pm.dpm.platform_caps = power_info->pplib.ulPlatformCaps;
  adev->pm.dpm.backbias_response_time = (u32 )power_info->pplib.usBackbiasTime;
  adev->pm.dpm.voltage_response_time = (u32 )power_info->pplib.usVoltageTime;
  i = 0;
  goto ldv_48406;
  ldv_48405:
  power_state = (union pplib_power_state *)power_state_offset;
  non_clock_array_index = (int )power_state->v2.nonClockInfoIndex;
  non_clock_info = (struct _ATOM_PPLIB_NONCLOCK_INFO *)(& non_clock_info_array->nonClockInfo) + (unsigned long )non_clock_array_index;
  tmp___2 = kzalloc(108UL, 208U);
  ps = (struct cz_ps *)tmp___2;
  if ((unsigned long )ps == (unsigned long )((struct cz_ps *)0)) {
    kfree((void const *)adev->pm.dpm.ps);
    return (-12);
  } else {
  }
  (adev->pm.dpm.ps + (unsigned long )i)->ps_priv = (void *)ps;
  k = 0;
  j = 0;
  goto ldv_48404;
  ldv_48403:
  clock_array_index = (int )power_state->v2.clockInfoIndex[j];
  if ((int )clock_info_array->ucNumEntries <= clock_array_index) {
    goto ldv_48401;
  } else {
  }
  if (k > 7) {
    goto ldv_48402;
  } else {
  }
  clock_info = (union pplib_clock_info___1 *)(& clock_info_array->clockInfo) + (unsigned long )((int )clock_info_array->ucEntrySize * clock_array_index);
  cz_parse_pplib_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, k, clock_info);
  k = k + 1;
  ldv_48401:
  j = j + 1;
  ldv_48404: ;
  if ((int )power_state->v2.ucNumDPMLevels > j) {
    goto ldv_48403;
  } else {
  }
  ldv_48402:
  cz_parse_pplib_non_clock_info(adev, adev->pm.dpm.ps + (unsigned long )i, non_clock_info,
                                (int )non_clock_info_array->ucEntrySize);
  power_state_offset = power_state_offset + (unsigned long )((int )power_state->v2.ucNumDPMLevels + 2);
  i = i + 1;
  ldv_48406: ;
  if ((int )state_array->ucNumEntries > i) {
    goto ldv_48405;
  } else {
  }
  adev->pm.dpm.num_ps = (int )state_array->ucNumEntries;
  return (0);
}
}
static int cz_process_firmware_header(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  u32 tmp___0 ;
  int ret ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ret = cz_read_smc_sram_dword(adev, 130972U, & tmp___0, pi->sram_end);
  if (ret == 0) {
    pi->dpm_table_start = tmp___0;
  } else {
  }
  return (ret);
}
}
static int cz_dpm_init(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  int ret ;
  int i ;
  void *tmp ;
  {
  tmp = kzalloc(648UL, 208U);
  pi = (struct cz_power_info *)tmp;
  if ((unsigned long )pi == (unsigned long )((struct cz_power_info *)0)) {
    return (-12);
  } else {
  }
  adev->pm.dpm.priv = (void *)pi;
  ret = amdgpu_get_platform_caps(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = amdgpu_parse_extended_power_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->sram_end = 262144U;
  i = 0;
  goto ldv_48421;
  ldv_48420:
  pi->active_target[i] = 30U;
  i = i + 1;
  ldv_48421: ;
  if (i <= 7) {
    goto ldv_48420;
  } else {
  }
  pi->mgcg_cgtt_local0 = 0U;
  pi->mgcg_cgtt_local1 = 0U;
  pi->clock_slow_down_step = 25000U;
  pi->skip_clock_slow_down = 1U;
  pi->enable_nb_ps_policy = 0;
  pi->caps_power_containment = 1;
  pi->caps_cac = 1;
  pi->didt_enabled = 0;
  if ((int )pi->didt_enabled) {
    pi->caps_sq_ramping = 1;
    pi->caps_db_ramping = 1;
    pi->caps_td_ramping = 1;
    pi->caps_tcp_ramping = 1;
  } else {
  }
  pi->caps_sclk_ds = 1;
  pi->voting_clients = 12582963U;
  pi->auto_thermal_throttling_enabled = 1;
  pi->bapm_enabled = 0;
  pi->disable_nb_ps3_in_battery = 0;
  pi->voltage_drop_threshold = 0U;
  pi->caps_sclk_throttle_low_notification = 0;
  pi->gfx_pg_threshold = 500U;
  pi->caps_fps = 1;
  pi->caps_uvd_pg = (adev->pg_flags & 8U) != 0U;
  pi->caps_uvd_dpm = 1;
  pi->caps_vce_pg = (adev->pg_flags & 16U) != 0U;
  pi->caps_vce_dpm = 1;
  pi->caps_acp_pg = (adev->pg_flags & 512U) != 0U;
  pi->caps_acp_dpm = 1;
  pi->caps_stable_power_state = 0;
  pi->nb_dpm_enabled_by_driver = 1;
  pi->nb_dpm_enabled = 0;
  pi->caps_voltage_island = 0;
  pi->need_pptable_upload = 1;
  ret = cz_parse_sys_info_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  cz_patch_voltage_values(adev);
  cz_construct_boot_state(adev);
  ret = cz_parse_power_table(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  ret = cz_process_firmware_header(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  pi->dpm_enabled = 1;
  pi->uvd_dynamic_pg = 0;
  return (0);
}
}
static void cz_dpm_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_48428;
  ldv_48427:
  kfree((void const *)(adev->pm.dpm.ps + (unsigned long )i)->ps_priv);
  i = i + 1;
  ldv_48428: ;
  if (adev->pm.dpm.num_ps > i) {
    goto ldv_48427;
  } else {
  }
  kfree((void const *)adev->pm.dpm.ps);
  kfree((void const *)adev->pm.dpm.priv);
  amdgpu_free_extended_power_table(adev);
  return;
}
}
static void cz_dpm_debugfs_print_current_performance_level(struct amdgpu_device *adev ,
                                                           struct seq_file *m )
{
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 current_index ;
  u32 tmp ;
  u32 sclk ;
  u32 tmp___0 ;
  u16 vddc ;
  u32 tmp___1 ;
  {
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  tmp = (*(adev->smc_rreg))(adev, 3491758100U);
  current_index = (tmp & 2031616U) >> 16;
  if (current_index > 7U) {
    seq_printf(m, "invalid dpm profile %d\n", current_index);
  } else {
    sclk = (table->entries + (unsigned long )current_index)->clk;
    tmp___1 = (*(adev->smc_rreg))(adev, 3491758228U);
    tmp___0 = (tmp___1 & 510U) >> 1;
    vddc = cz_convert_8bit_index_to_voltage(adev, (int )((unsigned short )tmp___0));
    seq_printf(m, "power level %d    sclk: %u vddc: %u\n", current_index, sclk, (int )vddc);
  }
  return;
}
}
static void cz_dpm_print_power_state(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  int i ;
  struct cz_ps *ps ;
  struct cz_ps *tmp ;
  struct cz_pl *pl ;
  u16 tmp___0 ;
  {
  tmp = cz_get_ps(rps);
  ps = tmp;
  amdgpu_dpm_print_class_info(rps->class, rps->class2);
  amdgpu_dpm_print_cap_info(rps->caps);
  printk("\016[drm] \tuvd    vclk: %d dclk: %d\n", rps->vclk, rps->dclk);
  i = 0;
  goto ldv_48447;
  ldv_48446:
  pl = (struct cz_pl *)(& ps->levels) + (unsigned long )i;
  tmp___0 = cz_convert_8bit_index_to_voltage(adev, (int )pl->vddc_index);
  printk("\016[drm] \t\tpower level %d    sclk: %u vddc: %u\n", i, pl->sclk, (int )tmp___0);
  i = i + 1;
  ldv_48447: ;
  if ((u32 )i < ps->num_levels) {
    goto ldv_48446;
  } else {
  }
  amdgpu_dpm_print_ps_status(adev, rps);
  return;
}
}
static void cz_dpm_set_funcs(struct amdgpu_device *adev ) ;
static int cz_dpm_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  cz_dpm_set_funcs(adev);
  return (0);
}
}
static int cz_dpm_late_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  if (amdgpu_dpm != 0) {
    cz_dpm_powergate_uvd(adev, 1);
    cz_dpm_powergate_vce(adev, 1);
  } else {
  }
  return (0);
}
}
static int cz_dpm_sw_init(void *handle )
{
  struct amdgpu_device *adev ;
  int ret ;
  struct amdgpu_ps *tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = 0;
  adev->pm.dpm.state = 3;
  adev->pm.dpm.user_state = 3;
  adev->pm.dpm.forced_level = 0;
  adev->pm.default_sclk = adev->clock.default_sclk;
  adev->pm.default_mclk = adev->clock.default_mclk;
  adev->pm.current_sclk = adev->clock.default_sclk;
  adev->pm.current_mclk = adev->clock.default_mclk;
  adev->pm.int_thermal_type = 0;
  if (amdgpu_dpm == 0) {
    return (0);
  } else {
  }
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = cz_dpm_init(adev);
  if (ret != 0) {
    goto dpm_init_failed;
  } else {
  }
  tmp = adev->pm.dpm.boot_ps;
  adev->pm.dpm.requested_ps = tmp;
  adev->pm.dpm.current_ps = tmp;
  if (amdgpu_dpm == 1) {
    amdgpu_pm_print_power_states(adev);
  } else {
  }
  ret = amdgpu_pm_sysfs_init(adev);
  if (ret != 0) {
    goto dpm_init_failed;
  } else {
  }
  mutex_unlock(& adev->pm.mutex);
  printk("\016[drm] amdgpu: dpm initialized\n");
  return (0);
  dpm_init_failed:
  cz_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  drm_err("amdgpu: dpm initialization failed\n");
  return (ret);
}
}
static int cz_dpm_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  amdgpu_pm_sysfs_fini(adev);
  cz_dpm_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static void cz_reset_ap_mask(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  pi->active_process_mask = 0U;
  return;
}
}
static int cz_dpm_download_pptable_from_smu(struct amdgpu_device *adev , void **table )
{
  int ret ;
  {
  ret = 0;
  ret = cz_smu_download_pptable(adev, table);
  return (ret);
}
}
static int cz_dpm_upload_pptable_to_smu(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct SMU8_Fusion_ClkTable *clock_table ;
  struct atom_clock_dividers dividers ;
  void *table ;
  uint8_t i ;
  int ret ;
  struct amdgpu_clock_voltage_dependency_table *vddc_table ;
  struct amdgpu_clock_voltage_dependency_table *vddgfx_table ;
  struct amdgpu_uvd_clock_voltage_dependency_table *uvd_table ;
  struct amdgpu_vce_clock_voltage_dependency_table *vce_table ;
  struct amdgpu_clock_voltage_dependency_table *acp_table ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = (void *)0;
  i = 0U;
  ret = 0;
  vddc_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  vddgfx_table = & adev->pm.dpm.dyn_state.vddgfx_dependency_on_sclk;
  uvd_table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  vce_table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  acp_table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  if (! pi->need_pptable_upload) {
    return (0);
  } else {
  }
  ret = cz_dpm_download_pptable_from_smu(adev, & table);
  if (ret != 0) {
    drm_err("amdgpu: Failed to get power play table from SMU!\n");
    return (-22);
  } else {
  }
  clock_table = (struct SMU8_Fusion_ClkTable *)table;
  if ((((vddc_table->count > 8U || vddgfx_table->count > 8U) || (unsigned int )uvd_table->count > 8U) || (unsigned int )vce_table->count > 8U) || acp_table->count > 8U) {
    drm_err("amdgpu: Invalid Clock Voltage Dependency Table!\n");
    return (-22);
  } else {
  }
  i = 0U;
  goto ldv_48493;
  ldv_48492:
  clock_table->SclkBreakdownTable.ClkLevel[(int )i].GnbVid = (u32 )i < vddc_table->count ? (uint8_t )(vddc_table->entries + (unsigned long )i)->v : 0U;
  clock_table->SclkBreakdownTable.ClkLevel[(int )i].Frequency = (u32 )i < vddc_table->count ? (vddc_table->entries + (unsigned long )i)->clk : 0U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, clock_table->SclkBreakdownTable.ClkLevel[(int )i].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  clock_table->SclkBreakdownTable.ClkLevel[(int )i].DfsDid = (unsigned char )dividers.post_divider;
  clock_table->SclkBreakdownTable.ClkLevel[(int )i].GfxVid = (u32 )i < vddgfx_table->count ? (uint8_t )(vddgfx_table->entries + (unsigned long )i)->v : 0U;
  clock_table->AclkBreakdownTable.ClkLevel[(int )i].GfxVid = (u32 )i < acp_table->count ? (uint8_t )(acp_table->entries + (unsigned long )i)->v : 0U;
  clock_table->AclkBreakdownTable.ClkLevel[(int )i].Frequency = (u32 )i < acp_table->count ? (acp_table->entries + (unsigned long )i)->clk : 0U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, clock_table->SclkBreakdownTable.ClkLevel[(int )i].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  clock_table->AclkBreakdownTable.ClkLevel[(int )i].DfsDid = (unsigned char )dividers.post_divider;
  clock_table->VclkBreakdownTable.ClkLevel[(int )i].GfxVid = (int )uvd_table->count > (int )i ? (uint8_t )(uvd_table->entries + (unsigned long )i)->v : 0U;
  clock_table->VclkBreakdownTable.ClkLevel[(int )i].Frequency = (int )uvd_table->count > (int )i ? (uvd_table->entries + (unsigned long )i)->vclk : 0U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, clock_table->VclkBreakdownTable.ClkLevel[(int )i].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  clock_table->VclkBreakdownTable.ClkLevel[(int )i].DfsDid = (unsigned char )dividers.post_divider;
  clock_table->DclkBreakdownTable.ClkLevel[(int )i].GfxVid = (int )uvd_table->count > (int )i ? (uint8_t )(uvd_table->entries + (unsigned long )i)->v : 0U;
  clock_table->DclkBreakdownTable.ClkLevel[(int )i].Frequency = (int )uvd_table->count > (int )i ? (uvd_table->entries + (unsigned long )i)->dclk : 0U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, clock_table->DclkBreakdownTable.ClkLevel[(int )i].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  clock_table->DclkBreakdownTable.ClkLevel[(int )i].DfsDid = (unsigned char )dividers.post_divider;
  clock_table->EclkBreakdownTable.ClkLevel[(int )i].GfxVid = (int )vce_table->count > (int )i ? (uint8_t )(vce_table->entries + (unsigned long )i)->v : 0U;
  clock_table->EclkBreakdownTable.ClkLevel[(int )i].Frequency = (int )vce_table->count > (int )i ? (vce_table->entries + (unsigned long )i)->ecclk : 0U;
  ret = amdgpu_atombios_get_clock_dividers(adev, 0, clock_table->EclkBreakdownTable.ClkLevel[(int )i].Frequency,
                                           0, & dividers);
  if (ret != 0) {
    return (ret);
  } else {
  }
  clock_table->EclkBreakdownTable.ClkLevel[(int )i].DfsDid = (unsigned char )dividers.post_divider;
  i = (uint8_t )((int )i + 1);
  ldv_48493: ;
  if ((unsigned int )i <= 7U) {
    goto ldv_48492;
  } else {
  }
  ret = cz_smu_upload_pptable(adev);
  if (ret != 0) {
    drm_err("amdgpu: Failed to put power play table to SMU!\n");
    return (ret);
  } else {
  }
  return (0);
}
}
static void cz_init_sclk_limit(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 clock ;
  u32 level ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  clock = 0U;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) || table->count == 0U) {
    drm_err("Invalid Voltage Dependency table.\n");
    return;
  } else {
  }
  pi->sclk_dpm.soft_min_clk = 0U;
  pi->sclk_dpm.hard_min_clk = 0U;
  cz_send_msg_to_smc(adev, 59);
  level = cz_get_argument(adev);
  if (table->count > level) {
    clock = (table->entries + (unsigned long )level)->clk;
  } else {
    drm_err("Invalid SLCK Voltage Dependency table entry.\n");
    clock = (table->entries + (unsigned long )(table->count - 1U))->clk;
  }
  pi->sclk_dpm.soft_max_clk = clock;
  pi->sclk_dpm.hard_max_clk = clock;
  return;
}
}
static void cz_init_uvd_limit(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_uvd_clock_voltage_dependency_table *table ;
  u32 clock ;
  u32 level ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.uvd_clock_voltage_dependency_table;
  clock = 0U;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_uvd_clock_voltage_dependency_table *)0) || (unsigned int )table->count == 0U) {
    drm_err("Invalid Voltage Dependency table.\n");
    return;
  } else {
  }
  pi->uvd_dpm.soft_min_clk = 0U;
  pi->uvd_dpm.hard_min_clk = 0U;
  cz_send_msg_to_smc(adev, 61);
  level = cz_get_argument(adev);
  if ((u32 )table->count > level) {
    clock = (table->entries + (unsigned long )level)->vclk;
  } else {
    drm_err("Invalid UVD Voltage Dependency table entry.\n");
    clock = (table->entries + ((unsigned long )table->count + 0xffffffffffffffffUL))->vclk;
  }
  pi->uvd_dpm.soft_max_clk = clock;
  pi->uvd_dpm.hard_max_clk = clock;
  return;
}
}
static void cz_init_vce_limit(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  u32 clock ;
  u32 level ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  clock = 0U;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_vce_clock_voltage_dependency_table *)0) || (unsigned int )table->count == 0U) {
    drm_err("Invalid Voltage Dependency table.\n");
    return;
  } else {
  }
  pi->vce_dpm.soft_min_clk = (table->entries)->ecclk;
  pi->vce_dpm.hard_min_clk = (table->entries)->ecclk;
  cz_send_msg_to_smc(adev, 62);
  level = cz_get_argument(adev);
  if ((u32 )table->count > level) {
    clock = (table->entries + (unsigned long )level)->ecclk;
  } else {
    drm_err("Invalid VCE Voltage Dependency table entry.\n");
    clock = (table->entries + ((unsigned long )table->count + 0xffffffffffffffffUL))->ecclk;
  }
  pi->vce_dpm.soft_max_clk = clock;
  pi->vce_dpm.hard_max_clk = clock;
  return;
}
}
static void cz_init_acp_limit(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  u32 clock ;
  u32 level ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.acp_clock_voltage_dependency_table;
  clock = 0U;
  if ((unsigned long )table == (unsigned long )((struct amdgpu_clock_voltage_dependency_table *)0) || table->count == 0U) {
    drm_err("Invalid Voltage Dependency table.\n");
    return;
  } else {
  }
  pi->acp_dpm.soft_min_clk = 0U;
  pi->acp_dpm.hard_min_clk = 0U;
  cz_send_msg_to_smc(adev, 63);
  level = cz_get_argument(adev);
  if (table->count > level) {
    clock = (table->entries + (unsigned long )level)->clk;
  } else {
    drm_err("Invalid ACP Voltage Dependency table entry.\n");
    clock = (table->entries + (unsigned long )(table->count - 1U))->clk;
  }
  pi->acp_dpm.soft_max_clk = clock;
  pi->acp_dpm.hard_max_clk = clock;
  return;
}
}
static void cz_init_pg_state(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  pi->uvd_power_gated = 0;
  pi->vce_power_gated = 0;
  pi->acp_power_gated = 0;
  return;
}
}
static void cz_init_sclk_threshold(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  pi->low_sclk_interrupt_threshold = 0U;
  return;
}
}
static void cz_dpm_setup_asic(struct amdgpu_device *adev )
{
  {
  cz_reset_ap_mask(adev);
  cz_dpm_upload_pptable_to_smu(adev);
  cz_init_sclk_limit(adev);
  cz_init_uvd_limit(adev);
  cz_init_vce_limit(adev);
  cz_init_acp_limit(adev);
  cz_init_pg_state(adev);
  cz_init_sclk_threshold(adev);
  return;
}
}
static bool cz_check_smu_feature(struct amdgpu_device *adev , u32 feature )
{
  u32 smu_feature ;
  int ret ;
  {
  smu_feature = 0U;
  ret = cz_send_msg_to_smc_with_parameter(adev, 2, 0U);
  if (ret != 0) {
    drm_err("Failed to get SMU features from SMC.\n");
    return (0);
  } else {
    smu_feature = cz_get_argument(adev);
    if ((feature & smu_feature) != 0U) {
      return (1);
    } else {
    }
  }
  return (0);
}
}
static bool cz_check_for_dpm_enabled(struct amdgpu_device *adev )
{
  bool tmp ;
  {
  tmp = cz_check_smu_feature(adev, 2097152U);
  if ((int )tmp) {
    return (1);
  } else {
  }
  return (0);
}
}
static void cz_program_voting_clients(struct amdgpu_device *adev )
{
  {
  (*(adev->smc_wreg))(adev, 3491758504U, 1073725698U);
  return;
}
}
static void cz_clear_voting_clients(struct amdgpu_device *adev )
{
  {
  (*(adev->smc_wreg))(adev, 3491758504U, 0U);
  return;
}
}
static int cz_start_dpm(struct amdgpu_device *adev )
{
  int ret ;
  {
  ret = 0;
  if (amdgpu_dpm != 0) {
    ret = cz_send_msg_to_smc_with_parameter(adev, 3, 524288U);
    if (ret != 0) {
      drm_err("SMU feature: SCLK_DPM enable failed\n");
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int cz_stop_dpm(struct amdgpu_device *adev )
{
  int ret ;
  {
  ret = 0;
  if (amdgpu_dpm != 0 && (int )adev->pm.dpm_enabled) {
    ret = cz_send_msg_to_smc_with_parameter(adev, 4, 524288U);
    if (ret != 0) {
      drm_err("SMU feature: SCLK_DPM disable failed\n");
      return (-22);
    } else {
    }
  } else {
  }
  return (0);
}
}
static u32 cz_get_sclk_level(struct amdgpu_device *adev , u32 clock , u16 msg )
{
  int i ;
  struct amdgpu_clock_voltage_dependency_table *table ;
  {
  i = 0;
  table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  switch ((int )msg) {
  case 18: ;
  case 20:
  i = 0;
  goto ldv_48568;
  ldv_48567: ;
  if ((table->entries + (unsigned long )i)->clk >= clock) {
    goto ldv_48566;
  } else {
  }
  i = i + 1;
  ldv_48568: ;
  if ((u32 )i < table->count) {
    goto ldv_48567;
  } else {
  }
  ldv_48566: ;
  if ((u32 )i == table->count) {
    i = (int )(table->count - 1U);
  } else {
  }
  goto ldv_48569;
  case 19: ;
  case 21:
  i = (int )(table->count - 1U);
  goto ldv_48574;
  ldv_48573: ;
  if ((table->entries + (unsigned long )i)->clk <= clock) {
    goto ldv_48572;
  } else {
  }
  i = i - 1;
  ldv_48574: ;
  if (i >= 0) {
    goto ldv_48573;
  } else {
  }
  ldv_48572: ;
  if (i < 0) {
    i = 0;
  } else {
  }
  goto ldv_48569;
  default: ;
  goto ldv_48569;
  }
  ldv_48569: ;
  return ((u32 )i);
}
}
static u32 cz_get_eclk_level(struct amdgpu_device *adev , u32 clock , u16 msg )
{
  int i ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  {
  i = 0;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  if ((unsigned int )table->count == 0U) {
    return (0U);
  } else {
  }
  switch ((int )msg) {
  case 30: ;
  case 32:
  i = 0;
  goto ldv_48587;
  ldv_48586: ;
  if ((table->entries + (unsigned long )i)->ecclk >= clock) {
    goto ldv_48585;
  } else {
  }
  i = i + 1;
  ldv_48587: ;
  if ((int )table->count + -1 > i) {
    goto ldv_48586;
  } else {
  }
  ldv_48585: ;
  goto ldv_48588;
  case 31: ;
  case 33:
  i = (int )table->count + -1;
  goto ldv_48593;
  ldv_48592: ;
  if ((table->entries + (unsigned long )i)->ecclk <= clock) {
    goto ldv_48591;
  } else {
  }
  i = i - 1;
  ldv_48593: ;
  if (i > 0) {
    goto ldv_48592;
  } else {
  }
  ldv_48591: ;
  goto ldv_48588;
  default: ;
  goto ldv_48588;
  }
  ldv_48588: ;
  return ((u32 )i);
}
}
static int cz_program_bootup_state(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  u32 soft_min_clk ;
  u32 soft_max_clk ;
  int ret ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  soft_min_clk = 0U;
  soft_max_clk = 0U;
  ret = 0;
  pi->sclk_dpm.soft_min_clk = pi->sys_info.bootup_sclk;
  pi->sclk_dpm.soft_max_clk = pi->sys_info.bootup_sclk;
  soft_min_clk = cz_get_sclk_level(adev, pi->sclk_dpm.soft_min_clk, 18);
  soft_max_clk = cz_get_sclk_level(adev, pi->sclk_dpm.soft_max_clk, 19);
  ret = cz_send_msg_to_smc_with_parameter(adev, 18, soft_min_clk);
  if (ret != 0) {
    return (-22);
  } else {
  }
  ret = cz_send_msg_to_smc_with_parameter(adev, 19, soft_max_clk);
  if (ret != 0) {
    return (-22);
  } else {
  }
  return (0);
}
}
static int cz_disable_cgpg(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static int cz_enable_cgpg(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static int cz_program_pt_config_registers(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static void cz_do_enable_didt(struct amdgpu_device *adev , bool enable )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  u32 reg ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  reg = 0U;
  if ((int )pi->caps_sq_ramping) {
    reg = (*(adev->didt_rreg))(adev, 0U);
    if ((int )enable) {
      reg = reg | 1U;
    } else {
      reg = reg & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 0U, reg);
  } else {
  }
  if ((int )pi->caps_db_ramping) {
    reg = (*(adev->didt_rreg))(adev, 32U);
    if ((int )enable) {
      reg = reg | 1U;
    } else {
      reg = reg & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 32U, reg);
  } else {
  }
  if ((int )pi->caps_td_ramping) {
    reg = (*(adev->didt_rreg))(adev, 64U);
    if ((int )enable) {
      reg = reg | 1U;
    } else {
      reg = reg & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 64U, reg);
  } else {
  }
  if ((int )pi->caps_tcp_ramping) {
    reg = (*(adev->didt_rreg))(adev, 96U);
    if ((int )enable) {
      reg = reg | 1U;
    } else {
      reg = reg & 4294967294U;
    }
    (*(adev->didt_wreg))(adev, 96U, reg);
  } else {
  }
  return;
}
}
static int cz_enable_didt(struct amdgpu_device *adev , bool enable )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  if ((((int )pi->caps_sq_ramping || (int )pi->caps_db_ramping) || (int )pi->caps_td_ramping) || (int )pi->caps_tcp_ramping) {
    if (adev->gfx.gfx_current_status != 1U) {
      ret = cz_disable_cgpg(adev);
      if (ret != 0) {
        drm_err("Pre Di/Dt disable cg/pg failed\n");
        return (-22);
      } else {
      }
      adev->gfx.gfx_current_status = 1U;
    } else {
    }
    ret = cz_program_pt_config_registers(adev);
    if (ret != 0) {
      drm_err("Di/Dt config failed\n");
      return (-22);
    } else {
    }
    cz_do_enable_didt(adev, (int )enable);
    if (adev->gfx.gfx_current_status == 1U) {
      ret = cz_enable_cgpg(adev);
      if (ret != 0) {
        drm_err("Post Di/Dt enable cg/pg failed\n");
        return (-22);
      } else {
      }
      adev->gfx.gfx_current_status = 0U;
    } else {
    }
  } else {
  }
  return (0);
}
}
static void cz_reset_acp_boot_level(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void cz_update_current_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct cz_ps *ps ;
  struct cz_ps *tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  tmp___0 = cz_get_ps(rps);
  ps = tmp___0;
  pi->current_ps = *ps;
  pi->current_rps = *rps;
  pi->current_rps.ps_priv = (void *)ps;
  return;
}
}
static void cz_update_requested_ps(struct amdgpu_device *adev , struct amdgpu_ps *rps )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct cz_ps *ps ;
  struct cz_ps *tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  tmp___0 = cz_get_ps(rps);
  ps = tmp___0;
  pi->requested_ps = *ps;
  pi->requested_rps = *rps;
  pi->requested_rps.ps_priv = (void *)ps;
  return;
}
}
static void cz_apply_state_adjust_rules(struct amdgpu_device *adev , struct amdgpu_ps *new_rps ,
                                        struct amdgpu_ps *old_rps )
{
  struct cz_ps *ps ;
  struct cz_ps *tmp ;
  struct cz_power_info *pi ;
  struct cz_power_info *tmp___0 ;
  struct amdgpu_clock_and_voltage_limits *limits ;
  u32 mclk ;
  {
  tmp = cz_get_ps(new_rps);
  ps = tmp;
  tmp___0 = cz_get_pi(adev);
  pi = tmp___0;
  limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  mclk = 0U;
  ps->force_high = 0;
  ps->need_dfs_bypass = 1;
  pi->video_start = (bool )(((new_rps->dclk != 0U || new_rps->vclk != 0U) || new_rps->evclk != 0U) || new_rps->ecclk != 0U);
  if ((new_rps->class & 7U) == 1U) {
    pi->battery_state = 1;
  } else {
    pi->battery_state = 0;
  }
  if ((int )pi->caps_stable_power_state) {
    mclk = limits->mclk;
  } else {
  }
  if (pi->sys_info.nbp_memory_clock[1] < mclk) {
    ps->force_high = 1;
  } else {
  }
  return;
}
}
static int cz_dpm_enable(struct amdgpu_device *adev )
{
  int ret ;
  bool tmp ;
  {
  ret = 0;
  tmp = cz_check_for_dpm_enabled(adev);
  if ((int )tmp) {
    return (-22);
  } else {
  }
  cz_program_voting_clients(adev);
  ret = cz_start_dpm(adev);
  if (ret != 0) {
    drm_err("Carrizo DPM enable failed\n");
    return (-22);
  } else {
  }
  ret = cz_program_bootup_state(adev);
  if (ret != 0) {
    drm_err("Carrizo bootup state program failed\n");
    return (-22);
  } else {
  }
  ret = cz_enable_didt(adev, 1);
  if (ret != 0) {
    drm_err("Carrizo enable di/dt failed\n");
    return (-22);
  } else {
  }
  cz_reset_acp_boot_level(adev);
  cz_update_current_ps(adev, adev->pm.dpm.boot_ps);
  return (0);
}
}
static int cz_dpm_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  int ret ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = 0;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = cz_smu_init(adev);
  if (ret != 0) {
    drm_err("amdgpu: smc initialization failed\n");
    mutex_unlock(& adev->pm.mutex);
    return (ret);
  } else {
  }
  ret = cz_smu_start(adev);
  if (ret != 0) {
    drm_err("amdgpu: smc start failed\n");
    mutex_unlock(& adev->pm.mutex);
    return (ret);
  } else {
  }
  if (amdgpu_dpm == 0) {
    adev->pm.dpm_enabled = 0;
    mutex_unlock(& adev->pm.mutex);
    return (ret);
  } else {
  }
  cz_dpm_setup_asic(adev);
  ret = cz_dpm_enable(adev);
  if (ret != 0) {
    adev->pm.dpm_enabled = 0;
  } else {
    adev->pm.dpm_enabled = 1;
  }
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static int cz_dpm_disable(struct amdgpu_device *adev )
{
  int ret ;
  bool tmp ;
  int tmp___0 ;
  {
  ret = 0;
  tmp = cz_check_for_dpm_enabled(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  ret = cz_enable_didt(adev, 0);
  if (ret != 0) {
    drm_err("Carrizo disable di/dt failed\n");
    return (-22);
  } else {
  }
  cz_dpm_powergate_uvd(adev, 0);
  cz_dpm_powergate_vce(adev, 0);
  cz_clear_voting_clients(adev);
  cz_stop_dpm(adev);
  cz_update_current_ps(adev, adev->pm.dpm.boot_ps);
  return (0);
}
}
static int cz_dpm_hw_fini(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  struct amdgpu_ps *tmp ;
  {
  ret = 0;
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  cz_smu_fini(adev);
  if ((int )adev->pm.dpm_enabled) {
    ret = cz_dpm_disable(adev);
    tmp = adev->pm.dpm.boot_ps;
    adev->pm.dpm.requested_ps = tmp;
    adev->pm.dpm.current_ps = tmp;
  } else {
  }
  adev->pm.dpm_enabled = 0;
  mutex_unlock(& adev->pm.mutex);
  return (ret);
}
}
static int cz_dpm_suspend(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  struct amdgpu_ps *tmp ;
  {
  ret = 0;
  adev = (struct amdgpu_device *)handle;
  if ((int )adev->pm.dpm_enabled) {
    mutex_lock_nested(& adev->pm.mutex, 0U);
    ret = cz_dpm_disable(adev);
    tmp = adev->pm.dpm.boot_ps;
    adev->pm.dpm.requested_ps = tmp;
    adev->pm.dpm.current_ps = tmp;
    mutex_unlock(& adev->pm.mutex);
  } else {
  }
  return (ret);
}
}
static int cz_dpm_resume(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  ret = 0;
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = cz_smu_start(adev);
  if (ret != 0) {
    drm_err("amdgpu: smc start failed\n");
    mutex_unlock(& adev->pm.mutex);
    return (ret);
  } else {
  }
  if (amdgpu_dpm == 0) {
    adev->pm.dpm_enabled = 0;
    mutex_unlock(& adev->pm.mutex);
    return (ret);
  } else {
  }
  cz_dpm_setup_asic(adev);
  ret = cz_dpm_enable(adev);
  if (ret != 0) {
    adev->pm.dpm_enabled = 0;
  } else {
    adev->pm.dpm_enabled = 1;
  }
  mutex_unlock(& adev->pm.mutex);
  if ((int )adev->pm.dpm_enabled) {
    amdgpu_pm_compute_clocks(adev);
  } else {
  }
  return (0);
}
}
static int cz_dpm_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int cz_dpm_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
static int cz_dpm_get_temperature(struct amdgpu_device *adev )
{
  int actual_temp ;
  u32 temp ;
  u32 tmp ;
  {
  actual_temp = 0;
  tmp = (*(adev->smc_rreg))(adev, 3224374796U);
  temp = tmp;
  if (temp != 0U) {
    actual_temp = (int )((temp / 8U) * 1000U + 4294918296U);
  } else {
  }
  return (actual_temp);
}
}
static int cz_dpm_pre_set_power_state(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_ps requested_ps ;
  struct amdgpu_ps *new_ps ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  requested_ps = *(adev->pm.dpm.requested_ps);
  new_ps = & requested_ps;
  cz_update_requested_ps(adev, new_ps);
  cz_apply_state_adjust_rules(adev, & pi->requested_rps, & pi->current_rps);
  return (0);
}
}
static int cz_dpm_update_sclk_limit(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_clock_and_voltage_limits *limits ;
  u32 clock ;
  u32 stable_ps_clock ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  limits = & adev->pm.dpm.dyn_state.max_clock_voltage_on_ac;
  stable_ps_clock = 0U;
  clock = pi->sclk_dpm.soft_min_clk;
  if ((int )pi->caps_stable_power_state) {
    stable_ps_clock = (limits->sclk * 75U) / 100U;
    if (clock < stable_ps_clock) {
      clock = stable_ps_clock;
    } else {
    }
  } else {
  }
  if (pi->sclk_dpm.soft_min_clk != clock) {
    pi->sclk_dpm.soft_min_clk = clock;
    tmp___0 = cz_get_sclk_level(adev, clock, 18);
    cz_send_msg_to_smc_with_parameter(adev, 18, tmp___0);
  } else {
  }
  if ((int )pi->caps_stable_power_state && pi->sclk_dpm.soft_max_clk != clock) {
    pi->sclk_dpm.soft_max_clk = clock;
    tmp___1 = cz_get_sclk_level(adev, clock, 19);
    cz_send_msg_to_smc_with_parameter(adev, 19, tmp___1);
  } else {
    tmp___2 = cz_get_sclk_level(adev, pi->sclk_dpm.soft_max_clk, 19);
    cz_send_msg_to_smc_with_parameter(adev, 19, tmp___2);
  }
  return (0);
}
}
static int cz_dpm_set_deep_sleep_sclk_threshold(struct amdgpu_device *adev )
{
  int ret ;
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  ret = 0;
  tmp = cz_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_sclk_ds) {
    cz_send_msg_to_smc_with_parameter(adev, 17, 800U);
  } else {
  }
  return (ret);
}
}
static int cz_dpm_set_watermark_threshold(struct amdgpu_device *adev )
{
  int ret ;
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  ret = 0;
  tmp = cz_get_pi(adev);
  pi = tmp;
  cz_send_msg_to_smc_with_parameter(adev, 622, pi->sclk_dpm.soft_max_clk);
  return (ret);
}
}
static int cz_dpm_enable_nbdpm(struct amdgpu_device *adev )
{
  int ret ;
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  ret = 0;
  tmp = cz_get_pi(adev);
  pi = tmp;
  if ((int )pi->nb_dpm_enabled_by_driver && ! pi->nb_dpm_enabled) {
    ret = cz_send_msg_to_smc_with_parameter(adev, 3, 2048U);
    if (ret != 0) {
      drm_err("amdgpu: nb dpm enable failed\n");
      return (ret);
    } else {
    }
    pi->nb_dpm_enabled = 1;
  } else {
  }
  return (ret);
}
}
static void cz_dpm_nbdpm_lm_pstate_enable(struct amdgpu_device *adev , bool enable )
{
  {
  if ((int )enable) {
    cz_send_msg_to_smc(adev, 47);
  } else {
    cz_send_msg_to_smc(adev, 46);
  }
  return;
}
}
static int cz_dpm_update_low_memory_pstate(struct amdgpu_device *adev )
{
  int ret ;
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct cz_ps *ps ;
  {
  ret = 0;
  tmp = cz_get_pi(adev);
  pi = tmp;
  ps = & pi->requested_ps;
  if (pi->sys_info.nb_dpm_enable != 0U) {
    if ((int )ps->force_high) {
      cz_dpm_nbdpm_lm_pstate_enable(adev, 1);
    } else {
      cz_dpm_nbdpm_lm_pstate_enable(adev, 0);
    }
  } else {
  }
  return (ret);
}
}
static int cz_dpm_set_power_state(struct amdgpu_device *adev )
{
  int ret ;
  {
  ret = 0;
  cz_dpm_update_sclk_limit(adev);
  cz_dpm_set_deep_sleep_sclk_threshold(adev);
  cz_dpm_set_watermark_threshold(adev);
  cz_dpm_enable_nbdpm(adev);
  cz_dpm_update_low_memory_pstate(adev);
  return (ret);
}
}
static void cz_dpm_post_set_power_state(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_ps *ps ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ps = & pi->requested_rps;
  cz_update_current_ps(adev, ps);
  return;
}
}
static int cz_dpm_force_highest(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  u32 tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ret = 0;
  if (pi->sclk_dpm.soft_min_clk != pi->sclk_dpm.soft_max_clk) {
    pi->sclk_dpm.soft_min_clk = pi->sclk_dpm.soft_max_clk;
    tmp___0 = cz_get_sclk_level(adev, pi->sclk_dpm.soft_min_clk, 18);
    ret = cz_send_msg_to_smc_with_parameter(adev, 18, tmp___0);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  return (ret);
}
}
static int cz_dpm_force_lowest(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  u32 tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ret = 0;
  if (pi->sclk_dpm.soft_max_clk != pi->sclk_dpm.soft_min_clk) {
    pi->sclk_dpm.soft_max_clk = pi->sclk_dpm.soft_min_clk;
    tmp___0 = cz_get_sclk_level(adev, pi->sclk_dpm.soft_max_clk, 19);
    ret = cz_send_msg_to_smc_with_parameter(adev, 19, tmp___0);
    if (ret != 0) {
      return (ret);
    } else {
    }
  } else {
  }
  return (ret);
}
}
static u32 cz_dpm_get_max_sclk_level(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  u32 tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  if (pi->max_sclk_level == 0U) {
    cz_send_msg_to_smc(adev, 59);
    tmp___0 = cz_get_argument(adev);
    pi->max_sclk_level = tmp___0 + 1U;
  } else {
  }
  if (pi->max_sclk_level > 8U) {
    drm_err("Invalid max sclk level!\n");
    return (4294967274U);
  } else {
  }
  return (pi->max_sclk_level);
}
}
static int cz_dpm_unforce_dpm_levels(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_clock_voltage_dependency_table *dep_table ;
  u32 level ;
  int ret ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  dep_table = & adev->pm.dpm.dyn_state.vddc_dependency_on_sclk;
  level = 0U;
  ret = 0;
  pi->sclk_dpm.soft_min_clk = (dep_table->entries)->clk;
  tmp___0 = cz_dpm_get_max_sclk_level(adev);
  level = tmp___0 - 1U;
  if (dep_table->count > level) {
    pi->sclk_dpm.soft_max_clk = (dep_table->entries + (unsigned long )level)->clk;
  } else {
    pi->sclk_dpm.soft_max_clk = (dep_table->entries + (unsigned long )(dep_table->count - 1U))->clk;
  }
  tmp___1 = cz_get_sclk_level(adev, pi->sclk_dpm.soft_min_clk, 18);
  ret = cz_send_msg_to_smc_with_parameter(adev, 18, tmp___1);
  if (ret != 0) {
    return (ret);
  } else {
  }
  tmp___2 = cz_get_sclk_level(adev, pi->sclk_dpm.soft_max_clk, 19);
  ret = cz_send_msg_to_smc_with_parameter(adev, 19, tmp___2);
  if (ret != 0) {
    return (ret);
  } else {
  }
  printk("\016[drm] DPM unforce state min=%d, max=%d.\n", pi->sclk_dpm.soft_min_clk,
         pi->sclk_dpm.soft_max_clk);
  return (0);
}
}
static int cz_dpm_force_dpm_level(struct amdgpu_device *adev , enum amdgpu_dpm_forced_level level )
{
  int ret ;
  {
  ret = 0;
  switch ((unsigned int )level) {
  case 2U:
  ret = cz_dpm_force_highest(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  goto ldv_48762;
  case 1U:
  ret = cz_dpm_force_lowest(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  goto ldv_48762;
  case 0U:
  ret = cz_dpm_unforce_dpm_levels(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  goto ldv_48762;
  default: ;
  goto ldv_48762;
  }
  ldv_48762: ;
  return (ret);
}
}
static void cz_dpm_display_configuration_changed(struct amdgpu_device *adev )
{
  {
  return;
}
}
static u32 cz_dpm_get_sclk(struct amdgpu_device *adev , bool low )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct cz_ps *requested_state ;
  struct cz_ps *tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  tmp___0 = cz_get_ps(& pi->requested_rps);
  requested_state = tmp___0;
  if ((int )low) {
    return (requested_state->levels[0].sclk);
  } else {
    return (requested_state->levels[requested_state->num_levels - 1U].sclk);
  }
}
}
static u32 cz_dpm_get_mclk(struct amdgpu_device *adev , bool low )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  return (pi->sys_info.bootup_uma_clk);
}
}
static int cz_enable_uvd_dpm(struct amdgpu_device *adev , bool enable )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )enable && (int )pi->caps_uvd_dpm) {
    pi->dpm_flags = pi->dpm_flags | 2U;
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("cz_enable_uvd_dpm", "UVD DPM Enabled.\n");
    } else {
    }
    ret = cz_send_msg_to_smc_with_parameter(adev, 3, 65536U);
  } else {
    pi->dpm_flags = pi->dpm_flags & 4294967293U;
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("cz_enable_uvd_dpm", "UVD DPM Stopped\n");
    } else {
    }
    ret = cz_send_msg_to_smc_with_parameter(adev, 4, 65536U);
  }
  return (ret);
}
}
static int cz_update_uvd_dpm(struct amdgpu_device *adev , bool gate )
{
  int tmp ;
  {
  tmp = cz_enable_uvd_dpm(adev, (int )((bool )(! ((int )gate != 0))));
  return (tmp);
}
}
static void cz_dpm_powergate_uvd(struct amdgpu_device *adev , bool gate )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  if ((int )pi->uvd_power_gated == (int )gate) {
    return;
  } else {
  }
  pi->uvd_power_gated = gate;
  if ((int )gate) {
    if ((int )pi->caps_uvd_pg) {
      ret = amdgpu_set_clockgating_state(adev, 7, 1);
      ret = amdgpu_set_powergating_state(adev, 7, 0);
    } else {
    }
    cz_update_uvd_dpm(adev, (int )gate);
    if ((int )pi->caps_uvd_pg) {
      cz_send_msg_to_smc(adev, 7);
    } else {
    }
  } else {
    if ((int )pi->caps_uvd_pg) {
      if ((int )pi->uvd_dynamic_pg) {
        cz_send_msg_to_smc_with_parameter(adev, 8, 1U);
      } else {
        cz_send_msg_to_smc_with_parameter(adev, 8, 0U);
      }
      ret = amdgpu_set_powergating_state(adev, 7, 1);
      ret = amdgpu_set_clockgating_state(adev, 7, 0);
    } else {
    }
    cz_update_uvd_dpm(adev, (int )gate);
  }
  return;
}
}
static int cz_enable_vce_dpm(struct amdgpu_device *adev , bool enable )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  int ret ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  ret = 0;
  if ((int )enable && (int )pi->caps_vce_dpm) {
    pi->dpm_flags = pi->dpm_flags | 4U;
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("cz_enable_vce_dpm", "VCE DPM Enabled.\n");
    } else {
    }
    ret = cz_send_msg_to_smc_with_parameter(adev, 3, 4194304U);
  } else {
    pi->dpm_flags = pi->dpm_flags & 4294967291U;
    tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("cz_enable_vce_dpm", "VCE DPM Stopped\n");
    } else {
    }
    ret = cz_send_msg_to_smc_with_parameter(adev, 4, 4194304U);
  }
  return (ret);
}
}
static int cz_update_vce_dpm(struct amdgpu_device *adev )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  struct amdgpu_vce_clock_voltage_dependency_table *table ;
  u32 tmp___0 ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  table = & adev->pm.dpm.dyn_state.vce_clock_voltage_dependency_table;
  if ((int )pi->caps_stable_power_state) {
    pi->vce_dpm.hard_min_clk = (table->entries + ((unsigned long )table->count + 0xffffffffffffffffUL))->ecclk;
  } else {
    pi->vce_dpm.hard_min_clk = (table->entries)->ecclk;
  }
  tmp___0 = cz_get_eclk_level(adev, pi->vce_dpm.hard_min_clk, 32);
  cz_send_msg_to_smc_with_parameter(adev, 32, tmp___0);
  return (0);
}
}
static void cz_dpm_powergate_vce(struct amdgpu_device *adev , bool gate )
{
  struct cz_power_info *pi ;
  struct cz_power_info *tmp ;
  {
  tmp = cz_get_pi(adev);
  pi = tmp;
  if ((int )pi->caps_vce_pg) {
    if ((int )pi->vce_power_gated != (int )gate) {
      if ((int )gate) {
        amdgpu_set_clockgating_state(adev, 8, 1);
        amdgpu_set_powergating_state(adev, 8, 0);
        cz_enable_vce_dpm(adev, 0);
        pi->vce_power_gated = 1;
      } else {
        cz_send_msg_to_smc(adev, 10);
        pi->vce_power_gated = 0;
        amdgpu_set_powergating_state(adev, 8, 1);
        amdgpu_set_clockgating_state(adev, 8, 0);
        cz_update_vce_dpm(adev);
        cz_enable_vce_dpm(adev, 1);
      }
    } else
    if (! pi->vce_power_gated) {
      cz_update_vce_dpm(adev);
    } else {
    }
  } else {
    cz_update_vce_dpm(adev);
    cz_enable_vce_dpm(adev, 1);
  }
  return;
}
}
struct amd_ip_funcs const cz_dpm_ip_funcs =
     {& cz_dpm_early_init, & cz_dpm_late_init, & cz_dpm_sw_init, & cz_dpm_sw_fini, & cz_dpm_hw_init,
    & cz_dpm_hw_fini, & cz_dpm_suspend, & cz_dpm_resume, (bool (*)(void * ))0, (int (*)(void * ))0,
    (int (*)(void * ))0, (void (*)(void * ))0, & cz_dpm_set_clockgating_state, & cz_dpm_set_powergating_state};
static struct amdgpu_dpm_funcs const cz_dpm_funcs =
     {& cz_dpm_get_temperature, & cz_dpm_pre_set_power_state, & cz_dpm_set_power_state,
    & cz_dpm_post_set_power_state, & cz_dpm_display_configuration_changed, & cz_dpm_get_sclk,
    & cz_dpm_get_mclk, & cz_dpm_print_power_state, & cz_dpm_debugfs_print_current_performance_level,
    & cz_dpm_force_dpm_level, (bool (*)(struct amdgpu_device * ))0, & cz_dpm_powergate_uvd,
    & cz_dpm_powergate_vce, 0, 0, 0, 0, 0};
static void cz_dpm_set_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.funcs == (unsigned long )((struct amdgpu_dpm_funcs const *)0)) {
    adev->pm.funcs = & cz_dpm_funcs;
  } else {
  }
  return;
}
}
int ldv_retval_15 ;
extern int ldv_probe_67(void) ;
int ldv_retval_16 ;
extern int ldv_release_67(void) ;
void ldv_initialize_amdgpu_dpm_funcs_66(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(23352UL);
  cz_dpm_funcs_group0 = (struct amdgpu_device *)tmp;
  return;
}
}
void ldv_main_exported_67(void)
{
  void *ldvarg289 ;
  void *tmp ;
  void *ldvarg283 ;
  void *tmp___0 ;
  void *ldvarg279 ;
  void *tmp___1 ;
  void *ldvarg288 ;
  void *tmp___2 ;
  void *ldvarg280 ;
  void *tmp___3 ;
  enum amd_powergating_state ldvarg284 ;
  enum amd_clockgating_state ldvarg281 ;
  void *ldvarg286 ;
  void *tmp___4 ;
  void *ldvarg287 ;
  void *tmp___5 ;
  void *ldvarg282 ;
  void *tmp___6 ;
  void *ldvarg290 ;
  void *tmp___7 ;
  void *ldvarg285 ;
  void *tmp___8 ;
  int tmp___9 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg289 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg283 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg279 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg288 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg280 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg286 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg287 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg282 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg290 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg285 = tmp___8;
  ldv_memset((void *)(& ldvarg284), 0, 4UL);
  ldv_memset((void *)(& ldvarg281), 0, 4UL);
  tmp___9 = __VERIFIER_nondet_int();
  switch (tmp___9) {
  case 0: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_hw_fini(ldvarg290);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_hw_fini(ldvarg290);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_hw_fini(ldvarg290);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 1: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_early_init(ldvarg289);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_early_init(ldvarg289);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_early_init(ldvarg289);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 2: ;
  if (ldv_state_variable_67 == 2) {
    ldv_retval_16 = cz_dpm_suspend(ldvarg288);
    if (ldv_retval_16 == 0) {
      ldv_state_variable_67 = 3;
    } else {
    }
  } else {
  }
  goto ldv_48844;
  case 3: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_late_init(ldvarg287);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_late_init(ldvarg287);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_late_init(ldvarg287);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 4: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_sw_init(ldvarg286);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_sw_init(ldvarg286);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_sw_init(ldvarg286);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 5: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_set_powergating_state(ldvarg285, ldvarg284);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_set_powergating_state(ldvarg285, ldvarg284);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_set_powergating_state(ldvarg285, ldvarg284);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 6: ;
  if (ldv_state_variable_67 == 3) {
    ldv_retval_15 = cz_dpm_resume(ldvarg283);
    if (ldv_retval_15 == 0) {
      ldv_state_variable_67 = 2;
    } else {
    }
  } else {
  }
  goto ldv_48844;
  case 7: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_set_clockgating_state(ldvarg282, ldvarg281);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_set_clockgating_state(ldvarg282, ldvarg281);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_set_clockgating_state(ldvarg282, ldvarg281);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 8: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_hw_init(ldvarg280);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_hw_init(ldvarg280);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_hw_init(ldvarg280);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 9: ;
  if (ldv_state_variable_67 == 1) {
    cz_dpm_sw_fini(ldvarg279);
    ldv_state_variable_67 = 1;
  } else {
  }
  if (ldv_state_variable_67 == 3) {
    cz_dpm_sw_fini(ldvarg279);
    ldv_state_variable_67 = 3;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    cz_dpm_sw_fini(ldvarg279);
    ldv_state_variable_67 = 2;
  } else {
  }
  goto ldv_48844;
  case 10: ;
  if (ldv_state_variable_67 == 3) {
    ldv_release_67();
    ldv_state_variable_67 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_67 == 2) {
    ldv_release_67();
    ldv_state_variable_67 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_48844;
  case 11: ;
  if (ldv_state_variable_67 == 1) {
    ldv_probe_67();
    ldv_state_variable_67 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_48844;
  default:
  ldv_stop();
  }
  ldv_48844: ;
  return;
}
}
void ldv_main_exported_66(void)
{
  bool ldvarg815 ;
  struct amdgpu_ps *ldvarg812 ;
  void *tmp ;
  bool ldvarg811 ;
  struct seq_file *ldvarg816 ;
  void *tmp___0 ;
  bool ldvarg810 ;
  enum amdgpu_dpm_forced_level ldvarg813 ;
  bool ldvarg814 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(48UL);
  ldvarg812 = (struct amdgpu_ps *)tmp;
  tmp___0 = ldv_init_zalloc(256UL);
  ldvarg816 = (struct seq_file *)tmp___0;
  ldv_memset((void *)(& ldvarg815), 0, 1UL);
  ldv_memset((void *)(& ldvarg811), 0, 1UL);
  ldv_memset((void *)(& ldvarg810), 0, 1UL);
  ldv_memset((void *)(& ldvarg813), 0, 4UL);
  ldv_memset((void *)(& ldvarg814), 0, 1UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_debugfs_print_current_performance_level(cz_dpm_funcs_group0, ldvarg816);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 1: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_get_sclk(cz_dpm_funcs_group0, (int )ldvarg815);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 2: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_get_temperature(cz_dpm_funcs_group0);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 3: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_powergate_vce(cz_dpm_funcs_group0, (int )ldvarg814);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 4: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_force_dpm_level(cz_dpm_funcs_group0, ldvarg813);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 5: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_post_set_power_state(cz_dpm_funcs_group0);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 6: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_display_configuration_changed(cz_dpm_funcs_group0);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 7: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_print_power_state(cz_dpm_funcs_group0, ldvarg812);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 8: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_pre_set_power_state(cz_dpm_funcs_group0);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 9: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_set_power_state(cz_dpm_funcs_group0);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 10: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_get_mclk(cz_dpm_funcs_group0, (int )ldvarg811);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  case 11: ;
  if (ldv_state_variable_66 == 1) {
    cz_dpm_powergate_uvd(cz_dpm_funcs_group0, (int )ldvarg810);
    ldv_state_variable_66 = 1;
  } else {
  }
  goto ldv_48868;
  default:
  ldv_stop();
  }
  ldv_48868: ;
  return;
}
}
bool ldv_queue_work_on_823(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_824(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_825(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_826(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_827(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_837(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_839(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_838(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_841(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_840(struct workqueue_struct *ldv_func_arg1 ) ;
int tonga_smu_init(struct amdgpu_device *adev ) ;
int tonga_smu_fini(struct amdgpu_device *adev ) ;
int tonga_smu_start(struct amdgpu_device *adev ) ;
static int tonga_set_smc_sram_address(struct amdgpu_device *adev , u32 smc_address ,
                                      u32 limit )
{
  u32 val ;
  {
  if ((smc_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_address + 3U > limit) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 128U, smc_address, 0);
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val & 4294967294U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  return (0);
}
}
static int tonga_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                   uint8_t const *src , u32 byte_count , u32 limit )
{
  u32 addr ;
  u32 data ;
  u32 orig_data ;
  int result ;
  u32 extra_shift ;
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  uint8_t const *tmp___0 ;
  {
  result = 0;
  if ((smc_start_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_start_address + byte_count > limit) {
    return (-22);
  } else {
  }
  addr = smc_start_address;
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  goto ldv_43723;
  ldv_43722:
  data = (u32 )(((((int )*src << 24) + ((int )*(src + 1UL) << 16)) + ((int )*(src + 2UL) << 8)) + (int )*(src + 3UL));
  result = tonga_set_smc_sram_address(adev, addr, limit);
  if (result != 0) {
    goto out;
  } else {
  }
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  byte_count = byte_count - 4U;
  addr = addr + 4U;
  ldv_43723: ;
  if (byte_count > 3U) {
    goto ldv_43722;
  } else {
  }
  if (byte_count != 0U) {
    data = 0U;
    result = tonga_set_smc_sram_address(adev, addr, limit);
    if (result != 0) {
      goto out;
    } else {
    }
    orig_data = amdgpu_mm_rreg(adev, 129U, 0);
    extra_shift = (4U - byte_count) * 8U;
    goto ldv_43726;
    ldv_43725:
    tmp___0 = src;
    src = src + 1;
    data = (data << 8) + (u32 )*tmp___0;
    byte_count = byte_count - 1U;
    ldv_43726: ;
    if (byte_count != 0U) {
      goto ldv_43725;
    } else {
    }
    data = data << (int )extra_shift;
    data = (~ ((u32 )(0xffffffffffffffffUL << (int )extra_shift)) & orig_data) | data;
    result = tonga_set_smc_sram_address(adev, addr, limit);
    if (result != 0) {
      goto out;
    } else {
    }
    amdgpu_mm_wreg(adev, 129U, data, 0);
  } else {
  }
  out:
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (result);
}
}
static int tonga_program_jump_on_start(struct amdgpu_device *adev )
{
  unsigned char data[4U] ;
  {
  data[0] = 224U;
  data[1] = 0U;
  data[2] = 128U;
  data[3] = 64U;
  tonga_copy_bytes_to_smc(adev, 0U, (uint8_t const *)(& data), 4U, 5U);
  return (0);
}
}
static bool tonga_is_smc_ram_running(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  val = tmp;
  val = val & 1U;
  if (val == 0U) {
    tmp___0 = (*(adev->smc_rreg))(adev, 2147484528U);
    if (tmp___0 > 131327U) {
      tmp___1 = 1;
    } else {
      tmp___1 = 0;
    }
  } else {
    tmp___1 = 0;
  }
  return ((bool )tmp___1);
}
}
static int wait_smu_response(struct amdgpu_device *adev )
{
  int i ;
  u32 val ;
  {
  i = 0;
  goto ldv_43743;
  ldv_43742:
  val = amdgpu_mm_rreg(adev, 149U, 0);
  if ((val & 65535U) != 0U) {
    goto ldv_43741;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43743: ;
  if (adev->usec_timeout > i) {
    goto ldv_43742;
  } else {
  }
  ldv_43741: ;
  if (adev->usec_timeout == i) {
    return (-22);
  } else {
  }
  return (0);
}
}
static int tonga_send_msg_to_smc_offset(struct amdgpu_device *adev )
{
  int tmp ;
  int tmp___0 ;
  {
  tmp = wait_smu_response(adev);
  if (tmp != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 164U, 131072U, 0);
  amdgpu_mm_wreg(adev, 148U, 256U, 0);
  tmp___0 = wait_smu_response(adev);
  if (tmp___0 != 0) {
    drm_err("Failed to send message\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int tonga_send_msg_to_smc(struct amdgpu_device *adev , PPSMC_Msg msg )
{
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = tonga_is_smc_ram_running(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  tmp___1 = wait_smu_response(adev);
  if (tmp___1 != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 148U, (u32 )msg, 0);
  tmp___2 = wait_smu_response(adev);
  if (tmp___2 != 0) {
    drm_err("Failed to send message\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int tonga_send_msg_to_smc_without_waiting(struct amdgpu_device *adev , PPSMC_Msg msg )
{
  int tmp ;
  {
  tmp = wait_smu_response(adev);
  if (tmp != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 148U, (u32 )msg, 0);
  return (0);
}
}
static int tonga_send_msg_to_smc_with_parameter(struct amdgpu_device *adev , PPSMC_Msg msg ,
                                                u32 parameter )
{
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = tonga_is_smc_ram_running(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  tmp___1 = wait_smu_response(adev);
  if (tmp___1 != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp___2 = tonga_send_msg_to_smc(adev, (int )msg);
  return (tmp___2);
}
}
static int tonga_send_msg_to_smc_with_parameter_without_waiting(struct amdgpu_device *adev ,
                                                                PPSMC_Msg msg , u32 parameter )
{
  int tmp ;
  int tmp___0 ;
  {
  tmp = wait_smu_response(adev);
  if (tmp != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp___0 = tonga_send_msg_to_smc_without_waiting(adev, (int )msg);
  return (tmp___0);
}
}
static int tonga_smu_upload_firmware_image(struct amdgpu_device *adev )
{
  struct smc_firmware_header_v1_0 const *hdr ;
  u32 ucode_size ;
  u32 ucode_start_address ;
  uint8_t const *src ;
  u32 val ;
  u32 byte_count ;
  u32 *data ;
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  if ((unsigned long )adev->pm.fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct smc_firmware_header_v1_0 const *)(adev->pm.fw)->data;
  amdgpu_ucode_print_smc_hdr(& hdr->header);
  adev->pm.fw_version = hdr->header.ucode_version;
  ucode_size = hdr->header.ucode_size_bytes;
  ucode_start_address = hdr->ucode_start_addr;
  src = (uint8_t const *)(adev->pm.fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  if ((ucode_size & 3U) != 0U) {
    drm_err("SMC ucode is not 4 bytes aligned\n");
    return (-22);
  } else {
  }
  if (ucode_size > 131072U) {
    drm_err("SMC address is beyond the SMC RAM area\n");
    return (-22);
  } else {
  }
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, ucode_start_address, 0);
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val | 1U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  byte_count = ucode_size;
  data = (u32 *)src;
  goto ldv_43780;
  ldv_43779:
  amdgpu_mm_wreg(adev, 129U, *data, 0);
  data = data + 1;
  byte_count = byte_count - 4U;
  ldv_43780: ;
  if (byte_count > 3U) {
    goto ldv_43779;
  } else {
  }
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val & 4294967294U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (0);
}
}
static enum AMDGPU_UCODE_ID tonga_convert_fw_type(u32 fw_type )
{
  {
  switch (fw_type) {
  case 1U: ;
  return (0);
  case 2U: ;
  return (1);
  case 3U: ;
  return (2);
  case 4U: ;
  return (3);
  case 5U: ;
  return (4);
  case 6U: ;
  case 7U: ;
  return (5);
  case 8U: ;
  return (6);
  case 10U: ;
  return (7);
  default:
  drm_err("ucode type is out of range!\n");
  return (8);
  }
}
}
static int tonga_smu_populate_single_firmware_entry(struct amdgpu_device *adev , u32 fw_type ,
                                                    struct SMU_Entry *entry )
{
  enum AMDGPU_UCODE_ID id ;
  enum AMDGPU_UCODE_ID tmp ;
  struct amdgpu_firmware_info *ucode ;
  struct gfx_firmware_header_v1_0 const *header ;
  uint64_t gpu_addr ;
  u32 data_size ;
  {
  tmp = tonga_convert_fw_type(fw_type);
  id = tmp;
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )id;
  header = (struct gfx_firmware_header_v1_0 const *)0;
  if ((unsigned long )ucode->fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  gpu_addr = ucode->mc_addr;
  header = (struct gfx_firmware_header_v1_0 const *)(ucode->fw)->data;
  data_size = header->header.ucode_size_bytes;
  if (fw_type == 7U || fw_type == 8U) {
    gpu_addr = (uint64_t )(header->jt_offset << 2) + gpu_addr;
    data_size = header->jt_size << 2;
  } else {
  }
  entry->version = (unsigned short )header->header.ucode_version;
  entry->id = (unsigned short )fw_type;
  entry->image_addr_high = (unsigned int )(gpu_addr >> 32ULL);
  entry->image_addr_low = (unsigned int )gpu_addr;
  entry->meta_data_addr_high = 0U;
  entry->meta_data_addr_low = 0U;
  entry->data_size_byte = data_size;
  entry->num_register_entries = 0U;
  if (fw_type == 10U) {
    entry->flags = 1U;
  } else {
    entry->flags = 0U;
  }
  return (0);
}
}
static int tonga_smu_request_load_fw(struct amdgpu_device *adev )
{
  struct tonga_smu_private_data *private ;
  struct SMU_DRAMData_TOC *toc ;
  u32 fw_to_load ;
  u32 tmp ;
  int tmp___0 ;
  u32 tmp___1 ;
  int tmp___2 ;
  u32 tmp___3 ;
  int tmp___4 ;
  u32 tmp___5 ;
  int tmp___6 ;
  u32 tmp___7 ;
  int tmp___8 ;
  u32 tmp___9 ;
  int tmp___10 ;
  u32 tmp___11 ;
  int tmp___12 ;
  u32 tmp___13 ;
  int tmp___14 ;
  u32 tmp___15 ;
  int tmp___16 ;
  int tmp___17 ;
  {
  private = (struct tonga_smu_private_data *)adev->smu.priv;
  (*(adev->smc_wreg))(adev, 261204U, 0U);
  tonga_send_msg_to_smc_with_parameter(adev, 594, private->smu_buffer_addr_high);
  tonga_send_msg_to_smc_with_parameter(adev, 595, private->smu_buffer_addr_low);
  toc = (struct SMU_DRAMData_TOC *)private->header;
  toc->num_entries = 0U;
  toc->structure_version = 1U;
  if (! adev->firmware.smu_load) {
    return (0);
  } else {
  }
  tmp = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___0 = tonga_smu_populate_single_firmware_entry(adev, 10U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp);
  if (tmp___0 != 0) {
    drm_err("Failed to get firmware entry for RLC\n");
    return (-22);
  } else {
  }
  tmp___1 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___2 = tonga_smu_populate_single_firmware_entry(adev, 3U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___1);
  if (tmp___2 != 0) {
    drm_err("Failed to get firmware entry for CE\n");
    return (-22);
  } else {
  }
  tmp___3 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___4 = tonga_smu_populate_single_firmware_entry(adev, 4U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___3);
  if (tmp___4 != 0) {
    drm_err("Failed to get firmware entry for PFP\n");
    return (-22);
  } else {
  }
  tmp___5 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___6 = tonga_smu_populate_single_firmware_entry(adev, 5U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___5);
  if (tmp___6 != 0) {
    drm_err("Failed to get firmware entry for ME\n");
    return (-22);
  } else {
  }
  tmp___7 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___8 = tonga_smu_populate_single_firmware_entry(adev, 6U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___7);
  if (tmp___8 != 0) {
    drm_err("Failed to get firmware entry for MEC\n");
    return (-22);
  } else {
  }
  tmp___9 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___10 = tonga_smu_populate_single_firmware_entry(adev, 7U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___9);
  if (tmp___10 != 0) {
    drm_err("Failed to get firmware entry for MEC_JT1\n");
    return (-22);
  } else {
  }
  tmp___11 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___12 = tonga_smu_populate_single_firmware_entry(adev, 8U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___11);
  if (tmp___12 != 0) {
    drm_err("Failed to get firmware entry for MEC_JT2\n");
    return (-22);
  } else {
  }
  tmp___13 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___14 = tonga_smu_populate_single_firmware_entry(adev, 1U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___13);
  if (tmp___14 != 0) {
    drm_err("Failed to get firmware entry for SDMA0\n");
    return (-22);
  } else {
  }
  tmp___15 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___16 = tonga_smu_populate_single_firmware_entry(adev, 2U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___15);
  if (tmp___16 != 0) {
    drm_err("Failed to get firmware entry for SDMA1\n");
    return (-22);
  } else {
  }
  tonga_send_msg_to_smc_with_parameter(adev, 592, private->header_addr_high);
  tonga_send_msg_to_smc_with_parameter(adev, 593, private->header_addr_low);
  fw_to_load = 1150U;
  tmp___17 = tonga_send_msg_to_smc_with_parameter_without_waiting(adev, 596, fw_to_load);
  if (tmp___17 != 0) {
    drm_err("Fail to request SMU load ucode\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static u32 tonga_smu_get_mask_for_fw_type(u32 fw_type )
{
  {
  switch (fw_type) {
  case 0U: ;
  return (2U);
  case 1U: ;
  return (4U);
  case 2U: ;
  return (8U);
  case 3U: ;
  return (16U);
  case 4U: ;
  return (32U);
  case 5U: ;
  return (64U);
  case 6U: ;
  return (64U);
  case 7U: ;
  return (1024U);
  default:
  drm_err("ucode type is out of range!\n");
  return (0U);
  }
}
}
static int tonga_smu_check_fw_load_finish(struct amdgpu_device *adev , u32 fw_type )
{
  u32 fw_mask ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  {
  tmp = tonga_smu_get_mask_for_fw_type(fw_type);
  fw_mask = tmp;
  i = 0;
  goto ldv_43831;
  ldv_43830:
  tmp___0 = (*(adev->smc_rreg))(adev, 261204U);
  if ((tmp___0 & fw_mask) == fw_mask) {
    goto ldv_43829;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43831: ;
  if (adev->usec_timeout > i) {
    goto ldv_43830;
  } else {
  }
  ldv_43829: ;
  if (adev->usec_timeout == i) {
    drm_err("check firmware loading failed\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int tonga_smu_start_in_protection_mode(struct amdgpu_device *adev )
{
  int result ;
  u32 val ;
  int i ;
  {
  val = (*(adev->smc_rreg))(adev, 2147483648U);
  val = val | 1U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  result = tonga_smu_upload_firmware_image(adev);
  if (result != 0) {
    return (result);
  } else {
  }
  (*(adev->smc_wreg))(adev, 3758108808U, 0U);
  val = (*(adev->smc_rreg))(adev, 2147483652U);
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483652U, val);
  val = (*(adev->smc_rreg))(adev, 2147483648U);
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  val = (*(adev->smc_rreg))(adev, 3758108856U);
  val = val | 2147483648U;
  (*(adev->smc_wreg))(adev, 3758108856U, val);
  (*(adev->smc_wreg))(adev, 260096U, 0U);
  i = 0;
  goto ldv_43840;
  ldv_43839:
  val = (*(adev->smc_rreg))(adev, 3221225476U);
  if ((val & 65536U) >> 16 != 0U) {
    goto ldv_43838;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43840: ;
  if (adev->usec_timeout > i) {
    goto ldv_43839;
  } else {
  }
  ldv_43838: ;
  if (adev->usec_timeout == i) {
    drm_err("Interrupt is not enabled by firmware\n");
    return (-22);
  } else {
  }
  tonga_send_msg_to_smc_offset(adev);
  i = 0;
  goto ldv_43843;
  ldv_43842:
  val = (*(adev->smc_rreg))(adev, 3758108808U);
  if ((int )val & 1) {
    goto ldv_43841;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43843: ;
  if (adev->usec_timeout > i) {
    goto ldv_43842;
  } else {
  }
  ldv_43841: ;
  if (adev->usec_timeout == i) {
    drm_err("Timeout for SMU start\n");
    return (-22);
  } else {
  }
  val = (*(adev->smc_rreg))(adev, 3758108808U);
  if ((val & 2U) >> 1 == 0U) {
    drm_err("SMU Firmware start failed\n");
    return (-22);
  } else {
  }
  i = 0;
  goto ldv_43846;
  ldv_43845:
  val = (*(adev->smc_rreg))(adev, 260096U);
  if ((int )val & 1) {
    goto ldv_43844;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43846: ;
  if (adev->usec_timeout > i) {
    goto ldv_43845;
  } else {
  }
  ldv_43844: ;
  if (adev->usec_timeout == i) {
    drm_err("SMU firmware initialization failed\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int tonga_smu_start_in_non_protection_mode(struct amdgpu_device *adev )
{
  int i ;
  int result ;
  u32 val ;
  {
  i = 0;
  goto ldv_43855;
  ldv_43854:
  val = (*(adev->smc_rreg))(adev, 3221225476U);
  val = (val & 128U) >> 7;
  if (val != 0U) {
    goto ldv_43853;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43855: ;
  if (adev->usec_timeout > i) {
    goto ldv_43854;
  } else {
  }
  ldv_43853: ;
  if (adev->usec_timeout == i) {
    drm_err("SMC boot sequence is not completed\n");
    return (-22);
  } else {
  }
  (*(adev->smc_wreg))(adev, 260096U, 0U);
  val = (*(adev->smc_rreg))(adev, 2147483648U);
  val = val | 1U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  result = tonga_smu_upload_firmware_image(adev);
  if (result != 0) {
    return (result);
  } else {
  }
  tonga_program_jump_on_start(adev);
  val = (*(adev->smc_rreg))(adev, 2147483652U);
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483652U, val);
  val = (*(adev->smc_rreg))(adev, 2147483648U);
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  i = 0;
  goto ldv_43858;
  ldv_43857:
  val = (*(adev->smc_rreg))(adev, 260096U);
  if ((int )val & 1) {
    goto ldv_43856;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43858: ;
  if (adev->usec_timeout > i) {
    goto ldv_43857;
  } else {
  }
  ldv_43856: ;
  if (adev->usec_timeout == i) {
    drm_err("Timeout for SMC firmware initialization\n");
    return (-22);
  } else {
  }
  return (0);
}
}
int tonga_smu_start(struct amdgpu_device *adev )
{
  int result ;
  u32 val ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = tonga_is_smc_ram_running(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    val = (*(adev->smc_rreg))(adev, 3758108836U);
    if ((val & 65536U) >> 16 == 0U) {
      result = tonga_smu_start_in_non_protection_mode(adev);
      if (result != 0) {
        return (result);
      } else {
      }
    } else {
      result = tonga_smu_start_in_protection_mode(adev);
      if (result != 0) {
        return (result);
      } else {
      }
    }
  } else {
  }
  tmp___1 = tonga_smu_request_load_fw(adev);
  return (tmp___1);
}
}
static struct amdgpu_smumgr_funcs const tonga_smumgr_funcs = {& tonga_smu_check_fw_load_finish, (int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ,
                                                                                    u32 ))0};
int tonga_smu_init(struct amdgpu_device *adev )
{
  struct tonga_smu_private_data *private ;
  u32 image_size ;
  u32 smu_internal_buffer_size ;
  struct amdgpu_bo **toc_buf ;
  struct amdgpu_bo **smu_buf ;
  uint64_t mc_addr ;
  void *toc_buf_ptr ;
  void *smu_buf_ptr ;
  int ret ;
  void *tmp ;
  {
  image_size = 4096U;
  smu_internal_buffer_size = 819200U;
  toc_buf = & adev->smu.toc_buf;
  smu_buf = & adev->smu.smu_buf;
  tmp = kzalloc(24UL, 208U);
  private = (struct tonga_smu_private_data *)tmp;
  if ((unsigned long )private == (unsigned long )((struct tonga_smu_private_data *)0)) {
    return (-12);
  } else {
  }
  if ((int )adev->firmware.smu_load) {
    amdgpu_ucode_init_bo(adev);
  } else {
  }
  adev->smu.priv = (void *)private;
  adev->smu.fw_flags = 0U;
  ret = amdgpu_bo_create(adev, (unsigned long )image_size, 4096, 1, 4U, 0ULL, (struct sg_table *)0,
                         toc_buf);
  if (ret != 0) {
    drm_err("Failed to allocate memory for TOC buffer\n");
    return (-12);
  } else {
  }
  ret = amdgpu_bo_create(adev, (unsigned long )smu_internal_buffer_size, 4096, 1,
                         4U, 0ULL, (struct sg_table *)0, smu_buf);
  if (ret != 0) {
    drm_err("Failed to allocate memory for SMU internal buffer\n");
    return (-12);
  } else {
  }
  ret = amdgpu_bo_reserve(adev->smu.toc_buf, 0);
  if (ret != 0) {
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to reserve the TOC buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_pin(adev->smu.toc_buf, 4U, & mc_addr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.toc_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to pin the TOC buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_kmap(*toc_buf, & toc_buf_ptr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.toc_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to map the TOC buffer\n");
    return (-22);
  } else {
  }
  amdgpu_bo_unreserve(adev->smu.toc_buf);
  private->header_addr_low = (unsigned int )mc_addr;
  private->header_addr_high = (unsigned int )(mc_addr >> 32ULL);
  private->header = (uint8_t *)toc_buf_ptr;
  ret = amdgpu_bo_reserve(adev->smu.smu_buf, 0);
  if (ret != 0) {
    amdgpu_bo_unref(& adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to reserve the SMU internal buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_pin(adev->smu.smu_buf, 4U, & mc_addr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to pin the SMU internal buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_kmap(*smu_buf, & smu_buf_ptr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.smu_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to map the SMU internal buffer\n");
    return (-22);
  } else {
  }
  amdgpu_bo_unreserve(adev->smu.smu_buf);
  private->smu_buffer_addr_low = (unsigned int )mc_addr;
  private->smu_buffer_addr_high = (unsigned int )(mc_addr >> 32ULL);
  adev->smu.smumgr_funcs = & tonga_smumgr_funcs;
  return (0);
}
}
int tonga_smu_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_bo_unref(& adev->smu.toc_buf);
  amdgpu_bo_unref(& adev->smu.smu_buf);
  kfree((void const *)adev->smu.priv);
  adev->smu.priv = (void *)0;
  if ((unsigned long )adev->firmware.fw_buf != (unsigned long )((struct amdgpu_bo *)0)) {
    amdgpu_ucode_fini_bo(adev);
  } else {
  }
  return (0);
}
}
void ldv_main_exported_65(void)
{
  struct amdgpu_device *ldvarg160 ;
  void *tmp ;
  u32 ldvarg159 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  ldvarg160 = (struct amdgpu_device *)tmp;
  ldv_memset((void *)(& ldvarg159), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_65 == 1) {
    tonga_smu_check_fw_load_finish(ldvarg160, ldvarg159);
    ldv_state_variable_65 = 1;
  } else {
  }
  goto ldv_43886;
  default:
  ldv_stop();
  }
  ldv_43886: ;
  return;
}
}
bool ldv_queue_work_on_837(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_838(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_839(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_840(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_841(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_851(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_853(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_852(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_855(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_854(struct workqueue_struct *ldv_func_arg1 ) ;
static void tonga_dpm_set_funcs(struct amdgpu_device *adev ) ;
static int tonga_dpm_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  tonga_dpm_set_funcs(adev);
  return (0);
}
}
static int tonga_dpm_init_microcode(struct amdgpu_device *adev )
{
  char fw_name[30U] ;
  unsigned int tmp ;
  int err ;
  {
  fw_name[0] = 'a';
  fw_name[1] = 'm';
  fw_name[2] = 'd';
  fw_name[3] = 'g';
  fw_name[4] = 'p';
  fw_name[5] = 'u';
  fw_name[6] = '/';
  fw_name[7] = 't';
  fw_name[8] = 'o';
  fw_name[9] = 'n';
  fw_name[10] = 'g';
  fw_name[11] = 'a';
  fw_name[12] = '_';
  fw_name[13] = 's';
  fw_name[14] = 'm';
  fw_name[15] = 'c';
  fw_name[16] = '.';
  fw_name[17] = 'b';
  fw_name[18] = 'i';
  fw_name[19] = 'n';
  fw_name[20] = '\000';
  tmp = 21U;
  while (1) {
    if (tmp >= 30U) {
      break;
    } else {
    }
    fw_name[tmp] = (char)0;
    tmp = tmp + 1U;
  }
  err = request_firmware(& adev->pm.fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->pm.fw);
  out: ;
  if (err != 0) {
    drm_err("Failed to load firmware \"%s\"", (char *)(& fw_name));
    release_firmware(adev->pm.fw);
    adev->pm.fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static int tonga_dpm_sw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = tonga_dpm_init_microcode(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (0);
}
}
static int tonga_dpm_sw_fini(void *handle )
{
  {
  return (0);
}
}
static int tonga_dpm_hw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = tonga_smu_init(adev);
  if (ret != 0) {
    drm_err("SMU initialization failed\n");
    goto fail;
  } else {
  }
  ret = tonga_smu_start(adev);
  if (ret != 0) {
    drm_err("SMU start failed\n");
    goto fail;
  } else {
  }
  mutex_unlock(& adev->pm.mutex);
  return (0);
  fail:
  adev->firmware.smu_load = 0;
  mutex_unlock(& adev->pm.mutex);
  return (-22);
}
}
static int tonga_dpm_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  tonga_smu_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static int tonga_dpm_suspend(void *handle )
{
  {
  return (0);
}
}
static int tonga_dpm_resume(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = tonga_smu_start(adev);
  if (ret != 0) {
    drm_err("SMU start failed\n");
    goto fail;
  } else {
  }
  fail:
  mutex_unlock(& adev->pm.mutex);
  return (ret);
}
}
static int tonga_dpm_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int tonga_dpm_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const tonga_dpm_ip_funcs =
     {& tonga_dpm_early_init, (int (*)(void * ))0, & tonga_dpm_sw_init, & tonga_dpm_sw_fini,
    & tonga_dpm_hw_init, & tonga_dpm_hw_fini, & tonga_dpm_suspend, & tonga_dpm_resume,
    (bool (*)(void * ))0, (int (*)(void * ))0, (int (*)(void * ))0, (void (*)(void * ))0,
    & tonga_dpm_set_clockgating_state, & tonga_dpm_set_powergating_state};
static struct amdgpu_dpm_funcs const tonga_dpm_funcs =
     {(int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ))0,
    (void (*)(struct amdgpu_device * ))0, (void (*)(struct amdgpu_device * ))0, (u32 (*)(struct amdgpu_device * ,
                                                                                         bool ))0,
    (u32 (*)(struct amdgpu_device * , bool ))0, (void (*)(struct amdgpu_device * ,
                                                           struct amdgpu_ps * ))0,
    (void (*)(struct amdgpu_device * , struct seq_file * ))0, (int (*)(struct amdgpu_device * ,
                                                                       enum amdgpu_dpm_forced_level ))0,
    (bool (*)(struct amdgpu_device * ))0, (void (*)(struct amdgpu_device * , bool ))0,
    0, 0, 0, 0, 0, 0};
static void tonga_dpm_set_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.funcs == (unsigned long )((struct amdgpu_dpm_funcs const *)0)) {
    adev->pm.funcs = & tonga_dpm_funcs;
  } else {
  }
  return;
}
}
int ldv_retval_64 ;
int ldv_retval_63 ;
extern int ldv_release_64(void) ;
extern int ldv_probe_64(void) ;
void ldv_main_exported_64(void)
{
  void *ldvarg774 ;
  void *tmp ;
  enum amd_clockgating_state ldvarg767 ;
  void *ldvarg772 ;
  void *tmp___0 ;
  void *ldvarg775 ;
  void *tmp___1 ;
  void *ldvarg769 ;
  void *tmp___2 ;
  void *ldvarg771 ;
  void *tmp___3 ;
  void *ldvarg773 ;
  void *tmp___4 ;
  void *ldvarg765 ;
  void *tmp___5 ;
  void *ldvarg766 ;
  void *tmp___6 ;
  void *ldvarg768 ;
  void *tmp___7 ;
  enum amd_powergating_state ldvarg770 ;
  int tmp___8 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg774 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg772 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg775 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg769 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg771 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg773 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg765 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg766 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg768 = tmp___7;
  ldv_memset((void *)(& ldvarg767), 0, 4UL);
  ldv_memset((void *)(& ldvarg770), 0, 4UL);
  tmp___8 = __VERIFIER_nondet_int();
  switch (tmp___8) {
  case 0: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_hw_fini(ldvarg775);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_hw_fini(ldvarg775);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_hw_fini(ldvarg775);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 1: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_early_init(ldvarg774);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_early_init(ldvarg774);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_early_init(ldvarg774);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 2: ;
  if (ldv_state_variable_64 == 2) {
    ldv_retval_64 = tonga_dpm_suspend(ldvarg773);
    if (ldv_retval_64 == 0) {
      ldv_state_variable_64 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43759;
  case 3: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_sw_init(ldvarg772);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_sw_init(ldvarg772);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_sw_init(ldvarg772);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 4: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_set_powergating_state(ldvarg771, ldvarg770);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_set_powergating_state(ldvarg771, ldvarg770);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_set_powergating_state(ldvarg771, ldvarg770);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 5: ;
  if (ldv_state_variable_64 == 3) {
    ldv_retval_63 = tonga_dpm_resume(ldvarg769);
    if (ldv_retval_63 == 0) {
      ldv_state_variable_64 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43759;
  case 6: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_set_clockgating_state(ldvarg768, ldvarg767);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_set_clockgating_state(ldvarg768, ldvarg767);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_set_clockgating_state(ldvarg768, ldvarg767);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 7: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_hw_init(ldvarg766);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_hw_init(ldvarg766);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_hw_init(ldvarg766);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 8: ;
  if (ldv_state_variable_64 == 2) {
    tonga_dpm_sw_fini(ldvarg765);
    ldv_state_variable_64 = 2;
  } else {
  }
  if (ldv_state_variable_64 == 1) {
    tonga_dpm_sw_fini(ldvarg765);
    ldv_state_variable_64 = 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    tonga_dpm_sw_fini(ldvarg765);
    ldv_state_variable_64 = 3;
  } else {
  }
  goto ldv_43759;
  case 9: ;
  if (ldv_state_variable_64 == 2) {
    ldv_release_64();
    ldv_state_variable_64 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_64 == 3) {
    ldv_release_64();
    ldv_state_variable_64 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43759;
  case 10: ;
  if (ldv_state_variable_64 == 1) {
    ldv_probe_64();
    ldv_state_variable_64 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43759;
  default:
  ldv_stop();
  }
  ldv_43759: ;
  return;
}
}
bool ldv_queue_work_on_851(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_852(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_853(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_854(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_855(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_865(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_867(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_866(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_869(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_868(struct workqueue_struct *ldv_func_arg1 ) ;
int iceland_smu_init(struct amdgpu_device *adev ) ;
int iceland_smu_fini(struct amdgpu_device *adev ) ;
int iceland_smu_start(struct amdgpu_device *adev ) ;
static int iceland_set_smc_sram_address(struct amdgpu_device *adev , u32 smc_address ,
                                        u32 limit )
{
  u32 val ;
  {
  if ((smc_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_address + 3U > limit) {
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 128U, smc_address, 0);
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val & 4294967294U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  return (0);
}
}
static int iceland_copy_bytes_to_smc(struct amdgpu_device *adev , u32 smc_start_address ,
                                     uint8_t const *src , u32 byte_count , u32 limit )
{
  u32 addr ;
  u32 data ;
  u32 orig_data ;
  int result ;
  u32 extra_shift ;
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  uint8_t const *tmp___0 ;
  {
  result = 0;
  if ((smc_start_address & 3U) != 0U) {
    return (-22);
  } else {
  }
  if (smc_start_address + byte_count > limit) {
    return (-22);
  } else {
  }
  addr = smc_start_address;
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  goto ldv_43722;
  ldv_43721:
  data = (u32 )(((((int )*src << 24) + ((int )*(src + 1UL) << 16)) + ((int )*(src + 2UL) << 8)) + (int )*(src + 3UL));
  result = iceland_set_smc_sram_address(adev, addr, limit);
  if (result != 0) {
    goto out;
  } else {
  }
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  byte_count = byte_count - 4U;
  addr = addr + 4U;
  ldv_43722: ;
  if (byte_count > 3U) {
    goto ldv_43721;
  } else {
  }
  if (byte_count != 0U) {
    data = 0U;
    result = iceland_set_smc_sram_address(adev, addr, limit);
    if (result != 0) {
      goto out;
    } else {
    }
    orig_data = amdgpu_mm_rreg(adev, 129U, 0);
    extra_shift = (4U - byte_count) * 8U;
    goto ldv_43725;
    ldv_43724:
    tmp___0 = src;
    src = src + 1;
    data = (data << 8) + (u32 )*tmp___0;
    byte_count = byte_count - 1U;
    ldv_43725: ;
    if (byte_count != 0U) {
      goto ldv_43724;
    } else {
    }
    data = data << (int )extra_shift;
    data = (~ ((u32 )(0xffffffffffffffffUL << (int )extra_shift)) & orig_data) | data;
    result = iceland_set_smc_sram_address(adev, addr, limit);
    if (result != 0) {
      goto out;
    } else {
    }
    amdgpu_mm_wreg(adev, 129U, data, 0);
  } else {
  }
  out:
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (result);
}
}
void iceland_start_smc(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483648U);
  val = tmp;
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  return;
}
}
void iceland_reset_smc(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483648U);
  val = tmp;
  val = val | 1U;
  (*(adev->smc_wreg))(adev, 2147483648U, val);
  return;
}
}
static int iceland_program_jump_on_start(struct amdgpu_device *adev )
{
  unsigned char data[4U] ;
  {
  data[0] = 224U;
  data[1] = 0U;
  data[2] = 128U;
  data[3] = 64U;
  iceland_copy_bytes_to_smc(adev, 0U, (uint8_t const *)(& data), 4U, 5U);
  return (0);
}
}
void iceland_stop_smc_clock(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  val = tmp;
  val = val | 1U;
  (*(adev->smc_wreg))(adev, 2147483652U, val);
  return;
}
}
void iceland_start_smc_clock(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  val = tmp;
  val = val & 4294967294U;
  (*(adev->smc_wreg))(adev, 2147483652U, val);
  return;
}
}
static bool iceland_is_smc_ram_running(struct amdgpu_device *adev )
{
  u32 val ;
  u32 tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  {
  tmp = (*(adev->smc_rreg))(adev, 2147483652U);
  val = tmp;
  val = val & 1U;
  if (val == 0U) {
    tmp___0 = (*(adev->smc_rreg))(adev, 2147484528U);
    if (tmp___0 > 131327U) {
      tmp___1 = 1;
    } else {
      tmp___1 = 0;
    }
  } else {
    tmp___1 = 0;
  }
  return ((bool )tmp___1);
}
}
static int wait_smu_response___0(struct amdgpu_device *adev )
{
  int i ;
  u32 val ;
  {
  i = 0;
  goto ldv_43758;
  ldv_43757:
  val = amdgpu_mm_rreg(adev, 149U, 0);
  if ((val & 65535U) != 0U) {
    goto ldv_43756;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43758: ;
  if (adev->usec_timeout > i) {
    goto ldv_43757;
  } else {
  }
  ldv_43756: ;
  if (adev->usec_timeout == i) {
    return (-22);
  } else {
  }
  return (0);
}
}
static int iceland_send_msg_to_smc(struct amdgpu_device *adev , PPSMC_Msg msg )
{
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = iceland_is_smc_ram_running(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  tmp___1 = wait_smu_response___0(adev);
  if (tmp___1 != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 148U, (u32 )msg, 0);
  tmp___2 = wait_smu_response___0(adev);
  if (tmp___2 != 0) {
    drm_err("Failed to send message\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int iceland_send_msg_to_smc_without_waiting(struct amdgpu_device *adev , PPSMC_Msg msg )
{
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = iceland_is_smc_ram_running(adev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (-22);
  } else {
  }
  tmp___1 = wait_smu_response___0(adev);
  if (tmp___1 != 0) {
    drm_err("Failed to send previous message\n");
    return (-22);
  } else {
  }
  amdgpu_mm_wreg(adev, 148U, (u32 )msg, 0);
  return (0);
}
}
static int iceland_send_msg_to_smc_with_parameter(struct amdgpu_device *adev , PPSMC_Msg msg ,
                                                  u32 parameter )
{
  int tmp ;
  {
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp = iceland_send_msg_to_smc(adev, (int )msg);
  return (tmp);
}
}
static int iceland_send_msg_to_smc_with_parameter_without_waiting(struct amdgpu_device *adev ,
                                                                  PPSMC_Msg msg ,
                                                                  u32 parameter )
{
  int tmp ;
  {
  amdgpu_mm_wreg(adev, 164U, parameter, 0);
  tmp = iceland_send_msg_to_smc_without_waiting(adev, (int )msg);
  return (tmp);
}
}
static int iceland_smu_upload_firmware_image(struct amdgpu_device *adev )
{
  struct smc_firmware_header_v1_0 const *hdr ;
  u32 ucode_size ;
  u32 ucode_start_address ;
  uint8_t const *src ;
  u32 val ;
  u32 byte_count ;
  u32 data ;
  unsigned long flags ;
  int i ;
  raw_spinlock_t *tmp ;
  {
  if ((unsigned long )adev->pm.fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct smc_firmware_header_v1_0 const *)(adev->pm.fw)->data;
  amdgpu_ucode_print_smc_hdr(& hdr->header);
  adev->pm.fw_version = hdr->header.ucode_version;
  ucode_size = hdr->header.ucode_size_bytes;
  ucode_start_address = hdr->ucode_start_addr;
  src = (uint8_t const *)(adev->pm.fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  if ((ucode_size & 3U) != 0U) {
    drm_err("SMC ucode is not 4 bytes aligned\n");
    return (-22);
  } else {
  }
  if (ucode_size > 131072U) {
    drm_err("SMC address is beyond the SMC RAM area\n");
    return (-22);
  } else {
  }
  i = 0;
  goto ldv_43791;
  ldv_43790:
  val = (*(adev->smc_rreg))(adev, 3221225476U);
  if ((val & 128U) >> 7 == 0U) {
    goto ldv_43789;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43791: ;
  if (adev->usec_timeout > i) {
    goto ldv_43790;
  } else {
  }
  ldv_43789:
  val = (*(adev->smc_rreg))(adev, 2147483664U);
  (*(adev->smc_wreg))(adev, 2147483664U, val | 1U);
  iceland_stop_smc_clock(adev);
  iceland_reset_smc(adev);
  tmp = spinlock_check(& adev->smc_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, 128U, ucode_start_address, 0);
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val | 1U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  byte_count = ucode_size;
  goto ldv_43796;
  ldv_43795:
  data = (u32 )(((((int )*src << 24) + ((int )*(src + 1UL) << 16)) + ((int )*(src + 2UL) << 8)) + (int )*(src + 3UL));
  amdgpu_mm_wreg(adev, 129U, data, 0);
  src = src + 4UL;
  byte_count = byte_count - 4U;
  ldv_43796: ;
  if (byte_count > 3U) {
    goto ldv_43795;
  } else {
  }
  val = amdgpu_mm_rreg(adev, 146U, 0);
  val = val & 4294967294U;
  amdgpu_mm_wreg(adev, 146U, val, 0);
  spin_unlock_irqrestore(& adev->smc_idx_lock, flags);
  return (0);
}
}
static int iceland_smu_start_smc(struct amdgpu_device *adev )
{
  int i ;
  u32 val ;
  {
  iceland_program_jump_on_start(adev);
  iceland_start_smc_clock(adev);
  iceland_start_smc(adev);
  i = 0;
  goto ldv_43805;
  ldv_43804:
  val = (*(adev->smc_rreg))(adev, 208896U);
  if ((int )val & 1) {
    goto ldv_43803;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43805: ;
  if (adev->usec_timeout > i) {
    goto ldv_43804;
  } else {
  }
  ldv_43803: ;
  return (0);
}
}
static enum AMDGPU_UCODE_ID iceland_convert_fw_type(u32 fw_type )
{
  {
  switch (fw_type) {
  case 1U: ;
  return (0);
  case 2U: ;
  return (1);
  case 3U: ;
  return (2);
  case 4U: ;
  return (3);
  case 5U: ;
  return (4);
  case 6U: ;
  case 7U: ;
  return (5);
  case 8U: ;
  return (6);
  case 10U: ;
  return (7);
  default:
  drm_err("ucode type is out of range!\n");
  return (8);
  }
}
}
static u32 iceland_smu_get_mask_for_fw_type(u32 fw_type )
{
  {
  switch (fw_type) {
  case 0U: ;
  return (2U);
  case 1U: ;
  return (4U);
  case 2U: ;
  return (8U);
  case 3U: ;
  return (16U);
  case 4U: ;
  return (32U);
  case 5U: ;
  return (448U);
  case 6U: ;
  return (64U);
  case 7U: ;
  return (1024U);
  default:
  drm_err("ucode type is out of range!\n");
  return (0U);
  }
}
}
static int iceland_smu_populate_single_firmware_entry(struct amdgpu_device *adev ,
                                                      u32 fw_type , struct SMU_Entry *entry )
{
  enum AMDGPU_UCODE_ID id ;
  enum AMDGPU_UCODE_ID tmp ;
  struct amdgpu_firmware_info *ucode ;
  struct gfx_firmware_header_v1_0 const *header ;
  uint64_t gpu_addr ;
  u32 data_size ;
  {
  tmp = iceland_convert_fw_type(fw_type);
  id = tmp;
  ucode = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )id;
  header = (struct gfx_firmware_header_v1_0 const *)0;
  if ((unsigned long )ucode->fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  gpu_addr = ucode->mc_addr;
  header = (struct gfx_firmware_header_v1_0 const *)(ucode->fw)->data;
  data_size = header->header.ucode_size_bytes;
  entry->version = (unsigned short )header->header.ucode_version;
  entry->id = (unsigned short )fw_type;
  entry->image_addr_high = (unsigned int )(gpu_addr >> 32ULL);
  entry->image_addr_low = (unsigned int )gpu_addr;
  entry->meta_data_addr_high = 0U;
  entry->meta_data_addr_low = 0U;
  entry->data_size_byte = data_size;
  entry->num_register_entries = 0U;
  entry->flags = 0U;
  return (0);
}
}
static int iceland_smu_request_load_fw(struct amdgpu_device *adev )
{
  struct iceland_smu_private_data *private ;
  struct SMU_DRAMData_TOC *toc ;
  u32 fw_to_load ;
  u32 tmp ;
  int tmp___0 ;
  u32 tmp___1 ;
  int tmp___2 ;
  u32 tmp___3 ;
  int tmp___4 ;
  u32 tmp___5 ;
  int tmp___6 ;
  u32 tmp___7 ;
  int tmp___8 ;
  u32 tmp___9 ;
  int tmp___10 ;
  u32 tmp___11 ;
  int tmp___12 ;
  u32 tmp___13 ;
  int tmp___14 ;
  u32 tmp___15 ;
  int tmp___16 ;
  int tmp___17 ;
  {
  private = (struct iceland_smu_private_data *)adev->smu.priv;
  toc = (struct SMU_DRAMData_TOC *)private->header;
  toc->num_entries = 0U;
  toc->structure_version = 1U;
  if (! adev->firmware.smu_load) {
    return (0);
  } else {
  }
  tmp = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___0 = iceland_smu_populate_single_firmware_entry(adev, 10U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp);
  if (tmp___0 != 0) {
    drm_err("Failed to get firmware entry for RLC\n");
    return (-22);
  } else {
  }
  tmp___1 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___2 = iceland_smu_populate_single_firmware_entry(adev, 3U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___1);
  if (tmp___2 != 0) {
    drm_err("Failed to get firmware entry for CE\n");
    return (-22);
  } else {
  }
  tmp___3 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___4 = iceland_smu_populate_single_firmware_entry(adev, 4U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___3);
  if (tmp___4 != 0) {
    drm_err("Failed to get firmware entry for PFP\n");
    return (-22);
  } else {
  }
  tmp___5 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___6 = iceland_smu_populate_single_firmware_entry(adev, 5U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___5);
  if (tmp___6 != 0) {
    drm_err("Failed to get firmware entry for ME\n");
    return (-22);
  } else {
  }
  tmp___7 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___8 = iceland_smu_populate_single_firmware_entry(adev, 6U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___7);
  if (tmp___8 != 0) {
    drm_err("Failed to get firmware entry for MEC\n");
    return (-22);
  } else {
  }
  tmp___9 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___10 = iceland_smu_populate_single_firmware_entry(adev, 7U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___9);
  if (tmp___10 != 0) {
    drm_err("Failed to get firmware entry for MEC_JT1\n");
    return (-22);
  } else {
  }
  tmp___11 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___12 = iceland_smu_populate_single_firmware_entry(adev, 8U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___11);
  if (tmp___12 != 0) {
    drm_err("Failed to get firmware entry for MEC_JT2\n");
    return (-22);
  } else {
  }
  tmp___13 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___14 = iceland_smu_populate_single_firmware_entry(adev, 1U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___13);
  if (tmp___14 != 0) {
    drm_err("Failed to get firmware entry for SDMA0\n");
    return (-22);
  } else {
  }
  tmp___15 = toc->num_entries;
  toc->num_entries = toc->num_entries + 1U;
  tmp___16 = iceland_smu_populate_single_firmware_entry(adev, 2U, (struct SMU_Entry *)(& toc->entry) + (unsigned long )tmp___15);
  if (tmp___16 != 0) {
    drm_err("Failed to get firmware entry for SDMA1\n");
    return (-22);
  } else {
  }
  iceland_send_msg_to_smc_with_parameter(adev, 592, private->header_addr_high);
  iceland_send_msg_to_smc_with_parameter(adev, 593, private->header_addr_low);
  fw_to_load = 1534U;
  tmp___17 = iceland_send_msg_to_smc_with_parameter_without_waiting(adev, 596, fw_to_load);
  if (tmp___17 != 0) {
    drm_err("Fail to request SMU load ucode\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int iceland_smu_check_fw_load_finish(struct amdgpu_device *adev , u32 fw_type )
{
  u32 fw_mask ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  {
  tmp = iceland_smu_get_mask_for_fw_type(fw_type);
  fw_mask = tmp;
  i = 0;
  goto ldv_43855;
  ldv_43854:
  tmp___0 = (*(adev->smc_rreg))(adev, 211248U);
  if ((tmp___0 & fw_mask) == fw_mask) {
    goto ldv_43853;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_43855: ;
  if (adev->usec_timeout > i) {
    goto ldv_43854;
  } else {
  }
  ldv_43853: ;
  if (adev->usec_timeout == i) {
    drm_err("check firmware loading failed\n");
    return (-22);
  } else {
  }
  return (0);
}
}
int iceland_smu_start(struct amdgpu_device *adev )
{
  int result ;
  int tmp ;
  {
  result = iceland_smu_upload_firmware_image(adev);
  if (result != 0) {
    return (result);
  } else {
  }
  result = iceland_smu_start_smc(adev);
  if (result != 0) {
    return (result);
  } else {
  }
  tmp = iceland_smu_request_load_fw(adev);
  return (tmp);
}
}
static struct amdgpu_smumgr_funcs const iceland_smumgr_funcs = {& iceland_smu_check_fw_load_finish, (int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ,
                                                                                      u32 ))0};
int iceland_smu_init(struct amdgpu_device *adev )
{
  struct iceland_smu_private_data *private ;
  u32 image_size ;
  struct amdgpu_bo **toc_buf ;
  uint64_t mc_addr ;
  void *toc_buf_ptr ;
  int ret ;
  void *tmp ;
  {
  image_size = 4096U;
  toc_buf = & adev->smu.toc_buf;
  tmp = kzalloc(24UL, 208U);
  private = (struct iceland_smu_private_data *)tmp;
  if ((unsigned long )private == (unsigned long )((struct iceland_smu_private_data *)0)) {
    return (-12);
  } else {
  }
  if ((int )adev->firmware.smu_load) {
    amdgpu_ucode_init_bo(adev);
  } else {
  }
  adev->smu.priv = (void *)private;
  adev->smu.fw_flags = 0U;
  ret = amdgpu_bo_create(adev, (unsigned long )image_size, 4096, 1, 4U, 0ULL, (struct sg_table *)0,
                         toc_buf);
  if (ret != 0) {
    drm_err("Failed to allocate memory for TOC buffer\n");
    return (-12);
  } else {
  }
  ret = amdgpu_bo_reserve(adev->smu.toc_buf, 0);
  if (ret != 0) {
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to reserve the TOC buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_pin(adev->smu.toc_buf, 4U, & mc_addr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.toc_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to pin the TOC buffer\n");
    return (-22);
  } else {
  }
  ret = amdgpu_bo_kmap(*toc_buf, & toc_buf_ptr);
  if (ret != 0) {
    amdgpu_bo_unreserve(adev->smu.toc_buf);
    amdgpu_bo_unref(& adev->smu.toc_buf);
    drm_err("Failed to map the TOC buffer\n");
    return (-22);
  } else {
  }
  amdgpu_bo_unreserve(adev->smu.toc_buf);
  private->header_addr_low = (unsigned int )mc_addr;
  private->header_addr_high = (unsigned int )(mc_addr >> 32ULL);
  private->header = (uint8_t *)toc_buf_ptr;
  adev->smu.smumgr_funcs = & iceland_smumgr_funcs;
  return (0);
}
}
int iceland_smu_fini(struct amdgpu_device *adev )
{
  {
  amdgpu_bo_unref(& adev->smu.toc_buf);
  kfree((void const *)adev->smu.priv);
  adev->smu.priv = (void *)0;
  if ((unsigned long )adev->firmware.fw_buf != (unsigned long )((struct amdgpu_bo *)0)) {
    amdgpu_ucode_fini_bo(adev);
  } else {
  }
  return (0);
}
}
void ldv_main_exported_63(void)
{
  u32 ldvarg535 ;
  struct amdgpu_device *ldvarg536 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  ldvarg536 = (struct amdgpu_device *)tmp;
  ldv_memset((void *)(& ldvarg535), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_63 == 1) {
    iceland_smu_check_fw_load_finish(ldvarg536, ldvarg535);
    ldv_state_variable_63 = 1;
  } else {
  }
  goto ldv_43879;
  default:
  ldv_stop();
  }
  ldv_43879: ;
  return;
}
}
bool ldv_queue_work_on_865(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_866(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_867(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_868(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_869(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_879(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_881(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_880(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_883(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_882(struct workqueue_struct *ldv_func_arg1 ) ;
static void iceland_dpm_set_funcs(struct amdgpu_device *adev ) ;
static int iceland_dpm_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  iceland_dpm_set_funcs(adev);
  return (0);
}
}
static int iceland_dpm_init_microcode(struct amdgpu_device *adev )
{
  char fw_name[30U] ;
  unsigned int tmp ;
  int err ;
  {
  fw_name[0] = 'a';
  fw_name[1] = 'm';
  fw_name[2] = 'd';
  fw_name[3] = 'g';
  fw_name[4] = 'p';
  fw_name[5] = 'u';
  fw_name[6] = '/';
  fw_name[7] = 't';
  fw_name[8] = 'o';
  fw_name[9] = 'p';
  fw_name[10] = 'a';
  fw_name[11] = 'z';
  fw_name[12] = '_';
  fw_name[13] = 's';
  fw_name[14] = 'm';
  fw_name[15] = 'c';
  fw_name[16] = '.';
  fw_name[17] = 'b';
  fw_name[18] = 'i';
  fw_name[19] = 'n';
  fw_name[20] = '\000';
  tmp = 21U;
  while (1) {
    if (tmp >= 30U) {
      break;
    } else {
    }
    fw_name[tmp] = (char)0;
    tmp = tmp + 1U;
  }
  err = request_firmware(& adev->pm.fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->pm.fw);
  out: ;
  if (err != 0) {
    drm_err("Failed to load firmware \"%s\"", (char *)(& fw_name));
    release_firmware(adev->pm.fw);
    adev->pm.fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static int iceland_dpm_sw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  ret = iceland_dpm_init_microcode(adev);
  if (ret != 0) {
    return (ret);
  } else {
  }
  return (0);
}
}
static int iceland_dpm_sw_fini(void *handle )
{
  {
  return (0);
}
}
static int iceland_dpm_hw_init(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = iceland_smu_init(adev);
  if (ret != 0) {
    drm_err("SMU initialization failed\n");
    goto fail;
  } else {
  }
  ret = iceland_smu_start(adev);
  if (ret != 0) {
    drm_err("SMU start failed\n");
    goto fail;
  } else {
  }
  mutex_unlock(& adev->pm.mutex);
  return (0);
  fail:
  adev->firmware.smu_load = 0;
  mutex_unlock(& adev->pm.mutex);
  return (-22);
}
}
static int iceland_dpm_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  iceland_smu_fini(adev);
  mutex_unlock(& adev->pm.mutex);
  return (0);
}
}
static int iceland_dpm_suspend(void *handle )
{
  {
  return (0);
}
}
static int iceland_dpm_resume(void *handle )
{
  int ret ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  mutex_lock_nested(& adev->pm.mutex, 0U);
  ret = iceland_smu_start(adev);
  if (ret != 0) {
    drm_err("SMU start failed\n");
    goto fail;
  } else {
  }
  fail:
  mutex_unlock(& adev->pm.mutex);
  return (ret);
}
}
static int iceland_dpm_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int iceland_dpm_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const iceland_dpm_ip_funcs =
     {& iceland_dpm_early_init, (int (*)(void * ))0, & iceland_dpm_sw_init, & iceland_dpm_sw_fini,
    & iceland_dpm_hw_init, & iceland_dpm_hw_fini, & iceland_dpm_suspend, & iceland_dpm_resume,
    (bool (*)(void * ))0, (int (*)(void * ))0, (int (*)(void * ))0, (void (*)(void * ))0,
    & iceland_dpm_set_clockgating_state, & iceland_dpm_set_powergating_state};
static struct amdgpu_dpm_funcs const iceland_dpm_funcs =
     {(int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ))0, (int (*)(struct amdgpu_device * ))0,
    (void (*)(struct amdgpu_device * ))0, (void (*)(struct amdgpu_device * ))0, (u32 (*)(struct amdgpu_device * ,
                                                                                         bool ))0,
    (u32 (*)(struct amdgpu_device * , bool ))0, (void (*)(struct amdgpu_device * ,
                                                           struct amdgpu_ps * ))0,
    (void (*)(struct amdgpu_device * , struct seq_file * ))0, (int (*)(struct amdgpu_device * ,
                                                                       enum amdgpu_dpm_forced_level ))0,
    (bool (*)(struct amdgpu_device * ))0, (void (*)(struct amdgpu_device * , bool ))0,
    0, 0, 0, 0, 0, 0};
static void iceland_dpm_set_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->pm.funcs == (unsigned long )((struct amdgpu_dpm_funcs const *)0)) {
    adev->pm.funcs = & iceland_dpm_funcs;
  } else {
  }
  return;
}
}
int ldv_retval_14 ;
int ldv_retval_13 ;
extern int ldv_release_62(void) ;
extern int ldv_probe_62(void) ;
void ldv_main_exported_62(void)
{
  void *ldvarg274 ;
  void *tmp ;
  enum amd_powergating_state ldvarg273 ;
  void *ldvarg277 ;
  void *tmp___0 ;
  void *ldvarg275 ;
  void *tmp___1 ;
  void *ldvarg271 ;
  void *tmp___2 ;
  void *ldvarg278 ;
  void *tmp___3 ;
  void *ldvarg276 ;
  void *tmp___4 ;
  enum amd_clockgating_state ldvarg270 ;
  void *ldvarg268 ;
  void *tmp___5 ;
  void *ldvarg269 ;
  void *tmp___6 ;
  void *ldvarg272 ;
  void *tmp___7 ;
  int tmp___8 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg274 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg277 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg275 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg271 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg278 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg276 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg268 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg269 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg272 = tmp___7;
  ldv_memset((void *)(& ldvarg273), 0, 4UL);
  ldv_memset((void *)(& ldvarg270), 0, 4UL);
  tmp___8 = __VERIFIER_nondet_int();
  switch (tmp___8) {
  case 0: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_hw_fini(ldvarg278);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_hw_fini(ldvarg278);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_hw_fini(ldvarg278);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 1: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_early_init(ldvarg277);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_early_init(ldvarg277);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_early_init(ldvarg277);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 2: ;
  if (ldv_state_variable_62 == 2) {
    ldv_retval_14 = iceland_dpm_suspend(ldvarg276);
    if (ldv_retval_14 == 0) {
      ldv_state_variable_62 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43758;
  case 3: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_sw_init(ldvarg275);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_sw_init(ldvarg275);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_sw_init(ldvarg275);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 4: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_set_powergating_state(ldvarg274, ldvarg273);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_set_powergating_state(ldvarg274, ldvarg273);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_set_powergating_state(ldvarg274, ldvarg273);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 5: ;
  if (ldv_state_variable_62 == 3) {
    ldv_retval_13 = iceland_dpm_resume(ldvarg272);
    if (ldv_retval_13 == 0) {
      ldv_state_variable_62 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43758;
  case 6: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_set_clockgating_state(ldvarg271, ldvarg270);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_set_clockgating_state(ldvarg271, ldvarg270);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_set_clockgating_state(ldvarg271, ldvarg270);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 7: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_hw_init(ldvarg269);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_hw_init(ldvarg269);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_hw_init(ldvarg269);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 8: ;
  if (ldv_state_variable_62 == 1) {
    iceland_dpm_sw_fini(ldvarg268);
    ldv_state_variable_62 = 1;
  } else {
  }
  if (ldv_state_variable_62 == 3) {
    iceland_dpm_sw_fini(ldvarg268);
    ldv_state_variable_62 = 3;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    iceland_dpm_sw_fini(ldvarg268);
    ldv_state_variable_62 = 2;
  } else {
  }
  goto ldv_43758;
  case 9: ;
  if (ldv_state_variable_62 == 3) {
    ldv_release_62();
    ldv_state_variable_62 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_62 == 2) {
    ldv_release_62();
    ldv_state_variable_62 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43758;
  case 10: ;
  if (ldv_state_variable_62 == 1) {
    ldv_probe_62();
    ldv_state_variable_62 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43758;
  default:
  ldv_stop();
  }
  ldv_43758: ;
  return;
}
}
bool ldv_queue_work_on_879(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_880(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_881(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_882(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_883(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static int __atomic_add_unless___5(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___5(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___5(v, a, u);
  return (tmp != u);
}
}
void ldv_destroy_workqueue_898(struct workqueue_struct *ldv_func_arg1 ) ;
bool ldv_queue_work_on_893(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_895(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_894(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_897(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_896(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___5(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_893(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___4(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___5(system_wq, work);
  return (tmp);
}
}
__inline static int kref_put_mutex___5(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___5(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static void drm_gem_object_unreference_unlocked___5(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___5(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
static void dce_v10_0_set_display_funcs(struct amdgpu_device *adev ) ;
static void dce_v10_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const crtc_offsets___0[7U] = { 0U, 512U, 1024U, 9728U,
        10240U, 10752U, 11264U};
static u32 const hpd_offsets[6U] = { 0U, 8U, 16U, 24U,
        32U, 40U};
static u32 const dig_offsets___0[7U] = { 0U, 256U, 512U, 768U,
        1024U, 1280U, 2560U};
static struct __anonstruct_interrupt_status_offsets_324___0 const interrupt_status_offsets___0[6U] = { {6231U,
      8U, 4U, 131072U},
        {6232U, 8U, 4U, 131072U},
        {6233U, 8U, 4U, 131072U},
        {6234U, 8U, 4U, 131072U},
        {6235U, 8U, 4U, 131072U},
        {6236U, 8U, 4U, 131072U}};
static u32 const golden_settings_tonga_a11___0[12U] =
  { 793U, 128U, 0U, 668U,
        240U, 112U, 674U, 523313151U,
        305135616U, 18953U, 822083857U, 17U};
static u32 const tonga_mgcg_cgcg_init___1[6U] = { 996U, 4294967295U, 256U, 998U,
        257U, 0U};
static void dce_v10_0_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 6U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_mgcg_cgcg_init___1),
                                   6U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_tonga_a11___0),
                                   12U);
  goto ldv_49838;
  default: ;
  goto ldv_49838;
  }
  ldv_49838: ;
  return;
}
}
static u32 dce_v10_0_audio_endpt_rreg(struct amdgpu_device *adev , u32 block_offset ,
                                      u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6056U, reg, 0);
  r = amdgpu_mm_rreg(adev, block_offset + 6057U, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return (r);
}
}
static void dce_v10_0_audio_endpt_wreg(struct amdgpu_device *adev , u32 block_offset ,
                                       u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6056U, reg, 0);
  amdgpu_mm_wreg(adev, block_offset + 6057U, v, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return;
}
}
static bool dce_v10_0_is_in_vblank(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7075U, 0);
  if ((tmp & 16383U) != 0U) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v10_0_is_counter_moving(struct amdgpu_device *adev , int crtc )
{
  u32 pos1 ;
  u32 pos2 ;
  {
  pos1 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7076U, 0);
  pos2 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7076U, 0);
  if (pos1 != pos2) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v10_0_vblank_wait(struct amdgpu_device *adev , int crtc )
{
  unsigned int i ;
  u32 tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  unsigned int tmp___2 ;
  bool tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  unsigned int tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  {
  i = 0U;
  if (adev->mode_info.num_crtc <= crtc) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7068U, 0);
  if ((tmp & 1U) == 0U) {
    return;
  } else {
  }
  goto ldv_49877;
  ldv_49876:
  tmp___2 = i;
  i = i + 1U;
  if (tmp___2 % 100U == 0U) {
    tmp___0 = dce_v10_0_is_counter_moving(adev, crtc);
    if (tmp___0) {
      tmp___1 = 0;
    } else {
      tmp___1 = 1;
    }
    if (tmp___1) {
      goto ldv_49875;
    } else {
    }
  } else {
  }
  ldv_49877:
  tmp___3 = dce_v10_0_is_in_vblank(adev, crtc);
  if ((int )tmp___3) {
    goto ldv_49876;
  } else {
  }
  ldv_49875: ;
  goto ldv_49880;
  ldv_49879:
  tmp___6 = i;
  i = i + 1U;
  if (tmp___6 % 100U == 0U) {
    tmp___4 = dce_v10_0_is_counter_moving(adev, crtc);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto ldv_49878;
    } else {
    }
  } else {
  }
  ldv_49880:
  tmp___7 = dce_v10_0_is_in_vblank(adev, crtc);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
    goto ldv_49879;
  } else {
  }
  ldv_49878: ;
  return;
}
}
static u32 dce_v10_0_vblank_get_counter(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    return (0U);
  } else {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7078U, 0);
    return (tmp);
  }
}
}
static void dce_v10_0_page_flip(struct amdgpu_device *adev , int crtc_id , u64 crtc_base )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  u32 tmp ;
  u32 tmp___0 ;
  int i ;
  u32 tmp___1 ;
  long tmp___2 ;
  {
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  tmp___0 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  tmp = tmp___0;
  tmp = tmp | 65536U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )crtc_base,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )crtc_base,
                 0);
  i = 0;
  goto ldv_49895;
  ldv_49894:
  tmp___1 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  if ((tmp___1 & 4U) != 0U) {
    goto ldv_49893;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_49895: ;
  if (adev->usec_timeout > i) {
    goto ldv_49894;
  } else {
  }
  ldv_49893:
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v10_0_page_flip", "Update pending now high. Unlocking vupdate_lock.\n");
  } else {
  }
  tmp = tmp & 4294901759U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  return;
}
}
static int dce_v10_0_crtc_get_scanoutpos(struct amdgpu_device *adev , int crtc , u32 *vbl ,
                                         u32 *position )
{
  {
  if (crtc < 0 || adev->mode_info.num_crtc <= crtc) {
    return (-22);
  } else {
  }
  *vbl = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7053U, 0);
  *position = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 7076U,
                             0);
  return (0);
}
}
static bool dce_v10_0_hpd_sense(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  int idx ;
  bool connected ;
  u32 tmp ;
  {
  connected = 0;
  switch ((unsigned int )hpd) {
  case 0U:
  idx = 0;
  goto ldv_49910;
  case 1U:
  idx = 1;
  goto ldv_49910;
  case 2U:
  idx = 2;
  goto ldv_49910;
  case 3U:
  idx = 3;
  goto ldv_49910;
  case 4U:
  idx = 4;
  goto ldv_49910;
  case 5U:
  idx = 5;
  goto ldv_49910;
  default: ;
  return (connected);
  }
  ldv_49910:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[idx] + 6296U, 0);
  if ((tmp & 2U) != 0U) {
    connected = 1;
  } else {
  }
  return (connected);
}
}
static void dce_v10_0_hpd_set_polarity(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  u32 tmp ;
  bool connected ;
  bool tmp___0 ;
  int idx ;
  {
  tmp___0 = dce_v10_0_hpd_sense(adev, hpd);
  connected = tmp___0;
  switch ((unsigned int )hpd) {
  case 0U:
  idx = 0;
  goto ldv_49925;
  case 1U:
  idx = 1;
  goto ldv_49925;
  case 2U:
  idx = 2;
  goto ldv_49925;
  case 3U:
  idx = 3;
  goto ldv_49925;
  case 4U:
  idx = 4;
  goto ldv_49925;
  case 5U:
  idx = 5;
  goto ldv_49925;
  default: ;
  return;
  }
  ldv_49925:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[idx] + 6297U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[idx] + 6297U, tmp, 0);
  return;
}
}
static void dce_v10_0_hpd_init(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  u32 tmp ;
  int idx ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_49956;
  ldv_49955:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    goto ldv_49946;
  } else {
  }
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  idx = 0;
  goto ldv_49948;
  case 1U:
  idx = 1;
  goto ldv_49948;
  case 2U:
  idx = 2;
  goto ldv_49948;
  case 3U:
  idx = 3;
  goto ldv_49948;
  case 4U:
  idx = 4;
  goto ldv_49948;
  case 5U:
  idx = 5;
  goto ldv_49948;
  default: ;
  goto ldv_49946;
  }
  ldv_49948:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[idx] + 6298U, 0);
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[idx] + 6298U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[idx] + 6300U, 0);
  tmp = (tmp & 4294967040U) | 50U;
  tmp = (tmp & 4027580415U) | 10485760U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[idx] + 6300U, tmp, 0);
  dce_v10_0_hpd_set_polarity(adev, amdgpu_connector->hpd.hpd);
  amdgpu_irq_get(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  ldv_49946:
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_49956: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_49955;
  } else {
  }
  return;
}
}
static void dce_v10_0_hpd_fini(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  u32 tmp ;
  int idx ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_49982;
  ldv_49981:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  idx = 0;
  goto ldv_49973;
  case 1U:
  idx = 1;
  goto ldv_49973;
  case 2U:
  idx = 2;
  goto ldv_49973;
  case 3U:
  idx = 3;
  goto ldv_49973;
  case 4U:
  idx = 4;
  goto ldv_49973;
  case 5U:
  idx = 5;
  goto ldv_49973;
  default: ;
  goto ldv_49980;
  }
  ldv_49973:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[idx] + 6298U, 0);
  tmp = tmp & 4026531839U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[idx] + 6298U, tmp, 0);
  amdgpu_irq_put(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  ldv_49980:
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_49982: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_49981;
  } else {
  }
  return;
}
}
static u32 dce_v10_0_hpd_get_gpio_reg(struct amdgpu_device *adev )
{
  {
  return (18573U);
}
}
static bool dce_v10_0_is_display_hung(struct amdgpu_device *adev )
{
  u32 crtc_hung ;
  u32 crtc_status[6U] ;
  u32 i ;
  u32 j ;
  u32 tmp ;
  {
  crtc_hung = 0U;
  i = 0U;
  goto ldv_49996;
  ldv_49995:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7068U, 0);
  if ((int )tmp & 1) {
    crtc_status[i] = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7080U,
                                    0);
    crtc_hung = (u32 )(1 << (int )i) | crtc_hung;
  } else {
  }
  i = i + 1U;
  ldv_49996: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_49995;
  } else {
  }
  j = 0U;
  goto ldv_50002;
  ldv_50001:
  i = 0U;
  goto ldv_49999;
  ldv_49998: ;
  if (((u32 )(1 << (int )i) & crtc_hung) != 0U) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7080U, 0);
    if (crtc_status[i] != tmp) {
      crtc_hung = (u32 )(~ (1 << (int )i)) & crtc_hung;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_49999: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_49998;
  } else {
  }
  if (crtc_hung == 0U) {
    return (0);
  } else {
  }
  __const_udelay(429500UL);
  j = j + 1U;
  ldv_50002: ;
  if (j <= 9U) {
    goto ldv_50001;
  } else {
  }
  return (1);
}
}
static void dce_v10_0_stop_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 crtc_enabled ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  {
  save->vga_render_control = amdgpu_mm_rreg(adev, 192U, 0);
  save->vga_hdp_control = amdgpu_mm_rreg(adev, 202U, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  tmp = tmp & 4294770687U;
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  i = 0;
  goto ldv_50012;
  ldv_50011:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7068U, 0);
  crtc_enabled = tmp___0 & 1U;
  if (crtc_enabled != 0U) {
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7093U, 1U, 0);
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7068U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7068U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7093U, 0U, 0);
    save->crtc_enabled[i] = 0;
  } else {
    save->crtc_enabled[i] = 0;
  }
  i = i + 1;
  ldv_50012: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50011;
  } else {
  }
  return;
}
}
static void dce_v10_0_resume_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 tmp ;
  u32 frame_count ;
  int i ;
  int j ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  {
  i = 0;
  goto ldv_50029;
  ldv_50028:
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 6663U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 6664U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 6660U, (unsigned int )adev->mc.vram_start,
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 6661U, (unsigned int )adev->mc.vram_start,
                 0);
  if ((int )save->crtc_enabled[i]) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7102U, 0);
    if ((tmp & 7U) != 3U) {
      tmp = (tmp & 4294967288U) | 3U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7102U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 6673U, 0);
    if ((tmp & 65536U) >> 16 != 0U) {
      tmp = tmp & 4294901759U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 6673U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7101U, 0);
    if ((int )tmp & 1) {
      tmp = tmp & 4294967294U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7101U, tmp, 0);
    } else {
    }
    j = 0;
    goto ldv_50024;
    ldv_50023:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 6673U, 0);
    if ((tmp & 4U) >> 2 == 0U) {
      goto ldv_50022;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_50024: ;
    if (adev->usec_timeout > j) {
      goto ldv_50023;
    } else {
    }
    ldv_50022:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[i] + 7069U, 0);
    tmp = tmp & 4294967039U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7093U, 1U, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7069U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[i] + 7093U, 0U, 0);
    frame_count = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    j = 0;
    goto ldv_50027;
    ldv_50026:
    tmp___0 = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    if (tmp___0 != frame_count) {
      goto ldv_50025;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_50027: ;
    if (adev->usec_timeout > j) {
      goto ldv_50026;
    } else {
    }
    ldv_50025: ;
  } else {
  }
  i = i + 1;
  ldv_50029: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50028;
  } else {
  }
  amdgpu_mm_wreg(adev, 201U, (unsigned int )(adev->mc.vram_start >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 196U, (unsigned int )adev->mc.vram_start, 0);
  amdgpu_mm_wreg(adev, 202U, save->vga_hdp_control, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_50033;
    ldv_50032:
    __const_udelay(4295000UL);
    ldv_50033:
    tmp___1 = __ms;
    __ms = __ms - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_50032;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 192U, save->vga_render_control, 0);
  return;
}
}
static void dce_v10_0_set_vga_render_state(struct amdgpu_device *adev , bool render )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 202U, 0);
  if ((int )render) {
    tmp = tmp & 4294967279U;
  } else {
    tmp = tmp | 16U;
  }
  amdgpu_mm_wreg(adev, 202U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  if ((int )render) {
    tmp = (tmp & 4294770687U) | 65536U;
  } else {
    tmp = tmp & 4294770687U;
  }
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  return;
}
}
static void dce_v10_0_program_fmt(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  int bpc ;
  u32 tmp___0 ;
  enum amdgpu_connector_dither dither ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 0;
  tmp___0 = 0U;
  dither = 0;
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    bpc = amdgpu_connector_get_monitor_bpc(connector);
    dither = amdgpu_connector->dither;
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    return;
  } else {
  }
  if (amdgpu_encoder->encoder_id == 21U || amdgpu_encoder->encoder_id == 22U) {
    return;
  } else {
  }
  if (bpc == 0) {
    return;
  } else {
  }
  switch (bpc) {
  case 6: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = tmp___0 & 4294961151U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = tmp___0 & 4294967247U;
  }
  goto ldv_50059;
  case 8: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 16384U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = (tmp___0 & 4294961151U) | 2048U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = (tmp___0 & 4294967247U) | 16U;
  }
  goto ldv_50059;
  case 10: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 16384U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = (tmp___0 & 4294961151U) | 4096U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = (tmp___0 & 4294967247U) | 32U;
  }
  goto ldv_50059;
  default: ;
  goto ldv_50059;
  }
  ldv_50059:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7154U, tmp___0, 0);
  return;
}
}
static u32 dce_v10_0_line_buffer_adjust(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                        struct drm_display_mode *mode )
{
  u32 tmp ;
  u32 buffer_alloc ;
  u32 i ;
  u32 mem_cfg ;
  u32 pipe_offset ;
  long tmp___0 ;
  {
  pipe_offset = (u32 )amdgpu_crtc->crtc_id;
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    if (mode->crtc_hdisplay <= 1919) {
      mem_cfg = 1U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 2559) {
      mem_cfg = 2U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 4095) {
      mem_cfg = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    } else {
      tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("dce_v10_0_line_buffer_adjust", "Mode too big for LB!\n");
      } else {
      }
      mem_cfg = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    }
  } else {
    mem_cfg = 1U;
    buffer_alloc = 0U;
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6849U, 0);
  tmp = (tmp & 4291821567U) | ((mem_cfg << 20) & 3145728U);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6849U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, pipe_offset + 801U, 0);
  tmp = (tmp & 4294967288U) | (buffer_alloc & 7U);
  amdgpu_mm_wreg(adev, pipe_offset + 801U, tmp, 0);
  i = 0U;
  goto ldv_50076;
  ldv_50075:
  tmp = amdgpu_mm_rreg(adev, pipe_offset + 801U, 0);
  if ((tmp & 16U) >> 4 != 0U) {
    goto ldv_50074;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50076: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_50075;
  } else {
  }
  ldv_50074: ;
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    switch (mem_cfg) {
    case 0U: ;
    default: ;
    return (8192U);
    case 1U: ;
    return (3840U);
    case 2U: ;
    return (5120U);
    }
  } else {
  }
  return (0U);
}
}
static u32 cik_get_number_of_dram_channels___0(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 2049U, 0);
  tmp = tmp___0;
  switch ((tmp & 61440U) >> 12) {
  case 0U: ;
  default: ;
  return (1U);
  case 1U: ;
  return (2U);
  case 2U: ;
  return (4U);
  case 3U: ;
  return (8U);
  case 4U: ;
  return (3U);
  case 5U: ;
  return (6U);
  case 6U: ;
  return (10U);
  case 7U: ;
  return (12U);
  case 8U: ;
  return (16U);
  }
}
}
static u32 dce_v10_0_dram_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 dram_efficiency ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  dram_efficiency.full = 28672U;
  dram_efficiency.full = dfixed_div(dram_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )dram_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v10_0_dram_bandwidth_for_display(struct dce10_wm_params *wm )
{
  fixed20_12 disp_dram_allocation ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  disp_dram_allocation.full = 12288U;
  disp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )disp_dram_allocation.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v10_0_data_return_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 return_efficiency ;
  fixed20_12 sclk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  sclk.full = wm->sclk << 12;
  sclk.full = dfixed_div(sclk, a);
  a.full = 40960U;
  return_efficiency.full = 32768U;
  return_efficiency.full = dfixed_div(return_efficiency, a);
  a.full = 131072U;
  bandwidth.full = (u32 )(((unsigned long long )a.full * (unsigned long long )sclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )return_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v10_0_dmif_request_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 disp_clk_request_efficiency ;
  fixed20_12 disp_clk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  fixed20_12 b ;
  {
  a.full = 4096000U;
  disp_clk.full = wm->disp_clk << 12;
  disp_clk.full = dfixed_div(disp_clk, a);
  a.full = 131072U;
  b.full = (u32 )(((unsigned long long )a.full * (unsigned long long )disp_clk.full + 2048ULL) >> 12);
  a.full = 40960U;
  disp_clk_request_efficiency.full = 32768U;
  disp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )b.full * (unsigned long long )disp_clk_request_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v10_0_available_bandwidth(struct dce10_wm_params *wm )
{
  u32 dram_bandwidth ;
  u32 tmp ;
  u32 data_return_bandwidth ;
  u32 tmp___0 ;
  u32 dmif_req_bandwidth ;
  u32 tmp___1 ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  tmp = dce_v10_0_dram_bandwidth(wm);
  dram_bandwidth = tmp;
  tmp___0 = dce_v10_0_data_return_bandwidth(wm);
  data_return_bandwidth = tmp___0;
  tmp___1 = dce_v10_0_dmif_request_bandwidth(wm);
  dmif_req_bandwidth = tmp___1;
  _min1 = dram_bandwidth;
  _min1___0 = data_return_bandwidth;
  _min2___0 = dmif_req_bandwidth;
  _min2 = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  return (_min1 < _min2 ? _min1 : _min2);
}
}
static u32 dce_v10_0_average_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 bpp ;
  fixed20_12 line_time ;
  fixed20_12 src_width ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  line_time.full = (wm->active_time + wm->blank_time) << 12;
  line_time.full = dfixed_div(line_time, a);
  bpp.full = wm->bytes_per_pixel << 12;
  src_width.full = wm->src_width << 12;
  bandwidth.full = (u32 )(((unsigned long long )src_width.full * (unsigned long long )bpp.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )wm->vsc.full + 2048ULL) >> 12);
  bandwidth.full = dfixed_div(bandwidth, line_time);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v10_0_latency_watermark(struct dce10_wm_params *wm )
{
  u32 mc_latency ;
  u32 available_bandwidth ;
  u32 tmp ;
  u32 worst_chunk_return_time ;
  u32 cursor_line_pair_return_time ;
  u32 dc_latency ;
  u32 other_heads_data_return_time ;
  u32 latency ;
  u32 max_src_lines_per_dst_line ;
  u32 lb_fill_bw ;
  u32 line_fill_time ;
  u32 tmp___0 ;
  u32 dmif_size ;
  fixed20_12 a ;
  fixed20_12 b ;
  fixed20_12 c ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  mc_latency = 2000U;
  tmp = dce_v10_0_available_bandwidth(wm);
  available_bandwidth = tmp;
  worst_chunk_return_time = 4096000U / available_bandwidth;
  cursor_line_pair_return_time = 512000U / available_bandwidth;
  dc_latency = 40000000U / wm->disp_clk;
  other_heads_data_return_time = (wm->num_heads + 1U) * worst_chunk_return_time + wm->num_heads * cursor_line_pair_return_time;
  latency = (mc_latency + other_heads_data_return_time) + dc_latency;
  dmif_size = 12288U;
  if (wm->num_heads == 0U) {
    return (0U);
  } else {
  }
  a.full = 8192U;
  b.full = 4096U;
  if (((wm->vsc.full > a.full || (wm->vsc.full > b.full && wm->vtaps > 2U)) || wm->vtaps > 4U) || (wm->vsc.full >= a.full && (int )wm->interlaced)) {
    max_src_lines_per_dst_line = 4U;
  } else {
    max_src_lines_per_dst_line = 2U;
  }
  a.full = available_bandwidth << 12;
  b.full = wm->num_heads << 12;
  a.full = dfixed_div(a, b);
  b.full = (mc_latency + 512U) << 12;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(b, c);
  c.full = dmif_size << 12;
  b.full = dfixed_div(c, b);
  _min1 = a.full >> 12;
  _min2 = b.full >> 12;
  tmp___0 = _min1 < _min2 ? _min1 : _min2;
  b.full = 4096000U;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(c, b);
  c.full = wm->bytes_per_pixel << 12;
  b.full = (u32 )(((unsigned long long )b.full * (unsigned long long )c.full + 2048ULL) >> 12);
  _min1___0 = tmp___0;
  _min2___0 = b.full >> 12;
  lb_fill_bw = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  a.full = (wm->src_width * max_src_lines_per_dst_line) * wm->bytes_per_pixel << 12;
  b.full = 4096000U;
  c.full = lb_fill_bw << 12;
  b.full = dfixed_div(c, b);
  a.full = dfixed_div(a, b);
  line_fill_time = a.full >> 12;
  if (wm->active_time > line_fill_time) {
    return (latency);
  } else {
    return ((line_fill_time - wm->active_time) + latency);
  }
}
}
static bool dce_v10_0_average_bandwidth_vs_dram_bandwidth_for_display(struct dce10_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v10_0_average_bandwidth(wm);
  tmp___0 = dce_v10_0_dram_bandwidth_for_display(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v10_0_average_bandwidth_vs_available_bandwidth(struct dce10_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v10_0_average_bandwidth(wm);
  tmp___0 = dce_v10_0_available_bandwidth(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v10_0_check_latency_hiding(struct dce10_wm_params *wm )
{
  u32 lb_partitions ;
  u32 line_time ;
  u32 latency_tolerant_lines ;
  u32 latency_hiding ;
  fixed20_12 a ;
  u32 tmp ;
  {
  lb_partitions = wm->lb_size / wm->src_width;
  line_time = wm->active_time + wm->blank_time;
  a.full = 4096U;
  if (wm->vsc.full > a.full) {
    latency_tolerant_lines = 1U;
  } else
  if (wm->vtaps + 1U >= lb_partitions) {
    latency_tolerant_lines = 1U;
  } else {
    latency_tolerant_lines = 2U;
  }
  latency_hiding = latency_tolerant_lines * line_time + wm->blank_time;
  tmp = dce_v10_0_latency_watermark(wm);
  if (tmp <= latency_hiding) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v10_0_program_watermarks(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                         u32 lb_size , u32 num_heads )
{
  struct drm_display_mode *mode ;
  struct dce10_wm_params wm_low ;
  struct dce10_wm_params wm_high ;
  u32 pixel_period ;
  u32 line_time ;
  u32 latency_watermark_a ;
  u32 latency_watermark_b ;
  u32 tmp ;
  u32 wm_mask ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 _min1___0 ;
  u32 tmp___2 ;
  unsigned int _min2___0 ;
  long tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  bool tmp___6 ;
  int tmp___7 ;
  bool tmp___8 ;
  int tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 _min1___1 ;
  u32 tmp___12 ;
  unsigned int _min2___1 ;
  long tmp___13 ;
  bool tmp___14 ;
  int tmp___15 ;
  bool tmp___16 ;
  int tmp___17 ;
  bool tmp___18 ;
  int tmp___19 ;
  {
  mode = & amdgpu_crtc->base.mode;
  line_time = 0U;
  latency_watermark_a = 0U;
  latency_watermark_b = 0U;
  if (((int )amdgpu_crtc->base.enabled && num_heads != 0U) && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    pixel_period = 1000000U / (unsigned int )mode->clock;
    _min1 = (unsigned int )mode->crtc_htotal * pixel_period;
    _min2 = 65535U;
    line_time = _min1 < _min2 ? _min1 : _min2;
    if ((int )adev->pm.dpm_enabled) {
      tmp___0 = (*((adev->pm.funcs)->get_mclk))(adev, 0);
      wm_high.yclk = tmp___0 * 10U;
      tmp___1 = (*((adev->pm.funcs)->get_sclk))(adev, 0);
      wm_high.sclk = tmp___1 * 10U;
    } else {
      wm_high.yclk = adev->pm.current_mclk * 10U;
      wm_high.sclk = adev->pm.current_sclk * 10U;
    }
    wm_high.disp_clk = (u32 )mode->clock;
    wm_high.src_width = (u32 )mode->crtc_hdisplay;
    wm_high.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_high.blank_time = line_time - wm_high.active_time;
    wm_high.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_high.interlaced = 1;
    } else {
    }
    wm_high.vsc = amdgpu_crtc->vsc;
    wm_high.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_high.vtaps = 2U;
    } else {
    }
    wm_high.bytes_per_pixel = 4U;
    wm_high.lb_size = lb_size;
    wm_high.dram_channels = cik_get_number_of_dram_channels___0(adev);
    wm_high.num_heads = num_heads;
    tmp___2 = dce_v10_0_latency_watermark(& wm_high);
    _min1___0 = tmp___2;
    _min2___0 = 65535U;
    latency_watermark_a = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    tmp___4 = dce_v10_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_high);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto _L;
    } else {
      tmp___6 = dce_v10_0_average_bandwidth_vs_available_bandwidth(& wm_high);
      if (tmp___6) {
        tmp___7 = 0;
      } else {
        tmp___7 = 1;
      }
      if (tmp___7) {
        goto _L;
      } else {
        tmp___8 = dce_v10_0_check_latency_hiding(& wm_high);
        if (tmp___8) {
          tmp___9 = 0;
        } else {
          tmp___9 = 1;
        }
        if (tmp___9) {
          goto _L;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L:
          tmp___3 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___3 != 0L) {
            drm_ut_debug_printk("dce_v10_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
    if ((int )adev->pm.dpm_enabled) {
      tmp___10 = (*((adev->pm.funcs)->get_mclk))(adev, 1);
      wm_low.yclk = tmp___10 * 10U;
      tmp___11 = (*((adev->pm.funcs)->get_sclk))(adev, 1);
      wm_low.sclk = tmp___11 * 10U;
    } else {
      wm_low.yclk = adev->pm.current_mclk * 10U;
      wm_low.sclk = adev->pm.current_sclk * 10U;
    }
    wm_low.disp_clk = (u32 )mode->clock;
    wm_low.src_width = (u32 )mode->crtc_hdisplay;
    wm_low.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_low.blank_time = line_time - wm_low.active_time;
    wm_low.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_low.interlaced = 1;
    } else {
    }
    wm_low.vsc = amdgpu_crtc->vsc;
    wm_low.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_low.vtaps = 2U;
    } else {
    }
    wm_low.bytes_per_pixel = 4U;
    wm_low.lb_size = lb_size;
    wm_low.dram_channels = cik_get_number_of_dram_channels___0(adev);
    wm_low.num_heads = num_heads;
    tmp___12 = dce_v10_0_latency_watermark(& wm_low);
    _min1___1 = tmp___12;
    _min2___1 = 65535U;
    latency_watermark_b = _min1___1 < _min2___1 ? _min1___1 : _min2___1;
    tmp___14 = dce_v10_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_low);
    if (tmp___14) {
      tmp___15 = 0;
    } else {
      tmp___15 = 1;
    }
    if (tmp___15) {
      goto _L___0;
    } else {
      tmp___16 = dce_v10_0_average_bandwidth_vs_available_bandwidth(& wm_low);
      if (tmp___16) {
        tmp___17 = 0;
      } else {
        tmp___17 = 1;
      }
      if (tmp___17) {
        goto _L___0;
      } else {
        tmp___18 = dce_v10_0_check_latency_hiding(& wm_low);
        if (tmp___18) {
          tmp___19 = 0;
        } else {
          tmp___19 = 1;
        }
        if (tmp___19) {
          goto _L___0;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L___0:
          tmp___13 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___13 != 0L) {
            drm_ut_debug_printk("dce_v10_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
  } else {
  }
  wm_mask = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6962U, 0);
  tmp = (wm_mask & 4294966527U) | 256U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6963U, 0);
  tmp = (tmp & 4294901760U) | (latency_watermark_a & 65535U);
  tmp = (tmp & 65535U) | (line_time << 16);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, tmp, 0);
  tmp = (wm_mask & 4294966527U) | 512U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6963U, 0);
  tmp = (tmp & 4294901760U) | (latency_watermark_a & 65535U);
  tmp = (tmp & 65535U) | (line_time << 16);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, wm_mask, 0);
  amdgpu_crtc->line_time = line_time;
  amdgpu_crtc->wm_high = latency_watermark_a;
  amdgpu_crtc->wm_low = latency_watermark_b;
  return;
}
}
static void dce_v10_0_bandwidth_update(struct amdgpu_device *adev )
{
  struct drm_display_mode *mode ;
  u32 num_heads ;
  u32 lb_size ;
  int i ;
  {
  mode = (struct drm_display_mode *)0;
  num_heads = 0U;
  amdgpu_update_display_priority(adev);
  i = 0;
  goto ldv_50234;
  ldv_50233: ;
  if ((int )(adev->mode_info.crtcs[i])->base.enabled) {
    num_heads = num_heads + 1U;
  } else {
  }
  i = i + 1;
  ldv_50234: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50233;
  } else {
  }
  i = 0;
  goto ldv_50237;
  ldv_50236:
  mode = & (adev->mode_info.crtcs[i])->base.mode;
  lb_size = dce_v10_0_line_buffer_adjust(adev, adev->mode_info.crtcs[i], mode);
  dce_v10_0_program_watermarks(adev, adev->mode_info.crtcs[i], lb_size, num_heads);
  i = i + 1;
  ldv_50237: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50236;
  } else {
  }
  return;
}
}
static void dce_v10_0_audio_get_connected_pins(struct amdgpu_device *adev )
{
  int i ;
  u32 offset ;
  u32 tmp ;
  {
  i = 0;
  goto ldv_50246;
  ldv_50245:
  offset = adev->mode_info.audio.pin[i].offset;
  tmp = (*(adev->audio_endpt_rreg))(adev, offset, 86U);
  if (tmp >> 30 == 1U) {
    adev->mode_info.audio.pin[i].connected = 0;
  } else {
    adev->mode_info.audio.pin[i].connected = 1;
  }
  i = i + 1;
  ldv_50246: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50245;
  } else {
  }
  return;
}
}
static struct amdgpu_audio_pin *dce_v10_0_audio_get_pin(struct amdgpu_device *adev )
{
  int i ;
  {
  dce_v10_0_audio_get_connected_pins(adev);
  i = 0;
  goto ldv_50253;
  ldv_50252: ;
  if ((int )adev->mode_info.audio.pin[i].connected) {
    return ((struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i);
  } else {
  }
  i = i + 1;
  ldv_50253: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50252;
  } else {
  }
  drm_err("No connected audio pins found!\n");
  return ((struct amdgpu_audio_pin *)0);
}
}
static void dce_v10_0_afmt_audio_select_pin(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19013), 0);
  tmp = (tmp & 4294967288U) | (((dig->afmt)->pin)->id & 7U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19013), tmp, 0);
  return;
}
}
static void dce_v10_0_audio_write_latency_fields(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 tmp ;
  int interlace ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  interlace = 0;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_50285;
  ldv_50284: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_50283;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_50285: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_50284;
  } else {
  }
  ldv_50283: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  if ((mode->flags & 16U) != 0U) {
    interlace = 1;
  } else {
  }
  if ((int )connector->latency_present[interlace]) {
    tmp = (u32 )connector->video_latency[interlace] & 255U;
    tmp = (u32 )(connector->audio_latency[interlace] << 8) & 65535U;
  } else {
    tmp = 0U;
    tmp = 0U;
  }
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, 55U, tmp);
  return;
}
}
static void dce_v10_0_audio_write_speaker_allocation(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 tmp ;
  u8 *sadb ;
  int sad_count ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  sadb = (u8 *)0U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_50307;
  ldv_50306: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_50305;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_50307: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_50306;
  } else {
  }
  ldv_50305: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp___0 = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_speaker_allocation(tmp___0, & sadb);
  if (sad_count < 0) {
    drm_err("Couldn\'t read Speaker Allocation Data Block: %d\n", sad_count);
    sad_count = 0;
  } else {
  }
  tmp = (*(adev->audio_endpt_rreg))(adev, ((dig->afmt)->pin)->offset, 37U);
  tmp = tmp & 4294836223U;
  tmp = tmp | 65536U;
  if (sad_count != 0) {
    tmp = (tmp & 4294967168U) | ((u32 )*sadb & 127U);
  } else {
    tmp = (tmp & 4294967168U) | 5U;
  }
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, 37U, tmp);
  kfree((void const *)sadb);
  return;
}
}
static void dce_v10_0_audio_write_sad_regs(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct cea_sad *sads ;
  int i ;
  int sad_count ;
  u16 eld_reg_to_type[12U][2U] ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp ;
  long tmp___0 ;
  u32 tmp___1 ;
  u8 stereo_freqs ;
  int max_channels ;
  int j ;
  struct cea_sad *sad ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  eld_reg_to_type[0][0] = 40U;
  eld_reg_to_type[0][1] = 1U;
  eld_reg_to_type[1][0] = 41U;
  eld_reg_to_type[1][1] = 2U;
  eld_reg_to_type[2][0] = 42U;
  eld_reg_to_type[2][1] = 3U;
  eld_reg_to_type[3][0] = 43U;
  eld_reg_to_type[3][1] = 4U;
  eld_reg_to_type[4][0] = 44U;
  eld_reg_to_type[4][1] = 5U;
  eld_reg_to_type[5][0] = 45U;
  eld_reg_to_type[5][1] = 6U;
  eld_reg_to_type[6][0] = 46U;
  eld_reg_to_type[6][1] = 7U;
  eld_reg_to_type[7][0] = 47U;
  eld_reg_to_type[7][1] = 8U;
  eld_reg_to_type[8][0] = 49U;
  eld_reg_to_type[8][1] = 10U;
  eld_reg_to_type[9][0] = 50U;
  eld_reg_to_type[9][1] = 11U;
  eld_reg_to_type[10][0] = 51U;
  eld_reg_to_type[10][1] = 12U;
  eld_reg_to_type[11][0] = 53U;
  eld_reg_to_type[11][1] = 14U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_50330;
  ldv_50329: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_50328;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_50330: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_50329;
  } else {
  }
  ldv_50328: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_sad(tmp, & sads);
  if (sad_count <= 0) {
    drm_err("Couldn\'t read SADs: %d\n", sad_count);
    return;
  } else {
  }
  tmp___0 = ldv__builtin_expect((unsigned long )sads == (unsigned long )((struct cea_sad *)0),
                             0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/dce_v10_0.c"),
                         "i" (1561), "i" (12UL));
    ldv_50331: ;
    goto ldv_50331;
  } else {
  }
  i = 0;
  goto ldv_50343;
  ldv_50342:
  tmp___1 = 0U;
  stereo_freqs = 0U;
  max_channels = -1;
  j = 0;
  goto ldv_50341;
  ldv_50340:
  sad = sads + (unsigned long )j;
  if ((int )((unsigned short )sad->format) == (int )eld_reg_to_type[i][1]) {
    if ((int )sad->channels > max_channels) {
      tmp___1 = (tmp___1 & 4294967288U) | ((u32 )sad->channels & 7U);
      tmp___1 = (tmp___1 & 4278255615U) | ((u32 )((int )sad->byte2 << 16) & 16711680U);
      tmp___1 = (tmp___1 & 4294902015U) | ((u32 )((int )sad->freq << 8) & 65535U);
      max_channels = (int )sad->channels;
    } else {
    }
    if ((unsigned int )sad->format == 1U) {
      stereo_freqs = (u8 )((int )sad->freq | (int )stereo_freqs);
    } else {
      goto ldv_50339;
    }
  } else {
  }
  j = j + 1;
  ldv_50341: ;
  if (j < sad_count) {
    goto ldv_50340;
  } else {
  }
  ldv_50339:
  tmp___1 = (tmp___1 & 16777215U) | (u32 )((int )stereo_freqs << 24);
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, (u32 )eld_reg_to_type[i][0],
                              tmp___1);
  i = i + 1;
  ldv_50343: ;
  if ((unsigned int )i <= 11U) {
    goto ldv_50342;
  } else {
  }
  kfree((void const *)sads);
  return;
}
}
static void dce_v10_0_audio_enable(struct amdgpu_device *adev , struct amdgpu_audio_pin *pin ,
                                   bool enable )
{
  {
  if ((unsigned long )pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  (*(adev->audio_endpt_wreg))(adev, pin->offset, 84U, (int )enable ? 2147483648U : 0U);
  return;
}
}
static u32 const pin_offsets___0[7U] = { 0U, 4U, 8U, 12U,
        16U, 20U, 28U};
static int dce_v10_0_audio_init(struct amdgpu_device *adev )
{
  int i ;
  {
  if (amdgpu_audio == 0) {
    return (0);
  } else {
  }
  adev->mode_info.audio.enabled = 1;
  adev->mode_info.audio.num_pins = 7;
  i = 0;
  goto ldv_50356;
  ldv_50355:
  adev->mode_info.audio.pin[i].channels = -1;
  adev->mode_info.audio.pin[i].rate = -1;
  adev->mode_info.audio.pin[i].bits_per_sample = -1;
  adev->mode_info.audio.pin[i].status_bits = 0U;
  adev->mode_info.audio.pin[i].category_code = 0U;
  adev->mode_info.audio.pin[i].connected = 0;
  adev->mode_info.audio.pin[i].offset = pin_offsets___0[i];
  adev->mode_info.audio.pin[i].id = (u32 )i;
  dce_v10_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_50356: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50355;
  } else {
  }
  return (0);
}
}
static void dce_v10_0_audio_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  if (! adev->mode_info.audio.enabled) {
    return;
  } else {
  }
  i = 0;
  goto ldv_50363;
  ldv_50362:
  dce_v10_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_50363: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50362;
  } else {
  }
  adev->mode_info.audio.enabled = 0;
  return;
}
}
static void dce_v10_0_afmt_update_ACR(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_afmt_acr acr ;
  struct amdgpu_afmt_acr tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 tmp___0 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_afmt_acr(clock);
  acr = tmp;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18990), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_32khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18990), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18991), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_32khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18991), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18992), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_44_1khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18992), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18993), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_44_1khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18993), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18994), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_48khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18994), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18995), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_48khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18995), tmp___0, 0);
  return;
}
}
static void dce_v10_0_afmt_update_avi_infoframe(struct drm_encoder *encoder , void *buffer ,
                                                size_t size )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  uint8_t *frame ;
  uint8_t *header ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  frame = (uint8_t *)buffer + 3U;
  header = (uint8_t *)buffer;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18974), (u32 )((((int )*frame | ((int )*(frame + 1UL) << 8)) | ((int )*(frame + 2UL) << 16)) | ((int )*(frame + 3UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18975), (u32 )((((int )*(frame + 4UL) | ((int )*(frame + 5UL) << 8)) | ((int )*(frame + 6UL) << 16)) | ((int )*(frame + 7UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18976), (u32 )((((int )*(frame + 8UL) | ((int )*(frame + 9UL) << 8)) | ((int )*(frame + 10UL) << 16)) | ((int )*(frame + 11UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18977), (u32 )(((int )*(frame + 12UL) | ((int )*(frame + 13UL) << 8)) | ((int )*(header + 1UL) << 24)),
                 0);
  return;
}
}
static void dce_v10_0_audio_set_dto(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  u32 dto_phase ;
  u32 dto_modulo ;
  u32 tmp ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  dto_phase = 24000U;
  dto_modulo = clock;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 363U, 0);
  tmp = (tmp & 4294967288U) | ((u32 )amdgpu_crtc->crtc_id & 7U);
  amdgpu_mm_wreg(adev, 363U, tmp, 0);
  amdgpu_mm_wreg(adev, 364U, dto_phase, 0);
  amdgpu_mm_wreg(adev, 365U, dto_modulo, 0);
  return;
}
}
static void dce_v10_0_afmt_setmode(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  u8 buffer[17U] ;
  struct hdmi_avi_infoframe frame ;
  ssize_t err ;
  u32 tmp___0 ;
  int bpc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  int tmp___4 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 8;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if (! (dig->afmt)->enabled) {
    return;
  } else {
  }
  if ((unsigned long )encoder->crtc != (unsigned long )((struct drm_crtc *)0)) {
    __mptr___0 = (struct drm_crtc const *)encoder->crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    bpc = amdgpu_crtc->bpc;
  } else {
  }
  (dig->afmt)->pin = dce_v10_0_audio_get_pin(adev);
  dce_v10_0_audio_enable(adev, (dig->afmt)->pin, 0);
  dce_v10_0_audio_set_dto(encoder, (u32 )mode->clock);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18957), 0);
  tmp___0 = tmp___0 | 1U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18957), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19002), 4096U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18953), 0);
  switch (bpc) {
  case 0: ;
  case 6: ;
  case 8: ;
  case 16: ;
  default:
  tmp___0 = tmp___0 & 4278190079U;
  tmp___0 = tmp___0 & 3489660927U;
  tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___1 != 0L) {
    drm_ut_debug_printk("dce_v10_0_afmt_setmode", "%s: Disabling hdmi deep color for %d bpc.\n",
                        connector->name, bpc);
  } else {
  }
  goto ldv_50431;
  case 10:
  tmp___0 = tmp___0 | 16777216U;
  tmp___0 = (tmp___0 & 3489660927U) | 268435456U;
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v10_0_afmt_setmode", "%s: Enabling hdmi deep color 30 for 10 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_50431;
  case 12:
  tmp___0 = tmp___0 | 16777216U;
  tmp___0 = (tmp___0 & 3489660927U) | 536870912U;
  tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___3 != 0L) {
    drm_ut_debug_printk("dce_v10_0_afmt_setmode", "%s: Enabling hdmi deep color 36 for 12 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_50431;
  }
  ldv_50431:
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18953), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18957), 0);
  tmp___0 = tmp___0 | 1U;
  tmp___0 = tmp___0 | 16U;
  tmp___0 = tmp___0 | 32U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18957), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18958), 0);
  tmp___0 = tmp___0 | 16U;
  tmp___0 = tmp___0 | 32U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18958), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19012), 0);
  tmp___0 = tmp___0 | 128U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19012), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18959), 0);
  tmp___0 = (tmp___0 & 4294951167U) | 512U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18959), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18963), 0U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18955), 0);
  tmp___0 = (tmp___0 & 4294967247U) | 16U;
  tmp___0 = (tmp___0 & 4292935679U) | 196608U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18955), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19010), 0);
  tmp___0 = tmp___0 | 67108864U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19010), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18956), 0);
  if (bpc > 8) {
    tmp___0 = tmp___0 & 4294967039U;
  } else {
    tmp___0 = tmp___0 | 256U;
  }
  tmp___0 = tmp___0 | 4096U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18956), tmp___0, 0);
  dce_v10_0_afmt_update_ACR(encoder, (u32 )mode->clock);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19000), 0);
  tmp___0 = (tmp___0 & 4279238655U) | 1048576U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19000), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19001), 0);
  tmp___0 = (tmp___0 & 4279238655U) | 2097152U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19001), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19007), 0);
  tmp___0 = (tmp___0 & 4294967280U) | 3U;
  tmp___0 = (tmp___0 & 4294967055U) | 64U;
  tmp___0 = (tmp___0 & 4294963455U) | 1280U;
  tmp___0 = (tmp___0 & 4294905855U) | 24576U;
  tmp___0 = (tmp___0 & 4293984255U) | 458752U;
  tmp___0 = (tmp___0 & 4279238655U) | 8388608U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19007), tmp___0, 0);
  dce_v10_0_audio_write_speaker_allocation(encoder);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18964), 65280U, 0);
  dce_v10_0_afmt_audio_select_pin(encoder);
  dce_v10_0_audio_write_sad_regs(encoder);
  dce_v10_0_audio_write_latency_fields(encoder, mode);
  tmp___4 = drm_hdmi_avi_infoframe_from_display_mode(& frame, (struct drm_display_mode const *)mode);
  err = (ssize_t )tmp___4;
  if (err < 0L) {
    drm_err("failed to setup AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  err = hdmi_avi_infoframe_pack(& frame, (void *)(& buffer), 17UL);
  if (err < 0L) {
    drm_err("failed to pack AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  dce_v10_0_afmt_update_avi_infoframe(encoder, (void *)(& buffer), 17UL);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18958), 0);
  tmp___0 = tmp___0 | 1U;
  tmp___0 = tmp___0 | 2U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18958), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18959), 0);
  tmp___0 = (tmp___0 & 4294967232U) | 2U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18959), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19010), 0);
  tmp___0 = tmp___0 | 1U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19010), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19003), 16777215U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19004), 8388607U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19005), 1U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19006), 1U, 0);
  dce_v10_0_audio_enable(adev, (dig->afmt)->pin, 1);
  return;
}
}
static void dce_v10_0_afmt_enable(struct drm_encoder *encoder , bool enable )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  long tmp ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if ((int )enable && (int )(dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && ! (dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && (unsigned long )(dig->afmt)->pin != (unsigned long )((struct amdgpu_audio_pin *)0)) {
    dce_v10_0_audio_enable(adev, (dig->afmt)->pin, 0);
    (dig->afmt)->pin = (struct amdgpu_audio_pin *)0;
  } else {
  }
  (dig->afmt)->enabled = enable;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v10_0_afmt_enable", "%sabling AFMT interface @ 0x%04X for encoder 0x%x\n",
                        (int )enable ? (char *)"En" : (char *)"Dis", (dig->afmt)->offset,
                        amdgpu_encoder->encoder_id);
  } else {
  }
  return;
}
}
static void dce_v10_0_afmt_init(struct amdgpu_device *adev )
{
  int i ;
  void *tmp ;
  {
  i = 0;
  goto ldv_50450;
  ldv_50449:
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_50450: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_50449;
  } else {
  }
  i = 0;
  goto ldv_50453;
  ldv_50452:
  tmp = kzalloc(24UL, 208U);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)tmp;
  if ((unsigned long )adev->mode_info.afmt[i] != (unsigned long )((struct amdgpu_afmt *)0)) {
    (adev->mode_info.afmt[i])->offset = (int )dig_offsets___0[i];
    (adev->mode_info.afmt[i])->id = i;
  } else {
  }
  i = i + 1;
  ldv_50453: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_50452;
  } else {
  }
  return;
}
}
static void dce_v10_0_afmt_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_50460;
  ldv_50459:
  kfree((void const *)adev->mode_info.afmt[i]);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_50460: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_50459;
  } else {
  }
  return;
}
}
static u32 const vga_control_regs___0[6U] = { 204U, 206U, 248U, 249U,
        250U, 251U};
static void dce_v10_0_vga_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 vga_control ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_mm_rreg(adev, vga_control_regs___0[amdgpu_crtc->crtc_id], 0);
  vga_control = tmp & 4294967294U;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, vga_control_regs___0[amdgpu_crtc->crtc_id], vga_control | 1U,
                   0);
  } else {
    amdgpu_mm_wreg(adev, vga_control_regs___0[amdgpu_crtc->crtc_id], vga_control,
                   0);
  }
  return;
}
}
static void dce_v10_0_grph_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 1U, 0);
  } else {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 0U, 0);
  }
  return;
}
}
static int dce_v10_0_crtc_do_set_base(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                      int x , int y , int atomic )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct drm_framebuffer *target_fb ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *rbo ;
  uint64_t fb_location ;
  uint64_t tiling_flags ;
  u32 fb_format ;
  u32 fb_pitch_pixels ;
  u32 fb_swap ;
  u32 pipe_config ;
  u32 tmp ;
  u32 viewport_w ;
  u32 viewport_h ;
  int r ;
  bool bypass_lut ;
  long tmp___0 ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_framebuffer const *__mptr___1 ;
  struct drm_gem_object const *__mptr___2 ;
  long tmp___1 ;
  long tmp___2 ;
  char const *tmp___3 ;
  unsigned int bankw ;
  unsigned int bankh ;
  unsigned int mtaspect ;
  unsigned int tile_split ;
  unsigned int num_banks ;
  long tmp___4 ;
  struct drm_framebuffer const *__mptr___3 ;
  struct drm_gem_object const *__mptr___4 ;
  long tmp___5 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  fb_swap = 0U;
  bypass_lut = 0;
  if (atomic == 0 && (unsigned long )(crtc->primary)->fb == (unsigned long )((struct drm_framebuffer *)0)) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_do_set_base", "No FB bound\n");
    } else {
    }
    return (0);
  } else {
  }
  if (atomic != 0) {
    __mptr___0 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    target_fb = fb;
  } else {
    __mptr___1 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___1;
    target_fb = (crtc->primary)->fb;
  }
  obj = amdgpu_fb->obj;
  __mptr___2 = (struct drm_gem_object const *)obj;
  rbo = (struct amdgpu_bo *)__mptr___2 + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(rbo, 0);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    return (r);
  } else {
  }
  if (atomic != 0) {
    fb_location = amdgpu_bo_gpu_offset(rbo);
  } else {
    r = amdgpu_bo_pin(rbo, 4U, & fb_location);
    tmp___2 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___2 != 0L) {
      amdgpu_bo_unreserve(rbo);
      return (-22);
    } else {
    }
  }
  amdgpu_bo_get_tiling_flags(rbo, & tiling_flags);
  amdgpu_bo_unreserve(rbo);
  pipe_config = (u32 )(tiling_flags >> 4) & 31U;
  switch (target_fb->pixel_format) {
  case 538982467U:
  fb_format = 0U;
  fb_format = fb_format & 4294965503U;
  goto ldv_50517;
  case 842093144U: ;
  case 842093121U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 512U;
  goto ldv_50517;
  case 892424792U: ;
  case 892424769U:
  fb_format = 1U;
  fb_format = fb_format & 4294965503U;
  goto ldv_50517;
  case 892426306U: ;
  case 892420418U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 1280U;
  goto ldv_50517;
  case 909199186U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 256U;
  goto ldv_50517;
  case 875713112U: ;
  case 875713089U:
  fb_format = 2U;
  fb_format = fb_format & 4294965503U;
  goto ldv_50517;
  case 808669784U: ;
  case 808669761U:
  fb_format = 2U;
  fb_format = (fb_format & 4294965503U) | 256U;
  bypass_lut = 1;
  goto ldv_50517;
  case 808671298U: ;
  case 808665410U:
  fb_format = 2U;
  fb_format = (fb_format & 4294965503U) | 1024U;
  bypass_lut = 1;
  goto ldv_50517;
  default:
  tmp___3 = drm_get_format_name(target_fb->pixel_format);
  drm_err("Unsupported screen format %s\n", tmp___3);
  return (-22);
  }
  ldv_50517: ;
  if ((tiling_flags & 15ULL) == 4ULL) {
    bankw = (unsigned int )(tiling_flags >> 15) & 3U;
    bankh = (unsigned int )(tiling_flags >> 17) & 3U;
    mtaspect = (unsigned int )(tiling_flags >> 19) & 3U;
    tile_split = (unsigned int )(tiling_flags >> 9) & 7U;
    num_banks = (unsigned int )(tiling_flags >> 21) & 3U;
    fb_format = (fb_format & 4294967283U) | ((num_banks << 2) & 12U);
    fb_format = (fb_format & 4279238655U) | 4194304U;
    fb_format = (fb_format & 4294909951U) | ((tile_split << 13) & 65535U);
    fb_format = (fb_format & 4294967103U) | ((bankw << 6) & 255U);
    fb_format = (fb_format & 4294961151U) | ((bankh << 11) & 6144U);
    fb_format = (fb_format & 4294180863U) | ((mtaspect << 18) & 786432U);
    fb_format = fb_format & 2684354559U;
  } else
  if ((tiling_flags & 15ULL) == 2ULL) {
    fb_format = (fb_format & 4279238655U) | 2097152U;
  } else {
  }
  fb_format = (fb_format & 3774873599U) | ((pipe_config << 24) & 520093696U);
  dce_v10_0_vga_enable(crtc, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6657U, fb_format, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6659U, fb_swap, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6658U, 0);
  if ((int )bypass_lut) {
    tmp = tmp | 256U;
  } else {
    tmp = tmp & 4294967039U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6658U, tmp, 0);
  if ((int )bypass_lut) {
    tmp___4 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___4 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_do_set_base", "Bypassing hardware LUT due to 10 bit fb scanout.\n");
    } else {
    }
  } else {
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6665U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6666U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6667U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6668U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6669U, target_fb->width, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6670U, target_fb->height, 0);
  fb_pitch_pixels = target_fb->pitches[0] / (unsigned int )(target_fb->bits_per_pixel / 8);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6662U, fb_pitch_pixels, 0);
  dce_v10_0_grph_enable(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6851U, target_fb->height, 0);
  x = x & -4;
  y = y & -2;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7004U, (u32 )((x << 16) | y), 0);
  viewport_w = (u32 )crtc->mode.hdisplay;
  viewport_h = (u32 )(crtc->mode.vdisplay + 1) & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7005U, (viewport_w << 16) | viewport_h,
                 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6674U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6674U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7102U, 3U, 0);
  if ((atomic == 0 && (unsigned long )fb != (unsigned long )((struct drm_framebuffer *)0)) && (unsigned long )(crtc->primary)->fb != (unsigned long )fb) {
    __mptr___3 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___3;
    __mptr___4 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___4 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp___5 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___5 != 0L) {
      return (r);
    } else {
    }
    amdgpu_bo_unpin(rbo);
    amdgpu_bo_unreserve(rbo);
  } else {
  }
  dce_v10_0_bandwidth_update(adev);
  return (0);
}
}
static void dce_v10_0_set_interleave(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  u32 tmp ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6848U, 0);
  if ((mode->flags & 16U) != 0U) {
    tmp = tmp | 8U;
  } else {
    tmp = tmp & 4294967287U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6848U, tmp, 0);
  return;
}
}
static void dce_v10_0_crtc_load_lut(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int i ;
  u32 tmp ;
  long tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("dce_v10_0_crtc_load_lut", "%d\n", amdgpu_crtc->crtc_id);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6709U, 0);
  tmp = tmp & 4294967292U;
  tmp = tmp & 4294967247U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6709U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6701U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6701U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6705U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6705U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6672U, 0);
  tmp = tmp & 4294967292U;
  tmp = tmp & 4294967247U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6672U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6784U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6785U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6786U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6787U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6788U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6789U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6790U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6776U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6782U, 7U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6777U, 0U, 0);
  i = 0;
  goto ldv_50563;
  ldv_50562:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6780U, (u32 )((((int )amdgpu_crtc->lut_r[i] << 20) | ((int )amdgpu_crtc->lut_g[i] << 10)) | (int )amdgpu_crtc->lut_b[i]),
                 0);
  i = i + 1;
  ldv_50563: ;
  if (i <= 255) {
    goto ldv_50562;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6744U, 0);
  tmp = tmp & 4294967292U;
  tmp = tmp & 4294967247U;
  tmp = tmp & 4294955007U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6744U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6745U, 0);
  tmp = tmp & 4294967292U;
  tmp = tmp & 4294967247U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6745U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6816U, 0);
  tmp = tmp & 4294967288U;
  tmp = tmp & 4294967183U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6816U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6716U, 0);
  tmp = tmp & 4294967288U;
  tmp = tmp & 4294967183U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6716U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6736U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6844U, 0);
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6844U, tmp, 0);
  return;
}
}
static int dce_v10_0_pick_dig_encoder(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  if ((int )dig->linkb) {
    return (1);
  } else {
    return (0);
  }
  case 32U: ;
  if ((int )dig->linkb) {
    return (3);
  } else {
    return (2);
  }
  case 33U: ;
  if ((int )dig->linkb) {
    return (5);
  } else {
    return (4);
  }
  case 37U: ;
  return (6);
  default:
  drm_err("invalid encoder_id: 0x%x\n", amdgpu_encoder->encoder_id);
  return (0);
  }
}
}
static u32 dce_v10_0_pick_pll(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 pll_in_use ;
  int pll ;
  int tmp ;
  int tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
  if (tmp == 0) {
    goto _L;
  } else {
    tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___0 == 5) {
      _L:
      if (adev->clock.dp_extclk != 0U) {
        return (255U);
      } else {
        pll = amdgpu_pll_get_shared_dp_ppll(crtc);
        if (pll != 255) {
          return ((u32 )pll);
        } else {
        }
      }
    } else {
      pll = amdgpu_pll_get_shared_nondp_ppll(crtc);
      if (pll != 255) {
        return ((u32 )pll);
      } else {
      }
    }
  }
  pll_in_use = amdgpu_pll_get_use_mask(crtc);
  if ((pll_in_use & 2U) == 0U) {
    return (1U);
  } else {
  }
  if ((pll_in_use & 1U) == 0U) {
    return (0U);
  } else {
  }
  if ((pll_in_use & 4U) == 0U) {
    return (2U);
  } else {
  }
  drm_err("unable to allocate a PPLL\n");
  return (255U);
}
}
static void dce_v10_0_lock_cursor(struct drm_crtc *crtc , bool lock )
{
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  u32 cur_lock ;
  {
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  cur_lock = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6766U, 0);
  if ((int )lock) {
    cur_lock = cur_lock | 65536U;
  } else {
    cur_lock = cur_lock & 4294901759U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6766U, cur_lock, 0);
  return;
}
}
static void dce_v10_0_hide_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6758U, 1);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, tmp, 1);
  return;
}
}
static void dce_v10_0_show_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6758U, 1);
  tmp = tmp | 1U;
  tmp = (tmp & 4294966527U) | 512U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, tmp, 1);
  return;
}
}
static void dce_v10_0_set_cursor(struct drm_crtc *crtc , struct drm_gem_object *obj ,
                                 uint64_t gpu_addr )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6761U, (unsigned int )(gpu_addr >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6759U, (unsigned int )gpu_addr,
                 0);
  return;
}
}
static int dce_v10_0_crtc_cursor_move(struct drm_crtc *crtc , int x , int y )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  int xorigin ;
  int yorigin ;
  long tmp ;
  int _min1 ;
  int _min2 ;
  int _min1___0 ;
  int _min2___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  xorigin = 0;
  yorigin = 0;
  x = crtc->x + x;
  y = crtc->y + y;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v10_0_crtc_cursor_move", "x %d y %d c->x %d c->y %d\n",
                        x, y, crtc->x, crtc->y);
  } else {
  }
  if (x < 0) {
    _min1 = - x;
    _min2 = amdgpu_crtc->max_cursor_width + -1;
    xorigin = _min1 < _min2 ? _min1 : _min2;
    x = 0;
  } else {
  }
  if (y < 0) {
    _min1___0 = - y;
    _min2___0 = amdgpu_crtc->max_cursor_height + -1;
    yorigin = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    y = 0;
  } else {
  }
  dce_v10_0_lock_cursor(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6762U, (u32 )((x << 16) | y), 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6763U, (u32 )((xorigin << 16) | yorigin),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6760U, (u32 )(((amdgpu_crtc->cursor_width + -1) << 16) | (amdgpu_crtc->cursor_height + -1)),
                 0);
  dce_v10_0_lock_cursor(crtc, 0);
  return (0);
}
}
static int dce_v10_0_crtc_cursor_set(struct drm_crtc *crtc , struct drm_file *file_priv ,
                                     u32 handle , u32 width , u32 height )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *robj ;
  uint64_t gpu_addr ;
  int ret ;
  struct drm_gem_object const *__mptr___0 ;
  long tmp ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (handle == 0U) {
    dce_v10_0_hide_cursor(crtc);
    obj = (struct drm_gem_object *)0;
    goto unpin;
  } else {
  }
  if ((u32 )amdgpu_crtc->max_cursor_width < width || (u32 )amdgpu_crtc->max_cursor_height < height) {
    drm_err("bad cursor width or height %d x %d\n", width, height);
    return (-22);
  } else {
  }
  obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    drm_err("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
    return (-2);
  } else {
  }
  __mptr___0 = (struct drm_gem_object const *)obj;
  robj = (struct amdgpu_bo *)__mptr___0 + 0xfffffffffffffbc0UL;
  ret = amdgpu_bo_reserve(robj, 0);
  tmp = ldv__builtin_expect(ret != 0, 0L);
  if (tmp != 0L) {
    goto fail;
  } else {
  }
  ret = amdgpu_bo_pin_restricted(robj, 4U, 0ULL, 0ULL, & gpu_addr);
  amdgpu_bo_unreserve(robj);
  if (ret != 0) {
    goto fail;
  } else {
  }
  amdgpu_crtc->cursor_width = (int )width;
  amdgpu_crtc->cursor_height = (int )height;
  dce_v10_0_lock_cursor(crtc, 1);
  dce_v10_0_set_cursor(crtc, obj, gpu_addr);
  dce_v10_0_show_cursor(crtc);
  dce_v10_0_lock_cursor(crtc, 0);
  unpin: ;
  if ((unsigned long )amdgpu_crtc->cursor_bo != (unsigned long )((struct drm_gem_object *)0)) {
    __mptr___1 = (struct drm_gem_object const *)amdgpu_crtc->cursor_bo;
    robj = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    ret = amdgpu_bo_reserve(robj, 0);
    tmp___0 = ldv__builtin_expect(ret == 0, 1L);
    if (tmp___0 != 0L) {
      amdgpu_bo_unpin(robj);
      amdgpu_bo_unreserve(robj);
    } else {
    }
    drm_gem_object_unreference_unlocked___5(amdgpu_crtc->cursor_bo);
  } else {
  }
  amdgpu_crtc->cursor_bo = obj;
  return (0);
  fail:
  drm_gem_object_unreference_unlocked___5(obj);
  return (ret);
}
}
static void dce_v10_0_crtc_gamma_set(struct drm_crtc *crtc , u16 *red , u16 *green ,
                                     u16 *blue , u32 start , u32 size )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  int end ;
  int i ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  end = (int )(256U < start + size ? 256U : start + size);
  i = (int )start;
  goto ldv_50673;
  ldv_50672:
  amdgpu_crtc->lut_r[i] = (u16 )((int )*(red + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_g[i] = (u16 )((int )*(green + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_b[i] = (u16 )((int )*(blue + (unsigned long )i) >> 6);
  i = i + 1;
  ldv_50673: ;
  if (i < end) {
    goto ldv_50672;
  } else {
  }
  dce_v10_0_crtc_load_lut(crtc);
  return;
}
}
static void dce_v10_0_crtc_destroy(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  drm_crtc_cleanup(crtc);
  ldv_destroy_workqueue_898(amdgpu_crtc->pflip_queue);
  kfree((void const *)amdgpu_crtc);
  return;
}
}
static struct drm_crtc_funcs const dce_v10_0_crtc_funcs =
     {0, 0, 0, & dce_v10_0_crtc_cursor_set, 0, & dce_v10_0_crtc_cursor_move, & dce_v10_0_crtc_gamma_set,
    & dce_v10_0_crtc_destroy, & amdgpu_crtc_set_config, & amdgpu_crtc_page_flip, 0,
    0, 0, 0, 0};
static void dce_v10_0_crtc_dpms(struct drm_crtc *crtc , int mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  switch (mode) {
  case 0:
  amdgpu_crtc->enabled = 1;
  amdgpu_atombios_crtc_enable(crtc, 1);
  dce_v10_0_vga_enable(crtc, 1);
  amdgpu_atombios_crtc_blank(crtc, 0);
  dce_v10_0_vga_enable(crtc, 0);
  drm_vblank_post_modeset(dev, amdgpu_crtc->crtc_id);
  dce_v10_0_crtc_load_lut(crtc);
  goto ldv_50692;
  case 1: ;
  case 2: ;
  case 3:
  drm_vblank_pre_modeset(dev, amdgpu_crtc->crtc_id);
  if ((int )amdgpu_crtc->enabled) {
    dce_v10_0_vga_enable(crtc, 1);
    amdgpu_atombios_crtc_blank(crtc, 1);
    dce_v10_0_vga_enable(crtc, 0);
  } else {
  }
  amdgpu_atombios_crtc_enable(crtc, 0);
  amdgpu_crtc->enabled = 0;
  goto ldv_50692;
  }
  ldv_50692:
  amdgpu_pm_compute_clocks(adev);
  return;
}
}
static void dce_v10_0_crtc_prepare(struct drm_crtc *crtc )
{
  {
  amdgpu_atombios_crtc_powergate(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 1);
  dce_v10_0_crtc_dpms(crtc, 3);
  return;
}
}
static void dce_v10_0_crtc_commit(struct drm_crtc *crtc )
{
  {
  dce_v10_0_crtc_dpms(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 0);
  return;
}
}
static void dce_v10_0_crtc_disable(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_atom_ss ss ;
  int i ;
  int r ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct amdgpu_bo *rbo ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  dce_v10_0_crtc_dpms(crtc, 3);
  if ((unsigned long )(crtc->primary)->fb != (unsigned long )((struct drm_framebuffer *)0)) {
    __mptr___0 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    __mptr___1 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      drm_err("failed to reserve rbo before unpin\n");
    } else {
      amdgpu_bo_unpin(rbo);
      amdgpu_bo_unreserve(rbo);
    }
  } else {
  }
  dce_v10_0_grph_enable(crtc, 0);
  amdgpu_atombios_crtc_powergate(crtc, 1);
  i = 0;
  goto ldv_50721;
  ldv_50720: ;
  if ((((unsigned long )adev->mode_info.crtcs[i] != (unsigned long )((struct amdgpu_crtc *)0) && (int )(adev->mode_info.crtcs[i])->enabled) && amdgpu_crtc->crtc_id != i) && amdgpu_crtc->pll_id == (adev->mode_info.crtcs[i])->pll_id) {
    goto done;
  } else {
  }
  i = i + 1;
  ldv_50721: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50720;
  } else {
  }
  switch (amdgpu_crtc->pll_id) {
  case 2U: ;
  case 0U: ;
  case 1U:
  amdgpu_atombios_crtc_program_pll(crtc, (u32 )amdgpu_crtc->crtc_id, (int )amdgpu_crtc->pll_id,
                                   0U, 0U, 0U, 0U, 0U, 0U, 0U, 0, 0, & ss);
  goto ldv_50726;
  default: ;
  goto ldv_50726;
  }
  ldv_50726: ;
  done:
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  return;
}
}
static int dce_v10_0_crtc_mode_set(struct drm_crtc *crtc , struct drm_display_mode *mode ,
                                   struct drm_display_mode *adjusted_mode , int x ,
                                   int y , struct drm_framebuffer *old_fb )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (amdgpu_crtc->adjusted_clock == 0U) {
    return (-22);
  } else {
  }
  amdgpu_atombios_crtc_set_pll(crtc, adjusted_mode);
  amdgpu_atombios_crtc_set_dtd_timing(crtc, adjusted_mode);
  dce_v10_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  amdgpu_atombios_crtc_overscan_setup(crtc, mode, adjusted_mode);
  amdgpu_atombios_crtc_scaler_setup(crtc);
  amdgpu_crtc->hw_mode = *adjusted_mode;
  return (0);
}
}
static bool dce_v10_0_crtc_mode_fixup(struct drm_crtc *crtc , struct drm_display_mode const *mode ,
                                      struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  __mptr___0 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___0 + 0xfffffffffffffff8UL;
  goto ldv_50755;
  ldv_50754: ;
  if ((unsigned long )encoder->crtc == (unsigned long )crtc) {
    amdgpu_crtc->encoder = encoder;
    amdgpu_crtc->connector = amdgpu_get_connector_for_encoder(encoder);
    goto ldv_50753;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_50755: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_50754;
  } else {
  }
  ldv_50753: ;
  if ((unsigned long )amdgpu_crtc->encoder == (unsigned long )((struct drm_encoder *)0) || (unsigned long )amdgpu_crtc->connector == (unsigned long )((struct drm_connector *)0)) {
    amdgpu_crtc->encoder = (struct drm_encoder *)0;
    amdgpu_crtc->connector = (struct drm_connector *)0;
    return (0);
  } else {
  }
  tmp = amdgpu_crtc_scaling_mode_fixup(crtc, mode, adjusted_mode);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (0);
  } else {
  }
  tmp___1 = amdgpu_atombios_crtc_prepare_pll(crtc, adjusted_mode);
  if (tmp___1 != 0) {
    return (0);
  } else {
  }
  amdgpu_crtc->pll_id = dce_v10_0_pick_pll(crtc);
  if (amdgpu_crtc->pll_id == 255U) {
    tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___2 != 0) {
      tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
      if (tmp___3 != 5) {
        return (0);
      } else {
      }
    } else {
    }
  } else {
  }
  return (1);
}
}
static int dce_v10_0_crtc_set_base(struct drm_crtc *crtc , int x , int y , struct drm_framebuffer *old_fb )
{
  int tmp ;
  {
  tmp = dce_v10_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  return (tmp);
}
}
static int dce_v10_0_crtc_set_base_atomic(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                          int x , int y , enum mode_set_atomic state )
{
  int tmp ;
  {
  tmp = dce_v10_0_crtc_do_set_base(crtc, fb, x, y, 1);
  return (tmp);
}
}
static struct drm_crtc_helper_funcs const dce_v10_0_crtc_helper_funcs =
     {& dce_v10_0_crtc_dpms, & dce_v10_0_crtc_prepare, & dce_v10_0_crtc_commit, & dce_v10_0_crtc_mode_fixup,
    & dce_v10_0_crtc_mode_set, 0, & dce_v10_0_crtc_set_base, & dce_v10_0_crtc_set_base_atomic,
    & dce_v10_0_crtc_load_lut, & dce_v10_0_crtc_disable, 0, 0, 0, 0};
static int dce_v10_0_crtc_init(struct amdgpu_device *adev , int index )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  int i ;
  void *tmp ;
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp___0 ;
  {
  tmp = kzalloc(3312UL, 208U);
  amdgpu_crtc = (struct amdgpu_crtc *)tmp;
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (-12);
  } else {
  }
  drm_crtc_init(adev->ddev, & amdgpu_crtc->base, & dce_v10_0_crtc_funcs);
  drm_mode_crtc_set_gamma_size(& amdgpu_crtc->base, 256);
  amdgpu_crtc->crtc_id = index;
  __lock_name = "\"%s\"\"amdgpu-pageflip-queue\"";
  tmp___0 = __alloc_workqueue_key("%s", 131082U, 1, & __key, __lock_name, (char *)"amdgpu-pageflip-queue");
  amdgpu_crtc->pflip_queue = tmp___0;
  adev->mode_info.crtcs[index] = amdgpu_crtc;
  amdgpu_crtc->max_cursor_width = 128;
  amdgpu_crtc->max_cursor_height = 128;
  (adev->ddev)->mode_config.cursor_width = (u32 )amdgpu_crtc->max_cursor_width;
  (adev->ddev)->mode_config.cursor_height = (u32 )amdgpu_crtc->max_cursor_height;
  i = 0;
  goto ldv_50780;
  ldv_50779:
  amdgpu_crtc->lut_r[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_g[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_b[i] = (int )((u16 )i) << 2U;
  i = i + 1;
  ldv_50780: ;
  if (i <= 255) {
    goto ldv_50779;
  } else {
  }
  switch (amdgpu_crtc->crtc_id) {
  case 0: ;
  default:
  amdgpu_crtc->crtc_offset = 0U;
  goto ldv_50784;
  case 1:
  amdgpu_crtc->crtc_offset = 512U;
  goto ldv_50784;
  case 2:
  amdgpu_crtc->crtc_offset = 1024U;
  goto ldv_50784;
  case 3:
  amdgpu_crtc->crtc_offset = 9728U;
  goto ldv_50784;
  case 4:
  amdgpu_crtc->crtc_offset = 10240U;
  goto ldv_50784;
  case 5:
  amdgpu_crtc->crtc_offset = 10752U;
  goto ldv_50784;
  }
  ldv_50784:
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  drm_crtc_helper_add(& amdgpu_crtc->base, & dce_v10_0_crtc_helper_funcs);
  return (0);
}
}
static int dce_v10_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->audio_endpt_rreg = & dce_v10_0_audio_endpt_rreg;
  adev->audio_endpt_wreg = & dce_v10_0_audio_endpt_wreg;
  dce_v10_0_set_display_funcs(adev);
  dce_v10_0_set_irq_funcs(adev);
  switch ((unsigned int )adev->asic_type) {
  case 6U:
  adev->mode_info.num_crtc = 6;
  adev->mode_info.num_hpd = 6;
  adev->mode_info.num_dig = 7;
  goto ldv_50795;
  default: ;
  return (-22);
  }
  ldv_50795: ;
  return (0);
}
}
static int dce_v10_0_sw_init(void *handle )
{
  int r ;
  int i ;
  struct amdgpu_device *adev ;
  bool tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0;
  goto ldv_50804;
  ldv_50803:
  r = amdgpu_irq_add_id(adev, (unsigned int )(i + 1), & adev->crtc_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_50804: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50803;
  } else {
  }
  i = 8;
  goto ldv_50807;
  ldv_50806:
  r = amdgpu_irq_add_id(adev, (unsigned int )i, & adev->pageflip_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 2;
  ldv_50807: ;
  if (i <= 19) {
    goto ldv_50806;
  } else {
  }
  r = amdgpu_irq_add_id(adev, 42U, & adev->hpd_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->mode_info.mode_config_initialized = 1;
  (adev->ddev)->mode_config.funcs = & amdgpu_mode_funcs;
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  (adev->ddev)->mode_config.preferred_depth = 24U;
  (adev->ddev)->mode_config.prefer_shadow = 1U;
  (adev->ddev)->mode_config.fb_base = adev->mc.aper_base;
  r = amdgpu_modeset_create_props(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  i = 0;
  goto ldv_50810;
  ldv_50809:
  r = dce_v10_0_crtc_init(adev, i);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_50810: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_50809;
  } else {
  }
  tmp = amdgpu_atombios_get_connector_info_from_object_table(adev);
  if ((int )tmp) {
    amdgpu_print_display_setup(adev->ddev);
  } else {
    return (-22);
  }
  dce_v10_0_afmt_init(adev);
  r = dce_v10_0_audio_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  drm_kms_helper_poll_init(adev->ddev);
  return (r);
}
}
static int dce_v10_0_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  kfree((void const *)adev->mode_info.bios_hardcoded_edid);
  drm_kms_helper_poll_fini(adev->ddev);
  dce_v10_0_audio_fini(adev);
  dce_v10_0_afmt_fini(adev);
  drm_mode_config_cleanup(adev->ddev);
  adev->mode_info.mode_config_initialized = 0;
  return (0);
}
}
static int dce_v10_0_hw_init(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v10_0_init_golden_registers(adev);
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  dce_v10_0_hpd_init(adev);
  i = 0;
  goto ldv_50822;
  ldv_50821:
  dce_v10_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_50822: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50821;
  } else {
  }
  return (0);
}
}
static int dce_v10_0_hw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v10_0_hpd_fini(adev);
  i = 0;
  goto ldv_50830;
  ldv_50829:
  dce_v10_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_50830: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_50829;
  } else {
  }
  return (0);
}
}
static int dce_v10_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_atombios_scratch_regs_save(adev);
  dce_v10_0_hpd_fini(adev);
  return (0);
}
}
static int dce_v10_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  u8 bl_level ;
  u8 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v10_0_init_golden_registers(adev);
  amdgpu_atombios_scratch_regs_restore(adev);
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  if ((unsigned long )adev->mode_info.bl_encoder != (unsigned long )((struct amdgpu_encoder *)0)) {
    tmp = (*((adev->mode_info.funcs)->backlight_get_level))(adev->mode_info.bl_encoder);
    bl_level = tmp;
    (*((adev->mode_info.funcs)->backlight_set_level))(adev->mode_info.bl_encoder,
                                                      (int )bl_level);
  } else {
  }
  dce_v10_0_hpd_init(adev);
  return (0);
}
}
static bool dce_v10_0_is_idle(void *handle )
{
  {
  return (1);
}
}
static int dce_v10_0_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void dce_v10_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "DCE 10.x registers\n");
  return;
}
}
static int dce_v10_0_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  bool tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = dce_v10_0_is_display_hung(adev);
  if ((int )tmp___0) {
    srbm_soft_reset = srbm_soft_reset | 32U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    dce_v10_0_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    dce_v10_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static void dce_v10_0_set_crtc_vblank_interrupt_state(struct amdgpu_device *adev ,
                                                      int crtc , enum amdgpu_interrupt_state state )
{
  u32 lb_interrupt_mask ;
  long tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v10_0_set_crtc_vblank_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_50865;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_50865;
  default: ;
  goto ldv_50865;
  }
  ldv_50865: ;
  return;
}
}
static void dce_v10_0_set_crtc_vline_interrupt_state(struct amdgpu_device *adev ,
                                                     int crtc , enum amdgpu_interrupt_state state )
{
  u32 lb_interrupt_mask ;
  long tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v10_0_set_crtc_vline_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967279U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_50876;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_50876;
  default: ;
  goto ldv_50876;
  }
  ldv_50876: ;
  return;
}
}
static int dce_v10_0_set_hpd_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                       unsigned int hpd , enum amdgpu_interrupt_state state )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if ((unsigned int )adev->mode_info.num_hpd <= hpd) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_set_hpd_irq_state", "invalid hdp %d\n", hpd);
    } else {
    }
    return (0);
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, 0);
  tmp = tmp & 4294901759U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, tmp, 0);
  goto ldv_50888;
  case 1U:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, 0);
  tmp = tmp | 65536U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, tmp, 0);
  goto ldv_50888;
  default: ;
  goto ldv_50888;
  }
  ldv_50888: ;
  return (0);
}
}
static int dce_v10_0_set_crtc_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  switch (type) {
  case 0U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 0, state);
  goto ldv_50898;
  case 1U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 1, state);
  goto ldv_50898;
  case 2U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 2, state);
  goto ldv_50898;
  case 3U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 3, state);
  goto ldv_50898;
  case 4U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 4, state);
  goto ldv_50898;
  case 5U:
  dce_v10_0_set_crtc_vblank_interrupt_state(adev, 5, state);
  goto ldv_50898;
  case 6U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 0, state);
  goto ldv_50898;
  case 7U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 1, state);
  goto ldv_50898;
  case 8U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 2, state);
  goto ldv_50898;
  case 9U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 3, state);
  goto ldv_50898;
  case 10U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 4, state);
  goto ldv_50898;
  case 11U:
  dce_v10_0_set_crtc_vline_interrupt_state(adev, 5, state);
  goto ldv_50898;
  default: ;
  goto ldv_50898;
  }
  ldv_50898: ;
  return (0);
}
}
static int dce_v10_0_set_pageflip_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                            unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 reg ;
  u32 reg_block ;
  {
  switch (type) {
  case 0U:
  reg_block = 0U;
  goto ldv_50920;
  case 1U:
  reg_block = 512U;
  goto ldv_50920;
  case 2U:
  reg_block = 1024U;
  goto ldv_50920;
  case 3U:
  reg_block = 9728U;
  goto ldv_50920;
  case 4U:
  reg_block = 10240U;
  goto ldv_50920;
  case 5U:
  reg_block = 10752U;
  goto ldv_50920;
  default:
  drm_err("invalid pageflip crtc %d\n", type);
  return (-22);
  }
  ldv_50920:
  reg = amdgpu_mm_rreg(adev, reg_block + 6679U, 0);
  if ((unsigned int )state == 0U) {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg & 4294967294U, 0);
  } else {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg | 1U, 0);
  }
  return (0);
}
}
static int dce_v10_0_pageflip_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                  struct amdgpu_iv_entry *entry )
{
  int reg_block ;
  unsigned long flags ;
  unsigned int crtc_id ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct amdgpu_flip_work *works ;
  u32 tmp ;
  raw_spinlock_t *tmp___0 ;
  long tmp___1 ;
  {
  crtc_id = (entry->src_id - 8U) >> 1;
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  switch (crtc_id) {
  case 0U:
  reg_block = 0;
  goto ldv_50938;
  case 1U:
  reg_block = 512;
  goto ldv_50938;
  case 2U:
  reg_block = 1024;
  goto ldv_50938;
  case 3U:
  reg_block = 9728;
  goto ldv_50938;
  case 4U:
  reg_block = 10240;
  goto ldv_50938;
  case 5U:
  reg_block = 10752;
  goto ldv_50938;
  default:
  drm_err("invalid pageflip crtc %d\n", crtc_id);
  return (-22);
  }
  ldv_50938:
  tmp = amdgpu_mm_rreg(adev, (u32 )(reg_block + 6678), 0);
  if ((int )tmp & 1) {
    amdgpu_mm_wreg(adev, (u32 )(reg_block + 6678), 256U, 0);
  } else {
  }
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (0);
  } else {
  }
  tmp___0 = spinlock_check(& (adev->ddev)->event_lock);
  flags = _raw_spin_lock_irqsave(tmp___0);
  works = amdgpu_crtc->pflip_works;
  if ((unsigned int )amdgpu_crtc->pflip_status != 2U) {
    tmp___1 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("dce_v10_0_pageflip_irq", "amdgpu_crtc->pflip_status = %d != AMDGPU_FLIP_SUBMITTED(%d)\n",
                          (unsigned int )amdgpu_crtc->pflip_status, 2);
    } else {
    }
    spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
    return (0);
  } else {
  }
  amdgpu_crtc->pflip_status = 0;
  amdgpu_crtc->pflip_works = (struct amdgpu_flip_work *)0;
  if ((unsigned long )works->event != (unsigned long )((struct drm_pending_vblank_event *)0)) {
    drm_send_vblank_event(adev->ddev, (int )crtc_id, works->event);
  } else {
  }
  spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
  drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
  amdgpu_irq_put(adev, & adev->pageflip_irq, crtc_id);
  queue_work___5(amdgpu_crtc->pflip_queue, & works->unpin_work);
  return (0);
}
}
static void dce_v10_0_hpd_int_ack(struct amdgpu_device *adev , int hpd )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_hpd <= hpd) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_hpd_int_ack", "invalid hdp %d\n", hpd);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets[hpd] + 6297U, tmp, 0);
  return;
}
}
static void dce_v10_0_crtc_vblank_int_ack(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_vblank_int_ack", "invalid crtc %d\n", crtc);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6859U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6859U, tmp, 0);
  return;
}
}
static void dce_v10_0_crtc_vline_int_ack(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_vline_int_ack", "invalid crtc %d\n", crtc);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6857U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___0[crtc] + 6857U, tmp, 0);
  return;
}
}
static int dce_v10_0_crtc_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                              struct amdgpu_iv_entry *entry )
{
  unsigned int crtc ;
  u32 disp_int ;
  u32 tmp ;
  unsigned int irq_type ;
  int tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  {
  crtc = entry->src_id - 1U;
  tmp = amdgpu_mm_rreg(adev, interrupt_status_offsets___0[crtc].reg, 0);
  disp_int = tmp;
  tmp___0 = amdgpu_crtc_idx_to_irq_type(adev, (int )crtc);
  irq_type = (unsigned int )tmp___0;
  switch (entry->src_data) {
  case 0U: ;
  if (((u32 )interrupt_status_offsets___0[crtc].vblank & disp_int) != 0U) {
    dce_v10_0_crtc_vblank_int_ack(adev, (int )crtc);
    tmp___1 = amdgpu_irq_enabled(adev, source, irq_type);
    if ((int )tmp___1) {
      drm_handle_vblank(adev->ddev, (int )crtc);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_irq", "IH: D%d vblank\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_50977;
  case 1U: ;
  if (((u32 )interrupt_status_offsets___0[crtc].vline & disp_int) != 0U) {
    dce_v10_0_crtc_vline_int_ack(adev, (int )crtc);
    tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___3 != 0L) {
      drm_ut_debug_printk("dce_v10_0_crtc_irq", "IH: D%d vline\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_50977;
  default:
  tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___4 != 0L) {
    drm_ut_debug_printk("dce_v10_0_crtc_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                        entry->src_data);
  } else {
  }
  goto ldv_50977;
  }
  ldv_50977: ;
  return (0);
}
}
static int dce_v10_0_hpd_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                             struct amdgpu_iv_entry *entry )
{
  u32 disp_int ;
  u32 mask ;
  unsigned int hpd ;
  long tmp ;
  long tmp___0 ;
  {
  if (entry->src_data >= (unsigned int )adev->mode_info.num_hpd) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v10_0_hpd_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                          entry->src_data);
    } else {
    }
    return (0);
  } else {
  }
  hpd = entry->src_data;
  disp_int = amdgpu_mm_rreg(adev, interrupt_status_offsets___0[hpd].reg, 0);
  mask = interrupt_status_offsets___0[hpd].hpd;
  if ((disp_int & mask) != 0U) {
    dce_v10_0_hpd_int_ack(adev, (int )hpd);
    schedule_work___4(& adev->hotplug_work);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v10_0_hpd_irq", "IH: HPD%d\n", hpd + 1U);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int dce_v10_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int dce_v10_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const dce_v10_0_ip_funcs =
     {& dce_v10_0_early_init, (int (*)(void * ))0, & dce_v10_0_sw_init, & dce_v10_0_sw_fini,
    & dce_v10_0_hw_init, & dce_v10_0_hw_fini, & dce_v10_0_suspend, & dce_v10_0_resume,
    & dce_v10_0_is_idle, & dce_v10_0_wait_for_idle, & dce_v10_0_soft_reset, & dce_v10_0_print_status,
    & dce_v10_0_set_clockgating_state, & dce_v10_0_set_powergating_state};
static void dce_v10_0_encoder_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                       struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  int tmp ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_encoder->pixel_clock = (u32 )adjusted_mode->clock;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  dce_v10_0_set_interleave(encoder->crtc, mode);
  tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  if (tmp == 3) {
    dce_v10_0_afmt_enable(encoder, 1);
    dce_v10_0_afmt_setmode(encoder, adjusted_mode);
  } else {
  }
  return;
}
}
static void dce_v10_0_encoder_prepare(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  struct amdgpu_encoder_atom_dig *dig ;
  u16 tmp___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  if (((long )amdgpu_encoder->active_device & 3818L) != 0L) {
    goto _L;
  } else {
    tmp___0 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    if ((unsigned int )tmp___0 != 0U) {
      _L:
      dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
      if ((unsigned long )dig != (unsigned long )((struct amdgpu_encoder_atom_dig *)0)) {
        dig->dig_encoder = dce_v10_0_pick_dig_encoder(encoder);
        if (((long )amdgpu_encoder->active_device & 3784L) != 0L) {
          dig->afmt = adev->mode_info.afmt[dig->dig_encoder];
        } else {
        }
      } else {
      }
    } else {
    }
  }
  amdgpu_atombios_scratch_regs_lock(adev, 1);
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    if ((int )amdgpu_connector->router.cd_valid) {
      amdgpu_i2c_router_select_cd_port(amdgpu_connector);
    } else {
    }
    if (connector->connector_type == 14) {
      amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
    } else {
    }
  } else {
  }
  amdgpu_atombios_encoder_set_crtc_source(encoder);
  dce_v10_0_program_fmt(encoder);
  return;
}
}
static void dce_v10_0_encoder_commit(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_atombios_encoder_dpms(encoder, 0);
  amdgpu_atombios_scratch_regs_lock(adev, 0);
  return;
}
}
static void dce_v10_0_encoder_disable(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  int tmp ;
  bool tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  tmp___0 = amdgpu_atombios_encoder_is_digital(encoder);
  if ((int )tmp___0) {
    tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp == 3) {
      dce_v10_0_afmt_enable(encoder, 0);
    } else {
    }
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    dig->dig_encoder = -1;
  } else {
  }
  amdgpu_encoder->active_device = 0U;
  return;
}
}
static void dce_v10_0_ext_prepare(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v10_0_ext_commit(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v10_0_ext_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                   struct drm_display_mode *adjusted_mode )
{
  {
  return;
}
}
static void dce_v10_0_ext_disable(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v10_0_ext_dpms(struct drm_encoder *encoder , int mode )
{
  {
  return;
}
}
static bool dce_v10_0_ext_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode const *mode ,
                                     struct drm_display_mode *adjusted_mode )
{
  {
  return (1);
}
}
static struct drm_encoder_helper_funcs const dce_v10_0_ext_helper_funcs =
     {& dce_v10_0_ext_dpms, 0, 0, & dce_v10_0_ext_mode_fixup, & dce_v10_0_ext_prepare,
    & dce_v10_0_ext_commit, & dce_v10_0_ext_mode_set, 0, 0, & dce_v10_0_ext_disable,
    0, 0};
static struct drm_encoder_helper_funcs const dce_v10_0_dig_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v10_0_encoder_prepare,
    & dce_v10_0_encoder_commit, & dce_v10_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dig_detect,
    & dce_v10_0_encoder_disable, 0, 0};
static struct drm_encoder_helper_funcs const dce_v10_0_dac_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v10_0_encoder_prepare,
    & dce_v10_0_encoder_commit, & dce_v10_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dac_detect,
    0, 0, 0};
static void dce_v10_0_encoder_destroy(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_atombios_encoder_fini_backlight(amdgpu_encoder);
  } else {
  }
  kfree((void const *)amdgpu_encoder->enc_priv);
  drm_encoder_cleanup(encoder);
  kfree((void const *)amdgpu_encoder);
  return;
}
}
static struct drm_encoder_funcs const dce_v10_0_encoder_funcs = {0, & dce_v10_0_encoder_destroy};
static void dce_v10_0_encoder_add(struct amdgpu_device *adev , u32 encoder_enum ,
                                  u32 supported_device , u16 caps )
{
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct list_head const *__mptr ;
  struct drm_encoder const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  void *tmp ;
  struct amdgpu_encoder_atom_dig *tmp___0 ;
  struct amdgpu_encoder_atom_dig *tmp___1 ;
  struct amdgpu_encoder_atom_dig *tmp___2 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_51079;
  ldv_51078:
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  if (amdgpu_encoder->encoder_enum == encoder_enum) {
    amdgpu_encoder->devices = amdgpu_encoder->devices | supported_device;
    return;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_51079: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_51078;
  } else {
  }
  tmp = kzalloc(360UL, 208U);
  amdgpu_encoder = (struct amdgpu_encoder *)tmp;
  if ((unsigned long )amdgpu_encoder == (unsigned long )((struct amdgpu_encoder *)0)) {
    return;
  } else {
  }
  encoder = & amdgpu_encoder->base;
  switch (adev->mode_info.num_crtc) {
  case 1:
  encoder->possible_crtcs = 1U;
  goto ldv_51082;
  case 2: ;
  default:
  encoder->possible_crtcs = 3U;
  goto ldv_51082;
  case 4:
  encoder->possible_crtcs = 15U;
  goto ldv_51082;
  case 6:
  encoder->possible_crtcs = 63U;
  goto ldv_51082;
  }
  ldv_51082:
  amdgpu_encoder->enc_priv = (void *)0;
  amdgpu_encoder->encoder_enum = encoder_enum;
  amdgpu_encoder->encoder_id = encoder_enum & 255U;
  amdgpu_encoder->devices = supported_device;
  amdgpu_encoder->rmx_type = 0;
  amdgpu_encoder->underscan_type = 0;
  amdgpu_encoder->is_ext_encoder = 0;
  amdgpu_encoder->caps = caps;
  switch (amdgpu_encoder->encoder_id) {
  case 21U: ;
  case 22U:
  drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 1);
  drm_encoder_helper_add(encoder, & dce_v10_0_dac_helper_funcs);
  goto ldv_51089;
  case 20U: ;
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_encoder->rmx_type = 1;
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 3);
    tmp___0 = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___0;
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 1);
    tmp___1 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___1;
  } else {
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 2);
    tmp___2 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___2;
  }
  drm_encoder_helper_add(encoder, & dce_v10_0_dig_helper_funcs);
  goto ldv_51089;
  case 8U: ;
  case 9U: ;
  case 12U: ;
  case 13U: ;
  case 14U: ;
  case 16U: ;
  case 17U: ;
  case 35U: ;
  case 34U:
  amdgpu_encoder->is_ext_encoder = 1;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 3);
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 1);
  } else {
    drm_encoder_init(dev, encoder, & dce_v10_0_encoder_funcs, 2);
  }
  drm_encoder_helper_add(encoder, & dce_v10_0_ext_helper_funcs);
  goto ldv_51089;
  }
  ldv_51089: ;
  return;
}
}
static struct amdgpu_display_funcs const dce_v10_0_display_funcs =
     {& dce_v10_0_set_vga_render_state, & dce_v10_0_bandwidth_update, & dce_v10_0_vblank_get_counter,
    & dce_v10_0_vblank_wait, & dce_v10_0_is_display_hung, & amdgpu_atombios_encoder_set_backlight_level,
    & amdgpu_atombios_encoder_get_backlight_level, & dce_v10_0_hpd_sense, & dce_v10_0_hpd_set_polarity,
    & dce_v10_0_hpd_get_gpio_reg, & dce_v10_0_page_flip, & dce_v10_0_crtc_get_scanoutpos,
    & dce_v10_0_encoder_add, & amdgpu_connector_add, & dce_v10_0_stop_mc_access, & dce_v10_0_resume_mc_access};
static void dce_v10_0_set_display_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.funcs == (unsigned long )((struct amdgpu_display_funcs const *)0)) {
    adev->mode_info.funcs = & dce_v10_0_display_funcs;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const dce_v10_0_crtc_irq_funcs = {& dce_v10_0_set_crtc_irq_state, & dce_v10_0_crtc_irq};
static struct amdgpu_irq_src_funcs const dce_v10_0_pageflip_irq_funcs = {& dce_v10_0_set_pageflip_irq_state, & dce_v10_0_pageflip_irq};
static struct amdgpu_irq_src_funcs const dce_v10_0_hpd_irq_funcs = {& dce_v10_0_set_hpd_irq_state, & dce_v10_0_hpd_irq};
static void dce_v10_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->crtc_irq.num_types = 12U;
  adev->crtc_irq.funcs = & dce_v10_0_crtc_irq_funcs;
  adev->pageflip_irq.num_types = 6U;
  adev->pageflip_irq.funcs = & dce_v10_0_pageflip_irq_funcs;
  adev->hpd_irq.num_types = 6U;
  adev->hpd_irq.funcs = & dce_v10_0_hpd_irq_funcs;
  return;
}
}
int ldv_retval_75 ;
extern int ldv_probe_61(void) ;
extern int ldv_release_58(void) ;
int ldv_retval_74 ;
extern int ldv_bind_60(void) ;
extern int ldv_release_59(void) ;
extern int ldv_probe_55(void) ;
extern int ldv_probe_59(void) ;
extern int ldv_connect_58(void) ;
extern int ldv_connect_57(void) ;
extern int ldv_bind_57(void) ;
extern int ldv_connect_60(void) ;
extern int ldv_release_57(void) ;
extern int ldv_bind_58(void) ;
extern int ldv_release_60(void) ;
void ldv_initialize_drm_encoder_helper_funcs_58(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v10_0_ext_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v10_0_ext_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_display_funcs_54(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v10_0_display_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(16UL);
  dce_v10_0_display_funcs_group1 = (struct amdgpu_mode_mc_save *)tmp___0;
  tmp___1 = ldv_init_zalloc(360UL);
  dce_v10_0_display_funcs_group2 = (struct amdgpu_encoder *)tmp___1;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_51(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v10_0_hpd_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v10_0_hpd_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_53(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v10_0_crtc_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v10_0_crtc_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_crtc_helper_funcs_60(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v10_0_crtc_helper_funcs_group0 = (struct drm_crtc *)tmp;
  tmp___0 = ldv_init_zalloc(168UL);
  dce_v10_0_crtc_helper_funcs_group1 = (struct drm_framebuffer *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  dce_v10_0_crtc_helper_funcs_group2 = (struct drm_display_mode *)tmp___1;
  return;
}
}
void ldv_initialize_drm_crtc_funcs_61(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v10_0_crtc_funcs_group0 = (struct drm_crtc *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_52(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v10_0_pageflip_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v10_0_pageflip_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_56(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v10_0_dac_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v10_0_dac_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_57(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v10_0_dig_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v10_0_dig_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_main_exported_53(void)
{
  unsigned int ldvarg1009 ;
  enum amdgpu_interrupt_state ldvarg1010 ;
  struct amdgpu_iv_entry *ldvarg1008 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1008 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1009), 0, 4UL);
  ldv_memset((void *)(& ldvarg1010), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_53 == 1) {
    dce_v10_0_set_crtc_irq_state(dce_v10_0_crtc_irq_funcs_group0, dce_v10_0_crtc_irq_funcs_group1,
                                 ldvarg1009, ldvarg1010);
    ldv_state_variable_53 = 1;
  } else {
  }
  goto ldv_51176;
  case 1: ;
  if (ldv_state_variable_53 == 1) {
    dce_v10_0_crtc_irq(dce_v10_0_crtc_irq_funcs_group0, dce_v10_0_crtc_irq_funcs_group1,
                       ldvarg1008);
    ldv_state_variable_53 = 1;
  } else {
  }
  goto ldv_51176;
  default:
  ldv_stop();
  }
  ldv_51176: ;
  return;
}
}
void ldv_main_exported_57(void)
{
  struct drm_display_mode *ldvarg90 ;
  void *tmp ;
  struct drm_display_mode *ldvarg92 ;
  void *tmp___0 ;
  struct drm_connector *ldvarg91 ;
  void *tmp___1 ;
  int ldvarg93 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg90 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg92 = (struct drm_display_mode *)tmp___0;
  tmp___1 = ldv_init_zalloc(936UL);
  ldvarg91 = (struct drm_connector *)tmp___1;
  ldv_memset((void *)(& ldvarg93), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_57 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v10_0_dig_helper_funcs_group0, ldvarg93);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    amdgpu_atombios_encoder_dpms(dce_v10_0_dig_helper_funcs_group0, ldvarg93);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    amdgpu_atombios_encoder_dpms(dce_v10_0_dig_helper_funcs_group0, ldvarg93);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 1: ;
  if (ldv_state_variable_57 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v10_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg92,
                                       dce_v10_0_dig_helper_funcs_group1);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    amdgpu_atombios_encoder_mode_fixup(dce_v10_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg92,
                                       dce_v10_0_dig_helper_funcs_group1);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    amdgpu_atombios_encoder_mode_fixup(dce_v10_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg92,
                                       dce_v10_0_dig_helper_funcs_group1);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 2: ;
  if (ldv_state_variable_57 == 1) {
    amdgpu_atombios_encoder_dig_detect(dce_v10_0_dig_helper_funcs_group0, ldvarg91);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    amdgpu_atombios_encoder_dig_detect(dce_v10_0_dig_helper_funcs_group0, ldvarg91);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    amdgpu_atombios_encoder_dig_detect(dce_v10_0_dig_helper_funcs_group0, ldvarg91);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 3: ;
  if (ldv_state_variable_57 == 1) {
    dce_v10_0_encoder_mode_set(dce_v10_0_dig_helper_funcs_group0, dce_v10_0_dig_helper_funcs_group1,
                               ldvarg90);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    dce_v10_0_encoder_mode_set(dce_v10_0_dig_helper_funcs_group0, dce_v10_0_dig_helper_funcs_group1,
                               ldvarg90);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    dce_v10_0_encoder_mode_set(dce_v10_0_dig_helper_funcs_group0, dce_v10_0_dig_helper_funcs_group1,
                               ldvarg90);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 4: ;
  if (ldv_state_variable_57 == 3) {
    dce_v10_0_encoder_disable(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 5: ;
  if (ldv_state_variable_57 == 1) {
    dce_v10_0_encoder_prepare(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    dce_v10_0_encoder_prepare(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    dce_v10_0_encoder_prepare(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 6: ;
  if (ldv_state_variable_57 == 1) {
    dce_v10_0_encoder_commit(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 1;
  } else {
  }
  if (ldv_state_variable_57 == 3) {
    dce_v10_0_encoder_commit(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 3;
  } else {
  }
  if (ldv_state_variable_57 == 2) {
    dce_v10_0_encoder_commit(dce_v10_0_dig_helper_funcs_group0);
    ldv_state_variable_57 = 2;
  } else {
  }
  goto ldv_51187;
  case 7: ;
  if (ldv_state_variable_57 == 2) {
    ldv_release_57();
    ldv_state_variable_57 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51187;
  case 8: ;
  if (ldv_state_variable_57 == 1) {
    ldv_bind_57();
    ldv_state_variable_57 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51187;
  case 9: ;
  if (ldv_state_variable_57 == 2) {
    ldv_connect_57();
    ldv_state_variable_57 = 3;
  } else {
  }
  goto ldv_51187;
  default:
  ldv_stop();
  }
  ldv_51187: ;
  return;
}
}
void ldv_main_exported_61(void)
{
  struct drm_file *ldvarg619 ;
  void *tmp ;
  u32 ldvarg610 ;
  u32 ldvarg621 ;
  u32 ldvarg620 ;
  u16 *ldvarg615 ;
  void *tmp___0 ;
  int ldvarg612 ;
  int ldvarg611 ;
  u32 ldvarg617 ;
  struct drm_pending_vblank_event *ldvarg608 ;
  void *tmp___1 ;
  u16 *ldvarg614 ;
  void *tmp___2 ;
  u32 ldvarg613 ;
  u16 *ldvarg616 ;
  void *tmp___3 ;
  struct drm_mode_set *ldvarg607 ;
  void *tmp___4 ;
  struct drm_framebuffer *ldvarg609 ;
  void *tmp___5 ;
  u32 ldvarg618 ;
  int tmp___6 ;
  {
  tmp = ldv_init_zalloc(744UL);
  ldvarg619 = (struct drm_file *)tmp;
  tmp___0 = ldv_init_zalloc(2UL);
  ldvarg615 = (u16 *)tmp___0;
  tmp___1 = ldv_init_zalloc(88UL);
  ldvarg608 = (struct drm_pending_vblank_event *)tmp___1;
  tmp___2 = ldv_init_zalloc(2UL);
  ldvarg614 = (u16 *)tmp___2;
  tmp___3 = ldv_init_zalloc(2UL);
  ldvarg616 = (u16 *)tmp___3;
  tmp___4 = ldv_init_zalloc(48UL);
  ldvarg607 = (struct drm_mode_set *)tmp___4;
  tmp___5 = ldv_init_zalloc(168UL);
  ldvarg609 = (struct drm_framebuffer *)tmp___5;
  ldv_memset((void *)(& ldvarg610), 0, 4UL);
  ldv_memset((void *)(& ldvarg621), 0, 4UL);
  ldv_memset((void *)(& ldvarg620), 0, 4UL);
  ldv_memset((void *)(& ldvarg612), 0, 4UL);
  ldv_memset((void *)(& ldvarg611), 0, 4UL);
  ldv_memset((void *)(& ldvarg617), 0, 4UL);
  ldv_memset((void *)(& ldvarg613), 0, 4UL);
  ldv_memset((void *)(& ldvarg618), 0, 4UL);
  tmp___6 = __VERIFIER_nondet_int();
  switch (tmp___6) {
  case 0: ;
  if (ldv_state_variable_61 == 2) {
    dce_v10_0_crtc_cursor_set(dce_v10_0_crtc_funcs_group0, ldvarg619, ldvarg618, ldvarg620,
                              ldvarg621);
    ldv_state_variable_61 = 2;
  } else {
  }
  if (ldv_state_variable_61 == 1) {
    dce_v10_0_crtc_cursor_set(dce_v10_0_crtc_funcs_group0, ldvarg619, ldvarg618, ldvarg620,
                              ldvarg621);
    ldv_state_variable_61 = 1;
  } else {
  }
  goto ldv_51217;
  case 1: ;
  if (ldv_state_variable_61 == 2) {
    dce_v10_0_crtc_destroy(dce_v10_0_crtc_funcs_group0);
    ldv_state_variable_61 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51217;
  case 2: ;
  if (ldv_state_variable_61 == 2) {
    dce_v10_0_crtc_gamma_set(dce_v10_0_crtc_funcs_group0, ldvarg615, ldvarg614, ldvarg616,
                             ldvarg617, ldvarg613);
    ldv_state_variable_61 = 2;
  } else {
  }
  if (ldv_state_variable_61 == 1) {
    dce_v10_0_crtc_gamma_set(dce_v10_0_crtc_funcs_group0, ldvarg615, ldvarg614, ldvarg616,
                             ldvarg617, ldvarg613);
    ldv_state_variable_61 = 1;
  } else {
  }
  goto ldv_51217;
  case 3: ;
  if (ldv_state_variable_61 == 2) {
    dce_v10_0_crtc_cursor_move(dce_v10_0_crtc_funcs_group0, ldvarg612, ldvarg611);
    ldv_state_variable_61 = 2;
  } else {
  }
  if (ldv_state_variable_61 == 1) {
    dce_v10_0_crtc_cursor_move(dce_v10_0_crtc_funcs_group0, ldvarg612, ldvarg611);
    ldv_state_variable_61 = 1;
  } else {
  }
  goto ldv_51217;
  case 4: ;
  if (ldv_state_variable_61 == 2) {
    amdgpu_crtc_page_flip(dce_v10_0_crtc_funcs_group0, ldvarg609, ldvarg608, ldvarg610);
    ldv_state_variable_61 = 2;
  } else {
  }
  if (ldv_state_variable_61 == 1) {
    amdgpu_crtc_page_flip(dce_v10_0_crtc_funcs_group0, ldvarg609, ldvarg608, ldvarg610);
    ldv_state_variable_61 = 1;
  } else {
  }
  goto ldv_51217;
  case 5: ;
  if (ldv_state_variable_61 == 2) {
    amdgpu_crtc_set_config(ldvarg607);
    ldv_state_variable_61 = 2;
  } else {
  }
  if (ldv_state_variable_61 == 1) {
    amdgpu_crtc_set_config(ldvarg607);
    ldv_state_variable_61 = 1;
  } else {
  }
  goto ldv_51217;
  case 6: ;
  if (ldv_state_variable_61 == 1) {
    ldv_probe_61();
    ldv_state_variable_61 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51217;
  default:
  ldv_stop();
  }
  ldv_51217: ;
  return;
}
}
void ldv_main_exported_51(void)
{
  struct amdgpu_iv_entry *ldvarg1066 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg1068 ;
  unsigned int ldvarg1067 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1066 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1068), 0, 4UL);
  ldv_memset((void *)(& ldvarg1067), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_51 == 1) {
    dce_v10_0_set_hpd_irq_state(dce_v10_0_hpd_irq_funcs_group0, dce_v10_0_hpd_irq_funcs_group1,
                                ldvarg1067, ldvarg1068);
    ldv_state_variable_51 = 1;
  } else {
  }
  goto ldv_51232;
  case 1: ;
  if (ldv_state_variable_51 == 1) {
    dce_v10_0_hpd_irq(dce_v10_0_hpd_irq_funcs_group0, dce_v10_0_hpd_irq_funcs_group1,
                      ldvarg1066);
    ldv_state_variable_51 = 1;
  } else {
  }
  goto ldv_51232;
  default:
  ldv_stop();
  }
  ldv_51232: ;
  return;
}
}
void ldv_main_exported_58(void)
{
  struct drm_display_mode *ldvarg186 ;
  void *tmp ;
  int ldvarg188 ;
  struct drm_display_mode *ldvarg187 ;
  void *tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg186 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg187 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg188), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_58 == 1) {
    dce_v10_0_ext_dpms(dce_v10_0_ext_helper_funcs_group0, ldvarg188);
    ldv_state_variable_58 = 1;
  } else {
  }
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_dpms(dce_v10_0_ext_helper_funcs_group0, ldvarg188);
    ldv_state_variable_58 = 3;
  } else {
  }
  if (ldv_state_variable_58 == 2) {
    dce_v10_0_ext_dpms(dce_v10_0_ext_helper_funcs_group0, ldvarg188);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 1: ;
  if (ldv_state_variable_58 == 1) {
    dce_v10_0_ext_mode_fixup(dce_v10_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg187,
                             dce_v10_0_ext_helper_funcs_group1);
    ldv_state_variable_58 = 1;
  } else {
  }
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_mode_fixup(dce_v10_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg187,
                             dce_v10_0_ext_helper_funcs_group1);
    ldv_state_variable_58 = 3;
  } else {
  }
  if (ldv_state_variable_58 == 2) {
    dce_v10_0_ext_mode_fixup(dce_v10_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg187,
                             dce_v10_0_ext_helper_funcs_group1);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 2: ;
  if (ldv_state_variable_58 == 1) {
    dce_v10_0_ext_mode_set(dce_v10_0_ext_helper_funcs_group0, dce_v10_0_ext_helper_funcs_group1,
                           ldvarg186);
    ldv_state_variable_58 = 1;
  } else {
  }
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_mode_set(dce_v10_0_ext_helper_funcs_group0, dce_v10_0_ext_helper_funcs_group1,
                           ldvarg186);
    ldv_state_variable_58 = 3;
  } else {
  }
  if (ldv_state_variable_58 == 2) {
    dce_v10_0_ext_mode_set(dce_v10_0_ext_helper_funcs_group0, dce_v10_0_ext_helper_funcs_group1,
                           ldvarg186);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 3: ;
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_disable(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 4: ;
  if (ldv_state_variable_58 == 1) {
    dce_v10_0_ext_prepare(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 1;
  } else {
  }
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_prepare(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 3;
  } else {
  }
  if (ldv_state_variable_58 == 2) {
    dce_v10_0_ext_prepare(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 5: ;
  if (ldv_state_variable_58 == 1) {
    dce_v10_0_ext_commit(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 1;
  } else {
  }
  if (ldv_state_variable_58 == 3) {
    dce_v10_0_ext_commit(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 3;
  } else {
  }
  if (ldv_state_variable_58 == 2) {
    dce_v10_0_ext_commit(dce_v10_0_ext_helper_funcs_group0);
    ldv_state_variable_58 = 2;
  } else {
  }
  goto ldv_51242;
  case 6: ;
  if (ldv_state_variable_58 == 2) {
    ldv_release_58();
    ldv_state_variable_58 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51242;
  case 7: ;
  if (ldv_state_variable_58 == 1) {
    ldv_bind_58();
    ldv_state_variable_58 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51242;
  case 8: ;
  if (ldv_state_variable_58 == 2) {
    ldv_connect_58();
    ldv_state_variable_58 = 3;
  } else {
  }
  goto ldv_51242;
  default:
  ldv_stop();
  }
  ldv_51242: ;
  return;
}
}
void ldv_main_exported_59(void)
{
  void *ldvarg952 ;
  void *tmp ;
  void *ldvarg961 ;
  void *tmp___0 ;
  void *ldvarg958 ;
  void *tmp___1 ;
  void *ldvarg947 ;
  void *tmp___2 ;
  void *ldvarg959 ;
  void *tmp___3 ;
  enum amd_powergating_state ldvarg955 ;
  enum amd_clockgating_state ldvarg951 ;
  void *ldvarg950 ;
  void *tmp___4 ;
  void *ldvarg949 ;
  void *tmp___5 ;
  void *ldvarg954 ;
  void *tmp___6 ;
  void *ldvarg948 ;
  void *tmp___7 ;
  void *ldvarg956 ;
  void *tmp___8 ;
  void *ldvarg953 ;
  void *tmp___9 ;
  void *ldvarg960 ;
  void *tmp___10 ;
  void *ldvarg957 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg952 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg961 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg958 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg947 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg959 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg950 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg949 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg954 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg948 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg956 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg953 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg960 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg957 = tmp___11;
  ldv_memset((void *)(& ldvarg955), 0, 4UL);
  ldv_memset((void *)(& ldvarg951), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_hw_fini(ldvarg961);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_hw_fini(ldvarg961);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_hw_fini(ldvarg961);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 1: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_print_status(ldvarg960);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_print_status(ldvarg960);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_print_status(ldvarg960);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 2: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_early_init(ldvarg959);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_early_init(ldvarg959);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_early_init(ldvarg959);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 3: ;
  if (ldv_state_variable_59 == 2) {
    ldv_retval_75 = dce_v10_0_suspend(ldvarg958);
    if (ldv_retval_75 == 0) {
      ldv_state_variable_59 = 3;
    } else {
    }
  } else {
  }
  goto ldv_51271;
  case 4: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_sw_init(ldvarg957);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_sw_init(ldvarg957);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_sw_init(ldvarg957);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 5: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_set_powergating_state(ldvarg956, ldvarg955);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_set_powergating_state(ldvarg956, ldvarg955);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_set_powergating_state(ldvarg956, ldvarg955);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 6: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_wait_for_idle(ldvarg954);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_wait_for_idle(ldvarg954);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_wait_for_idle(ldvarg954);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 7: ;
  if (ldv_state_variable_59 == 3) {
    ldv_retval_74 = dce_v10_0_resume(ldvarg953);
    if (ldv_retval_74 == 0) {
      ldv_state_variable_59 = 2;
    } else {
    }
  } else {
  }
  goto ldv_51271;
  case 8: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_set_clockgating_state(ldvarg952, ldvarg951);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_set_clockgating_state(ldvarg952, ldvarg951);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_set_clockgating_state(ldvarg952, ldvarg951);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 9: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_hw_init(ldvarg950);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_hw_init(ldvarg950);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_hw_init(ldvarg950);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 10: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_soft_reset(ldvarg949);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_soft_reset(ldvarg949);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_soft_reset(ldvarg949);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 11: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_sw_fini(ldvarg948);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_sw_fini(ldvarg948);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_sw_fini(ldvarg948);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 12: ;
  if (ldv_state_variable_59 == 2) {
    dce_v10_0_is_idle(ldvarg947);
    ldv_state_variable_59 = 2;
  } else {
  }
  if (ldv_state_variable_59 == 1) {
    dce_v10_0_is_idle(ldvarg947);
    ldv_state_variable_59 = 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    dce_v10_0_is_idle(ldvarg947);
    ldv_state_variable_59 = 3;
  } else {
  }
  goto ldv_51271;
  case 13: ;
  if (ldv_state_variable_59 == 2) {
    ldv_release_59();
    ldv_state_variable_59 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_59 == 3) {
    ldv_release_59();
    ldv_state_variable_59 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51271;
  case 14: ;
  if (ldv_state_variable_59 == 1) {
    ldv_probe_59();
    ldv_state_variable_59 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51271;
  default:
  ldv_stop();
  }
  ldv_51271: ;
  return;
}
}
void ldv_main_exported_52(void)
{
  struct amdgpu_iv_entry *ldvarg798 ;
  void *tmp ;
  unsigned int ldvarg799 ;
  enum amdgpu_interrupt_state ldvarg800 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg798 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg799), 0, 4UL);
  ldv_memset((void *)(& ldvarg800), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_52 == 1) {
    dce_v10_0_set_pageflip_irq_state(dce_v10_0_pageflip_irq_funcs_group0, dce_v10_0_pageflip_irq_funcs_group1,
                                     ldvarg799, ldvarg800);
    ldv_state_variable_52 = 1;
  } else {
  }
  goto ldv_51294;
  case 1: ;
  if (ldv_state_variable_52 == 1) {
    dce_v10_0_pageflip_irq(dce_v10_0_pageflip_irq_funcs_group0, dce_v10_0_pageflip_irq_funcs_group1,
                           ldvarg798);
    ldv_state_variable_52 = 1;
  } else {
  }
  goto ldv_51294;
  default:
  ldv_stop();
  }
  ldv_51294: ;
  return;
}
}
void ldv_main_exported_60(void)
{
  int ldvarg223 ;
  struct drm_display_mode *ldvarg224 ;
  void *tmp ;
  int ldvarg222 ;
  int ldvarg218 ;
  enum mode_set_atomic ldvarg220 ;
  int ldvarg219 ;
  struct drm_display_mode *ldvarg221 ;
  void *tmp___0 ;
  int ldvarg226 ;
  int ldvarg227 ;
  int ldvarg225 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg224 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg221 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg223), 0, 4UL);
  ldv_memset((void *)(& ldvarg222), 0, 4UL);
  ldv_memset((void *)(& ldvarg218), 0, 4UL);
  ldv_memset((void *)(& ldvarg220), 0, 4UL);
  ldv_memset((void *)(& ldvarg219), 0, 4UL);
  ldv_memset((void *)(& ldvarg226), 0, 4UL);
  ldv_memset((void *)(& ldvarg227), 0, 4UL);
  ldv_memset((void *)(& ldvarg225), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_set_base(dce_v10_0_crtc_helper_funcs_group0, ldvarg227, ldvarg226,
                            dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_set_base(dce_v10_0_crtc_helper_funcs_group0, ldvarg227, ldvarg226,
                            dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_set_base(dce_v10_0_crtc_helper_funcs_group0, ldvarg227, ldvarg226,
                            dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 1: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_dpms(dce_v10_0_crtc_helper_funcs_group0, ldvarg225);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_dpms(dce_v10_0_crtc_helper_funcs_group0, ldvarg225);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_dpms(dce_v10_0_crtc_helper_funcs_group0, ldvarg225);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 2: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_mode_fixup(dce_v10_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg224,
                              dce_v10_0_crtc_helper_funcs_group2);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_mode_fixup(dce_v10_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg224,
                              dce_v10_0_crtc_helper_funcs_group2);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_mode_fixup(dce_v10_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg224,
                              dce_v10_0_crtc_helper_funcs_group2);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 3: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_mode_set(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group2,
                            ldvarg221, ldvarg222, ldvarg223, dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_mode_set(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group2,
                            ldvarg221, ldvarg222, ldvarg223, dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_mode_set(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group2,
                            ldvarg221, ldvarg222, ldvarg223, dce_v10_0_crtc_helper_funcs_group1);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 4: ;
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_disable(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 5: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_prepare(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_prepare(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_prepare(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 6: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_load_lut(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_load_lut(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_load_lut(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 7: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_commit(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_commit(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_commit(dce_v10_0_crtc_helper_funcs_group0);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 8: ;
  if (ldv_state_variable_60 == 1) {
    dce_v10_0_crtc_set_base_atomic(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group1,
                                   ldvarg218, ldvarg219, ldvarg220);
    ldv_state_variable_60 = 1;
  } else {
  }
  if (ldv_state_variable_60 == 3) {
    dce_v10_0_crtc_set_base_atomic(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group1,
                                   ldvarg218, ldvarg219, ldvarg220);
    ldv_state_variable_60 = 3;
  } else {
  }
  if (ldv_state_variable_60 == 2) {
    dce_v10_0_crtc_set_base_atomic(dce_v10_0_crtc_helper_funcs_group0, dce_v10_0_crtc_helper_funcs_group1,
                                   ldvarg218, ldvarg219, ldvarg220);
    ldv_state_variable_60 = 2;
  } else {
  }
  goto ldv_51311;
  case 9: ;
  if (ldv_state_variable_60 == 2) {
    ldv_release_60();
    ldv_state_variable_60 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51311;
  case 10: ;
  if (ldv_state_variable_60 == 1) {
    ldv_bind_60();
    ldv_state_variable_60 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51311;
  case 11: ;
  if (ldv_state_variable_60 == 2) {
    ldv_connect_60();
    ldv_state_variable_60 = 3;
  } else {
  }
  goto ldv_51311;
  default:
  ldv_stop();
  }
  ldv_51311: ;
  return;
}
}
void ldv_main_exported_56(void)
{
  struct drm_display_mode *ldvarg804 ;
  void *tmp ;
  int ldvarg805 ;
  struct drm_connector *ldvarg803 ;
  void *tmp___0 ;
  struct drm_display_mode *ldvarg802 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg804 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(936UL);
  ldvarg803 = (struct drm_connector *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  ldvarg802 = (struct drm_display_mode *)tmp___1;
  ldv_memset((void *)(& ldvarg805), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_56 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v10_0_dac_helper_funcs_group0, ldvarg805);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  case 1: ;
  if (ldv_state_variable_56 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v10_0_dac_helper_funcs_group0, (struct drm_display_mode const *)ldvarg804,
                                       dce_v10_0_dac_helper_funcs_group1);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  case 2: ;
  if (ldv_state_variable_56 == 1) {
    amdgpu_atombios_encoder_dac_detect(dce_v10_0_dac_helper_funcs_group0, ldvarg803);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  case 3: ;
  if (ldv_state_variable_56 == 1) {
    dce_v10_0_encoder_mode_set(dce_v10_0_dac_helper_funcs_group0, dce_v10_0_dac_helper_funcs_group1,
                               ldvarg802);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  case 4: ;
  if (ldv_state_variable_56 == 1) {
    dce_v10_0_encoder_prepare(dce_v10_0_dac_helper_funcs_group0);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  case 5: ;
  if (ldv_state_variable_56 == 1) {
    dce_v10_0_encoder_commit(dce_v10_0_dac_helper_funcs_group0);
    ldv_state_variable_56 = 1;
  } else {
  }
  goto ldv_51332;
  default:
  ldv_stop();
  }
  ldv_51332: ;
  return;
}
}
void ldv_main_exported_54(void)
{
  struct amdgpu_hpd *ldvarg836 ;
  void *tmp ;
  int ldvarg839 ;
  int ldvarg830 ;
  u32 *ldvarg829 ;
  void *tmp___0 ;
  u32 *ldvarg831 ;
  void *tmp___1 ;
  int ldvarg826 ;
  int ldvarg832 ;
  u32 ldvarg837 ;
  struct amdgpu_router *ldvarg838 ;
  void *tmp___2 ;
  struct amdgpu_i2c_bus_rec *ldvarg840 ;
  void *tmp___3 ;
  u16 ldvarg824 ;
  u8 ldvarg833 ;
  enum amdgpu_hpd_id ldvarg828 ;
  enum amdgpu_hpd_id ldvarg821 ;
  bool ldvarg827 ;
  u32 ldvarg822 ;
  u32 ldvarg835 ;
  int ldvarg820 ;
  u32 ldvarg823 ;
  u16 ldvarg834 ;
  u64 ldvarg825 ;
  int tmp___4 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg836 = (struct amdgpu_hpd *)tmp;
  tmp___0 = ldv_init_zalloc(4UL);
  ldvarg829 = (u32 *)tmp___0;
  tmp___1 = ldv_init_zalloc(4UL);
  ldvarg831 = (u32 *)tmp___1;
  tmp___2 = ldv_init_zalloc(92UL);
  ldvarg838 = (struct amdgpu_router *)tmp___2;
  tmp___3 = ldv_init_zalloc(76UL);
  ldvarg840 = (struct amdgpu_i2c_bus_rec *)tmp___3;
  ldv_memset((void *)(& ldvarg839), 0, 4UL);
  ldv_memset((void *)(& ldvarg830), 0, 4UL);
  ldv_memset((void *)(& ldvarg826), 0, 4UL);
  ldv_memset((void *)(& ldvarg832), 0, 4UL);
  ldv_memset((void *)(& ldvarg837), 0, 4UL);
  ldv_memset((void *)(& ldvarg824), 0, 2UL);
  ldv_memset((void *)(& ldvarg833), 0, 1UL);
  ldv_memset((void *)(& ldvarg828), 0, 4UL);
  ldv_memset((void *)(& ldvarg821), 0, 4UL);
  ldv_memset((void *)(& ldvarg827), 0, 1UL);
  ldv_memset((void *)(& ldvarg822), 0, 4UL);
  ldv_memset((void *)(& ldvarg835), 0, 4UL);
  ldv_memset((void *)(& ldvarg820), 0, 4UL);
  ldv_memset((void *)(& ldvarg823), 0, 4UL);
  ldv_memset((void *)(& ldvarg834), 0, 2UL);
  ldv_memset((void *)(& ldvarg825), 0, 8UL);
  tmp___4 = __VERIFIER_nondet_int();
  switch (tmp___4) {
  case 0: ;
  if (ldv_state_variable_54 == 1) {
    amdgpu_connector_add(dce_v10_0_display_funcs_group0, ldvarg837, ldvarg835, ldvarg839,
                         ldvarg840, (int )ldvarg834, ldvarg836, ldvarg838);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 1: ;
  if (ldv_state_variable_54 == 1) {
    amdgpu_atombios_encoder_set_backlight_level(dce_v10_0_display_funcs_group2, (int )ldvarg833);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 2: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_vblank_wait(dce_v10_0_display_funcs_group0, ldvarg832);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 3: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_stop_mc_access(dce_v10_0_display_funcs_group0, dce_v10_0_display_funcs_group1);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 4: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_crtc_get_scanoutpos(dce_v10_0_display_funcs_group0, ldvarg830, ldvarg829,
                                  ldvarg831);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 5: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_hpd_get_gpio_reg(dce_v10_0_display_funcs_group0);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 6: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_resume_mc_access(dce_v10_0_display_funcs_group0, dce_v10_0_display_funcs_group1);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 7: ;
  if (ldv_state_variable_54 == 1) {
    amdgpu_atombios_encoder_get_backlight_level(dce_v10_0_display_funcs_group2);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 8: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_hpd_set_polarity(dce_v10_0_display_funcs_group0, ldvarg828);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 9: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_is_display_hung(dce_v10_0_display_funcs_group0);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 10: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_set_vga_render_state(dce_v10_0_display_funcs_group0, (int )ldvarg827);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 11: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_page_flip(dce_v10_0_display_funcs_group0, ldvarg826, ldvarg825);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 12: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_encoder_add(dce_v10_0_display_funcs_group0, ldvarg823, ldvarg822, (int )ldvarg824);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 13: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_hpd_sense(dce_v10_0_display_funcs_group0, ldvarg821);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 14: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_bandwidth_update(dce_v10_0_display_funcs_group0);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  case 15: ;
  if (ldv_state_variable_54 == 1) {
    dce_v10_0_vblank_get_counter(dce_v10_0_display_funcs_group0, ldvarg820);
    ldv_state_variable_54 = 1;
  } else {
  }
  goto ldv_51364;
  default:
  ldv_stop();
  }
  ldv_51364: ;
  return;
}
}
void ldv_main_exported_55(void)
{
  struct drm_encoder *ldvarg46 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  ldvarg46 = (struct drm_encoder *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_55 == 2) {
    dce_v10_0_encoder_destroy(ldvarg46);
    ldv_state_variable_55 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_51386;
  case 1: ;
  if (ldv_state_variable_55 == 1) {
    ldv_probe_55();
    ldv_state_variable_55 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_51386;
  default:
  ldv_stop();
  }
  ldv_51386: ;
  return;
}
}
bool ldv_queue_work_on_893(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_894(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_895(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_896(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_897(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
void ldv_destroy_workqueue_898(struct workqueue_struct *ldv_func_arg1 )
{
  {
  destroy_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static int __atomic_add_unless___6(atomic_t *v , int a , int u )
{
  int c ;
  int old ;
  long tmp ;
  long tmp___0 ;
  {
  c = atomic_read((atomic_t const *)v);
  ldv_5708:
  tmp = ldv__builtin_expect(c == u, 0L);
  if (tmp != 0L) {
    goto ldv_5707;
  } else {
  }
  old = atomic_cmpxchg(v, c, c + a);
  tmp___0 = ldv__builtin_expect(old == c, 1L);
  if (tmp___0 != 0L) {
    goto ldv_5707;
  } else {
  }
  c = old;
  goto ldv_5708;
  ldv_5707: ;
  return (c);
}
}
__inline static int atomic_add_unless___6(atomic_t *v , int a , int u )
{
  int tmp ;
  {
  tmp = __atomic_add_unless___6(v, a, u);
  return (tmp != u);
}
}
void ldv_destroy_workqueue_914(struct workqueue_struct *ldv_func_arg1 ) ;
bool ldv_queue_work_on_909(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_911(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_910(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_913(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_912(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___6(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_909(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___5(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___6(system_wq, work);
  return (tmp);
}
}
__inline static int kref_put_mutex___6(struct kref *kref , void (*release)(struct kref * ) ,
                                       struct mutex *lock )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  int tmp___2 ;
  long tmp___3 ;
  {
  __ret_warn_on = (unsigned long )release == (unsigned long )((void (*)(struct kref * ))0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/kref.h", 138);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___2 = atomic_add_unless___6(& kref->refcount, -1, 1);
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    mutex_lock_nested(lock, 0U);
    tmp___0 = atomic_dec_and_test(& kref->refcount);
    tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
    if (tmp___1 != 0L) {
      mutex_unlock(lock);
      return (0);
    } else {
    }
    (*release)(kref);
    return (1);
  } else {
  }
  return (0);
}
}
__inline static void drm_gem_object_unreference_unlocked___6(struct drm_gem_object *obj )
{
  struct drm_device *dev ;
  int tmp ;
  {
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    return;
  } else {
  }
  dev = obj->dev;
  tmp = kref_put_mutex___6(& obj->refcount, & drm_gem_object_free, & dev->struct_mutex);
  if (tmp != 0) {
    mutex_unlock(& dev->struct_mutex);
  } else {
    lock_acquire(& dev->struct_mutex.dep_map, 0U, 0, 0, 1, (struct lockdep_map *)0,
                 0UL);
    lock_release(& dev->struct_mutex.dep_map, 0, 0UL);
  }
  return;
}
}
static void dce_v11_0_set_display_funcs(struct amdgpu_device *adev ) ;
static void dce_v11_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const crtc_offsets___1[7U] = { 0U, 512U, 1024U, 9728U,
        10240U, 10752U, 11264U};
static u32 const hpd_offsets___0[6U] = { 0U, 8U, 16U, 24U,
        32U, 40U};
static u32 const dig_offsets___1[9U] =
  { 0U, 256U, 512U, 768U,
        1024U, 1280U, 2560U, 3072U,
        3328U};
static struct __anonstruct_interrupt_status_offsets_324___1 const interrupt_status_offsets___1[6U] = { {6231U,
      8U, 4U, 131072U},
        {6232U, 8U, 4U, 131072U},
        {6233U, 8U, 4U, 131072U},
        {6234U, 8U, 4U, 131072U},
        {6235U, 8U, 4U, 131072U},
        {6236U, 8U, 4U, 131072U}};
static u32 const cz_golden_settings_a11[6U] = { 7094U, 65793U, 65536U, 674U,
        523313151U, 338690048U};
static u32 const cz_mgcg_cgcg_init___1[6U] = { 996U, 4294967295U, 256U, 998U,
        257U, 0U};
static void dce_v11_0_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 7U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_mgcg_cgcg_init___1),
                                   6U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_golden_settings_a11),
                                   6U);
  goto ldv_54194;
  default: ;
  goto ldv_54194;
  }
  ldv_54194: ;
  return;
}
}
static u32 dce_v11_0_audio_endpt_rreg(struct amdgpu_device *adev , u32 block_offset ,
                                      u32 reg )
{
  unsigned long flags ;
  u32 r ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6056U, reg, 0);
  r = amdgpu_mm_rreg(adev, block_offset + 6057U, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return (r);
}
}
static void dce_v11_0_audio_endpt_wreg(struct amdgpu_device *adev , u32 block_offset ,
                                       u32 reg , u32 v )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& adev->audio_endpt_idx_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  amdgpu_mm_wreg(adev, block_offset + 6056U, reg, 0);
  amdgpu_mm_wreg(adev, block_offset + 6057U, v, 0);
  spin_unlock_irqrestore(& adev->audio_endpt_idx_lock, flags);
  return;
}
}
static bool dce_v11_0_is_in_vblank(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7075U, 0);
  if ((tmp & 16383U) != 0U) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v11_0_is_counter_moving(struct amdgpu_device *adev , int crtc )
{
  u32 pos1 ;
  u32 pos2 ;
  {
  pos1 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7076U, 0);
  pos2 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7076U, 0);
  if (pos1 != pos2) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v11_0_vblank_wait(struct amdgpu_device *adev , int crtc )
{
  unsigned int i ;
  u32 tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  unsigned int tmp___2 ;
  bool tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  unsigned int tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  {
  i = 0U;
  if (adev->mode_info.num_crtc <= crtc) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7068U, 0);
  if ((tmp & 1U) == 0U) {
    return;
  } else {
  }
  goto ldv_54233;
  ldv_54232:
  tmp___2 = i;
  i = i + 1U;
  if (tmp___2 % 100U == 0U) {
    tmp___0 = dce_v11_0_is_counter_moving(adev, crtc);
    if (tmp___0) {
      tmp___1 = 0;
    } else {
      tmp___1 = 1;
    }
    if (tmp___1) {
      goto ldv_54231;
    } else {
    }
  } else {
  }
  ldv_54233:
  tmp___3 = dce_v11_0_is_in_vblank(adev, crtc);
  if ((int )tmp___3) {
    goto ldv_54232;
  } else {
  }
  ldv_54231: ;
  goto ldv_54236;
  ldv_54235:
  tmp___6 = i;
  i = i + 1U;
  if (tmp___6 % 100U == 0U) {
    tmp___4 = dce_v11_0_is_counter_moving(adev, crtc);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto ldv_54234;
    } else {
    }
  } else {
  }
  ldv_54236:
  tmp___7 = dce_v11_0_is_in_vblank(adev, crtc);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
    goto ldv_54235;
  } else {
  }
  ldv_54234: ;
  return;
}
}
static u32 dce_v11_0_vblank_get_counter(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    return (0U);
  } else {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7078U, 0);
    return (tmp);
  }
}
}
static void dce_v11_0_page_flip(struct amdgpu_device *adev , int crtc_id , u64 crtc_base )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  u32 tmp ;
  u32 tmp___0 ;
  int i ;
  u32 tmp___1 ;
  long tmp___2 ;
  {
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  tmp___0 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  tmp = tmp___0;
  tmp = tmp | 65536U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )crtc_base,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(crtc_base >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )crtc_base,
                 0);
  i = 0;
  goto ldv_54251;
  ldv_54250:
  tmp___1 = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6673U, 0);
  if ((tmp___1 & 4U) != 0U) {
    goto ldv_54249;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1;
  ldv_54251: ;
  if (adev->usec_timeout > i) {
    goto ldv_54250;
  } else {
  }
  ldv_54249:
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v11_0_page_flip", "Update pending now high. Unlocking vupdate_lock.\n");
  } else {
  }
  tmp = tmp & 4294901759U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6673U, tmp, 0);
  return;
}
}
static int dce_v11_0_crtc_get_scanoutpos(struct amdgpu_device *adev , int crtc , u32 *vbl ,
                                         u32 *position )
{
  {
  if (crtc < 0 || adev->mode_info.num_crtc <= crtc) {
    return (-22);
  } else {
  }
  *vbl = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7053U, 0);
  *position = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 7076U,
                             0);
  return (0);
}
}
static bool dce_v11_0_hpd_sense(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  int idx ;
  bool connected ;
  u32 tmp ;
  {
  connected = 0;
  switch ((unsigned int )hpd) {
  case 0U:
  idx = 0;
  goto ldv_54266;
  case 1U:
  idx = 1;
  goto ldv_54266;
  case 2U:
  idx = 2;
  goto ldv_54266;
  case 3U:
  idx = 3;
  goto ldv_54266;
  case 4U:
  idx = 4;
  goto ldv_54266;
  case 5U:
  idx = 5;
  goto ldv_54266;
  default: ;
  return (connected);
  }
  ldv_54266:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[idx] + 6296U, 0);
  if ((tmp & 2U) != 0U) {
    connected = 1;
  } else {
  }
  return (connected);
}
}
static void dce_v11_0_hpd_set_polarity(struct amdgpu_device *adev , enum amdgpu_hpd_id hpd )
{
  u32 tmp ;
  bool connected ;
  bool tmp___0 ;
  int idx ;
  {
  tmp___0 = dce_v11_0_hpd_sense(adev, hpd);
  connected = tmp___0;
  switch ((unsigned int )hpd) {
  case 0U:
  idx = 0;
  goto ldv_54281;
  case 1U:
  idx = 1;
  goto ldv_54281;
  case 2U:
  idx = 2;
  goto ldv_54281;
  case 3U:
  idx = 3;
  goto ldv_54281;
  case 4U:
  idx = 4;
  goto ldv_54281;
  case 5U:
  idx = 5;
  goto ldv_54281;
  default: ;
  return;
  }
  ldv_54281:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[idx] + 6297U, 0);
  if ((int )connected) {
    tmp = tmp & 4294967039U;
  } else {
    tmp = tmp | 256U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[idx] + 6297U, tmp, 0);
  return;
}
}
static void dce_v11_0_hpd_init(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  u32 tmp ;
  int idx ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_54312;
  ldv_54311:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  if (connector->connector_type == 14 || connector->connector_type == 7) {
    goto ldv_54302;
  } else {
  }
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  idx = 0;
  goto ldv_54304;
  case 1U:
  idx = 1;
  goto ldv_54304;
  case 2U:
  idx = 2;
  goto ldv_54304;
  case 3U:
  idx = 3;
  goto ldv_54304;
  case 4U:
  idx = 4;
  goto ldv_54304;
  case 5U:
  idx = 5;
  goto ldv_54304;
  default: ;
  goto ldv_54302;
  }
  ldv_54304:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[idx] + 6298U, 0);
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[idx] + 6298U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[idx] + 6300U, 0);
  tmp = (tmp & 4294967040U) | 50U;
  tmp = (tmp & 4027580415U) | 10485760U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[idx] + 6300U, tmp, 0);
  dce_v11_0_hpd_set_polarity(adev, amdgpu_connector->hpd.hpd);
  amdgpu_irq_get(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  ldv_54302:
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_54312: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_54311;
  } else {
  }
  return;
}
}
static void dce_v11_0_hpd_fini(struct amdgpu_device *adev )
{
  struct drm_device *dev ;
  struct drm_connector *connector ;
  u32 tmp ;
  int idx ;
  struct list_head const *__mptr ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr + 0xffffffffffffffe8UL;
  goto ldv_54338;
  ldv_54337:
  __mptr___0 = (struct drm_connector const *)connector;
  amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
  switch ((unsigned int )amdgpu_connector->hpd.hpd) {
  case 0U:
  idx = 0;
  goto ldv_54329;
  case 1U:
  idx = 1;
  goto ldv_54329;
  case 2U:
  idx = 2;
  goto ldv_54329;
  case 3U:
  idx = 3;
  goto ldv_54329;
  case 4U:
  idx = 4;
  goto ldv_54329;
  case 5U:
  idx = 5;
  goto ldv_54329;
  default: ;
  goto ldv_54336;
  }
  ldv_54329:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[idx] + 6298U, 0);
  tmp = tmp & 4026531839U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[idx] + 6298U, tmp, 0);
  amdgpu_irq_put(adev, & adev->hpd_irq, (unsigned int )amdgpu_connector->hpd.hpd);
  ldv_54336:
  __mptr___1 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___1 + 0xffffffffffffffe8UL;
  ldv_54338: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& dev->mode_config.connector_list)) {
    goto ldv_54337;
  } else {
  }
  return;
}
}
static u32 dce_v11_0_hpd_get_gpio_reg(struct amdgpu_device *adev )
{
  {
  return (18573U);
}
}
static bool dce_v11_0_is_display_hung(struct amdgpu_device *adev )
{
  u32 crtc_hung ;
  u32 crtc_status[6U] ;
  u32 i ;
  u32 j ;
  u32 tmp ;
  {
  crtc_hung = 0U;
  i = 0U;
  goto ldv_54352;
  ldv_54351:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7068U, 0);
  if ((int )tmp & 1) {
    crtc_status[i] = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7080U,
                                    0);
    crtc_hung = (u32 )(1 << (int )i) | crtc_hung;
  } else {
  }
  i = i + 1U;
  ldv_54352: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_54351;
  } else {
  }
  j = 0U;
  goto ldv_54358;
  ldv_54357:
  i = 0U;
  goto ldv_54355;
  ldv_54354: ;
  if (((u32 )(1 << (int )i) & crtc_hung) != 0U) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7080U, 0);
    if (crtc_status[i] != tmp) {
      crtc_hung = (u32 )(~ (1 << (int )i)) & crtc_hung;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_54355: ;
  if ((u32 )adev->mode_info.num_crtc > i) {
    goto ldv_54354;
  } else {
  }
  if (crtc_hung == 0U) {
    return (0);
  } else {
  }
  __const_udelay(429500UL);
  j = j + 1U;
  ldv_54358: ;
  if (j <= 9U) {
    goto ldv_54357;
  } else {
  }
  return (1);
}
}
static void dce_v11_0_stop_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 crtc_enabled ;
  u32 tmp ;
  int i ;
  u32 tmp___0 ;
  {
  save->vga_render_control = amdgpu_mm_rreg(adev, 192U, 0);
  save->vga_hdp_control = amdgpu_mm_rreg(adev, 202U, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  tmp = tmp & 4294770687U;
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  i = 0;
  goto ldv_54368;
  ldv_54367:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7068U, 0);
  crtc_enabled = tmp___0 & 1U;
  if (crtc_enabled != 0U) {
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7093U, 1U, 0);
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7068U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7068U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7093U, 0U, 0);
    save->crtc_enabled[i] = 0;
  } else {
    save->crtc_enabled[i] = 0;
  }
  i = i + 1;
  ldv_54368: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54367;
  } else {
  }
  return;
}
}
static void dce_v11_0_resume_mc_access(struct amdgpu_device *adev , struct amdgpu_mode_mc_save *save )
{
  u32 tmp ;
  u32 frame_count ;
  int i ;
  int j ;
  u32 tmp___0 ;
  unsigned long __ms ;
  unsigned long tmp___1 ;
  {
  i = 0;
  goto ldv_54385;
  ldv_54384:
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 6663U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 6664U, (unsigned int )(adev->mc.vram_start >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 6660U, (unsigned int )adev->mc.vram_start,
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 6661U, (unsigned int )adev->mc.vram_start,
                 0);
  if ((int )save->crtc_enabled[i]) {
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7102U, 0);
    if ((tmp & 7U) != 3U) {
      tmp = (tmp & 4294967288U) | 3U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7102U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 6673U, 0);
    if ((tmp & 65536U) >> 16 != 0U) {
      tmp = tmp & 4294901759U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 6673U, tmp, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7101U, 0);
    if ((int )tmp & 1) {
      tmp = tmp & 4294967294U;
      amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7101U, tmp, 0);
    } else {
    }
    j = 0;
    goto ldv_54380;
    ldv_54379:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 6673U, 0);
    if ((tmp & 4U) >> 2 == 0U) {
      goto ldv_54378;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_54380: ;
    if (adev->usec_timeout > j) {
      goto ldv_54379;
    } else {
    }
    ldv_54378:
    tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[i] + 7069U, 0);
    tmp = tmp & 4294967039U;
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7093U, 1U, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7069U, tmp, 0);
    amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[i] + 7093U, 0U, 0);
    frame_count = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    j = 0;
    goto ldv_54383;
    ldv_54382:
    tmp___0 = (*((adev->mode_info.funcs)->vblank_get_counter))(adev, i);
    if (tmp___0 != frame_count) {
      goto ldv_54381;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_54383: ;
    if (adev->usec_timeout > j) {
      goto ldv_54382;
    } else {
    }
    ldv_54381: ;
  } else {
  }
  i = i + 1;
  ldv_54385: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54384;
  } else {
  }
  amdgpu_mm_wreg(adev, 201U, (unsigned int )(adev->mc.vram_start >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 196U, (unsigned int )adev->mc.vram_start, 0);
  amdgpu_mm_wreg(adev, 202U, save->vga_hdp_control, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_54389;
    ldv_54388:
    __const_udelay(4295000UL);
    ldv_54389:
    tmp___1 = __ms;
    __ms = __ms - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_54388;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 192U, save->vga_render_control, 0);
  return;
}
}
static void dce_v11_0_set_vga_render_state(struct amdgpu_device *adev , bool render )
{
  u32 tmp ;
  {
  tmp = amdgpu_mm_rreg(adev, 202U, 0);
  if ((int )render) {
    tmp = tmp & 4294967279U;
  } else {
    tmp = tmp | 16U;
  }
  amdgpu_mm_wreg(adev, 202U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, 192U, 0);
  if ((int )render) {
    tmp = (tmp & 4294770687U) | 65536U;
  } else {
    tmp = tmp & 4294770687U;
  }
  amdgpu_mm_wreg(adev, 192U, tmp, 0);
  return;
}
}
static void dce_v11_0_program_fmt(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  int bpc ;
  u32 tmp___0 ;
  enum amdgpu_connector_dither dither ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___1 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 0;
  tmp___0 = 0U;
  dither = 0;
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    bpc = amdgpu_connector_get_monitor_bpc(connector);
    dither = amdgpu_connector->dither;
  } else {
  }
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    return;
  } else {
  }
  if (amdgpu_encoder->encoder_id == 21U || amdgpu_encoder->encoder_id == 22U) {
    return;
  } else {
  }
  if (bpc == 0) {
    return;
  } else {
  }
  switch (bpc) {
  case 6: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = tmp___0 & 4294961151U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = tmp___0 & 4294967247U;
  }
  goto ldv_54415;
  case 8: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 16384U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = (tmp___0 & 4294961151U) | 2048U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = (tmp___0 & 4294967247U) | 16U;
  }
  goto ldv_54415;
  case 10: ;
  if ((unsigned int )dither == 1U) {
    tmp___0 = tmp___0 | 8192U;
    tmp___0 = tmp___0 | 32768U;
    tmp___0 = tmp___0 | 16384U;
    tmp___0 = tmp___0 | 256U;
    tmp___0 = (tmp___0 & 4294961151U) | 4096U;
  } else {
    tmp___0 = tmp___0 | 1U;
    tmp___0 = (tmp___0 & 4294967247U) | 32U;
  }
  goto ldv_54415;
  default: ;
  goto ldv_54415;
  }
  ldv_54415:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7154U, tmp___0, 0);
  return;
}
}
static u32 dce_v11_0_line_buffer_adjust(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                        struct drm_display_mode *mode )
{
  u32 tmp ;
  u32 buffer_alloc ;
  u32 i ;
  u32 mem_cfg ;
  u32 pipe_offset ;
  long tmp___0 ;
  {
  pipe_offset = (u32 )amdgpu_crtc->crtc_id;
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    if (mode->crtc_hdisplay <= 1919) {
      mem_cfg = 1U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 2559) {
      mem_cfg = 2U;
      buffer_alloc = 2U;
    } else
    if (mode->crtc_hdisplay <= 4095) {
      mem_cfg = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    } else {
      tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("dce_v11_0_line_buffer_adjust", "Mode too big for LB!\n");
      } else {
      }
      mem_cfg = 0U;
      buffer_alloc = (adev->flags & 131072UL) != 0UL ? 2U : 4U;
    }
  } else {
    mem_cfg = 1U;
    buffer_alloc = 0U;
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6849U, 0);
  tmp = (tmp & 4291821567U) | ((mem_cfg << 20) & 3145728U);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6849U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, pipe_offset + 801U, 0);
  tmp = (tmp & 4294967288U) | (buffer_alloc & 7U);
  amdgpu_mm_wreg(adev, pipe_offset + 801U, tmp, 0);
  i = 0U;
  goto ldv_54432;
  ldv_54431:
  tmp = amdgpu_mm_rreg(adev, pipe_offset + 801U, 0);
  if ((tmp & 16U) >> 4 != 0U) {
    goto ldv_54430;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_54432: ;
  if ((u32 )adev->usec_timeout > i) {
    goto ldv_54431;
  } else {
  }
  ldv_54430: ;
  if ((int )amdgpu_crtc->base.enabled && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    switch (mem_cfg) {
    case 0U: ;
    default: ;
    return (8192U);
    case 1U: ;
    return (3840U);
    case 2U: ;
    return (5120U);
    }
  } else {
  }
  return (0U);
}
}
static u32 cik_get_number_of_dram_channels___1(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 2049U, 0);
  tmp = tmp___0;
  switch ((tmp & 61440U) >> 12) {
  case 0U: ;
  default: ;
  return (1U);
  case 1U: ;
  return (2U);
  case 2U: ;
  return (4U);
  case 3U: ;
  return (8U);
  case 4U: ;
  return (3U);
  case 5U: ;
  return (6U);
  case 6U: ;
  return (10U);
  case 7U: ;
  return (12U);
  case 8U: ;
  return (16U);
  }
}
}
static u32 dce_v11_0_dram_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 dram_efficiency ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  dram_efficiency.full = 28672U;
  dram_efficiency.full = dfixed_div(dram_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )dram_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v11_0_dram_bandwidth_for_display(struct dce10_wm_params *wm )
{
  fixed20_12 disp_dram_allocation ;
  fixed20_12 yclk ;
  fixed20_12 dram_channels ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  yclk.full = wm->yclk << 12;
  yclk.full = dfixed_div(yclk, a);
  dram_channels.full = wm->dram_channels * 4U << 12;
  a.full = 40960U;
  disp_dram_allocation.full = 12288U;
  disp_dram_allocation.full = dfixed_div(disp_dram_allocation, a);
  bandwidth.full = (u32 )(((unsigned long long )dram_channels.full * (unsigned long long )yclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )disp_dram_allocation.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v11_0_data_return_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 return_efficiency ;
  fixed20_12 sclk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  sclk.full = wm->sclk << 12;
  sclk.full = dfixed_div(sclk, a);
  a.full = 40960U;
  return_efficiency.full = 32768U;
  return_efficiency.full = dfixed_div(return_efficiency, a);
  a.full = 131072U;
  bandwidth.full = (u32 )(((unsigned long long )a.full * (unsigned long long )sclk.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )return_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v11_0_dmif_request_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 disp_clk_request_efficiency ;
  fixed20_12 disp_clk ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  fixed20_12 b ;
  {
  a.full = 4096000U;
  disp_clk.full = wm->disp_clk << 12;
  disp_clk.full = dfixed_div(disp_clk, a);
  a.full = 131072U;
  b.full = (u32 )(((unsigned long long )a.full * (unsigned long long )disp_clk.full + 2048ULL) >> 12);
  a.full = 40960U;
  disp_clk_request_efficiency.full = 32768U;
  disp_clk_request_efficiency.full = dfixed_div(disp_clk_request_efficiency, a);
  bandwidth.full = (u32 )(((unsigned long long )b.full * (unsigned long long )disp_clk_request_efficiency.full + 2048ULL) >> 12);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v11_0_available_bandwidth(struct dce10_wm_params *wm )
{
  u32 dram_bandwidth ;
  u32 tmp ;
  u32 data_return_bandwidth ;
  u32 tmp___0 ;
  u32 dmif_req_bandwidth ;
  u32 tmp___1 ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  tmp = dce_v11_0_dram_bandwidth(wm);
  dram_bandwidth = tmp;
  tmp___0 = dce_v11_0_data_return_bandwidth(wm);
  data_return_bandwidth = tmp___0;
  tmp___1 = dce_v11_0_dmif_request_bandwidth(wm);
  dmif_req_bandwidth = tmp___1;
  _min1 = dram_bandwidth;
  _min1___0 = data_return_bandwidth;
  _min2___0 = dmif_req_bandwidth;
  _min2 = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  return (_min1 < _min2 ? _min1 : _min2);
}
}
static u32 dce_v11_0_average_bandwidth(struct dce10_wm_params *wm )
{
  fixed20_12 bpp ;
  fixed20_12 line_time ;
  fixed20_12 src_width ;
  fixed20_12 bandwidth ;
  fixed20_12 a ;
  {
  a.full = 4096000U;
  line_time.full = (wm->active_time + wm->blank_time) << 12;
  line_time.full = dfixed_div(line_time, a);
  bpp.full = wm->bytes_per_pixel << 12;
  src_width.full = wm->src_width << 12;
  bandwidth.full = (u32 )(((unsigned long long )src_width.full * (unsigned long long )bpp.full + 2048ULL) >> 12);
  bandwidth.full = (u32 )(((unsigned long long )bandwidth.full * (unsigned long long )wm->vsc.full + 2048ULL) >> 12);
  bandwidth.full = dfixed_div(bandwidth, line_time);
  return (bandwidth.full >> 12);
}
}
static u32 dce_v11_0_latency_watermark(struct dce10_wm_params *wm )
{
  u32 mc_latency ;
  u32 available_bandwidth ;
  u32 tmp ;
  u32 worst_chunk_return_time ;
  u32 cursor_line_pair_return_time ;
  u32 dc_latency ;
  u32 other_heads_data_return_time ;
  u32 latency ;
  u32 max_src_lines_per_dst_line ;
  u32 lb_fill_bw ;
  u32 line_fill_time ;
  u32 tmp___0 ;
  u32 dmif_size ;
  fixed20_12 a ;
  fixed20_12 b ;
  fixed20_12 c ;
  u32 _min1 ;
  u32 _min2 ;
  u32 _min1___0 ;
  u32 _min2___0 ;
  {
  mc_latency = 2000U;
  tmp = dce_v11_0_available_bandwidth(wm);
  available_bandwidth = tmp;
  worst_chunk_return_time = 4096000U / available_bandwidth;
  cursor_line_pair_return_time = 512000U / available_bandwidth;
  dc_latency = 40000000U / wm->disp_clk;
  other_heads_data_return_time = (wm->num_heads + 1U) * worst_chunk_return_time + wm->num_heads * cursor_line_pair_return_time;
  latency = (mc_latency + other_heads_data_return_time) + dc_latency;
  dmif_size = 12288U;
  if (wm->num_heads == 0U) {
    return (0U);
  } else {
  }
  a.full = 8192U;
  b.full = 4096U;
  if (((wm->vsc.full > a.full || (wm->vsc.full > b.full && wm->vtaps > 2U)) || wm->vtaps > 4U) || (wm->vsc.full >= a.full && (int )wm->interlaced)) {
    max_src_lines_per_dst_line = 4U;
  } else {
    max_src_lines_per_dst_line = 2U;
  }
  a.full = available_bandwidth << 12;
  b.full = wm->num_heads << 12;
  a.full = dfixed_div(a, b);
  b.full = (mc_latency + 512U) << 12;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(b, c);
  c.full = dmif_size << 12;
  b.full = dfixed_div(c, b);
  _min1 = a.full >> 12;
  _min2 = b.full >> 12;
  tmp___0 = _min1 < _min2 ? _min1 : _min2;
  b.full = 4096000U;
  c.full = wm->disp_clk << 12;
  b.full = dfixed_div(c, b);
  c.full = wm->bytes_per_pixel << 12;
  b.full = (u32 )(((unsigned long long )b.full * (unsigned long long )c.full + 2048ULL) >> 12);
  _min1___0 = tmp___0;
  _min2___0 = b.full >> 12;
  lb_fill_bw = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
  a.full = (wm->src_width * max_src_lines_per_dst_line) * wm->bytes_per_pixel << 12;
  b.full = 4096000U;
  c.full = lb_fill_bw << 12;
  b.full = dfixed_div(c, b);
  a.full = dfixed_div(a, b);
  line_fill_time = a.full >> 12;
  if (wm->active_time > line_fill_time) {
    return (latency);
  } else {
    return ((line_fill_time - wm->active_time) + latency);
  }
}
}
static bool dce_v11_0_average_bandwidth_vs_dram_bandwidth_for_display(struct dce10_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v11_0_average_bandwidth(wm);
  tmp___0 = dce_v11_0_dram_bandwidth_for_display(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v11_0_average_bandwidth_vs_available_bandwidth(struct dce10_wm_params *wm )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp = dce_v11_0_average_bandwidth(wm);
  tmp___0 = dce_v11_0_available_bandwidth(wm);
  if (tmp <= tmp___0 / wm->num_heads) {
    return (1);
  } else {
    return (0);
  }
}
}
static bool dce_v11_0_check_latency_hiding(struct dce10_wm_params *wm )
{
  u32 lb_partitions ;
  u32 line_time ;
  u32 latency_tolerant_lines ;
  u32 latency_hiding ;
  fixed20_12 a ;
  u32 tmp ;
  {
  lb_partitions = wm->lb_size / wm->src_width;
  line_time = wm->active_time + wm->blank_time;
  a.full = 4096U;
  if (wm->vsc.full > a.full) {
    latency_tolerant_lines = 1U;
  } else
  if (wm->vtaps + 1U >= lb_partitions) {
    latency_tolerant_lines = 1U;
  } else {
    latency_tolerant_lines = 2U;
  }
  latency_hiding = latency_tolerant_lines * line_time + wm->blank_time;
  tmp = dce_v11_0_latency_watermark(wm);
  if (tmp <= latency_hiding) {
    return (1);
  } else {
    return (0);
  }
}
}
static void dce_v11_0_program_watermarks(struct amdgpu_device *adev , struct amdgpu_crtc *amdgpu_crtc ,
                                         u32 lb_size , u32 num_heads )
{
  struct drm_display_mode *mode ;
  struct dce10_wm_params wm_low ;
  struct dce10_wm_params wm_high ;
  u32 pixel_period ;
  u32 line_time ;
  u32 latency_watermark_a ;
  u32 latency_watermark_b ;
  u32 tmp ;
  u32 wm_mask ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 _min1___0 ;
  u32 tmp___2 ;
  unsigned int _min2___0 ;
  long tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  bool tmp___6 ;
  int tmp___7 ;
  bool tmp___8 ;
  int tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 _min1___1 ;
  u32 tmp___12 ;
  unsigned int _min2___1 ;
  long tmp___13 ;
  bool tmp___14 ;
  int tmp___15 ;
  bool tmp___16 ;
  int tmp___17 ;
  bool tmp___18 ;
  int tmp___19 ;
  {
  mode = & amdgpu_crtc->base.mode;
  line_time = 0U;
  latency_watermark_a = 0U;
  latency_watermark_b = 0U;
  if (((int )amdgpu_crtc->base.enabled && num_heads != 0U) && (unsigned long )mode != (unsigned long )((struct drm_display_mode *)0)) {
    pixel_period = 1000000U / (unsigned int )mode->clock;
    _min1 = (unsigned int )mode->crtc_htotal * pixel_period;
    _min2 = 65535U;
    line_time = _min1 < _min2 ? _min1 : _min2;
    if ((int )adev->pm.dpm_enabled) {
      tmp___0 = (*((adev->pm.funcs)->get_mclk))(adev, 0);
      wm_high.yclk = tmp___0 * 10U;
      tmp___1 = (*((adev->pm.funcs)->get_sclk))(adev, 0);
      wm_high.sclk = tmp___1 * 10U;
    } else {
      wm_high.yclk = adev->pm.current_mclk * 10U;
      wm_high.sclk = adev->pm.current_sclk * 10U;
    }
    wm_high.disp_clk = (u32 )mode->clock;
    wm_high.src_width = (u32 )mode->crtc_hdisplay;
    wm_high.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_high.blank_time = line_time - wm_high.active_time;
    wm_high.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_high.interlaced = 1;
    } else {
    }
    wm_high.vsc = amdgpu_crtc->vsc;
    wm_high.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_high.vtaps = 2U;
    } else {
    }
    wm_high.bytes_per_pixel = 4U;
    wm_high.lb_size = lb_size;
    wm_high.dram_channels = cik_get_number_of_dram_channels___1(adev);
    wm_high.num_heads = num_heads;
    tmp___2 = dce_v11_0_latency_watermark(& wm_high);
    _min1___0 = tmp___2;
    _min2___0 = 65535U;
    latency_watermark_a = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    tmp___4 = dce_v11_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_high);
    if (tmp___4) {
      tmp___5 = 0;
    } else {
      tmp___5 = 1;
    }
    if (tmp___5) {
      goto _L;
    } else {
      tmp___6 = dce_v11_0_average_bandwidth_vs_available_bandwidth(& wm_high);
      if (tmp___6) {
        tmp___7 = 0;
      } else {
        tmp___7 = 1;
      }
      if (tmp___7) {
        goto _L;
      } else {
        tmp___8 = dce_v11_0_check_latency_hiding(& wm_high);
        if (tmp___8) {
          tmp___9 = 0;
        } else {
          tmp___9 = 1;
        }
        if (tmp___9) {
          goto _L;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L:
          tmp___3 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___3 != 0L) {
            drm_ut_debug_printk("dce_v11_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
    if ((int )adev->pm.dpm_enabled) {
      tmp___10 = (*((adev->pm.funcs)->get_mclk))(adev, 1);
      wm_low.yclk = tmp___10 * 10U;
      tmp___11 = (*((adev->pm.funcs)->get_sclk))(adev, 1);
      wm_low.sclk = tmp___11 * 10U;
    } else {
      wm_low.yclk = adev->pm.current_mclk * 10U;
      wm_low.sclk = adev->pm.current_sclk * 10U;
    }
    wm_low.disp_clk = (u32 )mode->clock;
    wm_low.src_width = (u32 )mode->crtc_hdisplay;
    wm_low.active_time = (u32 )mode->crtc_hdisplay * pixel_period;
    wm_low.blank_time = line_time - wm_low.active_time;
    wm_low.interlaced = 0;
    if ((mode->flags & 16U) != 0U) {
      wm_low.interlaced = 1;
    } else {
    }
    wm_low.vsc = amdgpu_crtc->vsc;
    wm_low.vtaps = 1U;
    if ((unsigned int )amdgpu_crtc->rmx_type != 0U) {
      wm_low.vtaps = 2U;
    } else {
    }
    wm_low.bytes_per_pixel = 4U;
    wm_low.lb_size = lb_size;
    wm_low.dram_channels = cik_get_number_of_dram_channels___1(adev);
    wm_low.num_heads = num_heads;
    tmp___12 = dce_v11_0_latency_watermark(& wm_low);
    _min1___1 = tmp___12;
    _min2___1 = 65535U;
    latency_watermark_b = _min1___1 < _min2___1 ? _min1___1 : _min2___1;
    tmp___14 = dce_v11_0_average_bandwidth_vs_dram_bandwidth_for_display(& wm_low);
    if (tmp___14) {
      tmp___15 = 0;
    } else {
      tmp___15 = 1;
    }
    if (tmp___15) {
      goto _L___0;
    } else {
      tmp___16 = dce_v11_0_average_bandwidth_vs_available_bandwidth(& wm_low);
      if (tmp___16) {
        tmp___17 = 0;
      } else {
        tmp___17 = 1;
      }
      if (tmp___17) {
        goto _L___0;
      } else {
        tmp___18 = dce_v11_0_check_latency_hiding(& wm_low);
        if (tmp___18) {
          tmp___19 = 0;
        } else {
          tmp___19 = 1;
        }
        if (tmp___19) {
          goto _L___0;
        } else
        if (adev->mode_info.disp_priority == 2) {
          _L___0:
          tmp___13 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
          if (tmp___13 != 0L) {
            drm_ut_debug_printk("dce_v11_0_program_watermarks", "force priority to high\n");
          } else {
          }
        } else {
        }
      }
    }
  } else {
  }
  wm_mask = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6962U, 0);
  tmp = (wm_mask & 4294966527U) | 256U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6963U, 0);
  tmp = (tmp & 4294901760U) | (latency_watermark_a & 65535U);
  tmp = (tmp & 65535U) | (line_time << 16);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, tmp, 0);
  tmp = (wm_mask & 4294966527U) | 512U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6963U, 0);
  tmp = (tmp & 4294901760U) | (latency_watermark_a & 65535U);
  tmp = (tmp & 65535U) | (line_time << 16);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6963U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6962U, wm_mask, 0);
  amdgpu_crtc->line_time = line_time;
  amdgpu_crtc->wm_high = latency_watermark_a;
  amdgpu_crtc->wm_low = latency_watermark_b;
  return;
}
}
static void dce_v11_0_bandwidth_update(struct amdgpu_device *adev )
{
  struct drm_display_mode *mode ;
  u32 num_heads ;
  u32 lb_size ;
  int i ;
  {
  mode = (struct drm_display_mode *)0;
  num_heads = 0U;
  amdgpu_update_display_priority(adev);
  i = 0;
  goto ldv_54590;
  ldv_54589: ;
  if ((int )(adev->mode_info.crtcs[i])->base.enabled) {
    num_heads = num_heads + 1U;
  } else {
  }
  i = i + 1;
  ldv_54590: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54589;
  } else {
  }
  i = 0;
  goto ldv_54593;
  ldv_54592:
  mode = & (adev->mode_info.crtcs[i])->base.mode;
  lb_size = dce_v11_0_line_buffer_adjust(adev, adev->mode_info.crtcs[i], mode);
  dce_v11_0_program_watermarks(adev, adev->mode_info.crtcs[i], lb_size, num_heads);
  i = i + 1;
  ldv_54593: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_54592;
  } else {
  }
  return;
}
}
static void dce_v11_0_audio_get_connected_pins(struct amdgpu_device *adev )
{
  int i ;
  u32 offset ;
  u32 tmp ;
  {
  i = 0;
  goto ldv_54602;
  ldv_54601:
  offset = adev->mode_info.audio.pin[i].offset;
  tmp = (*(adev->audio_endpt_rreg))(adev, offset, 86U);
  if (tmp >> 30 == 1U) {
    adev->mode_info.audio.pin[i].connected = 0;
  } else {
    adev->mode_info.audio.pin[i].connected = 1;
  }
  i = i + 1;
  ldv_54602: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54601;
  } else {
  }
  return;
}
}
static struct amdgpu_audio_pin *dce_v11_0_audio_get_pin(struct amdgpu_device *adev )
{
  int i ;
  {
  dce_v11_0_audio_get_connected_pins(adev);
  i = 0;
  goto ldv_54609;
  ldv_54608: ;
  if ((int )adev->mode_info.audio.pin[i].connected) {
    return ((struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i);
  } else {
  }
  i = i + 1;
  ldv_54609: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54608;
  } else {
  }
  drm_err("No connected audio pins found!\n");
  return ((struct amdgpu_audio_pin *)0);
}
}
static void dce_v11_0_afmt_audio_select_pin(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19013), 0);
  tmp = (tmp & 4294967288U) | (((dig->afmt)->pin)->id & 7U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19013), tmp, 0);
  return;
}
}
static void dce_v11_0_audio_write_latency_fields(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 tmp ;
  int interlace ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  interlace = 0;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_54641;
  ldv_54640: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_54639;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_54641: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_54640;
  } else {
  }
  ldv_54639: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  if ((mode->flags & 16U) != 0U) {
    interlace = 1;
  } else {
  }
  if ((int )connector->latency_present[interlace]) {
    tmp = (u32 )connector->video_latency[interlace] & 255U;
    tmp = (u32 )(connector->audio_latency[interlace] << 8) & 65535U;
  } else {
    tmp = 0U;
    tmp = 0U;
  }
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, 55U, tmp);
  return;
}
}
static void dce_v11_0_audio_write_speaker_allocation(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  u32 tmp ;
  u8 *sadb ;
  int sad_count ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  sadb = (u8 *)0U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_54663;
  ldv_54662: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_54661;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_54663: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_54662;
  } else {
  }
  ldv_54661: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp___0 = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_speaker_allocation(tmp___0, & sadb);
  if (sad_count < 0) {
    drm_err("Couldn\'t read Speaker Allocation Data Block: %d\n", sad_count);
    sad_count = 0;
  } else {
  }
  tmp = (*(adev->audio_endpt_rreg))(adev, ((dig->afmt)->pin)->offset, 37U);
  tmp = tmp & 4294836223U;
  tmp = tmp | 65536U;
  if (sad_count != 0) {
    tmp = (tmp & 4294967168U) | ((u32 )*sadb & 127U);
  } else {
    tmp = (tmp & 4294967168U) | 5U;
  }
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, 37U, tmp);
  kfree((void const *)sadb);
  return;
}
}
static void dce_v11_0_audio_write_sad_regs(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct amdgpu_connector *amdgpu_connector ;
  struct cea_sad *sads ;
  int i ;
  int sad_count ;
  u16 eld_reg_to_type[12U][2U] ;
  struct list_head const *__mptr___0 ;
  struct drm_connector const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct edid *tmp ;
  long tmp___0 ;
  u32 tmp___1 ;
  u8 stereo_freqs ;
  int max_channels ;
  int j ;
  struct cea_sad *sad ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  amdgpu_connector = (struct amdgpu_connector *)0;
  eld_reg_to_type[0][0] = 40U;
  eld_reg_to_type[0][1] = 1U;
  eld_reg_to_type[1][0] = 41U;
  eld_reg_to_type[1][1] = 2U;
  eld_reg_to_type[2][0] = 42U;
  eld_reg_to_type[2][1] = 3U;
  eld_reg_to_type[3][0] = 43U;
  eld_reg_to_type[3][1] = 4U;
  eld_reg_to_type[4][0] = 44U;
  eld_reg_to_type[4][1] = 5U;
  eld_reg_to_type[5][0] = 45U;
  eld_reg_to_type[5][1] = 6U;
  eld_reg_to_type[6][0] = 46U;
  eld_reg_to_type[6][1] = 7U;
  eld_reg_to_type[7][0] = 47U;
  eld_reg_to_type[7][1] = 8U;
  eld_reg_to_type[8][0] = 49U;
  eld_reg_to_type[8][1] = 10U;
  eld_reg_to_type[9][0] = 50U;
  eld_reg_to_type[9][1] = 11U;
  eld_reg_to_type[10][0] = 51U;
  eld_reg_to_type[10][1] = 12U;
  eld_reg_to_type[11][0] = 53U;
  eld_reg_to_type[11][1] = 14U;
  if (((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) || (unsigned long )(dig->afmt)->pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  __mptr___0 = (struct list_head const *)(encoder->dev)->mode_config.connector_list.next;
  connector = (struct drm_connector *)__mptr___0 + 0xffffffffffffffe8UL;
  goto ldv_54686;
  ldv_54685: ;
  if ((unsigned long )connector->encoder == (unsigned long )encoder) {
    __mptr___1 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___1;
    goto ldv_54684;
  } else {
  }
  __mptr___2 = (struct list_head const *)connector->head.next;
  connector = (struct drm_connector *)__mptr___2 + 0xffffffffffffffe8UL;
  ldv_54686: ;
  if ((unsigned long )(& connector->head) != (unsigned long )(& (encoder->dev)->mode_config.connector_list)) {
    goto ldv_54685;
  } else {
  }
  ldv_54684: ;
  if ((unsigned long )amdgpu_connector == (unsigned long )((struct amdgpu_connector *)0)) {
    drm_err("Couldn\'t find encoder\'s connector\n");
    return;
  } else {
  }
  tmp = amdgpu_connector_edid(connector);
  sad_count = drm_edid_to_sad(tmp, & sads);
  if (sad_count <= 0) {
    drm_err("Couldn\'t read SADs: %d\n", sad_count);
    return;
  } else {
  }
  tmp___0 = ldv__builtin_expect((unsigned long )sads == (unsigned long )((struct cea_sad *)0),
                             0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/dce_v11_0.c"),
                         "i" (1559), "i" (12UL));
    ldv_54687: ;
    goto ldv_54687;
  } else {
  }
  i = 0;
  goto ldv_54699;
  ldv_54698:
  tmp___1 = 0U;
  stereo_freqs = 0U;
  max_channels = -1;
  j = 0;
  goto ldv_54697;
  ldv_54696:
  sad = sads + (unsigned long )j;
  if ((int )((unsigned short )sad->format) == (int )eld_reg_to_type[i][1]) {
    if ((int )sad->channels > max_channels) {
      tmp___1 = (tmp___1 & 4294967288U) | ((u32 )sad->channels & 7U);
      tmp___1 = (tmp___1 & 4278255615U) | ((u32 )((int )sad->byte2 << 16) & 16711680U);
      tmp___1 = (tmp___1 & 4294902015U) | ((u32 )((int )sad->freq << 8) & 65535U);
      max_channels = (int )sad->channels;
    } else {
    }
    if ((unsigned int )sad->format == 1U) {
      stereo_freqs = (u8 )((int )sad->freq | (int )stereo_freqs);
    } else {
      goto ldv_54695;
    }
  } else {
  }
  j = j + 1;
  ldv_54697: ;
  if (j < sad_count) {
    goto ldv_54696;
  } else {
  }
  ldv_54695:
  tmp___1 = (tmp___1 & 16777215U) | (u32 )((int )stereo_freqs << 24);
  (*(adev->audio_endpt_wreg))(adev, ((dig->afmt)->pin)->offset, (u32 )eld_reg_to_type[i][0],
                              tmp___1);
  i = i + 1;
  ldv_54699: ;
  if ((unsigned int )i <= 11U) {
    goto ldv_54698;
  } else {
  }
  kfree((void const *)sads);
  return;
}
}
static void dce_v11_0_audio_enable(struct amdgpu_device *adev , struct amdgpu_audio_pin *pin ,
                                   bool enable )
{
  {
  if ((unsigned long )pin == (unsigned long )((struct amdgpu_audio_pin *)0)) {
    return;
  } else {
  }
  (*(adev->audio_endpt_wreg))(adev, pin->offset, 84U, (int )enable ? 2147483648U : 0U);
  return;
}
}
static u32 const pin_offsets___1[7U] = { 0U, 4U, 8U, 12U,
        16U, 20U, 28U};
static int dce_v11_0_audio_init(struct amdgpu_device *adev )
{
  int i ;
  {
  if (amdgpu_audio == 0) {
    return (0);
  } else {
  }
  adev->mode_info.audio.enabled = 1;
  adev->mode_info.audio.num_pins = 7;
  i = 0;
  goto ldv_54712;
  ldv_54711:
  adev->mode_info.audio.pin[i].channels = -1;
  adev->mode_info.audio.pin[i].rate = -1;
  adev->mode_info.audio.pin[i].bits_per_sample = -1;
  adev->mode_info.audio.pin[i].status_bits = 0U;
  adev->mode_info.audio.pin[i].category_code = 0U;
  adev->mode_info.audio.pin[i].connected = 0;
  adev->mode_info.audio.pin[i].offset = pin_offsets___1[i];
  adev->mode_info.audio.pin[i].id = (u32 )i;
  dce_v11_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_54712: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54711;
  } else {
  }
  return (0);
}
}
static void dce_v11_0_audio_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  if (! adev->mode_info.audio.enabled) {
    return;
  } else {
  }
  i = 0;
  goto ldv_54719;
  ldv_54718:
  dce_v11_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_54719: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_54718;
  } else {
  }
  adev->mode_info.audio.enabled = 0;
  return;
}
}
static void dce_v11_0_afmt_update_ACR(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_afmt_acr acr ;
  struct amdgpu_afmt_acr tmp ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  u32 tmp___0 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_afmt_acr(clock);
  acr = tmp;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18990), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_32khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18990), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18991), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_32khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18991), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18992), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_44_1khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18992), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18993), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_44_1khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18993), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18994), 0);
  tmp___0 = (tmp___0 & 4095U) | (u32 )(acr.cts_48khz << 12);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18994), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18995), 0);
  tmp___0 = (tmp___0 & 4293918720U) | ((u32 )acr.n_48khz & 1048575U);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18995), tmp___0, 0);
  return;
}
}
static void dce_v11_0_afmt_update_avi_infoframe(struct drm_encoder *encoder , void *buffer ,
                                                size_t size )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  uint8_t *frame ;
  uint8_t *header ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  frame = (uint8_t *)buffer + 3U;
  header = (uint8_t *)buffer;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18974), (u32 )((((int )*frame | ((int )*(frame + 1UL) << 8)) | ((int )*(frame + 2UL) << 16)) | ((int )*(frame + 3UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18975), (u32 )((((int )*(frame + 4UL) | ((int )*(frame + 5UL) << 8)) | ((int )*(frame + 6UL) << 16)) | ((int )*(frame + 7UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18976), (u32 )((((int )*(frame + 8UL) | ((int )*(frame + 9UL) << 8)) | ((int )*(frame + 10UL) << 16)) | ((int )*(frame + 11UL) << 24)),
                 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18977), (u32 )(((int )*(frame + 12UL) | ((int )*(frame + 13UL) << 8)) | ((int )*(header + 1UL) << 24)),
                 0);
  return;
}
}
static void dce_v11_0_audio_set_dto(struct drm_encoder *encoder , u32 clock )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  u32 dto_phase ;
  u32 dto_modulo ;
  u32 tmp ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  __mptr___0 = (struct drm_crtc const *)encoder->crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
  dto_phase = 24000U;
  dto_modulo = clock;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 363U, 0);
  tmp = (tmp & 4294967288U) | ((u32 )amdgpu_crtc->crtc_id & 7U);
  amdgpu_mm_wreg(adev, 363U, tmp, 0);
  amdgpu_mm_wreg(adev, 364U, dto_phase, 0);
  amdgpu_mm_wreg(adev, 365U, dto_modulo, 0);
  return;
}
}
static void dce_v11_0_afmt_setmode(struct drm_encoder *encoder , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  u8 buffer[17U] ;
  struct hdmi_avi_infoframe frame ;
  ssize_t err ;
  u32 tmp___0 ;
  int bpc ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr___0 ;
  long tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  int tmp___4 ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  bpc = 8;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if (! (dig->afmt)->enabled) {
    return;
  } else {
  }
  if ((unsigned long )encoder->crtc != (unsigned long )((struct drm_crtc *)0)) {
    __mptr___0 = (struct drm_crtc const *)encoder->crtc;
    amdgpu_crtc = (struct amdgpu_crtc *)__mptr___0;
    bpc = amdgpu_crtc->bpc;
  } else {
  }
  (dig->afmt)->pin = dce_v11_0_audio_get_pin(adev);
  dce_v11_0_audio_enable(adev, (dig->afmt)->pin, 0);
  dce_v11_0_audio_set_dto(encoder, (u32 )mode->clock);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18957), 0);
  tmp___0 = tmp___0 | 1U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18957), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19002), 4096U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18953), 0);
  switch (bpc) {
  case 0: ;
  case 6: ;
  case 8: ;
  case 16: ;
  default:
  tmp___0 = tmp___0 & 4278190079U;
  tmp___0 = tmp___0 & 3489660927U;
  tmp___1 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___1 != 0L) {
    drm_ut_debug_printk("dce_v11_0_afmt_setmode", "%s: Disabling hdmi deep color for %d bpc.\n",
                        connector->name, bpc);
  } else {
  }
  goto ldv_54787;
  case 10:
  tmp___0 = tmp___0 | 16777216U;
  tmp___0 = (tmp___0 & 3489660927U) | 268435456U;
  tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("dce_v11_0_afmt_setmode", "%s: Enabling hdmi deep color 30 for 10 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_54787;
  case 12:
  tmp___0 = tmp___0 | 16777216U;
  tmp___0 = (tmp___0 & 3489660927U) | 536870912U;
  tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___3 != 0L) {
    drm_ut_debug_printk("dce_v11_0_afmt_setmode", "%s: Enabling hdmi deep color 36 for 12 bpc.\n",
                        connector->name);
  } else {
  }
  goto ldv_54787;
  }
  ldv_54787:
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18953), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18957), 0);
  tmp___0 = tmp___0 | 1U;
  tmp___0 = tmp___0 | 16U;
  tmp___0 = tmp___0 | 32U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18957), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18958), 0);
  tmp___0 = tmp___0 | 16U;
  tmp___0 = tmp___0 | 32U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18958), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19012), 0);
  tmp___0 = tmp___0 | 128U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19012), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18959), 0);
  tmp___0 = (tmp___0 & 4294951167U) | 512U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18959), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18963), 0U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18955), 0);
  tmp___0 = (tmp___0 & 4294967247U) | 16U;
  tmp___0 = (tmp___0 & 4292935679U) | 196608U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18955), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19010), 0);
  tmp___0 = tmp___0 | 67108864U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19010), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18956), 0);
  if (bpc > 8) {
    tmp___0 = tmp___0 & 4294967039U;
  } else {
    tmp___0 = tmp___0 | 256U;
  }
  tmp___0 = tmp___0 | 4096U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18956), tmp___0, 0);
  dce_v11_0_afmt_update_ACR(encoder, (u32 )mode->clock);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19000), 0);
  tmp___0 = (tmp___0 & 4279238655U) | 1048576U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19000), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19001), 0);
  tmp___0 = (tmp___0 & 4279238655U) | 2097152U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19001), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19007), 0);
  tmp___0 = (tmp___0 & 4294967280U) | 3U;
  tmp___0 = (tmp___0 & 4294967055U) | 64U;
  tmp___0 = (tmp___0 & 4294963455U) | 1280U;
  tmp___0 = (tmp___0 & 4294905855U) | 24576U;
  tmp___0 = (tmp___0 & 4293984255U) | 458752U;
  tmp___0 = (tmp___0 & 4279238655U) | 8388608U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19007), tmp___0, 0);
  dce_v11_0_audio_write_speaker_allocation(encoder);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18964), 65280U, 0);
  dce_v11_0_afmt_audio_select_pin(encoder);
  dce_v11_0_audio_write_sad_regs(encoder);
  dce_v11_0_audio_write_latency_fields(encoder, mode);
  tmp___4 = drm_hdmi_avi_infoframe_from_display_mode(& frame, (struct drm_display_mode const *)mode);
  err = (ssize_t )tmp___4;
  if (err < 0L) {
    drm_err("failed to setup AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  err = hdmi_avi_infoframe_pack(& frame, (void *)(& buffer), 17UL);
  if (err < 0L) {
    drm_err("failed to pack AVI infoframe: %zd\n", err);
    return;
  } else {
  }
  dce_v11_0_afmt_update_avi_infoframe(encoder, (void *)(& buffer), 17UL);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18958), 0);
  tmp___0 = tmp___0 | 1U;
  tmp___0 = tmp___0 | 2U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18958), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 18959), 0);
  tmp___0 = (tmp___0 & 4294967232U) | 2U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 18959), tmp___0, 0);
  tmp___0 = amdgpu_mm_rreg(adev, (u32 )((dig->afmt)->offset + 19010), 0);
  tmp___0 = tmp___0 | 1U;
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19010), tmp___0, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19003), 16777215U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19004), 8388607U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19005), 1U, 0);
  amdgpu_mm_wreg(adev, (u32 )((dig->afmt)->offset + 19006), 1U, 0);
  dce_v11_0_audio_enable(adev, (dig->afmt)->pin, 1);
  return;
}
}
static void dce_v11_0_afmt_enable(struct drm_encoder *encoder , bool enable )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  long tmp ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  if ((unsigned long )dig == (unsigned long )((struct amdgpu_encoder_atom_dig *)0) || (unsigned long )dig->afmt == (unsigned long )((struct amdgpu_afmt *)0)) {
    return;
  } else {
  }
  if ((int )enable && (int )(dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && ! (dig->afmt)->enabled) {
    return;
  } else {
  }
  if (! enable && (unsigned long )(dig->afmt)->pin != (unsigned long )((struct amdgpu_audio_pin *)0)) {
    dce_v11_0_audio_enable(adev, (dig->afmt)->pin, 0);
    (dig->afmt)->pin = (struct amdgpu_audio_pin *)0;
  } else {
  }
  (dig->afmt)->enabled = enable;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v11_0_afmt_enable", "%sabling AFMT interface @ 0x%04X for encoder 0x%x\n",
                        (int )enable ? (char *)"En" : (char *)"Dis", (dig->afmt)->offset,
                        amdgpu_encoder->encoder_id);
  } else {
  }
  return;
}
}
static void dce_v11_0_afmt_init(struct amdgpu_device *adev )
{
  int i ;
  void *tmp ;
  {
  i = 0;
  goto ldv_54806;
  ldv_54805:
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_54806: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_54805;
  } else {
  }
  i = 0;
  goto ldv_54809;
  ldv_54808:
  tmp = kzalloc(24UL, 208U);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)tmp;
  if ((unsigned long )adev->mode_info.afmt[i] != (unsigned long )((struct amdgpu_afmt *)0)) {
    (adev->mode_info.afmt[i])->offset = (int )dig_offsets___1[i];
    (adev->mode_info.afmt[i])->id = i;
  } else {
  }
  i = i + 1;
  ldv_54809: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_54808;
  } else {
  }
  return;
}
}
static void dce_v11_0_afmt_fini(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_54816;
  ldv_54815:
  kfree((void const *)adev->mode_info.afmt[i]);
  adev->mode_info.afmt[i] = (struct amdgpu_afmt *)0;
  i = i + 1;
  ldv_54816: ;
  if (adev->mode_info.num_dig > i) {
    goto ldv_54815;
  } else {
  }
  return;
}
}
static u32 const vga_control_regs___1[6U] = { 204U, 206U, 248U, 249U,
        250U, 251U};
static void dce_v11_0_vga_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 vga_control ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_mm_rreg(adev, vga_control_regs___1[amdgpu_crtc->crtc_id], 0);
  vga_control = tmp & 4294967294U;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, vga_control_regs___1[amdgpu_crtc->crtc_id], vga_control | 1U,
                   0);
  } else {
    amdgpu_mm_wreg(adev, vga_control_regs___1[amdgpu_crtc->crtc_id], vga_control,
                   0);
  }
  return;
}
}
static void dce_v11_0_grph_enable(struct drm_crtc *crtc , bool enable )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  if ((int )enable) {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 1U, 0);
  } else {
    amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6656U, 0U, 0);
  }
  return;
}
}
static int dce_v11_0_crtc_do_set_base(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                      int x , int y , int atomic )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct drm_framebuffer *target_fb ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *rbo ;
  uint64_t fb_location ;
  uint64_t tiling_flags ;
  u32 fb_format ;
  u32 fb_pitch_pixels ;
  u32 fb_swap ;
  u32 pipe_config ;
  u32 tmp ;
  u32 viewport_w ;
  u32 viewport_h ;
  int r ;
  bool bypass_lut ;
  long tmp___0 ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_framebuffer const *__mptr___1 ;
  struct drm_gem_object const *__mptr___2 ;
  long tmp___1 ;
  long tmp___2 ;
  char const *tmp___3 ;
  unsigned int bankw ;
  unsigned int bankh ;
  unsigned int mtaspect ;
  unsigned int tile_split ;
  unsigned int num_banks ;
  long tmp___4 ;
  struct drm_framebuffer const *__mptr___3 ;
  struct drm_gem_object const *__mptr___4 ;
  long tmp___5 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  fb_swap = 0U;
  bypass_lut = 0;
  if (atomic == 0 && (unsigned long )(crtc->primary)->fb == (unsigned long )((struct drm_framebuffer *)0)) {
    tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_do_set_base", "No FB bound\n");
    } else {
    }
    return (0);
  } else {
  }
  if (atomic != 0) {
    __mptr___0 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    target_fb = fb;
  } else {
    __mptr___1 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___1;
    target_fb = (crtc->primary)->fb;
  }
  obj = amdgpu_fb->obj;
  __mptr___2 = (struct drm_gem_object const *)obj;
  rbo = (struct amdgpu_bo *)__mptr___2 + 0xfffffffffffffbc0UL;
  r = amdgpu_bo_reserve(rbo, 0);
  tmp___1 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___1 != 0L) {
    return (r);
  } else {
  }
  if (atomic != 0) {
    fb_location = amdgpu_bo_gpu_offset(rbo);
  } else {
    r = amdgpu_bo_pin(rbo, 4U, & fb_location);
    tmp___2 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___2 != 0L) {
      amdgpu_bo_unreserve(rbo);
      return (-22);
    } else {
    }
  }
  amdgpu_bo_get_tiling_flags(rbo, & tiling_flags);
  amdgpu_bo_unreserve(rbo);
  pipe_config = (u32 )(tiling_flags >> 4) & 31U;
  switch (target_fb->pixel_format) {
  case 538982467U:
  fb_format = 0U;
  fb_format = fb_format & 4294965503U;
  goto ldv_54873;
  case 842093144U: ;
  case 842093121U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 512U;
  goto ldv_54873;
  case 892424792U: ;
  case 892424769U:
  fb_format = 1U;
  fb_format = fb_format & 4294965503U;
  goto ldv_54873;
  case 892426306U: ;
  case 892420418U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 1280U;
  goto ldv_54873;
  case 909199186U:
  fb_format = 1U;
  fb_format = (fb_format & 4294965503U) | 256U;
  goto ldv_54873;
  case 875713112U: ;
  case 875713089U:
  fb_format = 2U;
  fb_format = fb_format & 4294965503U;
  goto ldv_54873;
  case 808669784U: ;
  case 808669761U:
  fb_format = 2U;
  fb_format = (fb_format & 4294965503U) | 256U;
  bypass_lut = 1;
  goto ldv_54873;
  case 808671298U: ;
  case 808665410U:
  fb_format = 2U;
  fb_format = (fb_format & 4294965503U) | 1024U;
  bypass_lut = 1;
  goto ldv_54873;
  default:
  tmp___3 = drm_get_format_name(target_fb->pixel_format);
  drm_err("Unsupported screen format %s\n", tmp___3);
  return (-22);
  }
  ldv_54873: ;
  if ((tiling_flags & 15ULL) == 4ULL) {
    bankw = (unsigned int )(tiling_flags >> 15) & 3U;
    bankh = (unsigned int )(tiling_flags >> 17) & 3U;
    mtaspect = (unsigned int )(tiling_flags >> 19) & 3U;
    tile_split = (unsigned int )(tiling_flags >> 9) & 7U;
    num_banks = (unsigned int )(tiling_flags >> 21) & 3U;
    fb_format = (fb_format & 4294967283U) | ((num_banks << 2) & 12U);
    fb_format = (fb_format & 4279238655U) | 4194304U;
    fb_format = (fb_format & 4294909951U) | ((tile_split << 13) & 65535U);
    fb_format = (fb_format & 4294967103U) | ((bankw << 6) & 255U);
    fb_format = (fb_format & 4294961151U) | ((bankh << 11) & 6144U);
    fb_format = (fb_format & 4294180863U) | ((mtaspect << 18) & 786432U);
    fb_format = fb_format & 2684354559U;
  } else
  if ((tiling_flags & 15ULL) == 2ULL) {
    fb_format = (fb_format & 4279238655U) | 2097152U;
  } else {
  }
  fb_format = (fb_format & 3774873599U) | ((pipe_config << 24) & 520093696U);
  dce_v11_0_vga_enable(crtc, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6663U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6664U, (unsigned int )(fb_location >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6660U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6661U, (unsigned int )fb_location & 4294967040U,
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6657U, fb_format, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6659U, fb_swap, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6658U, 0);
  if ((int )bypass_lut) {
    tmp = tmp | 256U;
  } else {
    tmp = tmp & 4294967039U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6658U, tmp, 0);
  if ((int )bypass_lut) {
    tmp___4 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
    if (tmp___4 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_do_set_base", "Bypassing hardware LUT due to 10 bit fb scanout.\n");
    } else {
    }
  } else {
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6665U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6666U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6667U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6668U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6669U, target_fb->width, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6670U, target_fb->height, 0);
  fb_pitch_pixels = target_fb->pitches[0] / (unsigned int )(target_fb->bits_per_pixel / 8);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6662U, fb_pitch_pixels, 0);
  dce_v11_0_grph_enable(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6851U, target_fb->height, 0);
  x = x & -4;
  y = y & -2;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7004U, (u32 )((x << 16) | y), 0);
  viewport_w = (u32 )crtc->mode.hdisplay;
  viewport_h = (u32 )(crtc->mode.vdisplay + 1) & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7005U, (viewport_w << 16) | viewport_h,
                 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6674U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6674U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 7102U, 3U, 0);
  if ((atomic == 0 && (unsigned long )fb != (unsigned long )((struct drm_framebuffer *)0)) && (unsigned long )(crtc->primary)->fb != (unsigned long )fb) {
    __mptr___3 = (struct drm_framebuffer const *)fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___3;
    __mptr___4 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___4 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp___5 = ldv__builtin_expect(r != 0, 0L);
    if (tmp___5 != 0L) {
      return (r);
    } else {
    }
    amdgpu_bo_unpin(rbo);
    amdgpu_bo_unreserve(rbo);
  } else {
  }
  dce_v11_0_bandwidth_update(adev);
  return (0);
}
}
static void dce_v11_0_set_interleave(struct drm_crtc *crtc , struct drm_display_mode *mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  u32 tmp ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6848U, 0);
  if ((mode->flags & 16U) != 0U) {
    tmp = tmp | 8U;
  } else {
    tmp = tmp & 4294967287U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6848U, tmp, 0);
  return;
}
}
static void dce_v11_0_crtc_load_lut(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  int i ;
  u32 tmp ;
  long tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp___0 = ldv__builtin_expect((drm_debug & 4U) != 0U, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("dce_v11_0_crtc_load_lut", "%d\n", amdgpu_crtc->crtc_id);
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6709U, 0);
  tmp = tmp & 4294967292U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6709U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6701U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6701U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6672U, 0);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6672U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6784U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6785U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6786U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6787U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6788U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6789U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6790U, 65535U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6776U, 0U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6782U, 7U, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6777U, 0U, 0);
  i = 0;
  goto ldv_54919;
  ldv_54918:
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6780U, (u32 )((((int )amdgpu_crtc->lut_r[i] << 20) | ((int )amdgpu_crtc->lut_g[i] << 10)) | (int )amdgpu_crtc->lut_b[i]),
                 0);
  i = i + 1;
  ldv_54919: ;
  if (i <= 255) {
    goto ldv_54918;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6744U, 0);
  tmp = tmp & 4294967292U;
  tmp = tmp & 4294955007U;
  tmp = tmp & 4294966527U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6744U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6745U, 0);
  tmp = tmp & 4294967292U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6745U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6816U, 0);
  tmp = tmp & 4294967288U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6816U, tmp, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6716U, 0);
  tmp = tmp & 4294967288U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6716U, tmp, 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6736U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6844U, 0);
  tmp = tmp | 2U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6844U, tmp, 0);
  return;
}
}
static int dce_v11_0_pick_dig_encoder(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
  switch (amdgpu_encoder->encoder_id) {
  case 30U: ;
  if ((int )dig->linkb) {
    return (1);
  } else {
    return (0);
  }
  case 32U: ;
  if ((int )dig->linkb) {
    return (3);
  } else {
    return (2);
  }
  case 33U: ;
  if ((int )dig->linkb) {
    return (5);
  } else {
    return (4);
  }
  case 37U: ;
  return (6);
  default:
  drm_err("invalid encoder_id: 0x%x\n", amdgpu_encoder->encoder_id);
  return (0);
  }
}
}
static u32 dce_v11_0_pick_pll(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  u32 pll_in_use ;
  int pll ;
  int tmp ;
  int tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  tmp = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
  if (tmp == 0) {
    goto _L;
  } else {
    tmp___0 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___0 == 5) {
      _L:
      if (adev->clock.dp_extclk != 0U) {
        return (255U);
      } else {
        pll = amdgpu_pll_get_shared_dp_ppll(crtc);
        if (pll != 255) {
          return ((u32 )pll);
        } else {
        }
      }
    } else {
      pll = amdgpu_pll_get_shared_nondp_ppll(crtc);
      if (pll != 255) {
        return ((u32 )pll);
      } else {
      }
    }
  }
  pll_in_use = amdgpu_pll_get_use_mask(crtc);
  if ((unsigned int )adev->asic_type == 7U) {
    if ((pll_in_use & 1U) == 0U) {
      return (0U);
    } else {
    }
    if ((pll_in_use & 4U) == 0U) {
      return (2U);
    } else {
    }
    drm_err("unable to allocate a PPLL\n");
    return (255U);
  } else {
    if ((pll_in_use & 2U) == 0U) {
      return (1U);
    } else {
    }
    if ((pll_in_use & 1U) == 0U) {
      return (0U);
    } else {
    }
    if ((pll_in_use & 4U) == 0U) {
      return (2U);
    } else {
    }
    drm_err("unable to allocate a PPLL\n");
    return (255U);
  }
  return (255U);
}
}
static void dce_v11_0_lock_cursor(struct drm_crtc *crtc , bool lock )
{
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  u32 cur_lock ;
  {
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  cur_lock = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6766U, 0);
  if ((int )lock) {
    cur_lock = cur_lock | 65536U;
  } else {
    cur_lock = cur_lock & 4294901759U;
  }
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6766U, cur_lock, 0);
  return;
}
}
static void dce_v11_0_hide_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6758U, 1);
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, tmp, 1);
  return;
}
}
static void dce_v11_0_show_cursor(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  tmp = amdgpu_mm_rreg(adev, amdgpu_crtc->crtc_offset + 6758U, 1);
  tmp = tmp | 1U;
  tmp = (tmp & 4294966527U) | 512U;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6758U, tmp, 1);
  return;
}
}
static void dce_v11_0_set_cursor(struct drm_crtc *crtc , struct drm_gem_object *obj ,
                                 uint64_t gpu_addr )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6761U, (unsigned int )(gpu_addr >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6759U, (unsigned int )gpu_addr,
                 0);
  return;
}
}
static int dce_v11_0_crtc_cursor_move(struct drm_crtc *crtc , int x , int y )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct amdgpu_device *adev ;
  int xorigin ;
  int yorigin ;
  long tmp ;
  int _min1 ;
  int _min2 ;
  int _min1___0 ;
  int _min2___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  adev = (struct amdgpu_device *)(crtc->dev)->dev_private;
  xorigin = 0;
  yorigin = 0;
  x = crtc->x + x;
  y = crtc->y + y;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("dce_v11_0_crtc_cursor_move", "x %d y %d c->x %d c->y %d\n",
                        x, y, crtc->x, crtc->y);
  } else {
  }
  if (x < 0) {
    _min1 = - x;
    _min2 = amdgpu_crtc->max_cursor_width + -1;
    xorigin = _min1 < _min2 ? _min1 : _min2;
    x = 0;
  } else {
  }
  if (y < 0) {
    _min1___0 = - y;
    _min2___0 = amdgpu_crtc->max_cursor_height + -1;
    yorigin = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    y = 0;
  } else {
  }
  dce_v11_0_lock_cursor(crtc, 1);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6762U, (u32 )((x << 16) | y), 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6763U, (u32 )((xorigin << 16) | yorigin),
                 0);
  amdgpu_mm_wreg(adev, amdgpu_crtc->crtc_offset + 6760U, (u32 )(((amdgpu_crtc->cursor_width + -1) << 16) | (amdgpu_crtc->cursor_height + -1)),
                 0);
  dce_v11_0_lock_cursor(crtc, 0);
  return (0);
}
}
static int dce_v11_0_crtc_cursor_set(struct drm_crtc *crtc , struct drm_file *file_priv ,
                                     u32 handle , u32 width , u32 height )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_gem_object *obj ;
  struct amdgpu_bo *robj ;
  uint64_t gpu_addr ;
  int ret ;
  struct drm_gem_object const *__mptr___0 ;
  long tmp ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp___0 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (handle == 0U) {
    dce_v11_0_hide_cursor(crtc);
    obj = (struct drm_gem_object *)0;
    goto unpin;
  } else {
  }
  if ((u32 )amdgpu_crtc->max_cursor_width < width || (u32 )amdgpu_crtc->max_cursor_height < height) {
    drm_err("bad cursor width or height %d x %d\n", width, height);
    return (-22);
  } else {
  }
  obj = drm_gem_object_lookup(crtc->dev, file_priv, handle);
  if ((unsigned long )obj == (unsigned long )((struct drm_gem_object *)0)) {
    drm_err("Cannot find cursor object %x for crtc %d\n", handle, amdgpu_crtc->crtc_id);
    return (-2);
  } else {
  }
  __mptr___0 = (struct drm_gem_object const *)obj;
  robj = (struct amdgpu_bo *)__mptr___0 + 0xfffffffffffffbc0UL;
  ret = amdgpu_bo_reserve(robj, 0);
  tmp = ldv__builtin_expect(ret != 0, 0L);
  if (tmp != 0L) {
    goto fail;
  } else {
  }
  ret = amdgpu_bo_pin_restricted(robj, 4U, 0ULL, 0ULL, & gpu_addr);
  amdgpu_bo_unreserve(robj);
  if (ret != 0) {
    goto fail;
  } else {
  }
  amdgpu_crtc->cursor_width = (int )width;
  amdgpu_crtc->cursor_height = (int )height;
  dce_v11_0_lock_cursor(crtc, 1);
  dce_v11_0_set_cursor(crtc, obj, gpu_addr);
  dce_v11_0_show_cursor(crtc);
  dce_v11_0_lock_cursor(crtc, 0);
  unpin: ;
  if ((unsigned long )amdgpu_crtc->cursor_bo != (unsigned long )((struct drm_gem_object *)0)) {
    __mptr___1 = (struct drm_gem_object const *)amdgpu_crtc->cursor_bo;
    robj = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    ret = amdgpu_bo_reserve(robj, 0);
    tmp___0 = ldv__builtin_expect(ret == 0, 1L);
    if (tmp___0 != 0L) {
      amdgpu_bo_unpin(robj);
      amdgpu_bo_unreserve(robj);
    } else {
    }
    drm_gem_object_unreference_unlocked___6(amdgpu_crtc->cursor_bo);
  } else {
  }
  amdgpu_crtc->cursor_bo = obj;
  return (0);
  fail:
  drm_gem_object_unreference_unlocked___6(obj);
  return (ret);
}
}
static void dce_v11_0_crtc_gamma_set(struct drm_crtc *crtc , u16 *red , u16 *green ,
                                     u16 *blue , u32 start , u32 size )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  int end ;
  int i ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  end = (int )(256U < start + size ? 256U : start + size);
  i = (int )start;
  goto ldv_55029;
  ldv_55028:
  amdgpu_crtc->lut_r[i] = (u16 )((int )*(red + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_g[i] = (u16 )((int )*(green + (unsigned long )i) >> 6);
  amdgpu_crtc->lut_b[i] = (u16 )((int )*(blue + (unsigned long )i) >> 6);
  i = i + 1;
  ldv_55029: ;
  if (i < end) {
    goto ldv_55028;
  } else {
  }
  dce_v11_0_crtc_load_lut(crtc);
  return;
}
}
static void dce_v11_0_crtc_destroy(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  drm_crtc_cleanup(crtc);
  ldv_destroy_workqueue_914(amdgpu_crtc->pflip_queue);
  kfree((void const *)amdgpu_crtc);
  return;
}
}
static struct drm_crtc_funcs const dce_v11_0_crtc_funcs =
     {0, 0, 0, & dce_v11_0_crtc_cursor_set, 0, & dce_v11_0_crtc_cursor_move, & dce_v11_0_crtc_gamma_set,
    & dce_v11_0_crtc_destroy, & amdgpu_crtc_set_config, & amdgpu_crtc_page_flip, 0,
    0, 0, 0, 0};
static void dce_v11_0_crtc_dpms(struct drm_crtc *crtc , int mode )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  switch (mode) {
  case 0:
  amdgpu_crtc->enabled = 1;
  amdgpu_atombios_crtc_enable(crtc, 1);
  dce_v11_0_vga_enable(crtc, 1);
  amdgpu_atombios_crtc_blank(crtc, 0);
  dce_v11_0_vga_enable(crtc, 0);
  drm_vblank_post_modeset(dev, amdgpu_crtc->crtc_id);
  dce_v11_0_crtc_load_lut(crtc);
  goto ldv_55048;
  case 1: ;
  case 2: ;
  case 3:
  drm_vblank_pre_modeset(dev, amdgpu_crtc->crtc_id);
  if ((int )amdgpu_crtc->enabled) {
    dce_v11_0_vga_enable(crtc, 1);
    amdgpu_atombios_crtc_blank(crtc, 1);
    dce_v11_0_vga_enable(crtc, 0);
  } else {
  }
  amdgpu_atombios_crtc_enable(crtc, 0);
  amdgpu_crtc->enabled = 0;
  goto ldv_55048;
  }
  ldv_55048:
  amdgpu_pm_compute_clocks(adev);
  return;
}
}
static void dce_v11_0_crtc_prepare(struct drm_crtc *crtc )
{
  {
  amdgpu_atombios_crtc_powergate(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 1);
  dce_v11_0_crtc_dpms(crtc, 3);
  return;
}
}
static void dce_v11_0_crtc_commit(struct drm_crtc *crtc )
{
  {
  dce_v11_0_crtc_dpms(crtc, 0);
  amdgpu_atombios_crtc_lock(crtc, 0);
  return;
}
}
static void dce_v11_0_crtc_disable(struct drm_crtc *crtc )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  struct amdgpu_atom_ss ss ;
  int i ;
  int r ;
  struct amdgpu_framebuffer *amdgpu_fb ;
  struct amdgpu_bo *rbo ;
  struct drm_framebuffer const *__mptr___0 ;
  struct drm_gem_object const *__mptr___1 ;
  long tmp ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  dce_v11_0_crtc_dpms(crtc, 3);
  if ((unsigned long )(crtc->primary)->fb != (unsigned long )((struct drm_framebuffer *)0)) {
    __mptr___0 = (struct drm_framebuffer const *)(crtc->primary)->fb;
    amdgpu_fb = (struct amdgpu_framebuffer *)__mptr___0;
    __mptr___1 = (struct drm_gem_object const *)amdgpu_fb->obj;
    rbo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffbc0UL;
    r = amdgpu_bo_reserve(rbo, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      drm_err("failed to reserve rbo before unpin\n");
    } else {
      amdgpu_bo_unpin(rbo);
      amdgpu_bo_unreserve(rbo);
    }
  } else {
  }
  dce_v11_0_grph_enable(crtc, 0);
  amdgpu_atombios_crtc_powergate(crtc, 1);
  i = 0;
  goto ldv_55077;
  ldv_55076: ;
  if ((((unsigned long )adev->mode_info.crtcs[i] != (unsigned long )((struct amdgpu_crtc *)0) && (int )(adev->mode_info.crtcs[i])->enabled) && amdgpu_crtc->crtc_id != i) && amdgpu_crtc->pll_id == (adev->mode_info.crtcs[i])->pll_id) {
    goto done;
  } else {
  }
  i = i + 1;
  ldv_55077: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_55076;
  } else {
  }
  switch (amdgpu_crtc->pll_id) {
  case 2U: ;
  case 0U: ;
  case 1U:
  amdgpu_atombios_crtc_program_pll(crtc, (u32 )amdgpu_crtc->crtc_id, (int )amdgpu_crtc->pll_id,
                                   0U, 0U, 0U, 0U, 0U, 0U, 0U, 0, 0, & ss);
  goto ldv_55082;
  default: ;
  goto ldv_55082;
  }
  ldv_55082: ;
  done:
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  return;
}
}
static int dce_v11_0_crtc_mode_set(struct drm_crtc *crtc , struct drm_display_mode *mode ,
                                   struct drm_display_mode *adjusted_mode , int x ,
                                   int y , struct drm_framebuffer *old_fb )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  if (amdgpu_crtc->adjusted_clock == 0U) {
    return (-22);
  } else {
  }
  amdgpu_atombios_crtc_set_pll(crtc, adjusted_mode);
  amdgpu_atombios_crtc_set_dtd_timing(crtc, adjusted_mode);
  dce_v11_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  amdgpu_atombios_crtc_overscan_setup(crtc, mode, adjusted_mode);
  amdgpu_atombios_crtc_scaler_setup(crtc);
  amdgpu_crtc->hw_mode = *adjusted_mode;
  return (0);
}
}
static bool dce_v11_0_crtc_mode_fixup(struct drm_crtc *crtc , struct drm_display_mode const *mode ,
                                      struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  struct drm_crtc const *__mptr ;
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  __mptr = (struct drm_crtc const *)crtc;
  amdgpu_crtc = (struct amdgpu_crtc *)__mptr;
  dev = crtc->dev;
  __mptr___0 = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr___0 + 0xfffffffffffffff8UL;
  goto ldv_55111;
  ldv_55110: ;
  if ((unsigned long )encoder->crtc == (unsigned long )crtc) {
    amdgpu_crtc->encoder = encoder;
    amdgpu_crtc->connector = amdgpu_get_connector_for_encoder(encoder);
    goto ldv_55109;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_55111: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_55110;
  } else {
  }
  ldv_55109: ;
  if ((unsigned long )amdgpu_crtc->encoder == (unsigned long )((struct drm_encoder *)0) || (unsigned long )amdgpu_crtc->connector == (unsigned long )((struct drm_connector *)0)) {
    amdgpu_crtc->encoder = (struct drm_encoder *)0;
    amdgpu_crtc->connector = (struct drm_connector *)0;
    return (0);
  } else {
  }
  tmp = amdgpu_crtc_scaling_mode_fixup(crtc, mode, adjusted_mode);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return (0);
  } else {
  }
  tmp___1 = amdgpu_atombios_crtc_prepare_pll(crtc, adjusted_mode);
  if (tmp___1 != 0) {
    return (0);
  } else {
  }
  amdgpu_crtc->pll_id = dce_v11_0_pick_pll(crtc);
  if (amdgpu_crtc->pll_id == 255U) {
    tmp___2 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
    if (tmp___2 != 0) {
      tmp___3 = amdgpu_atombios_encoder_get_encoder_mode(amdgpu_crtc->encoder);
      if (tmp___3 != 5) {
        return (0);
      } else {
      }
    } else {
    }
  } else {
  }
  return (1);
}
}
static int dce_v11_0_crtc_set_base(struct drm_crtc *crtc , int x , int y , struct drm_framebuffer *old_fb )
{
  int tmp ;
  {
  tmp = dce_v11_0_crtc_do_set_base(crtc, old_fb, x, y, 0);
  return (tmp);
}
}
static int dce_v11_0_crtc_set_base_atomic(struct drm_crtc *crtc , struct drm_framebuffer *fb ,
                                          int x , int y , enum mode_set_atomic state )
{
  int tmp ;
  {
  tmp = dce_v11_0_crtc_do_set_base(crtc, fb, x, y, 1);
  return (tmp);
}
}
static struct drm_crtc_helper_funcs const dce_v11_0_crtc_helper_funcs =
     {& dce_v11_0_crtc_dpms, & dce_v11_0_crtc_prepare, & dce_v11_0_crtc_commit, & dce_v11_0_crtc_mode_fixup,
    & dce_v11_0_crtc_mode_set, 0, & dce_v11_0_crtc_set_base, & dce_v11_0_crtc_set_base_atomic,
    & dce_v11_0_crtc_load_lut, & dce_v11_0_crtc_disable, 0, 0, 0, 0};
static int dce_v11_0_crtc_init(struct amdgpu_device *adev , int index )
{
  struct amdgpu_crtc *amdgpu_crtc ;
  int i ;
  void *tmp ;
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp___0 ;
  {
  tmp = kzalloc(3312UL, 208U);
  amdgpu_crtc = (struct amdgpu_crtc *)tmp;
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (-12);
  } else {
  }
  drm_crtc_init(adev->ddev, & amdgpu_crtc->base, & dce_v11_0_crtc_funcs);
  drm_mode_crtc_set_gamma_size(& amdgpu_crtc->base, 256);
  amdgpu_crtc->crtc_id = index;
  __lock_name = "\"%s\"\"amdgpu-pageflip-queue\"";
  tmp___0 = __alloc_workqueue_key("%s", 131082U, 1, & __key, __lock_name, (char *)"amdgpu-pageflip-queue");
  amdgpu_crtc->pflip_queue = tmp___0;
  adev->mode_info.crtcs[index] = amdgpu_crtc;
  amdgpu_crtc->max_cursor_width = 128;
  amdgpu_crtc->max_cursor_height = 128;
  (adev->ddev)->mode_config.cursor_width = (u32 )amdgpu_crtc->max_cursor_width;
  (adev->ddev)->mode_config.cursor_height = (u32 )amdgpu_crtc->max_cursor_height;
  i = 0;
  goto ldv_55136;
  ldv_55135:
  amdgpu_crtc->lut_r[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_g[i] = (int )((u16 )i) << 2U;
  amdgpu_crtc->lut_b[i] = (int )((u16 )i) << 2U;
  i = i + 1;
  ldv_55136: ;
  if (i <= 255) {
    goto ldv_55135;
  } else {
  }
  switch (amdgpu_crtc->crtc_id) {
  case 0: ;
  default:
  amdgpu_crtc->crtc_offset = 0U;
  goto ldv_55140;
  case 1:
  amdgpu_crtc->crtc_offset = 512U;
  goto ldv_55140;
  case 2:
  amdgpu_crtc->crtc_offset = 1024U;
  goto ldv_55140;
  case 3:
  amdgpu_crtc->crtc_offset = 9728U;
  goto ldv_55140;
  case 4:
  amdgpu_crtc->crtc_offset = 10240U;
  goto ldv_55140;
  case 5:
  amdgpu_crtc->crtc_offset = 10752U;
  goto ldv_55140;
  }
  ldv_55140:
  amdgpu_crtc->pll_id = 255U;
  amdgpu_crtc->adjusted_clock = 0U;
  amdgpu_crtc->encoder = (struct drm_encoder *)0;
  amdgpu_crtc->connector = (struct drm_connector *)0;
  drm_crtc_helper_add(& amdgpu_crtc->base, & dce_v11_0_crtc_helper_funcs);
  return (0);
}
}
static int dce_v11_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->audio_endpt_rreg = & dce_v11_0_audio_endpt_rreg;
  adev->audio_endpt_wreg = & dce_v11_0_audio_endpt_wreg;
  dce_v11_0_set_display_funcs(adev);
  dce_v11_0_set_irq_funcs(adev);
  switch ((unsigned int )adev->asic_type) {
  case 7U:
  adev->mode_info.num_crtc = 4;
  adev->mode_info.num_hpd = 6;
  adev->mode_info.num_dig = 9;
  goto ldv_55151;
  default: ;
  return (-22);
  }
  ldv_55151: ;
  return (0);
}
}
static int dce_v11_0_sw_init(void *handle )
{
  int r ;
  int i ;
  struct amdgpu_device *adev ;
  bool tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0;
  goto ldv_55160;
  ldv_55159:
  r = amdgpu_irq_add_id(adev, (unsigned int )(i + 1), & adev->crtc_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_55160: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_55159;
  } else {
  }
  i = 8;
  goto ldv_55163;
  ldv_55162:
  r = amdgpu_irq_add_id(adev, (unsigned int )i, & adev->pageflip_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 2;
  ldv_55163: ;
  if (i <= 19) {
    goto ldv_55162;
  } else {
  }
  r = amdgpu_irq_add_id(adev, 42U, & adev->hpd_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->mode_info.mode_config_initialized = 1;
  (adev->ddev)->mode_config.funcs = & amdgpu_mode_funcs;
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  (adev->ddev)->mode_config.preferred_depth = 24U;
  (adev->ddev)->mode_config.prefer_shadow = 1U;
  (adev->ddev)->mode_config.fb_base = adev->mc.aper_base;
  r = amdgpu_modeset_create_props(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  (adev->ddev)->mode_config.max_width = 16384;
  (adev->ddev)->mode_config.max_height = 16384;
  i = 0;
  goto ldv_55166;
  ldv_55165:
  r = dce_v11_0_crtc_init(adev, i);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_55166: ;
  if (adev->mode_info.num_crtc > i) {
    goto ldv_55165;
  } else {
  }
  tmp = amdgpu_atombios_get_connector_info_from_object_table(adev);
  if ((int )tmp) {
    amdgpu_print_display_setup(adev->ddev);
  } else {
    return (-22);
  }
  dce_v11_0_afmt_init(adev);
  r = dce_v11_0_audio_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  drm_kms_helper_poll_init(adev->ddev);
  return (r);
}
}
static int dce_v11_0_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  kfree((void const *)adev->mode_info.bios_hardcoded_edid);
  drm_kms_helper_poll_fini(adev->ddev);
  dce_v11_0_audio_fini(adev);
  dce_v11_0_afmt_fini(adev);
  adev->mode_info.mode_config_initialized = 0;
  return (0);
}
}
static int dce_v11_0_hw_init(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v11_0_init_golden_registers(adev);
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  dce_v11_0_hpd_init(adev);
  i = 0;
  goto ldv_55178;
  ldv_55177:
  dce_v11_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_55178: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_55177;
  } else {
  }
  return (0);
}
}
static int dce_v11_0_hw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v11_0_hpd_fini(adev);
  i = 0;
  goto ldv_55186;
  ldv_55185:
  dce_v11_0_audio_enable(adev, (struct amdgpu_audio_pin *)(& adev->mode_info.audio.pin) + (unsigned long )i,
                         0);
  i = i + 1;
  ldv_55186: ;
  if (adev->mode_info.audio.num_pins > i) {
    goto ldv_55185;
  } else {
  }
  return (0);
}
}
static int dce_v11_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_atombios_scratch_regs_save(adev);
  dce_v11_0_hpd_fini(adev);
  return (0);
}
}
static int dce_v11_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  u8 bl_level ;
  u8 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  dce_v11_0_init_golden_registers(adev);
  amdgpu_atombios_scratch_regs_restore(adev);
  amdgpu_atombios_crtc_powergate_init(adev);
  amdgpu_atombios_encoder_init_dig(adev);
  amdgpu_atombios_crtc_set_disp_eng_pll(adev, adev->clock.default_dispclk);
  if ((unsigned long )adev->mode_info.bl_encoder != (unsigned long )((struct amdgpu_encoder *)0)) {
    tmp = (*((adev->mode_info.funcs)->backlight_get_level))(adev->mode_info.bl_encoder);
    bl_level = tmp;
    (*((adev->mode_info.funcs)->backlight_set_level))(adev->mode_info.bl_encoder,
                                                      (int )bl_level);
  } else {
  }
  dce_v11_0_hpd_init(adev);
  return (0);
}
}
static bool dce_v11_0_is_idle(void *handle )
{
  {
  return (1);
}
}
static int dce_v11_0_wait_for_idle(void *handle )
{
  {
  return (0);
}
}
static void dce_v11_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "DCE 10.x registers\n");
  return;
}
}
static int dce_v11_0_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  bool tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = dce_v11_0_is_display_hung(adev);
  if ((int )tmp___0) {
    srbm_soft_reset = srbm_soft_reset | 32U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    dce_v11_0_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    dce_v11_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static void dce_v11_0_set_crtc_vblank_interrupt_state(struct amdgpu_device *adev ,
                                                      int crtc , enum amdgpu_interrupt_state state )
{
  u32 lb_interrupt_mask ;
  long tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v11_0_set_crtc_vblank_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_55221;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_55221;
  default: ;
  goto ldv_55221;
  }
  ldv_55221: ;
  return;
}
}
static void dce_v11_0_set_crtc_vline_interrupt_state(struct amdgpu_device *adev ,
                                                     int crtc , enum amdgpu_interrupt_state state )
{
  u32 lb_interrupt_mask ;
  long tmp ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v11_0_set_crtc_vline_interrupt_state", "invalid crtc %d\n",
                          crtc);
    } else {
    }
    return;
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask & 4294967279U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_55232;
  case 1U:
  lb_interrupt_mask = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U,
                                     0);
  lb_interrupt_mask = lb_interrupt_mask | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6856U, lb_interrupt_mask,
                 0);
  goto ldv_55232;
  default: ;
  goto ldv_55232;
  }
  ldv_55232: ;
  return;
}
}
static int dce_v11_0_set_hpd_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                       unsigned int hpd , enum amdgpu_interrupt_state state )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if ((unsigned int )adev->mode_info.num_hpd <= hpd) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_set_hpd_irq_state", "invalid hdp %d\n", hpd);
    } else {
    }
    return (0);
  } else {
  }
  switch ((unsigned int )state) {
  case 0U:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, 0);
  tmp = tmp & 4294901759U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, tmp, 0);
  goto ldv_55244;
  case 1U:
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, 0);
  tmp = tmp | 65536U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, tmp, 0);
  goto ldv_55244;
  default: ;
  goto ldv_55244;
  }
  ldv_55244: ;
  return (0);
}
}
static int dce_v11_0_set_crtc_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  switch (type) {
  case 0U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 0, state);
  goto ldv_55254;
  case 1U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 1, state);
  goto ldv_55254;
  case 2U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 2, state);
  goto ldv_55254;
  case 3U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 3, state);
  goto ldv_55254;
  case 4U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 4, state);
  goto ldv_55254;
  case 5U:
  dce_v11_0_set_crtc_vblank_interrupt_state(adev, 5, state);
  goto ldv_55254;
  case 6U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 0, state);
  goto ldv_55254;
  case 7U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 1, state);
  goto ldv_55254;
  case 8U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 2, state);
  goto ldv_55254;
  case 9U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 3, state);
  goto ldv_55254;
  case 10U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 4, state);
  goto ldv_55254;
  case 11U:
  dce_v11_0_set_crtc_vline_interrupt_state(adev, 5, state);
  goto ldv_55254;
  default: ;
  goto ldv_55254;
  }
  ldv_55254: ;
  return (0);
}
}
static int dce_v11_0_set_pageflip_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                            unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 reg ;
  u32 reg_block ;
  {
  switch (type) {
  case 0U:
  reg_block = 0U;
  goto ldv_55276;
  case 1U:
  reg_block = 512U;
  goto ldv_55276;
  case 2U:
  reg_block = 1024U;
  goto ldv_55276;
  case 3U:
  reg_block = 9728U;
  goto ldv_55276;
  case 4U:
  reg_block = 10240U;
  goto ldv_55276;
  case 5U:
  reg_block = 10752U;
  goto ldv_55276;
  default:
  drm_err("invalid pageflip crtc %d\n", type);
  return (-22);
  }
  ldv_55276:
  reg = amdgpu_mm_rreg(adev, reg_block + 6679U, 0);
  if ((unsigned int )state == 0U) {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg & 4294967294U, 0);
  } else {
    amdgpu_mm_wreg(adev, reg_block + 6679U, reg | 1U, 0);
  }
  return (0);
}
}
static int dce_v11_0_pageflip_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                  struct amdgpu_iv_entry *entry )
{
  int reg_block ;
  unsigned long flags ;
  unsigned int crtc_id ;
  struct amdgpu_crtc *amdgpu_crtc ;
  struct amdgpu_flip_work *works ;
  u32 tmp ;
  raw_spinlock_t *tmp___0 ;
  long tmp___1 ;
  {
  crtc_id = (entry->src_id - 8U) >> 1;
  amdgpu_crtc = adev->mode_info.crtcs[crtc_id];
  switch (crtc_id) {
  case 0U:
  reg_block = 0;
  goto ldv_55294;
  case 1U:
  reg_block = 512;
  goto ldv_55294;
  case 2U:
  reg_block = 1024;
  goto ldv_55294;
  case 3U:
  reg_block = 9728;
  goto ldv_55294;
  case 4U:
  reg_block = 10240;
  goto ldv_55294;
  case 5U:
  reg_block = 10752;
  goto ldv_55294;
  default:
  drm_err("invalid pageflip crtc %d\n", crtc_id);
  return (-22);
  }
  ldv_55294:
  tmp = amdgpu_mm_rreg(adev, (u32 )(reg_block + 6678), 0);
  if ((int )tmp & 1) {
    amdgpu_mm_wreg(adev, (u32 )(reg_block + 6678), 256U, 0);
  } else {
  }
  if ((unsigned long )amdgpu_crtc == (unsigned long )((struct amdgpu_crtc *)0)) {
    return (0);
  } else {
  }
  tmp___0 = spinlock_check(& (adev->ddev)->event_lock);
  flags = _raw_spin_lock_irqsave(tmp___0);
  works = amdgpu_crtc->pflip_works;
  if ((unsigned int )amdgpu_crtc->pflip_status != 2U) {
    tmp___1 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("dce_v11_0_pageflip_irq", "amdgpu_crtc->pflip_status = %d != AMDGPU_FLIP_SUBMITTED(%d)\n",
                          (unsigned int )amdgpu_crtc->pflip_status, 2);
    } else {
    }
    spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
    return (0);
  } else {
  }
  amdgpu_crtc->pflip_status = 0;
  amdgpu_crtc->pflip_works = (struct amdgpu_flip_work *)0;
  if ((unsigned long )works->event != (unsigned long )((struct drm_pending_vblank_event *)0)) {
    drm_send_vblank_event(adev->ddev, (int )crtc_id, works->event);
  } else {
  }
  spin_unlock_irqrestore(& (adev->ddev)->event_lock, flags);
  drm_vblank_put(adev->ddev, amdgpu_crtc->crtc_id);
  amdgpu_irq_put(adev, & adev->pageflip_irq, crtc_id);
  queue_work___6(amdgpu_crtc->pflip_queue, & works->unpin_work);
  return (0);
}
}
static void dce_v11_0_hpd_int_ack(struct amdgpu_device *adev , int hpd )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_hpd <= hpd) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_hpd_int_ack", "invalid hdp %d\n", hpd);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, 0);
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )hpd_offsets___0[hpd] + 6297U, tmp, 0);
  return;
}
}
static void dce_v11_0_crtc_vblank_int_ack(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_vblank_int_ack", "invalid crtc %d\n", crtc);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6859U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6859U, tmp, 0);
  return;
}
}
static void dce_v11_0_crtc_vline_int_ack(struct amdgpu_device *adev , int crtc )
{
  u32 tmp ;
  long tmp___0 ;
  {
  if (adev->mode_info.num_crtc <= crtc) {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_vline_int_ack", "invalid crtc %d\n", crtc);
    } else {
    }
    return;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6857U, 0);
  tmp = tmp | 16U;
  amdgpu_mm_wreg(adev, (unsigned int )crtc_offsets___1[crtc] + 6857U, tmp, 0);
  return;
}
}
static int dce_v11_0_crtc_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                              struct amdgpu_iv_entry *entry )
{
  unsigned int crtc ;
  u32 disp_int ;
  u32 tmp ;
  unsigned int irq_type ;
  int tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  {
  crtc = entry->src_id - 1U;
  tmp = amdgpu_mm_rreg(adev, interrupt_status_offsets___1[crtc].reg, 0);
  disp_int = tmp;
  tmp___0 = amdgpu_crtc_idx_to_irq_type(adev, (int )crtc);
  irq_type = (unsigned int )tmp___0;
  switch (entry->src_data) {
  case 0U: ;
  if (((u32 )interrupt_status_offsets___1[crtc].vblank & disp_int) != 0U) {
    dce_v11_0_crtc_vblank_int_ack(adev, (int )crtc);
    tmp___1 = amdgpu_irq_enabled(adev, source, irq_type);
    if ((int )tmp___1) {
      drm_handle_vblank(adev->ddev, (int )crtc);
    } else {
    }
    tmp___2 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___2 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_irq", "IH: D%d vblank\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_55333;
  case 1U: ;
  if (((u32 )interrupt_status_offsets___1[crtc].vline & disp_int) != 0U) {
    dce_v11_0_crtc_vline_int_ack(adev, (int )crtc);
    tmp___3 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___3 != 0L) {
      drm_ut_debug_printk("dce_v11_0_crtc_irq", "IH: D%d vline\n", crtc + 1U);
    } else {
    }
  } else {
  }
  goto ldv_55333;
  default:
  tmp___4 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp___4 != 0L) {
    drm_ut_debug_printk("dce_v11_0_crtc_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                        entry->src_data);
  } else {
  }
  goto ldv_55333;
  }
  ldv_55333: ;
  return (0);
}
}
static int dce_v11_0_hpd_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                             struct amdgpu_iv_entry *entry )
{
  u32 disp_int ;
  u32 mask ;
  unsigned int hpd ;
  long tmp ;
  long tmp___0 ;
  {
  if (entry->src_data >= (unsigned int )adev->mode_info.num_hpd) {
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("dce_v11_0_hpd_irq", "Unhandled interrupt: %d %d\n", entry->src_id,
                          entry->src_data);
    } else {
    }
    return (0);
  } else {
  }
  hpd = entry->src_data;
  disp_int = amdgpu_mm_rreg(adev, interrupt_status_offsets___1[hpd].reg, 0);
  mask = interrupt_status_offsets___1[hpd].hpd;
  if ((disp_int & mask) != 0U) {
    dce_v11_0_hpd_int_ack(adev, (int )hpd);
    schedule_work___5(& adev->hotplug_work);
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("dce_v11_0_hpd_irq", "IH: HPD%d\n", hpd + 1U);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int dce_v11_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int dce_v11_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const dce_v11_0_ip_funcs =
     {& dce_v11_0_early_init, (int (*)(void * ))0, & dce_v11_0_sw_init, & dce_v11_0_sw_fini,
    & dce_v11_0_hw_init, & dce_v11_0_hw_fini, & dce_v11_0_suspend, & dce_v11_0_resume,
    & dce_v11_0_is_idle, & dce_v11_0_wait_for_idle, & dce_v11_0_soft_reset, & dce_v11_0_print_status,
    & dce_v11_0_set_clockgating_state, & dce_v11_0_set_powergating_state};
static void dce_v11_0_encoder_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                       struct drm_display_mode *adjusted_mode )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  int tmp ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_encoder->pixel_clock = (u32 )adjusted_mode->clock;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  dce_v11_0_set_interleave(encoder->crtc, mode);
  tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
  if (tmp == 3) {
    dce_v11_0_afmt_enable(encoder, 1);
    dce_v11_0_afmt_setmode(encoder, adjusted_mode);
  } else {
  }
  return;
}
}
static void dce_v11_0_encoder_prepare(struct drm_encoder *encoder )
{
  struct amdgpu_device *adev ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct drm_connector *connector ;
  struct drm_connector *tmp ;
  struct amdgpu_encoder_atom_dig *dig ;
  u16 tmp___0 ;
  struct amdgpu_connector *amdgpu_connector ;
  struct drm_connector const *__mptr___0 ;
  {
  adev = (struct amdgpu_device *)(encoder->dev)->dev_private;
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  tmp = amdgpu_get_connector_for_encoder(encoder);
  connector = tmp;
  if (((long )amdgpu_encoder->active_device & 3818L) != 0L) {
    goto _L;
  } else {
    tmp___0 = amdgpu_encoder_get_dp_bridge_encoder_id(encoder);
    if ((unsigned int )tmp___0 != 0U) {
      _L:
      dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
      if ((unsigned long )dig != (unsigned long )((struct amdgpu_encoder_atom_dig *)0)) {
        dig->dig_encoder = dce_v11_0_pick_dig_encoder(encoder);
        if (((long )amdgpu_encoder->active_device & 3784L) != 0L) {
          dig->afmt = adev->mode_info.afmt[dig->dig_encoder];
        } else {
        }
      } else {
      }
    } else {
    }
  }
  amdgpu_atombios_scratch_regs_lock(adev, 1);
  if ((unsigned long )connector != (unsigned long )((struct drm_connector *)0)) {
    __mptr___0 = (struct drm_connector const *)connector;
    amdgpu_connector = (struct amdgpu_connector *)__mptr___0;
    if ((int )amdgpu_connector->router.cd_valid) {
      amdgpu_i2c_router_select_cd_port(amdgpu_connector);
    } else {
    }
    if (connector->connector_type == 14) {
      amdgpu_atombios_encoder_set_edp_panel_power(connector, 12);
    } else {
    }
  } else {
  }
  amdgpu_atombios_encoder_set_crtc_source(encoder);
  dce_v11_0_program_fmt(encoder);
  return;
}
}
static void dce_v11_0_encoder_commit(struct drm_encoder *encoder )
{
  struct drm_device *dev ;
  struct amdgpu_device *adev ;
  {
  dev = encoder->dev;
  adev = (struct amdgpu_device *)dev->dev_private;
  amdgpu_atombios_encoder_dpms(encoder, 0);
  amdgpu_atombios_scratch_regs_lock(adev, 0);
  return;
}
}
static void dce_v11_0_encoder_disable(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  struct amdgpu_encoder_atom_dig *dig ;
  int tmp ;
  bool tmp___0 ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  amdgpu_atombios_encoder_dpms(encoder, 3);
  tmp___0 = amdgpu_atombios_encoder_is_digital(encoder);
  if ((int )tmp___0) {
    tmp = amdgpu_atombios_encoder_get_encoder_mode(encoder);
    if (tmp == 3) {
      dce_v11_0_afmt_enable(encoder, 0);
    } else {
    }
    dig = (struct amdgpu_encoder_atom_dig *)amdgpu_encoder->enc_priv;
    dig->dig_encoder = -1;
  } else {
  }
  amdgpu_encoder->active_device = 0U;
  return;
}
}
static void dce_v11_0_ext_prepare(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v11_0_ext_commit(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v11_0_ext_mode_set(struct drm_encoder *encoder , struct drm_display_mode *mode ,
                                   struct drm_display_mode *adjusted_mode )
{
  {
  return;
}
}
static void dce_v11_0_ext_disable(struct drm_encoder *encoder )
{
  {
  return;
}
}
static void dce_v11_0_ext_dpms(struct drm_encoder *encoder , int mode )
{
  {
  return;
}
}
static bool dce_v11_0_ext_mode_fixup(struct drm_encoder *encoder , struct drm_display_mode const *mode ,
                                     struct drm_display_mode *adjusted_mode )
{
  {
  return (1);
}
}
static struct drm_encoder_helper_funcs const dce_v11_0_ext_helper_funcs =
     {& dce_v11_0_ext_dpms, 0, 0, & dce_v11_0_ext_mode_fixup, & dce_v11_0_ext_prepare,
    & dce_v11_0_ext_commit, & dce_v11_0_ext_mode_set, 0, 0, & dce_v11_0_ext_disable,
    0, 0};
static struct drm_encoder_helper_funcs const dce_v11_0_dig_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v11_0_encoder_prepare,
    & dce_v11_0_encoder_commit, & dce_v11_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dig_detect,
    & dce_v11_0_encoder_disable, 0, 0};
static struct drm_encoder_helper_funcs const dce_v11_0_dac_helper_funcs =
     {& amdgpu_atombios_encoder_dpms, 0, 0, & amdgpu_atombios_encoder_mode_fixup, & dce_v11_0_encoder_prepare,
    & dce_v11_0_encoder_commit, & dce_v11_0_encoder_mode_set, 0, & amdgpu_atombios_encoder_dac_detect,
    0, 0, 0};
static void dce_v11_0_encoder_destroy(struct drm_encoder *encoder )
{
  struct amdgpu_encoder *amdgpu_encoder ;
  struct drm_encoder const *__mptr ;
  {
  __mptr = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_atombios_encoder_fini_backlight(amdgpu_encoder);
  } else {
  }
  kfree((void const *)amdgpu_encoder->enc_priv);
  drm_encoder_cleanup(encoder);
  kfree((void const *)amdgpu_encoder);
  return;
}
}
static struct drm_encoder_funcs const dce_v11_0_encoder_funcs = {0, & dce_v11_0_encoder_destroy};
static void dce_v11_0_encoder_add(struct amdgpu_device *adev , u32 encoder_enum ,
                                  u32 supported_device , u16 caps )
{
  struct drm_device *dev ;
  struct drm_encoder *encoder ;
  struct amdgpu_encoder *amdgpu_encoder ;
  struct list_head const *__mptr ;
  struct drm_encoder const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  void *tmp ;
  struct amdgpu_encoder_atom_dig *tmp___0 ;
  struct amdgpu_encoder_atom_dig *tmp___1 ;
  struct amdgpu_encoder_atom_dig *tmp___2 ;
  {
  dev = adev->ddev;
  __mptr = (struct list_head const *)dev->mode_config.encoder_list.next;
  encoder = (struct drm_encoder *)__mptr + 0xfffffffffffffff8UL;
  goto ldv_55435;
  ldv_55434:
  __mptr___0 = (struct drm_encoder const *)encoder;
  amdgpu_encoder = (struct amdgpu_encoder *)__mptr___0;
  if (amdgpu_encoder->encoder_enum == encoder_enum) {
    amdgpu_encoder->devices = amdgpu_encoder->devices | supported_device;
    return;
  } else {
  }
  __mptr___1 = (struct list_head const *)encoder->head.next;
  encoder = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
  ldv_55435: ;
  if ((unsigned long )(& encoder->head) != (unsigned long )(& dev->mode_config.encoder_list)) {
    goto ldv_55434;
  } else {
  }
  tmp = kzalloc(360UL, 208U);
  amdgpu_encoder = (struct amdgpu_encoder *)tmp;
  if ((unsigned long )amdgpu_encoder == (unsigned long )((struct amdgpu_encoder *)0)) {
    return;
  } else {
  }
  encoder = & amdgpu_encoder->base;
  switch (adev->mode_info.num_crtc) {
  case 1:
  encoder->possible_crtcs = 1U;
  goto ldv_55438;
  case 2: ;
  default:
  encoder->possible_crtcs = 3U;
  goto ldv_55438;
  case 4:
  encoder->possible_crtcs = 15U;
  goto ldv_55438;
  case 6:
  encoder->possible_crtcs = 63U;
  goto ldv_55438;
  }
  ldv_55438:
  amdgpu_encoder->enc_priv = (void *)0;
  amdgpu_encoder->encoder_enum = encoder_enum;
  amdgpu_encoder->encoder_id = encoder_enum & 255U;
  amdgpu_encoder->devices = supported_device;
  amdgpu_encoder->rmx_type = 0;
  amdgpu_encoder->underscan_type = 0;
  amdgpu_encoder->is_ext_encoder = 0;
  amdgpu_encoder->caps = caps;
  switch (amdgpu_encoder->encoder_id) {
  case 21U: ;
  case 22U:
  drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 1);
  drm_encoder_helper_add(encoder, & dce_v11_0_dac_helper_funcs);
  goto ldv_55445;
  case 20U: ;
  case 30U: ;
  case 32U: ;
  case 33U: ;
  case 37U: ;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    amdgpu_encoder->rmx_type = 1;
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 3);
    tmp___0 = amdgpu_atombios_encoder_get_lcd_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___0;
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 1);
    tmp___1 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___1;
  } else {
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 2);
    tmp___2 = amdgpu_atombios_encoder_get_dig_info(amdgpu_encoder);
    amdgpu_encoder->enc_priv = (void *)tmp___2;
  }
  drm_encoder_helper_add(encoder, & dce_v11_0_dig_helper_funcs);
  goto ldv_55445;
  case 8U: ;
  case 9U: ;
  case 12U: ;
  case 13U: ;
  case 14U: ;
  case 16U: ;
  case 17U: ;
  case 35U: ;
  case 34U:
  amdgpu_encoder->is_ext_encoder = 1;
  if (((long )amdgpu_encoder->devices & 34L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 3);
  } else
  if (((long )amdgpu_encoder->devices & 17L) != 0L) {
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 1);
  } else {
    drm_encoder_init(dev, encoder, & dce_v11_0_encoder_funcs, 2);
  }
  drm_encoder_helper_add(encoder, & dce_v11_0_ext_helper_funcs);
  goto ldv_55445;
  }
  ldv_55445: ;
  return;
}
}
static struct amdgpu_display_funcs const dce_v11_0_display_funcs =
     {& dce_v11_0_set_vga_render_state, & dce_v11_0_bandwidth_update, & dce_v11_0_vblank_get_counter,
    & dce_v11_0_vblank_wait, & dce_v11_0_is_display_hung, & amdgpu_atombios_encoder_set_backlight_level,
    & amdgpu_atombios_encoder_get_backlight_level, & dce_v11_0_hpd_sense, & dce_v11_0_hpd_set_polarity,
    & dce_v11_0_hpd_get_gpio_reg, & dce_v11_0_page_flip, & dce_v11_0_crtc_get_scanoutpos,
    & dce_v11_0_encoder_add, & amdgpu_connector_add, & dce_v11_0_stop_mc_access, & dce_v11_0_resume_mc_access};
static void dce_v11_0_set_display_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mode_info.funcs == (unsigned long )((struct amdgpu_display_funcs const *)0)) {
    adev->mode_info.funcs = & dce_v11_0_display_funcs;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const dce_v11_0_crtc_irq_funcs = {& dce_v11_0_set_crtc_irq_state, & dce_v11_0_crtc_irq};
static struct amdgpu_irq_src_funcs const dce_v11_0_pageflip_irq_funcs = {& dce_v11_0_set_pageflip_irq_state, & dce_v11_0_pageflip_irq};
static struct amdgpu_irq_src_funcs const dce_v11_0_hpd_irq_funcs = {& dce_v11_0_set_hpd_irq_state, & dce_v11_0_hpd_irq};
static void dce_v11_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->crtc_irq.num_types = 12U;
  adev->crtc_irq.funcs = & dce_v11_0_crtc_irq_funcs;
  adev->pageflip_irq.num_types = 6U;
  adev->pageflip_irq.funcs = & dce_v11_0_pageflip_irq_funcs;
  adev->hpd_irq.num_types = 6U;
  adev->hpd_irq.funcs = & dce_v11_0_hpd_irq_funcs;
  return;
}
}
extern int ldv_connect_46(void) ;
extern int ldv_release_47(void) ;
int ldv_retval_57 ;
extern int ldv_connect_47(void) ;
extern int ldv_bind_46(void) ;
extern int ldv_probe_44(void) ;
extern int ldv_release_49(void) ;
extern int ldv_bind_49(void) ;
extern int ldv_release_46(void) ;
extern int ldv_bind_47(void) ;
extern int ldv_probe_50(void) ;
extern int ldv_connect_49(void) ;
int ldv_retval_56 ;
extern int ldv_release_48(void) ;
extern int ldv_probe_48(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_40(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v11_0_hpd_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v11_0_hpd_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_crtc_funcs_50(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v11_0_crtc_funcs_group0 = (struct drm_crtc *)tmp;
  return;
}
}
void ldv_initialize_drm_crtc_helper_funcs_49(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(1160UL);
  dce_v11_0_crtc_helper_funcs_group0 = (struct drm_crtc *)tmp;
  tmp___0 = ldv_init_zalloc(168UL);
  dce_v11_0_crtc_helper_funcs_group1 = (struct drm_framebuffer *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  dce_v11_0_crtc_helper_funcs_group2 = (struct drm_display_mode *)tmp___1;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_45(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v11_0_dac_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v11_0_dac_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_42(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v11_0_crtc_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v11_0_crtc_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_46(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v11_0_dig_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v11_0_dig_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_41(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v11_0_pageflip_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  dce_v11_0_pageflip_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_drm_encoder_helper_funcs_47(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  dce_v11_0_ext_helper_funcs_group0 = (struct drm_encoder *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  dce_v11_0_ext_helper_funcs_group1 = (struct drm_display_mode *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_display_funcs_43(void)
{
  void *tmp ;
  void *tmp___0 ;
  void *tmp___1 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  dce_v11_0_display_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(16UL);
  dce_v11_0_display_funcs_group1 = (struct amdgpu_mode_mc_save *)tmp___0;
  tmp___1 = ldv_init_zalloc(360UL);
  dce_v11_0_display_funcs_group2 = (struct amdgpu_encoder *)tmp___1;
  return;
}
}
void ldv_main_exported_50(void)
{
  u32 ldvarg745 ;
  u32 ldvarg748 ;
  u32 ldvarg741 ;
  struct drm_pending_vblank_event *ldvarg736 ;
  void *tmp ;
  struct drm_file *ldvarg747 ;
  void *tmp___0 ;
  int ldvarg740 ;
  u32 ldvarg738 ;
  u16 *ldvarg743 ;
  void *tmp___1 ;
  u32 ldvarg746 ;
  struct drm_framebuffer *ldvarg737 ;
  void *tmp___2 ;
  u16 *ldvarg742 ;
  void *tmp___3 ;
  int ldvarg739 ;
  u16 *ldvarg744 ;
  void *tmp___4 ;
  struct drm_mode_set *ldvarg735 ;
  void *tmp___5 ;
  u32 ldvarg749 ;
  int tmp___6 ;
  {
  tmp = ldv_init_zalloc(88UL);
  ldvarg736 = (struct drm_pending_vblank_event *)tmp;
  tmp___0 = ldv_init_zalloc(744UL);
  ldvarg747 = (struct drm_file *)tmp___0;
  tmp___1 = ldv_init_zalloc(2UL);
  ldvarg743 = (u16 *)tmp___1;
  tmp___2 = ldv_init_zalloc(168UL);
  ldvarg737 = (struct drm_framebuffer *)tmp___2;
  tmp___3 = ldv_init_zalloc(2UL);
  ldvarg742 = (u16 *)tmp___3;
  tmp___4 = ldv_init_zalloc(2UL);
  ldvarg744 = (u16 *)tmp___4;
  tmp___5 = ldv_init_zalloc(48UL);
  ldvarg735 = (struct drm_mode_set *)tmp___5;
  ldv_memset((void *)(& ldvarg745), 0, 4UL);
  ldv_memset((void *)(& ldvarg748), 0, 4UL);
  ldv_memset((void *)(& ldvarg741), 0, 4UL);
  ldv_memset((void *)(& ldvarg740), 0, 4UL);
  ldv_memset((void *)(& ldvarg738), 0, 4UL);
  ldv_memset((void *)(& ldvarg746), 0, 4UL);
  ldv_memset((void *)(& ldvarg739), 0, 4UL);
  ldv_memset((void *)(& ldvarg749), 0, 4UL);
  tmp___6 = __VERIFIER_nondet_int();
  switch (tmp___6) {
  case 0: ;
  if (ldv_state_variable_50 == 2) {
    dce_v11_0_crtc_cursor_set(dce_v11_0_crtc_funcs_group0, ldvarg747, ldvarg746, ldvarg748,
                              ldvarg749);
    ldv_state_variable_50 = 2;
  } else {
  }
  if (ldv_state_variable_50 == 1) {
    dce_v11_0_crtc_cursor_set(dce_v11_0_crtc_funcs_group0, ldvarg747, ldvarg746, ldvarg748,
                              ldvarg749);
    ldv_state_variable_50 = 1;
  } else {
  }
  goto ldv_55544;
  case 1: ;
  if (ldv_state_variable_50 == 2) {
    dce_v11_0_crtc_destroy(dce_v11_0_crtc_funcs_group0);
    ldv_state_variable_50 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55544;
  case 2: ;
  if (ldv_state_variable_50 == 2) {
    dce_v11_0_crtc_gamma_set(dce_v11_0_crtc_funcs_group0, ldvarg743, ldvarg742, ldvarg744,
                             ldvarg745, ldvarg741);
    ldv_state_variable_50 = 2;
  } else {
  }
  if (ldv_state_variable_50 == 1) {
    dce_v11_0_crtc_gamma_set(dce_v11_0_crtc_funcs_group0, ldvarg743, ldvarg742, ldvarg744,
                             ldvarg745, ldvarg741);
    ldv_state_variable_50 = 1;
  } else {
  }
  goto ldv_55544;
  case 3: ;
  if (ldv_state_variable_50 == 2) {
    dce_v11_0_crtc_cursor_move(dce_v11_0_crtc_funcs_group0, ldvarg740, ldvarg739);
    ldv_state_variable_50 = 2;
  } else {
  }
  if (ldv_state_variable_50 == 1) {
    dce_v11_0_crtc_cursor_move(dce_v11_0_crtc_funcs_group0, ldvarg740, ldvarg739);
    ldv_state_variable_50 = 1;
  } else {
  }
  goto ldv_55544;
  case 4: ;
  if (ldv_state_variable_50 == 2) {
    amdgpu_crtc_page_flip(dce_v11_0_crtc_funcs_group0, ldvarg737, ldvarg736, ldvarg738);
    ldv_state_variable_50 = 2;
  } else {
  }
  if (ldv_state_variable_50 == 1) {
    amdgpu_crtc_page_flip(dce_v11_0_crtc_funcs_group0, ldvarg737, ldvarg736, ldvarg738);
    ldv_state_variable_50 = 1;
  } else {
  }
  goto ldv_55544;
  case 5: ;
  if (ldv_state_variable_50 == 2) {
    amdgpu_crtc_set_config(ldvarg735);
    ldv_state_variable_50 = 2;
  } else {
  }
  if (ldv_state_variable_50 == 1) {
    amdgpu_crtc_set_config(ldvarg735);
    ldv_state_variable_50 = 1;
  } else {
  }
  goto ldv_55544;
  case 6: ;
  if (ldv_state_variable_50 == 1) {
    ldv_probe_50();
    ldv_state_variable_50 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55544;
  default:
  ldv_stop();
  }
  ldv_55544: ;
  return;
}
}
void ldv_main_exported_40(void)
{
  unsigned int ldvarg939 ;
  struct amdgpu_iv_entry *ldvarg938 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg940 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg938 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg939), 0, 4UL);
  ldv_memset((void *)(& ldvarg940), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_40 == 1) {
    dce_v11_0_set_hpd_irq_state(dce_v11_0_hpd_irq_funcs_group0, dce_v11_0_hpd_irq_funcs_group1,
                                ldvarg939, ldvarg940);
    ldv_state_variable_40 = 1;
  } else {
  }
  goto ldv_55559;
  case 1: ;
  if (ldv_state_variable_40 == 1) {
    dce_v11_0_hpd_irq(dce_v11_0_hpd_irq_funcs_group0, dce_v11_0_hpd_irq_funcs_group1,
                      ldvarg938);
    ldv_state_variable_40 = 1;
  } else {
  }
  goto ldv_55559;
  default:
  ldv_stop();
  }
  ldv_55559: ;
  return;
}
}
void ldv_main_exported_41(void)
{
  unsigned int ldvarg796 ;
  struct amdgpu_iv_entry *ldvarg795 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg797 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg795 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg796), 0, 4UL);
  ldv_memset((void *)(& ldvarg797), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_41 == 1) {
    dce_v11_0_set_pageflip_irq_state(dce_v11_0_pageflip_irq_funcs_group0, dce_v11_0_pageflip_irq_funcs_group1,
                                     ldvarg796, ldvarg797);
    ldv_state_variable_41 = 1;
  } else {
  }
  goto ldv_55569;
  case 1: ;
  if (ldv_state_variable_41 == 1) {
    dce_v11_0_pageflip_irq(dce_v11_0_pageflip_irq_funcs_group0, dce_v11_0_pageflip_irq_funcs_group1,
                           ldvarg795);
    ldv_state_variable_41 = 1;
  } else {
  }
  goto ldv_55569;
  default:
  ldv_stop();
  }
  ldv_55569: ;
  return;
}
}
void ldv_main_exported_48(void)
{
  enum amd_clockgating_state ldvarg678 ;
  void *ldvarg688 ;
  void *tmp ;
  void *ldvarg686 ;
  void *tmp___0 ;
  void *ldvarg680 ;
  void *tmp___1 ;
  void *ldvarg685 ;
  void *tmp___2 ;
  enum amd_powergating_state ldvarg682 ;
  void *ldvarg677 ;
  void *tmp___3 ;
  void *ldvarg674 ;
  void *tmp___4 ;
  void *ldvarg675 ;
  void *tmp___5 ;
  void *ldvarg683 ;
  void *tmp___6 ;
  void *ldvarg676 ;
  void *tmp___7 ;
  void *ldvarg687 ;
  void *tmp___8 ;
  void *ldvarg679 ;
  void *tmp___9 ;
  void *ldvarg684 ;
  void *tmp___10 ;
  void *ldvarg681 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg688 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg686 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg680 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg685 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg677 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg674 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg675 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg683 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg676 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg687 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg679 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg684 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg681 = tmp___11;
  ldv_memset((void *)(& ldvarg678), 0, 4UL);
  ldv_memset((void *)(& ldvarg682), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_hw_fini(ldvarg688);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_hw_fini(ldvarg688);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_hw_fini(ldvarg688);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 1: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_print_status(ldvarg687);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_print_status(ldvarg687);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_print_status(ldvarg687);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 2: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_early_init(ldvarg686);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_early_init(ldvarg686);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_early_init(ldvarg686);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 3: ;
  if (ldv_state_variable_48 == 2) {
    ldv_retval_57 = dce_v11_0_suspend(ldvarg685);
    if (ldv_retval_57 == 0) {
      ldv_state_variable_48 = 3;
    } else {
    }
  } else {
  }
  goto ldv_55591;
  case 4: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_sw_init(ldvarg684);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_sw_init(ldvarg684);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_sw_init(ldvarg684);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 5: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_set_powergating_state(ldvarg683, ldvarg682);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_set_powergating_state(ldvarg683, ldvarg682);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_set_powergating_state(ldvarg683, ldvarg682);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 6: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_wait_for_idle(ldvarg681);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_wait_for_idle(ldvarg681);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_wait_for_idle(ldvarg681);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 7: ;
  if (ldv_state_variable_48 == 3) {
    ldv_retval_56 = dce_v11_0_resume(ldvarg680);
    if (ldv_retval_56 == 0) {
      ldv_state_variable_48 = 2;
    } else {
    }
  } else {
  }
  goto ldv_55591;
  case 8: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_set_clockgating_state(ldvarg679, ldvarg678);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_set_clockgating_state(ldvarg679, ldvarg678);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_set_clockgating_state(ldvarg679, ldvarg678);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 9: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_hw_init(ldvarg677);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_hw_init(ldvarg677);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_hw_init(ldvarg677);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 10: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_soft_reset(ldvarg676);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_soft_reset(ldvarg676);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_soft_reset(ldvarg676);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 11: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_sw_fini(ldvarg675);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_sw_fini(ldvarg675);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_sw_fini(ldvarg675);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 12: ;
  if (ldv_state_variable_48 == 2) {
    dce_v11_0_is_idle(ldvarg674);
    ldv_state_variable_48 = 2;
  } else {
  }
  if (ldv_state_variable_48 == 1) {
    dce_v11_0_is_idle(ldvarg674);
    ldv_state_variable_48 = 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    dce_v11_0_is_idle(ldvarg674);
    ldv_state_variable_48 = 3;
  } else {
  }
  goto ldv_55591;
  case 13: ;
  if (ldv_state_variable_48 == 2) {
    ldv_release_48();
    ldv_state_variable_48 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_48 == 3) {
    ldv_release_48();
    ldv_state_variable_48 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55591;
  case 14: ;
  if (ldv_state_variable_48 == 1) {
    ldv_probe_48();
    ldv_state_variable_48 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55591;
  default:
  ldv_stop();
  }
  ldv_55591: ;
  return;
}
}
void ldv_main_exported_47(void)
{
  struct drm_display_mode *ldvarg454 ;
  void *tmp ;
  struct drm_display_mode *ldvarg455 ;
  void *tmp___0 ;
  int ldvarg456 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg454 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg455 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg456), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_47 == 1) {
    dce_v11_0_ext_dpms(dce_v11_0_ext_helper_funcs_group0, ldvarg456);
    ldv_state_variable_47 = 1;
  } else {
  }
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_dpms(dce_v11_0_ext_helper_funcs_group0, ldvarg456);
    ldv_state_variable_47 = 3;
  } else {
  }
  if (ldv_state_variable_47 == 2) {
    dce_v11_0_ext_dpms(dce_v11_0_ext_helper_funcs_group0, ldvarg456);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 1: ;
  if (ldv_state_variable_47 == 1) {
    dce_v11_0_ext_mode_fixup(dce_v11_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg455,
                             dce_v11_0_ext_helper_funcs_group1);
    ldv_state_variable_47 = 1;
  } else {
  }
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_mode_fixup(dce_v11_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg455,
                             dce_v11_0_ext_helper_funcs_group1);
    ldv_state_variable_47 = 3;
  } else {
  }
  if (ldv_state_variable_47 == 2) {
    dce_v11_0_ext_mode_fixup(dce_v11_0_ext_helper_funcs_group0, (struct drm_display_mode const *)ldvarg455,
                             dce_v11_0_ext_helper_funcs_group1);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 2: ;
  if (ldv_state_variable_47 == 1) {
    dce_v11_0_ext_mode_set(dce_v11_0_ext_helper_funcs_group0, dce_v11_0_ext_helper_funcs_group1,
                           ldvarg454);
    ldv_state_variable_47 = 1;
  } else {
  }
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_mode_set(dce_v11_0_ext_helper_funcs_group0, dce_v11_0_ext_helper_funcs_group1,
                           ldvarg454);
    ldv_state_variable_47 = 3;
  } else {
  }
  if (ldv_state_variable_47 == 2) {
    dce_v11_0_ext_mode_set(dce_v11_0_ext_helper_funcs_group0, dce_v11_0_ext_helper_funcs_group1,
                           ldvarg454);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 3: ;
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_disable(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 4: ;
  if (ldv_state_variable_47 == 1) {
    dce_v11_0_ext_prepare(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 1;
  } else {
  }
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_prepare(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 3;
  } else {
  }
  if (ldv_state_variable_47 == 2) {
    dce_v11_0_ext_prepare(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 5: ;
  if (ldv_state_variable_47 == 1) {
    dce_v11_0_ext_commit(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 1;
  } else {
  }
  if (ldv_state_variable_47 == 3) {
    dce_v11_0_ext_commit(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 3;
  } else {
  }
  if (ldv_state_variable_47 == 2) {
    dce_v11_0_ext_commit(dce_v11_0_ext_helper_funcs_group0);
    ldv_state_variable_47 = 2;
  } else {
  }
  goto ldv_55614;
  case 6: ;
  if (ldv_state_variable_47 == 2) {
    ldv_release_47();
    ldv_state_variable_47 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55614;
  case 7: ;
  if (ldv_state_variable_47 == 1) {
    ldv_bind_47();
    ldv_state_variable_47 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55614;
  case 8: ;
  if (ldv_state_variable_47 == 2) {
    ldv_connect_47();
    ldv_state_variable_47 = 3;
  } else {
  }
  goto ldv_55614;
  default:
  ldv_stop();
  }
  ldv_55614: ;
  return;
}
}
void ldv_main_exported_42(void)
{
  unsigned int ldvarg1017 ;
  struct amdgpu_iv_entry *ldvarg1016 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg1018 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1016 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1017), 0, 4UL);
  ldv_memset((void *)(& ldvarg1018), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_42 == 1) {
    dce_v11_0_set_crtc_irq_state(dce_v11_0_crtc_irq_funcs_group0, dce_v11_0_crtc_irq_funcs_group1,
                                 ldvarg1017, ldvarg1018);
    ldv_state_variable_42 = 1;
  } else {
  }
  goto ldv_55631;
  case 1: ;
  if (ldv_state_variable_42 == 1) {
    dce_v11_0_crtc_irq(dce_v11_0_crtc_irq_funcs_group0, dce_v11_0_crtc_irq_funcs_group1,
                       ldvarg1016);
    ldv_state_variable_42 = 1;
  } else {
  }
  goto ldv_55631;
  default:
  ldv_stop();
  }
  ldv_55631: ;
  return;
}
}
void ldv_main_exported_49(void)
{
  int ldvarg367 ;
  int ldvarg371 ;
  int ldvarg375 ;
  struct drm_display_mode *ldvarg370 ;
  void *tmp ;
  struct drm_display_mode *ldvarg373 ;
  void *tmp___0 ;
  int ldvarg368 ;
  enum mode_set_atomic ldvarg369 ;
  int ldvarg374 ;
  int ldvarg372 ;
  int ldvarg376 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg370 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg373 = (struct drm_display_mode *)tmp___0;
  ldv_memset((void *)(& ldvarg367), 0, 4UL);
  ldv_memset((void *)(& ldvarg371), 0, 4UL);
  ldv_memset((void *)(& ldvarg375), 0, 4UL);
  ldv_memset((void *)(& ldvarg368), 0, 4UL);
  ldv_memset((void *)(& ldvarg369), 0, 4UL);
  ldv_memset((void *)(& ldvarg374), 0, 4UL);
  ldv_memset((void *)(& ldvarg372), 0, 4UL);
  ldv_memset((void *)(& ldvarg376), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_set_base(dce_v11_0_crtc_helper_funcs_group0, ldvarg376, ldvarg375,
                            dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_set_base(dce_v11_0_crtc_helper_funcs_group0, ldvarg376, ldvarg375,
                            dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_set_base(dce_v11_0_crtc_helper_funcs_group0, ldvarg376, ldvarg375,
                            dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 1: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_dpms(dce_v11_0_crtc_helper_funcs_group0, ldvarg374);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_dpms(dce_v11_0_crtc_helper_funcs_group0, ldvarg374);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_dpms(dce_v11_0_crtc_helper_funcs_group0, ldvarg374);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 2: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_mode_fixup(dce_v11_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg373,
                              dce_v11_0_crtc_helper_funcs_group2);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_mode_fixup(dce_v11_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg373,
                              dce_v11_0_crtc_helper_funcs_group2);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_mode_fixup(dce_v11_0_crtc_helper_funcs_group0, (struct drm_display_mode const *)ldvarg373,
                              dce_v11_0_crtc_helper_funcs_group2);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 3: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_mode_set(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group2,
                            ldvarg370, ldvarg371, ldvarg372, dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_mode_set(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group2,
                            ldvarg370, ldvarg371, ldvarg372, dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_mode_set(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group2,
                            ldvarg370, ldvarg371, ldvarg372, dce_v11_0_crtc_helper_funcs_group1);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 4: ;
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_disable(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 5: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_prepare(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_prepare(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_prepare(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 6: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_load_lut(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_load_lut(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_load_lut(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 7: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_commit(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_commit(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_commit(dce_v11_0_crtc_helper_funcs_group0);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 8: ;
  if (ldv_state_variable_49 == 1) {
    dce_v11_0_crtc_set_base_atomic(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group1,
                                   ldvarg367, ldvarg368, ldvarg369);
    ldv_state_variable_49 = 1;
  } else {
  }
  if (ldv_state_variable_49 == 3) {
    dce_v11_0_crtc_set_base_atomic(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group1,
                                   ldvarg367, ldvarg368, ldvarg369);
    ldv_state_variable_49 = 3;
  } else {
  }
  if (ldv_state_variable_49 == 2) {
    dce_v11_0_crtc_set_base_atomic(dce_v11_0_crtc_helper_funcs_group0, dce_v11_0_crtc_helper_funcs_group1,
                                   ldvarg367, ldvarg368, ldvarg369);
    ldv_state_variable_49 = 2;
  } else {
  }
  goto ldv_55648;
  case 9: ;
  if (ldv_state_variable_49 == 2) {
    ldv_release_49();
    ldv_state_variable_49 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55648;
  case 10: ;
  if (ldv_state_variable_49 == 1) {
    ldv_bind_49();
    ldv_state_variable_49 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55648;
  case 11: ;
  if (ldv_state_variable_49 == 2) {
    ldv_connect_49();
    ldv_state_variable_49 = 3;
  } else {
  }
  goto ldv_55648;
  default:
  ldv_stop();
  }
  ldv_55648: ;
  return;
}
}
void ldv_main_exported_46(void)
{
  struct drm_display_mode *ldvarg1034 ;
  void *tmp ;
  struct drm_display_mode *ldvarg1032 ;
  void *tmp___0 ;
  struct drm_connector *ldvarg1033 ;
  void *tmp___1 ;
  int ldvarg1035 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(208UL);
  ldvarg1034 = (struct drm_display_mode *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg1032 = (struct drm_display_mode *)tmp___0;
  tmp___1 = ldv_init_zalloc(936UL);
  ldvarg1033 = (struct drm_connector *)tmp___1;
  ldv_memset((void *)(& ldvarg1035), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_46 == 2) {
    amdgpu_atombios_encoder_dpms(dce_v11_0_dig_helper_funcs_group0, ldvarg1035);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v11_0_dig_helper_funcs_group0, ldvarg1035);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    amdgpu_atombios_encoder_dpms(dce_v11_0_dig_helper_funcs_group0, ldvarg1035);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 1: ;
  if (ldv_state_variable_46 == 2) {
    amdgpu_atombios_encoder_mode_fixup(dce_v11_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1034,
                                       dce_v11_0_dig_helper_funcs_group1);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v11_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1034,
                                       dce_v11_0_dig_helper_funcs_group1);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    amdgpu_atombios_encoder_mode_fixup(dce_v11_0_dig_helper_funcs_group0, (struct drm_display_mode const *)ldvarg1034,
                                       dce_v11_0_dig_helper_funcs_group1);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 2: ;
  if (ldv_state_variable_46 == 2) {
    amdgpu_atombios_encoder_dig_detect(dce_v11_0_dig_helper_funcs_group0, ldvarg1033);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    amdgpu_atombios_encoder_dig_detect(dce_v11_0_dig_helper_funcs_group0, ldvarg1033);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    amdgpu_atombios_encoder_dig_detect(dce_v11_0_dig_helper_funcs_group0, ldvarg1033);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 3: ;
  if (ldv_state_variable_46 == 2) {
    dce_v11_0_encoder_mode_set(dce_v11_0_dig_helper_funcs_group0, dce_v11_0_dig_helper_funcs_group1,
                               ldvarg1032);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    dce_v11_0_encoder_mode_set(dce_v11_0_dig_helper_funcs_group0, dce_v11_0_dig_helper_funcs_group1,
                               ldvarg1032);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    dce_v11_0_encoder_mode_set(dce_v11_0_dig_helper_funcs_group0, dce_v11_0_dig_helper_funcs_group1,
                               ldvarg1032);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 4: ;
  if (ldv_state_variable_46 == 3) {
    dce_v11_0_encoder_disable(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 2;
  } else {
  }
  goto ldv_55669;
  case 5: ;
  if (ldv_state_variable_46 == 2) {
    dce_v11_0_encoder_prepare(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    dce_v11_0_encoder_prepare(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    dce_v11_0_encoder_prepare(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 6: ;
  if (ldv_state_variable_46 == 2) {
    dce_v11_0_encoder_commit(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 2;
  } else {
  }
  if (ldv_state_variable_46 == 1) {
    dce_v11_0_encoder_commit(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 1;
  } else {
  }
  if (ldv_state_variable_46 == 3) {
    dce_v11_0_encoder_commit(dce_v11_0_dig_helper_funcs_group0);
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  case 7: ;
  if (ldv_state_variable_46 == 2) {
    ldv_release_46();
    ldv_state_variable_46 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55669;
  case 8: ;
  if (ldv_state_variable_46 == 1) {
    ldv_bind_46();
    ldv_state_variable_46 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55669;
  case 9: ;
  if (ldv_state_variable_46 == 2) {
    ldv_connect_46();
    ldv_state_variable_46 = 3;
  } else {
  }
  goto ldv_55669;
  default:
  ldv_stop();
  }
  ldv_55669: ;
  return;
}
}
void ldv_main_exported_45(void)
{
  int ldvarg809 ;
  struct drm_connector *ldvarg807 ;
  void *tmp ;
  struct drm_display_mode *ldvarg806 ;
  void *tmp___0 ;
  struct drm_display_mode *ldvarg808 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(936UL);
  ldvarg807 = (struct drm_connector *)tmp;
  tmp___0 = ldv_init_zalloc(208UL);
  ldvarg806 = (struct drm_display_mode *)tmp___0;
  tmp___1 = ldv_init_zalloc(208UL);
  ldvarg808 = (struct drm_display_mode *)tmp___1;
  ldv_memset((void *)(& ldvarg809), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_45 == 1) {
    amdgpu_atombios_encoder_dpms(dce_v11_0_dac_helper_funcs_group0, ldvarg809);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  case 1: ;
  if (ldv_state_variable_45 == 1) {
    amdgpu_atombios_encoder_mode_fixup(dce_v11_0_dac_helper_funcs_group0, (struct drm_display_mode const *)ldvarg808,
                                       dce_v11_0_dac_helper_funcs_group1);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  case 2: ;
  if (ldv_state_variable_45 == 1) {
    amdgpu_atombios_encoder_dac_detect(dce_v11_0_dac_helper_funcs_group0, ldvarg807);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  case 3: ;
  if (ldv_state_variable_45 == 1) {
    dce_v11_0_encoder_mode_set(dce_v11_0_dac_helper_funcs_group0, dce_v11_0_dac_helper_funcs_group1,
                               ldvarg806);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  case 4: ;
  if (ldv_state_variable_45 == 1) {
    dce_v11_0_encoder_prepare(dce_v11_0_dac_helper_funcs_group0);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  case 5: ;
  if (ldv_state_variable_45 == 1) {
    dce_v11_0_encoder_commit(dce_v11_0_dac_helper_funcs_group0);
    ldv_state_variable_45 = 1;
  } else {
  }
  goto ldv_55688;
  default:
  ldv_stop();
  }
  ldv_55688: ;
  return;
}
}
void ldv_main_exported_43(void)
{
  u32 *ldvarg489 ;
  void *tmp ;
  u32 ldvarg493 ;
  u32 ldvarg480 ;
  struct amdgpu_i2c_bus_rec *ldvarg498 ;
  void *tmp___0 ;
  u16 ldvarg492 ;
  int ldvarg490 ;
  u32 ldvarg481 ;
  enum amdgpu_hpd_id ldvarg486 ;
  int ldvarg484 ;
  u64 ldvarg483 ;
  struct amdgpu_hpd *ldvarg494 ;
  void *tmp___1 ;
  struct amdgpu_router *ldvarg496 ;
  void *tmp___2 ;
  int ldvarg478 ;
  u8 ldvarg491 ;
  int ldvarg488 ;
  bool ldvarg485 ;
  u16 ldvarg482 ;
  u32 ldvarg495 ;
  enum amdgpu_hpd_id ldvarg479 ;
  u32 *ldvarg487 ;
  void *tmp___3 ;
  int ldvarg497 ;
  int tmp___4 ;
  {
  tmp = ldv_init_zalloc(4UL);
  ldvarg489 = (u32 *)tmp;
  tmp___0 = ldv_init_zalloc(76UL);
  ldvarg498 = (struct amdgpu_i2c_bus_rec *)tmp___0;
  tmp___1 = ldv_init_zalloc(24UL);
  ldvarg494 = (struct amdgpu_hpd *)tmp___1;
  tmp___2 = ldv_init_zalloc(92UL);
  ldvarg496 = (struct amdgpu_router *)tmp___2;
  tmp___3 = ldv_init_zalloc(4UL);
  ldvarg487 = (u32 *)tmp___3;
  ldv_memset((void *)(& ldvarg493), 0, 4UL);
  ldv_memset((void *)(& ldvarg480), 0, 4UL);
  ldv_memset((void *)(& ldvarg492), 0, 2UL);
  ldv_memset((void *)(& ldvarg490), 0, 4UL);
  ldv_memset((void *)(& ldvarg481), 0, 4UL);
  ldv_memset((void *)(& ldvarg486), 0, 4UL);
  ldv_memset((void *)(& ldvarg484), 0, 4UL);
  ldv_memset((void *)(& ldvarg483), 0, 8UL);
  ldv_memset((void *)(& ldvarg478), 0, 4UL);
  ldv_memset((void *)(& ldvarg491), 0, 1UL);
  ldv_memset((void *)(& ldvarg488), 0, 4UL);
  ldv_memset((void *)(& ldvarg485), 0, 1UL);
  ldv_memset((void *)(& ldvarg482), 0, 2UL);
  ldv_memset((void *)(& ldvarg495), 0, 4UL);
  ldv_memset((void *)(& ldvarg479), 0, 4UL);
  ldv_memset((void *)(& ldvarg497), 0, 4UL);
  tmp___4 = __VERIFIER_nondet_int();
  switch (tmp___4) {
  case 0: ;
  if (ldv_state_variable_43 == 1) {
    amdgpu_connector_add(dce_v11_0_display_funcs_group0, ldvarg495, ldvarg493, ldvarg497,
                         ldvarg498, (int )ldvarg492, ldvarg494, ldvarg496);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 1: ;
  if (ldv_state_variable_43 == 1) {
    amdgpu_atombios_encoder_set_backlight_level(dce_v11_0_display_funcs_group2, (int )ldvarg491);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 2: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_vblank_wait(dce_v11_0_display_funcs_group0, ldvarg490);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 3: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_stop_mc_access(dce_v11_0_display_funcs_group0, dce_v11_0_display_funcs_group1);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 4: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_crtc_get_scanoutpos(dce_v11_0_display_funcs_group0, ldvarg488, ldvarg487,
                                  ldvarg489);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 5: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_hpd_get_gpio_reg(dce_v11_0_display_funcs_group0);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 6: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_resume_mc_access(dce_v11_0_display_funcs_group0, dce_v11_0_display_funcs_group1);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 7: ;
  if (ldv_state_variable_43 == 1) {
    amdgpu_atombios_encoder_get_backlight_level(dce_v11_0_display_funcs_group2);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 8: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_hpd_set_polarity(dce_v11_0_display_funcs_group0, ldvarg486);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 9: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_is_display_hung(dce_v11_0_display_funcs_group0);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 10: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_set_vga_render_state(dce_v11_0_display_funcs_group0, (int )ldvarg485);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 11: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_page_flip(dce_v11_0_display_funcs_group0, ldvarg484, ldvarg483);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 12: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_encoder_add(dce_v11_0_display_funcs_group0, ldvarg481, ldvarg480, (int )ldvarg482);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 13: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_hpd_sense(dce_v11_0_display_funcs_group0, ldvarg479);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 14: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_bandwidth_update(dce_v11_0_display_funcs_group0);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  case 15: ;
  if (ldv_state_variable_43 == 1) {
    dce_v11_0_vblank_get_counter(dce_v11_0_display_funcs_group0, ldvarg478);
    ldv_state_variable_43 = 1;
  } else {
  }
  goto ldv_55720;
  default:
  ldv_stop();
  }
  ldv_55720: ;
  return;
}
}
void ldv_main_exported_44(void)
{
  struct drm_encoder *ldvarg45 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(96UL);
  ldvarg45 = (struct drm_encoder *)tmp;
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_44 == 2) {
    dce_v11_0_encoder_destroy(ldvarg45);
    ldv_state_variable_44 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_55742;
  case 1: ;
  if (ldv_state_variable_44 == 1) {
    ldv_probe_44();
    ldv_state_variable_44 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_55742;
  default:
  ldv_stop();
  }
  ldv_55742: ;
  return;
}
}
bool ldv_queue_work_on_909(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_910(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_911(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_912(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_913(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
void ldv_destroy_workqueue_914(struct workqueue_struct *ldv_func_arg1 )
{
  {
  destroy_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_work_on_925(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_927(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_926(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_929(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_928(struct workqueue_struct *ldv_func_arg1 ) ;
int amdgpu_gfx_scratch_get(struct amdgpu_device *adev , u32 *reg )
{
  int i ;
  {
  i = 0;
  goto ldv_43646;
  ldv_43645: ;
  if ((int )adev->gfx.scratch.free[i]) {
    adev->gfx.scratch.free[i] = 0;
    *reg = adev->gfx.scratch.reg[i];
    return (0);
  } else {
  }
  i = i + 1;
  ldv_43646: ;
  if ((unsigned int )i < adev->gfx.scratch.num_reg) {
    goto ldv_43645;
  } else {
  }
  return (-22);
}
}
void amdgpu_gfx_scratch_free(struct amdgpu_device *adev , u32 reg )
{
  int i ;
  {
  i = 0;
  goto ldv_43654;
  ldv_43653: ;
  if (adev->gfx.scratch.reg[i] == reg) {
    adev->gfx.scratch.free[i] = 1;
    return;
  } else {
  }
  i = i + 1;
  ldv_43654: ;
  if ((unsigned int )i < adev->gfx.scratch.num_reg) {
    goto ldv_43653;
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_925(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_926(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_927(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_928(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_929(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_939(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_941(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_940(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_943(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_942(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___7(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_939(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___6(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___7(system_wq, work);
  return (tmp);
}
}
static unsigned int const vi_SECT_CONTEXT_def_1[212U] =
  { 0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 1073758208U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 2147483648U, 1073758208U, 65535U,
        0U, 1073758208U, 0U, 1073758208U,
        0U, 1073758208U, 0U, 1073758208U,
        2862197418U, 0U, 4294967295U, 4294967295U,
        2147483648U, 1073758208U, 0U, 0U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        2147483648U, 1073758208U, 2147483648U, 1073758208U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U,
        0U, 1065353216U, 0U, 1065353216U};
static unsigned int const vi_SECT_CONTEXT_def_2[274U] =
  { 0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 4294967295U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        2U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U};
static unsigned int const vi_SECT_CONTEXT_def_3[6U] = { 0U, 0U, 0U, 0U,
        0U, 0U};
static unsigned int const vi_SECT_CONTEXT_def_4[157U] =
  { 0U, 0U, 0U, 0U,
        589824U, 4U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 256U, 128U, 2U,
        0U, 0U, 0U, 0U,
        0U};
static unsigned int const vi_SECT_CONTEXT_def_5[2U] = { 0U, 0U};
static unsigned int const vi_SECT_CONTEXT_def_6[1U] = { 0U};
static unsigned int const vi_SECT_CONTEXT_def_7[233U] =
  { 0U, 0U, 0U, 0U,
        0U, 255U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 4096U, 0U,
        5U, 1065353216U, 1065353216U, 1065353216U,
        1065353216U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 4294967295U, 4294967295U, 0U,
        0U, 0U, 0U, 0U,
        0U, 30U, 32U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U, 0U, 0U, 0U,
        0U};
static struct cs_extent_def const vi_SECT_CONTEXT_defs[8U] =
  { {(unsigned int const *)(& vi_SECT_CONTEXT_def_1), 40960U, 212U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_2), 41174U, 274U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_3), 41461U, 6U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_4), 41472U, 157U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_5), 41632U, 2U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_6), 41635U, 1U},
        {(unsigned int const *)(& vi_SECT_CONTEXT_def_7), 41637U, 233U},
        {(unsigned int const *)0U, 0U, 0U}};
static struct cs_section_def const vi_cs_data[2U] = { {(struct cs_extent_def const *)(& vi_SECT_CONTEXT_defs), 1},
        {(struct cs_extent_def const *)0, 0}};
static struct amdgpu_gds_reg_offset const amdgpu_gds_reg_offset___0[16U] =
  { {13056U, 13057U, 13088U, 13104U},
        {13058U, 13059U, 13089U, 13105U},
        {13060U, 13061U, 13090U, 13106U},
        {13062U, 13063U, 13091U, 13107U},
        {13064U, 13065U, 13092U, 13108U},
        {13066U, 13067U, 13093U, 13109U},
        {13068U, 13069U, 13094U, 13110U},
        {13070U, 13071U, 13095U, 13111U},
        {13072U, 13073U, 13096U, 13112U},
        {13074U, 13075U, 13097U, 13113U},
        {13076U, 13077U, 13098U, 13114U},
        {13078U, 13079U, 13099U, 13115U},
        {13080U, 13081U, 13100U, 13116U},
        {13082U, 13083U, 13101U, 13117U},
        {13084U, 13085U, 13102U, 13118U},
        {13086U, 13087U, 13103U, 13119U}};
static u32 const golden_settings_tonga_a11___1[45U] =
  { 9860U, 4294833103U, 29192U, 9859U,
        64U, 64U, 9741U, 4027580415U,
        1024U, 9792U, 15U, 0U,
        8956U, 4294967295U, 536870913U, 8853U,
        1023U, 252U, 49793U, 65295U,
        0U, 8963U, 2097151U, 1789U,
        9538U, 983055U, 720896U, 11136U,
        1048576U, 4078960511U, 11140U, 2U,
        2U, 11013U, 1023U, 763U,
        11012U, 4294967295U, 21563U, 11011U,
        4294967295U, 2837514358U, 8754U, 4U,
        4U};
static u32 const tonga_golden_common_all[24U] =
  { 49664U, 4294967295U, 3758096384U, 41172U,
        4294967295U, 369098770U, 41173U, 4294967295U,
        42U, 9790U, 4294967295U, 570494979U,
        12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const tonga_mgcg_cgcg_init___2[225U] =
  { 60488U, 4294967295U, 4294967295U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 256U, 61618U,
        4294967295U, 256U, 61617U, 4294967295U,
        1073742080U, 61600U, 4294967295U, 256U,
        61573U, 4294967295U, 100663552U, 61576U,
        4294967295U, 256U, 61574U, 4294967295U,
        100663552U, 61569U, 4294967295U, 256U,
        61624U, 4294967295U, 256U, 61577U,
        4294967295U, 256U, 61568U, 4294967295U,
        256U, 61580U, 4294967295U, 256U,
        61581U, 4294967295U, 256U, 61588U,
        4294967295U, 256U, 61589U, 4294967295U,
        256U, 61590U, 4294967295U, 256U,
        61591U, 4294967295U, 256U, 61592U,
        4294967295U, 256U, 61599U, 4294967295U,
        256U, 61598U, 4294967295U, 256U,
        61572U, 4294967295U, 100663552U, 61604U,
        4294967295U, 256U, 61597U, 4294967295U,
        256U, 61613U, 4294967295U, 256U,
        61612U, 4294967295U, 256U, 61596U,
        4294967295U, 256U, 49664U, 4294967295U,
        3758096384U, 61448U, 4294967295U, 65536U,
        61449U, 4294967295U, 196610U, 61450U,
        4294967295U, 262151U, 61451U, 4294967295U,
        393221U, 61452U, 4294967295U, 589832U,
        61453U, 4294967295U, 65536U, 61454U,
        4294967295U, 196610U, 61455U, 4294967295U,
        262151U, 61456U, 4294967295U, 393221U,
        61457U, 4294967295U, 589832U, 61458U,
        4294967295U, 65536U, 61459U, 4294967295U,
        196610U, 61460U, 4294967295U, 262151U,
        61461U, 4294967295U, 393221U, 61462U,
        4294967295U, 589832U, 61463U, 4294967295U,
        65536U, 61464U, 4294967295U, 196610U,
        61465U, 4294967295U, 262151U, 61466U,
        4294967295U, 393221U, 61467U, 4294967295U,
        589832U, 61468U, 4294967295U, 65536U,
        61469U, 4294967295U, 196610U, 61470U,
        4294967295U, 262151U, 61471U, 4294967295U,
        393221U, 61472U, 4294967295U, 589832U,
        61473U, 4294967295U, 65536U, 61474U,
        4294967295U, 196610U, 61475U, 4294967295U,
        262151U, 61476U, 4294967295U, 393221U,
        61477U, 4294967295U, 589832U, 61478U,
        4294967295U, 65536U, 61479U, 4294967295U,
        196610U, 61480U, 4294967295U, 262151U,
        61481U, 4294967295U, 393221U, 61482U,
        4294967295U, 589832U, 61483U, 4294967295U,
        65536U, 61484U, 4294967295U, 196610U,
        61485U, 4294967295U, 262151U, 61486U,
        4294967295U, 393221U, 61487U, 4294967295U,
        589832U, 61440U, 4294967295U, 2531262976U,
        8642U, 4294967295U, 9437440U, 60489U,
        4294967295U, 2097212U, 12409U, 1U,
        1U};
static u32 const golden_settings_iceland_a11___0[45U] =
  { 9859U, 64U, 64U, 9741U,
        4027580415U, 1024U, 9742U, 3221225472U,
        3221225472U, 9792U, 15U, 0U,
        8956U, 4294967295U, 536870913U, 49793U,
        65295U, 0U, 41172U, 1061158911U,
        2U, 41173U, 63U, 0U,
        8963U, 2097151U, 1789U, 9538U,
        983055U, 720896U, 11136U, 1048576U,
        4078960511U, 11140U, 2U, 2U,
        11013U, 1023U, 241U, 11012U,
        4294967295U, 0U, 11011U, 4294967295U,
        16U};
static u32 const iceland_golden_common_all[24U] =
  { 49664U, 4294967295U, 3758096384U, 41172U,
        4294967295U, 2U, 41173U, 4294967295U,
        0U, 9790U, 4294967295U, 570490881U,
        12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const iceland_mgcg_cgcg_init___1[192U] =
  { 60488U, 4294967295U, 4294967295U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 3221225728U, 61618U,
        4294967295U, 3221225728U, 61617U, 4294967295U,
        3221225728U, 61600U, 4294967295U, 256U,
        61573U, 4294967295U, 100663552U, 61576U,
        4294967295U, 256U, 61574U, 4294967295U,
        100663552U, 61569U, 4294967295U, 256U,
        61624U, 4294967295U, 256U, 61577U,
        4294967295U, 256U, 61568U, 4294967295U,
        256U, 61580U, 4294967295U, 256U,
        61581U, 4294967295U, 256U, 61588U,
        4294967295U, 256U, 61589U, 4294967295U,
        256U, 61590U, 4294967295U, 256U,
        61591U, 4294967295U, 256U, 61592U,
        4294967295U, 256U, 61599U, 4294967295U,
        4278190336U, 61598U, 4294967295U, 256U,
        61572U, 4294967295U, 100663552U, 61604U,
        4294967295U, 256U, 61597U, 4294967295U,
        256U, 61613U, 4294967295U, 256U,
        61612U, 4294967295U, 256U, 61596U,
        4294967295U, 256U, 49664U, 4294967295U,
        3758096384U, 61448U, 4294967295U, 65536U,
        61449U, 4294967295U, 196610U, 61450U,
        4294967295U, 260312967U, 61451U, 4294967295U,
        393221U, 61452U, 4294967295U, 589832U,
        61453U, 4294967295U, 65536U, 61454U,
        4294967295U, 196610U, 61455U, 4294967295U,
        262151U, 61456U, 4294967295U, 393221U,
        61457U, 4294967295U, 589832U, 61458U,
        4294967295U, 65536U, 61459U, 4294967295U,
        196610U, 61460U, 4294967295U, 262151U,
        61461U, 4294967295U, 393221U, 61462U,
        4294967295U, 589832U, 61463U, 4294967295U,
        65536U, 61464U, 4294967295U, 196610U,
        61465U, 4294967295U, 262151U, 61466U,
        4294967295U, 393221U, 61467U, 4294967295U,
        589832U, 61468U, 4294967295U, 65536U,
        61469U, 4294967295U, 196610U, 61470U,
        4294967295U, 260312967U, 61471U, 4294967295U,
        393221U, 61472U, 4294967295U, 589832U,
        61473U, 4294967295U, 65536U, 61474U,
        4294967295U, 196610U, 61475U, 4294967295U,
        262151U, 61476U, 4294967295U, 393221U,
        61477U, 4294967295U, 589832U, 61440U,
        4294967295U, 2531262976U, 8642U, 4294967295U,
        9437440U, 60489U, 4294967295U, 2097212U};
static u32 const cz_golden_settings_a11___0[30U] =
  { 9859U, 64U, 64U, 9741U,
        4027580415U, 1024U, 9792U, 15U,
        0U, 8956U, 4294967295U, 1U,
        49793U, 65295U, 0U, 8963U,
        2097151U, 1789U, 9538U, 983055U,
        65536U, 11140U, 2U, 2U,
        11013U, 15U, 243U, 11011U,
        4294967295U, 4866U};
static u32 const cz_golden_common_all[24U] =
  { 49664U, 4294967295U, 3758096384U, 41172U,
        4294967295U, 2U, 41173U, 4294967295U,
        0U, 9790U, 4294967295U, 570490881U,
        12764U, 4294967295U, 2048U, 12765U,
        4294967295U, 2048U, 12774U, 4294967295U,
        32703U, 12775U, 4294967295U, 32687U};
static u32 const cz_mgcg_cgcg_init___2[225U] =
  { 60488U, 4294967295U, 4294967295U, 49664U,
        4294967295U, 3758096384U, 61608U, 4294967295U,
        256U, 61570U, 4294967295U, 256U,
        61616U, 4294967295U, 256U, 61618U,
        4294967295U, 256U, 61617U, 4294967295U,
        256U, 61600U, 4294967295U, 256U,
        61573U, 4294967295U, 100663552U, 61576U,
        4294967295U, 256U, 61574U, 4294967295U,
        100663552U, 61569U, 4294967295U, 256U,
        61624U, 4294967295U, 256U, 61577U,
        4294967295U, 256U, 61568U, 4294967295U,
        256U, 61580U, 4294967295U, 256U,
        61581U, 4294967295U, 256U, 61588U,
        4294967295U, 256U, 61589U, 4294967295U,
        256U, 61590U, 4294967295U, 256U,
        61591U, 4294967295U, 256U, 61592U,
        4294967295U, 256U, 61599U, 4294967295U,
        256U, 61598U, 4294967295U, 256U,
        61572U, 4294967295U, 100663552U, 61604U,
        4294967295U, 256U, 61597U, 4294967295U,
        256U, 61613U, 4294967295U, 256U,
        61612U, 4294967295U, 256U, 61596U,
        4294967295U, 256U, 49664U, 4294967295U,
        3758096384U, 61448U, 4294967295U, 65536U,
        61449U, 4294967295U, 196610U, 61450U,
        4294967295U, 262151U, 61451U, 4294967295U,
        393221U, 61452U, 4294967295U, 589832U,
        61453U, 4294967295U, 65536U, 61454U,
        4294967295U, 196610U, 61455U, 4294967295U,
        262151U, 61456U, 4294967295U, 393221U,
        61457U, 4294967295U, 589832U, 61458U,
        4294967295U, 65536U, 61459U, 4294967295U,
        196610U, 61460U, 4294967295U, 262151U,
        61461U, 4294967295U, 393221U, 61462U,
        4294967295U, 589832U, 61463U, 4294967295U,
        65536U, 61464U, 4294967295U, 196610U,
        61465U, 4294967295U, 262151U, 61466U,
        4294967295U, 393221U, 61467U, 4294967295U,
        589832U, 61468U, 4294967295U, 65536U,
        61469U, 4294967295U, 196610U, 61470U,
        4294967295U, 262151U, 61471U, 4294967295U,
        393221U, 61472U, 4294967295U, 589832U,
        61473U, 4294967295U, 65536U, 61474U,
        4294967295U, 196610U, 61475U, 4294967295U,
        262151U, 61476U, 4294967295U, 393221U,
        61477U, 4294967295U, 589832U, 61478U,
        4294967295U, 65536U, 61479U, 4294967295U,
        196610U, 61480U, 4294967295U, 262151U,
        61481U, 4294967295U, 393221U, 61482U,
        4294967295U, 589832U, 61483U, 4294967295U,
        65536U, 61484U, 4294967295U, 196610U,
        61485U, 4294967295U, 262151U, 61486U,
        4294967295U, 393221U, 61487U, 4294967295U,
        589832U, 61440U, 4294967295U, 2531262976U,
        8642U, 4294967295U, 9437440U, 60489U,
        4294967295U, 2097215U, 12409U, 1U,
        1U};
static void gfx_v8_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void gfx_v8_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static void gfx_v8_0_set_gds_init(struct amdgpu_device *adev ) ;
static void gfx_v8_0_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& iceland_mgcg_cgcg_init___1),
                                   192U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_iceland_a11___0),
                                   45U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& iceland_golden_common_all),
                                   24U);
  goto ldv_49119;
  case 6U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_mgcg_cgcg_init___2),
                                   225U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_tonga_a11___1),
                                   45U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_golden_common_all),
                                   24U);
  goto ldv_49119;
  case 7U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_mgcg_cgcg_init___2),
                                   225U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_golden_settings_a11___0),
                                   30U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_golden_common_all),
                                   24U);
  goto ldv_49119;
  default: ;
  goto ldv_49119;
  }
  ldv_49119: ;
  return;
}
}
static void gfx_v8_0_scratch_init(struct amdgpu_device *adev )
{
  int i ;
  {
  adev->gfx.scratch.num_reg = 7U;
  adev->gfx.scratch.reg_base = 49216U;
  i = 0;
  goto ldv_49140;
  ldv_49139:
  adev->gfx.scratch.free[i] = 1;
  adev->gfx.scratch.reg[i] = adev->gfx.scratch.reg_base + (u32 )i;
  i = i + 1;
  ldv_49140: ;
  if ((unsigned int )i < adev->gfx.scratch.num_reg) {
    goto ldv_49139;
  } else {
  }
  return;
}
}
static int gfx_v8_0_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 scratch ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_gfx_scratch_get(adev, & scratch);
  if (r != 0) {
    drm_err("amdgpu: cp failed to get scratch reg (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_mm_wreg(adev, scratch, 3405700781U, 0);
  r = amdgpu_ring_lock(ring, 3U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring %d (%d).\n", ring->idx, r);
    amdgpu_gfx_scratch_free(adev, scratch);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 3221321984U);
  amdgpu_ring_write(ring, scratch - 49152U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_49152;
  ldv_49151:
  tmp = amdgpu_mm_rreg(adev, scratch, 0);
  if (tmp == 3735928559U) {
    goto ldv_49150;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49152: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49151;
  } else {
  }
  ldv_49150: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (scratch(0x%04X)=0x%08X)\n", ring->idx, scratch,
            tmp);
    r = -22;
  }
  amdgpu_gfx_scratch_free(adev, scratch);
  return (r);
}
}
static int gfx_v8_0_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ib ib ;
  u32 scratch ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_gfx_scratch_get(adev, & scratch);
  if (r != 0) {
    drm_err("amdgpu: failed to get scratch reg (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_mm_wreg(adev, scratch, 3405700781U, 0);
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 256U, & ib);
  if (r != 0) {
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    amdgpu_gfx_scratch_free(adev, scratch);
    return (r);
  } else {
  }
  *(ib.ptr) = 3221321984U;
  *(ib.ptr + 1UL) = scratch - 49152U;
  *(ib.ptr + 2UL) = 3735928559U;
  ib.length_dw = 3U;
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)0);
  if (r != 0) {
    amdgpu_gfx_scratch_free(adev, scratch);
    amdgpu_ib_free(adev, & ib);
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_fence_wait(ib.fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    amdgpu_gfx_scratch_free(adev, scratch);
    amdgpu_ib_free(adev, & ib);
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_49164;
  ldv_49163:
  tmp = amdgpu_mm_rreg(adev, scratch, 0);
  if (tmp == 3735928559U) {
    goto ldv_49162;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49164: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49163;
  } else {
  }
  ldv_49162: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ib test on ring %d succeeded in %u usecs\n", ((ib.fence)->ring)->idx,
           i);
  } else {
    drm_err("amdgpu: ib test failed (scratch(0x%04X)=0x%08X)\n", scratch, tmp);
    r = -22;
  }
  amdgpu_gfx_scratch_free(adev, scratch);
  amdgpu_ib_free(adev, & ib);
  return (r);
}
}
static int gfx_v8_0_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  struct amdgpu_firmware_info *info ;
  struct common_firmware_header const *header ;
  long tmp ;
  {
  info = (struct amdgpu_firmware_info *)0;
  header = (struct common_firmware_header const *)0;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gfx_v8_0_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  chip_name = "topaz";
  goto ldv_49175;
  case 6U:
  chip_name = "tonga";
  goto ldv_49175;
  case 7U:
  chip_name = "carrizo";
  goto ldv_49175;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c"),
                       "i" (604), "i" (12UL));
  ldv_49179: ;
  goto ldv_49179;
  }
  ldv_49175:
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_pfp.bin", chip_name);
  err = request_firmware(& adev->gfx.pfp_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.pfp_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_me.bin", chip_name);
  err = request_firmware(& adev->gfx.me_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.me_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_ce.bin", chip_name);
  err = request_firmware(& adev->gfx.ce_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.ce_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_rlc.bin", chip_name);
  err = request_firmware(& adev->gfx.rlc_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.rlc_fw);
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_mec.bin", chip_name);
  err = request_firmware(& adev->gfx.mec_fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->gfx.mec_fw);
  if (err != 0) {
    goto out;
  } else {
  }
  snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_mec2.bin", chip_name);
  err = request_firmware(& adev->gfx.mec2_fw, (char const *)(& fw_name), adev->dev);
  if (err == 0) {
    err = amdgpu_ucode_validate(adev->gfx.mec2_fw);
    if (err != 0) {
      goto out;
    } else {
    }
  } else {
    err = 0;
    adev->gfx.mec2_fw = (struct firmware const *)0;
  }
  if ((int )adev->firmware.smu_load) {
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 3UL;
    info->ucode_id = 3;
    info->fw = adev->gfx.pfp_fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 4UL;
    info->ucode_id = 4;
    info->fw = adev->gfx.me_fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 2UL;
    info->ucode_id = 2;
    info->fw = adev->gfx.ce_fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 7UL;
    info->ucode_id = 7;
    info->fw = adev->gfx.rlc_fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 5UL;
    info->ucode_id = 5;
    info->fw = adev->gfx.mec_fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    if ((unsigned long )adev->gfx.mec2_fw != (unsigned long )((struct firmware const *)0)) {
      info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + 6UL;
      info->ucode_id = 6;
      info->fw = adev->gfx.mec2_fw;
      header = (struct common_firmware_header const *)(info->fw)->data;
      adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
    } else {
    }
  } else {
  }
  out: ;
  if (err != 0) {
    dev_err((struct device const *)adev->dev, "gfx8: Failed to load firmware \"%s\"\n",
            (char *)(& fw_name));
    release_firmware(adev->gfx.pfp_fw);
    adev->gfx.pfp_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.me_fw);
    adev->gfx.me_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.ce_fw);
    adev->gfx.ce_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.rlc_fw);
    adev->gfx.rlc_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.mec_fw);
    adev->gfx.mec_fw = (struct firmware const *)0;
    release_firmware(adev->gfx.mec2_fw);
    adev->gfx.mec2_fw = (struct firmware const *)0;
  } else {
  }
  return (err);
}
}
static void gfx_v8_0_mec_fini(struct amdgpu_device *adev )
{
  int r ;
  long tmp ;
  {
  if ((unsigned long )adev->gfx.mec.hpd_eop_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(adev->gfx.mec.hpd_eop_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve HPD EOP bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(adev->gfx.mec.hpd_eop_obj);
    amdgpu_bo_unreserve(adev->gfx.mec.hpd_eop_obj);
    amdgpu_bo_unref(& adev->gfx.mec.hpd_eop_obj);
    adev->gfx.mec.hpd_eop_obj = (struct amdgpu_bo *)0;
  } else {
  }
  return;
}
}
static int gfx_v8_0_mec_init(struct amdgpu_device *adev )
{
  int r ;
  u32 *hpd ;
  long tmp ;
  {
  adev->gfx.mec.num_mec = 1U;
  adev->gfx.mec.num_pipe = 1U;
  adev->gfx.mec.num_queue = (adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 8U;
  if ((unsigned long )adev->gfx.mec.hpd_eop_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, (unsigned long )((adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 4096U),
                         4096, 1, 2U, 0ULL, (struct sg_table *)0, & adev->gfx.mec.hpd_eop_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) create HDP EOP bo failed\n",
               r);
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_reserve(adev->gfx.mec.hpd_eop_obj, 0);
  tmp = ldv__builtin_expect(r != 0, 0L);
  if (tmp != 0L) {
    gfx_v8_0_mec_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->gfx.mec.hpd_eop_obj, 2U, & adev->gfx.mec.hpd_eop_gpu_addr);
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) pin HDP EOP bo failed\n", r);
    gfx_v8_0_mec_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->gfx.mec.hpd_eop_obj, (void **)(& hpd));
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) map HDP EOP bo failed\n", r);
    gfx_v8_0_mec_fini(adev);
    return (r);
  } else {
  }
  memset((void *)hpd, 0, (size_t )((adev->gfx.mec.num_mec * adev->gfx.mec.num_pipe) * 4096U));
  amdgpu_bo_kunmap(adev->gfx.mec.hpd_eop_obj);
  amdgpu_bo_unreserve(adev->gfx.mec.hpd_eop_obj);
  return (0);
}
}
static int gfx_v8_0_sw_init(void *handle )
{
  int i ;
  int r ;
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  unsigned int irq_type ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 181U, & adev->gfx.eop_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 184U, & adev->gfx.priv_reg_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 185U, & adev->gfx.priv_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gfx.gfx_current_status = 0U;
  gfx_v8_0_scratch_init(adev);
  r = gfx_v8_0_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load gfx firmware!\n");
    return (r);
  } else {
  }
  r = gfx_v8_0_mec_init(adev);
  if (r != 0) {
    drm_err("Failed to init MEC BOs!\n");
    return (r);
  } else {
  }
  r = amdgpu_wb_get(adev, & adev->gfx.ce_sync_offs);
  if (r != 0) {
    drm_err("(%d) gfx.ce_sync_offs wb alloc failed\n", r);
    return (r);
  } else {
  }
  i = 0;
  goto ldv_49198;
  ldv_49197:
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring) + (unsigned long )i;
  ring->ring_obj = (struct amdgpu_bo *)0;
  sprintf((char *)(& ring->name), "gfx");
  if ((unsigned int )adev->asic_type != 5U) {
    ring->use_doorbell = 1;
    ring->doorbell_index = 32U;
  } else {
  }
  r = amdgpu_ring_init(adev, ring, 1048576U, 4294905856U, 15U, & adev->gfx.eop_irq,
                       0U, 0);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_49198: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_49197;
  } else {
  }
  i = 0;
  goto ldv_49203;
  ldv_49202: ;
  if (i > 31 || i > 7) {
    drm_err("Too many (%d) compute rings!\n", i);
    goto ldv_49201;
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 1;
  ring->doorbell_index = (u32 )(i + 16);
  ring->me = 1U;
  ring->pipe = (u32 )(i / 8);
  ring->queue = (u32 )(i % 8);
  sprintf((char *)(& ring->name), "comp %d.%d.%d", ring->me, ring->pipe, ring->queue);
  irq_type = ring->pipe + 1U;
  r = amdgpu_ring_init(adev, ring, 1048576U, 4294905856U, 15U, & adev->gfx.eop_irq,
                       irq_type, 1);
  if (r != 0) {
    return (r);
  } else {
  }
  i = i + 1;
  ldv_49203: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_49202;
  } else {
  }
  ldv_49201:
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.mem.gfx_partition_size, 4096,
                       1, 8U, 0ULL, (struct sg_table *)0, & adev->gds.gds_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.gws.gfx_partition_size, 4096,
                       1, 16U, 0ULL, (struct sg_table *)0, & adev->gds.gws_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_create(adev, (unsigned long )adev->gds.oa.gfx_partition_size, 4096,
                       1, 32U, 0ULL, (struct sg_table *)0, & adev->gds.oa_gfx_bo);
  if (r != 0) {
    return (r);
  } else {
  }
  adev->gfx.ce_ram_size = 32768U;
  return (0);
}
}
static int gfx_v8_0_sw_fini(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_bo_unref(& adev->gds.oa_gfx_bo);
  amdgpu_bo_unref(& adev->gds.gws_gfx_bo);
  amdgpu_bo_unref(& adev->gds.gds_gfx_bo);
  i = 0;
  goto ldv_49210;
  ldv_49209:
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->gfx.gfx_ring) + (unsigned long )i);
  i = i + 1;
  ldv_49210: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_49209;
  } else {
  }
  i = 0;
  goto ldv_49213;
  ldv_49212:
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i);
  i = i + 1;
  ldv_49213: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_49212;
  } else {
  }
  amdgpu_wb_free(adev, adev->gfx.ce_sync_offs);
  gfx_v8_0_mec_fini(adev);
  return (0);
}
}
static void gfx_v8_0_tiling_mode_table_init(struct amdgpu_device *adev )
{
  u32 num_tile_mode_states ;
  u32 num_secondary_tile_mode_states ;
  u32 reg_offset ;
  u32 gb_tile_moden ;
  u32 split_equal_to_row_size ;
  {
  num_tile_mode_states = 32U;
  num_secondary_tile_mode_states = 16U;
  switch (adev->gfx.config.mem_row_size_in_kb) {
  case 1U:
  split_equal_to_row_size = 4U;
  goto ldv_49224;
  case 2U: ;
  default:
  split_equal_to_row_size = 5U;
  goto ldv_49224;
  case 4U:
  split_equal_to_row_size = 6U;
  goto ldv_49224;
  }
  ldv_49224: ;
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  reg_offset = 0U;
  goto ldv_49263;
  ldv_49262: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8388624U;
  goto ldv_49230;
  case 1U:
  gb_tile_moden = 8390672U;
  goto ldv_49230;
  case 2U:
  gb_tile_moden = 8392720U;
  goto ldv_49230;
  case 3U:
  gb_tile_moden = 8394768U;
  goto ldv_49230;
  case 4U:
  gb_tile_moden = 8398864U;
  goto ldv_49230;
  case 5U:
  gb_tile_moden = 8398856U;
  goto ldv_49230;
  case 6U:
  gb_tile_moden = 8398868U;
  goto ldv_49230;
  case 8U:
  gb_tile_moden = 4U;
  goto ldv_49230;
  case 9U:
  gb_tile_moden = 33554440U;
  goto ldv_49230;
  case 10U:
  gb_tile_moden = 33554448U;
  goto ldv_49230;
  case 11U:
  gb_tile_moden = 100663316U;
  goto ldv_49230;
  case 13U:
  gb_tile_moden = 37748744U;
  goto ldv_49230;
  case 14U:
  gb_tile_moden = 37748752U;
  goto ldv_49230;
  case 15U:
  gb_tile_moden = 37748784U;
  goto ldv_49230;
  case 16U:
  gb_tile_moden = 104857620U;
  goto ldv_49230;
  case 18U:
  gb_tile_moden = 4194316U;
  goto ldv_49230;
  case 19U:
  gb_tile_moden = 16777228U;
  goto ldv_49230;
  case 20U:
  gb_tile_moden = 16777244U;
  goto ldv_49230;
  case 21U:
  gb_tile_moden = 16777268U;
  goto ldv_49230;
  case 22U:
  gb_tile_moden = 16777252U;
  goto ldv_49230;
  case 24U:
  gb_tile_moden = 4194332U;
  goto ldv_49230;
  case 25U:
  gb_tile_moden = 16777248U;
  goto ldv_49230;
  case 26U:
  gb_tile_moden = 16777272U;
  goto ldv_49230;
  case 27U:
  gb_tile_moden = 46137352U;
  goto ldv_49230;
  case 28U:
  gb_tile_moden = 46137360U;
  goto ldv_49230;
  case 29U:
  gb_tile_moden = 113246228U;
  goto ldv_49230;
  case 7U: ;
  case 12U: ;
  case 17U: ;
  case 23U: ;
  goto ldv_49260;
  default:
  gb_tile_moden = 0U;
  goto ldv_49230;
  }
  ldv_49230:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  ldv_49260:
  reg_offset = reg_offset + 1U;
  ldv_49263: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_49262;
  } else {
  }
  reg_offset = 0U;
  goto ldv_49284;
  ldv_49283: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 154U;
  goto ldv_49266;
  case 1U:
  gb_tile_moden = 154U;
  goto ldv_49266;
  case 2U:
  gb_tile_moden = 153U;
  goto ldv_49266;
  case 3U:
  gb_tile_moden = 168U;
  goto ldv_49266;
  case 4U:
  gb_tile_moden = 148U;
  goto ldv_49266;
  case 5U:
  gb_tile_moden = 144U;
  goto ldv_49266;
  case 6U:
  gb_tile_moden = 144U;
  goto ldv_49266;
  case 8U:
  gb_tile_moden = 238U;
  goto ldv_49266;
  case 9U:
  gb_tile_moden = 234U;
  goto ldv_49266;
  case 10U:
  gb_tile_moden = 233U;
  goto ldv_49266;
  case 11U:
  gb_tile_moden = 229U;
  goto ldv_49266;
  case 12U:
  gb_tile_moden = 228U;
  goto ldv_49266;
  case 13U:
  gb_tile_moden = 224U;
  goto ldv_49266;
  case 14U:
  gb_tile_moden = 144U;
  goto ldv_49266;
  case 7U: ;
  goto ldv_49281;
  default:
  gb_tile_moden = 0U;
  goto ldv_49266;
  }
  ldv_49266:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  ldv_49281:
  reg_offset = reg_offset + 1U;
  ldv_49284: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_49283;
  } else {
  }
  case 6U:
  reg_offset = 0U;
  goto ldv_49321;
  ldv_49320: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8389392U;
  goto ldv_49288;
  case 1U:
  gb_tile_moden = 8391440U;
  goto ldv_49288;
  case 2U:
  gb_tile_moden = 8393488U;
  goto ldv_49288;
  case 3U:
  gb_tile_moden = 8395536U;
  goto ldv_49288;
  case 4U:
  gb_tile_moden = 8399632U;
  goto ldv_49288;
  case 5U:
  gb_tile_moden = 8399624U;
  goto ldv_49288;
  case 6U:
  gb_tile_moden = 8399636U;
  goto ldv_49288;
  case 7U:
  gb_tile_moden = 8399188U;
  goto ldv_49288;
  case 8U:
  gb_tile_moden = 772U;
  goto ldv_49288;
  case 9U:
  gb_tile_moden = 33555208U;
  goto ldv_49288;
  case 10U:
  gb_tile_moden = 33555216U;
  goto ldv_49288;
  case 11U:
  gb_tile_moden = 100664084U;
  goto ldv_49288;
  case 12U:
  gb_tile_moden = 100663636U;
  goto ldv_49288;
  case 13U:
  gb_tile_moden = 37749512U;
  goto ldv_49288;
  case 14U:
  gb_tile_moden = 37749520U;
  goto ldv_49288;
  case 15U:
  gb_tile_moden = 37749552U;
  goto ldv_49288;
  case 16U:
  gb_tile_moden = 104858388U;
  goto ldv_49288;
  case 17U:
  gb_tile_moden = 104857940U;
  goto ldv_49288;
  case 18U:
  gb_tile_moden = 4195084U;
  goto ldv_49288;
  case 19U:
  gb_tile_moden = 16777996U;
  goto ldv_49288;
  case 20U:
  gb_tile_moden = 16778012U;
  goto ldv_49288;
  case 21U:
  gb_tile_moden = 16778036U;
  goto ldv_49288;
  case 22U:
  gb_tile_moden = 16778020U;
  goto ldv_49288;
  case 23U:
  gb_tile_moden = 16777572U;
  goto ldv_49288;
  case 24U:
  gb_tile_moden = 4195100U;
  goto ldv_49288;
  case 25U:
  gb_tile_moden = 16778016U;
  goto ldv_49288;
  case 26U:
  gb_tile_moden = 16778040U;
  goto ldv_49288;
  case 27U:
  gb_tile_moden = 46138120U;
  goto ldv_49288;
  case 28U:
  gb_tile_moden = 46138128U;
  goto ldv_49288;
  case 29U:
  gb_tile_moden = 113246996U;
  goto ldv_49288;
  case 30U:
  gb_tile_moden = 113246548U;
  goto ldv_49288;
  default:
  gb_tile_moden = 0U;
  goto ldv_49288;
  }
  ldv_49288:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  reg_offset = reg_offset + 1U;
  ldv_49321: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_49320;
  } else {
  }
  reg_offset = 0U;
  goto ldv_49342;
  ldv_49341: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 232U;
  goto ldv_49324;
  case 1U:
  gb_tile_moden = 232U;
  goto ldv_49324;
  case 2U:
  gb_tile_moden = 232U;
  goto ldv_49324;
  case 3U:
  gb_tile_moden = 232U;
  goto ldv_49324;
  case 4U:
  gb_tile_moden = 212U;
  goto ldv_49324;
  case 5U:
  gb_tile_moden = 192U;
  goto ldv_49324;
  case 6U:
  gb_tile_moden = 192U;
  goto ldv_49324;
  case 8U:
  gb_tile_moden = 236U;
  goto ldv_49324;
  case 9U:
  gb_tile_moden = 232U;
  goto ldv_49324;
  case 10U:
  gb_tile_moden = 212U;
  goto ldv_49324;
  case 11U:
  gb_tile_moden = 208U;
  goto ldv_49324;
  case 12U:
  gb_tile_moden = 128U;
  goto ldv_49324;
  case 13U:
  gb_tile_moden = 64U;
  goto ldv_49324;
  case 14U:
  gb_tile_moden = 64U;
  goto ldv_49324;
  case 7U: ;
  goto ldv_49339;
  default:
  gb_tile_moden = 0U;
  goto ldv_49324;
  }
  ldv_49324:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  ldv_49339:
  reg_offset = reg_offset + 1U;
  ldv_49342: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_49341;
  } else {
  }
  goto ldv_49344;
  case 7U: ;
  default:
  reg_offset = 0U;
  goto ldv_49381;
  ldv_49380: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 8388624U;
  goto ldv_49348;
  case 1U:
  gb_tile_moden = 8390672U;
  goto ldv_49348;
  case 2U:
  gb_tile_moden = 8392720U;
  goto ldv_49348;
  case 3U:
  gb_tile_moden = 8394768U;
  goto ldv_49348;
  case 4U:
  gb_tile_moden = 8398864U;
  goto ldv_49348;
  case 5U:
  gb_tile_moden = 8398856U;
  goto ldv_49348;
  case 6U:
  gb_tile_moden = 8398868U;
  goto ldv_49348;
  case 8U:
  gb_tile_moden = 4U;
  goto ldv_49348;
  case 9U:
  gb_tile_moden = 33554440U;
  goto ldv_49348;
  case 10U:
  gb_tile_moden = 33554448U;
  goto ldv_49348;
  case 11U:
  gb_tile_moden = 100663316U;
  goto ldv_49348;
  case 13U:
  gb_tile_moden = 37748744U;
  goto ldv_49348;
  case 14U:
  gb_tile_moden = 37748752U;
  goto ldv_49348;
  case 15U:
  gb_tile_moden = 37748784U;
  goto ldv_49348;
  case 16U:
  gb_tile_moden = 104857620U;
  goto ldv_49348;
  case 18U:
  gb_tile_moden = 4194316U;
  goto ldv_49348;
  case 19U:
  gb_tile_moden = 16777228U;
  goto ldv_49348;
  case 20U:
  gb_tile_moden = 16777244U;
  goto ldv_49348;
  case 21U:
  gb_tile_moden = 16777268U;
  goto ldv_49348;
  case 22U:
  gb_tile_moden = 16777252U;
  goto ldv_49348;
  case 24U:
  gb_tile_moden = 4194332U;
  goto ldv_49348;
  case 25U:
  gb_tile_moden = 16777248U;
  goto ldv_49348;
  case 26U:
  gb_tile_moden = 16777272U;
  goto ldv_49348;
  case 27U:
  gb_tile_moden = 46137352U;
  goto ldv_49348;
  case 28U:
  gb_tile_moden = 46137360U;
  goto ldv_49348;
  case 29U:
  gb_tile_moden = 113246228U;
  goto ldv_49348;
  case 7U: ;
  case 12U: ;
  case 17U: ;
  case 23U: ;
  goto ldv_49378;
  default:
  gb_tile_moden = 0U;
  goto ldv_49348;
  }
  ldv_49348:
  adev->gfx.config.tile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9796U, gb_tile_moden, 0);
  ldv_49378:
  reg_offset = reg_offset + 1U;
  ldv_49381: ;
  if (reg_offset < num_tile_mode_states) {
    goto ldv_49380;
  } else {
  }
  reg_offset = 0U;
  goto ldv_49402;
  ldv_49401: ;
  switch (reg_offset) {
  case 0U:
  gb_tile_moden = 168U;
  goto ldv_49384;
  case 1U:
  gb_tile_moden = 164U;
  goto ldv_49384;
  case 2U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 3U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 4U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 5U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 6U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 8U:
  gb_tile_moden = 238U;
  goto ldv_49384;
  case 9U:
  gb_tile_moden = 234U;
  goto ldv_49384;
  case 10U:
  gb_tile_moden = 233U;
  goto ldv_49384;
  case 11U:
  gb_tile_moden = 229U;
  goto ldv_49384;
  case 12U:
  gb_tile_moden = 228U;
  goto ldv_49384;
  case 13U:
  gb_tile_moden = 224U;
  goto ldv_49384;
  case 14U:
  gb_tile_moden = 144U;
  goto ldv_49384;
  case 7U: ;
  goto ldv_49399;
  default:
  gb_tile_moden = 0U;
  goto ldv_49384;
  }
  ldv_49384:
  adev->gfx.config.macrotile_mode_array[reg_offset] = gb_tile_moden;
  amdgpu_mm_wreg(adev, reg_offset + 9828U, gb_tile_moden, 0);
  ldv_49399:
  reg_offset = reg_offset + 1U;
  ldv_49402: ;
  if (reg_offset < num_secondary_tile_mode_states) {
    goto ldv_49401;
  } else {
  }
  }
  ldv_49344: ;
  return;
}
}
static u32 gfx_v8_0_create_bitmask(u32 bit_width )
{
  u32 i ;
  u32 mask ;
  {
  mask = 0U;
  i = 0U;
  goto ldv_49410;
  ldv_49409:
  mask = mask << 1;
  mask = mask | 1U;
  i = i + 1U;
  ldv_49410: ;
  if (i < bit_width) {
    goto ldv_49409;
  } else {
  }
  return (mask);
}
}
void gfx_v8_0_select_se_sh(struct amdgpu_device *adev , u32 se_num , u32 sh_num )
{
  u32 data ;
  {
  data = 1073741824U;
  if (se_num == 4294967295U && sh_num == 4294967295U) {
    data = data | 536870912U;
    data = data | 2147483648U;
  } else
  if (se_num == 4294967295U) {
    data = (data & 4294902015U) | ((sh_num << 8) & 65535U);
    data = data | 2147483648U;
  } else
  if (sh_num == 4294967295U) {
    data = data | 536870912U;
    data = (data & 4278255615U) | ((se_num << 16) & 16711680U);
  } else {
    data = (data & 4294902015U) | ((sh_num << 8) & 65535U);
    data = (data & 4278255615U) | ((se_num << 16) & 16711680U);
  }
  amdgpu_mm_wreg(adev, 49664U, data, 0);
  return;
}
}
static u32 gfx_v8_0_get_rb_disabled(struct amdgpu_device *adev , u32 max_rb_num_per_se ,
                                    u32 sh_per_se )
{
  u32 data ;
  u32 mask ;
  u32 tmp ;
  {
  data = amdgpu_mm_rreg(adev, 9789U, 0);
  if ((int )data & 1) {
    data = data & 16711680U;
  } else {
    data = 0U;
  }
  tmp = amdgpu_mm_rreg(adev, 9951U, 0);
  data = tmp | data;
  data = data >> 16;
  mask = gfx_v8_0_create_bitmask(max_rb_num_per_se / sh_per_se);
  return (data & mask);
}
}
static void gfx_v8_0_setup_rb(struct amdgpu_device *adev , u32 se_num , u32 sh_per_se ,
                              u32 max_rb_num_per_se )
{
  int i ;
  int j ;
  u32 data ;
  u32 mask ;
  u32 disabled_rbs ;
  u32 enabled_rbs ;
  {
  disabled_rbs = 0U;
  enabled_rbs = 0U;
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_49441;
  ldv_49440:
  j = 0;
  goto ldv_49438;
  ldv_49437:
  gfx_v8_0_select_se_sh(adev, (u32 )i, (u32 )j);
  data = gfx_v8_0_get_rb_disabled(adev, max_rb_num_per_se, sh_per_se);
  disabled_rbs = (data << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | disabled_rbs;
  j = j + 1;
  ldv_49438: ;
  if ((u32 )j < sh_per_se) {
    goto ldv_49437;
  } else {
  }
  i = i + 1;
  ldv_49441: ;
  if ((u32 )i < se_num) {
    goto ldv_49440;
  } else {
  }
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  mask = 1U;
  i = 0;
  goto ldv_49444;
  ldv_49443: ;
  if ((disabled_rbs & mask) == 0U) {
    enabled_rbs = enabled_rbs | mask;
  } else {
  }
  mask = mask << 1;
  i = i + 1;
  ldv_49444: ;
  if ((u32 )i < max_rb_num_per_se * se_num) {
    goto ldv_49443;
  } else {
  }
  adev->gfx.config.backend_enable_mask = enabled_rbs;
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_49456;
  ldv_49455:
  gfx_v8_0_select_se_sh(adev, (u32 )i, 4294967295U);
  data = 0U;
  j = 0;
  goto ldv_49453;
  ldv_49452: ;
  switch (enabled_rbs & 3U) {
  case 0U: ;
  if (j == 0) {
    data = data | 768U;
  } else {
    data = data;
  }
  goto ldv_49447;
  case 1U:
  data = data;
  goto ldv_49447;
  case 2U:
  data = (u32 )(3 << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | data;
  goto ldv_49447;
  case 3U: ;
  default:
  data = (u32 )(2 << (int )(((u32 )i * sh_per_se + (u32 )j) * 2U)) | data;
  goto ldv_49447;
  }
  ldv_49447:
  enabled_rbs = enabled_rbs >> 2;
  j = j + 1;
  ldv_49453: ;
  if ((u32 )j < sh_per_se) {
    goto ldv_49452;
  } else {
  }
  amdgpu_mm_wreg(adev, 41172U, data, 0);
  i = i + 1;
  ldv_49456: ;
  if ((u32 )i < se_num) {
    goto ldv_49455;
  } else {
  }
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  return;
}
}
static void gmc_v8_0_init_compute_vmid(struct amdgpu_device *adev )
{
  int i ;
  u32 sh_mem_config ;
  u32 sh_mem_bases ;
  {
  sh_mem_bases = 1610637312U;
  sh_mem_config = 94U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 8;
  goto ldv_49465;
  ldv_49464:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  amdgpu_mm_wreg(adev, 8973U, sh_mem_config, 0);
  amdgpu_mm_wreg(adev, 8971U, 1U, 0);
  amdgpu_mm_wreg(adev, 8972U, 0U, 0);
  amdgpu_mm_wreg(adev, 8970U, sh_mem_bases, 0);
  i = i + 1;
  ldv_49465: ;
  if (i <= 15) {
    goto ldv_49464;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  return;
}
}
static void gfx_v8_0_gpu_init(struct amdgpu_device *adev )
{
  u32 gb_addr_config ;
  u32 mc_shared_chmap ;
  u32 mc_arb_ramcfg ;
  u32 dimm00_addr_map ;
  u32 dimm01_addr_map ;
  u32 dimm10_addr_map ;
  u32 dimm11_addr_map ;
  u32 tmp ;
  int i ;
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  adev->gfx.config.max_shader_engines = 1U;
  adev->gfx.config.max_tile_pipes = 2U;
  adev->gfx.config.max_cu_per_sh = 6U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 2U;
  adev->gfx.config.max_texture_channel_caches = 2U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 570490881U;
  goto ldv_49480;
  case 6U:
  adev->gfx.config.max_shader_engines = 4U;
  adev->gfx.config.max_tile_pipes = 8U;
  adev->gfx.config.max_cu_per_sh = 8U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 2U;
  adev->gfx.config.max_texture_channel_caches = 8U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 570494979U;
  goto ldv_49480;
  case 7U:
  adev->gfx.config.max_shader_engines = 1U;
  adev->gfx.config.max_tile_pipes = 2U;
  adev->gfx.config.max_sh_per_se = 1U;
  switch ((int )(adev->pdev)->revision) {
  case 196: ;
  case 132: ;
  case 200: ;
  case 204:
  adev->gfx.config.max_cu_per_sh = 8U;
  adev->gfx.config.max_backends_per_se = 2U;
  goto ldv_49487;
  case 197: ;
  case 129: ;
  case 133: ;
  case 201: ;
  case 205:
  adev->gfx.config.max_cu_per_sh = 6U;
  adev->gfx.config.max_backends_per_se = 2U;
  goto ldv_49487;
  case 198: ;
  case 202: ;
  case 206:
  adev->gfx.config.max_cu_per_sh = 6U;
  adev->gfx.config.max_backends_per_se = 2U;
  goto ldv_49487;
  case 199: ;
  case 135: ;
  case 203: ;
  default:
  adev->gfx.config.max_cu_per_sh = 4U;
  adev->gfx.config.max_backends_per_se = 1U;
  goto ldv_49487;
  }
  ldv_49487:
  adev->gfx.config.max_texture_channel_caches = 2U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 570490881U;
  goto ldv_49480;
  default:
  adev->gfx.config.max_shader_engines = 2U;
  adev->gfx.config.max_tile_pipes = 4U;
  adev->gfx.config.max_cu_per_sh = 2U;
  adev->gfx.config.max_sh_per_se = 1U;
  adev->gfx.config.max_backends_per_se = 2U;
  adev->gfx.config.max_texture_channel_caches = 4U;
  adev->gfx.config.max_gprs = 256U;
  adev->gfx.config.max_gs_threads = 32U;
  adev->gfx.config.max_hw_contexts = 8U;
  adev->gfx.config.sc_prim_fifo_size_frontend = 32U;
  adev->gfx.config.sc_prim_fifo_size_backend = 256U;
  adev->gfx.config.sc_hiz_tile_fifo_size = 48U;
  adev->gfx.config.sc_earlyz_tile_fifo_size = 304U;
  gb_addr_config = 570494979U;
  goto ldv_49480;
  }
  ldv_49480:
  tmp = amdgpu_mm_rreg(adev, 8192U, 0);
  tmp = tmp | 255U;
  amdgpu_mm_wreg(adev, 8192U, tmp, 0);
  mc_shared_chmap = amdgpu_mm_rreg(adev, 2049U, 0);
  adev->gfx.config.mc_arb_ramcfg = amdgpu_mm_rreg(adev, 2520U, 0);
  mc_arb_ramcfg = adev->gfx.config.mc_arb_ramcfg;
  adev->gfx.config.num_tile_pipes = adev->gfx.config.max_tile_pipes;
  adev->gfx.config.mem_max_burst_length_bytes = 256U;
  if ((adev->flags & 131072UL) != 0UL) {
    tmp = amdgpu_mm_rreg(adev, 2577U, 0);
    dimm00_addr_map = tmp & 15U;
    dimm01_addr_map = (tmp & 240U) >> 4;
    tmp = amdgpu_mm_rreg(adev, 2578U, 0);
    dimm10_addr_map = tmp & 15U;
    dimm11_addr_map = (tmp & 240U) >> 4;
    if (((dimm00_addr_map == 0U || dimm00_addr_map == 3U) || dimm00_addr_map == 4U) || dimm00_addr_map > 12U) {
      dimm00_addr_map = 0U;
    } else {
    }
    if (((dimm01_addr_map == 0U || dimm01_addr_map == 3U) || dimm01_addr_map == 4U) || dimm01_addr_map > 12U) {
      dimm01_addr_map = 0U;
    } else {
    }
    if (((dimm10_addr_map == 0U || dimm10_addr_map == 3U) || dimm10_addr_map == 4U) || dimm10_addr_map > 12U) {
      dimm10_addr_map = 0U;
    } else {
    }
    if (((dimm11_addr_map == 0U || dimm11_addr_map == 3U) || dimm11_addr_map == 4U) || dimm11_addr_map > 12U) {
      dimm11_addr_map = 0U;
    } else {
    }
    if (((dimm00_addr_map == 11U || dimm01_addr_map == 11U) || dimm10_addr_map == 11U) || dimm11_addr_map == 11U) {
      adev->gfx.config.mem_row_size_in_kb = 2U;
    } else {
      adev->gfx.config.mem_row_size_in_kb = 1U;
    }
  } else {
    tmp = (mc_arb_ramcfg & 192U) >> 6;
    adev->gfx.config.mem_row_size_in_kb = (unsigned int )((4 << (int )(tmp + 8U)) / 1024);
    if (adev->gfx.config.mem_row_size_in_kb > 4U) {
      adev->gfx.config.mem_row_size_in_kb = 4U;
    } else {
    }
  }
  adev->gfx.config.shader_engine_tile_size = 32U;
  adev->gfx.config.num_gpus = 1U;
  adev->gfx.config.multi_gpu_tile_size = 64U;
  switch (adev->gfx.config.mem_row_size_in_kb) {
  case 1U: ;
  default:
  gb_addr_config = gb_addr_config & 3489660927U;
  goto ldv_49503;
  case 2U:
  gb_addr_config = (gb_addr_config & 3489660927U) | 268435456U;
  goto ldv_49503;
  case 4U:
  gb_addr_config = (gb_addr_config & 3489660927U) | 536870912U;
  goto ldv_49503;
  }
  ldv_49503:
  adev->gfx.config.gb_addr_config = gb_addr_config;
  amdgpu_mm_wreg(adev, 9790U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 3026U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 771U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 13318U, gb_addr_config & 112U, 0);
  amdgpu_mm_wreg(adev, 13830U, gb_addr_config & 112U, 0);
  amdgpu_mm_wreg(adev, 15315U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 15316U, gb_addr_config, 0);
  amdgpu_mm_wreg(adev, 15317U, gb_addr_config, 0);
  gfx_v8_0_tiling_mode_table_init(adev);
  gfx_v8_0_setup_rb(adev, adev->gfx.config.max_shader_engines, adev->gfx.config.max_sh_per_se,
                    adev->gfx.config.max_backends_per_se);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_49507;
  ldv_49506:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  if (i == 0) {
    tmp = 96U;
    tmp = (tmp & 4294965503U) | 768U;
    tmp = tmp | 24U;
    amdgpu_mm_wreg(adev, 8973U, tmp, 0);
  } else {
    tmp = 32U;
    tmp = (tmp & 4294965503U) | 256U;
    tmp = tmp | 24U;
    amdgpu_mm_wreg(adev, 8973U, tmp, 0);
  }
  amdgpu_mm_wreg(adev, 8971U, 1U, 0);
  amdgpu_mm_wreg(adev, 8972U, 0U, 0);
  amdgpu_mm_wreg(adev, 8970U, 0U, 0);
  i = i + 1;
  ldv_49507: ;
  if (i <= 15) {
    goto ldv_49506;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  gmc_v8_0_init_compute_vmid(adev);
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  amdgpu_mm_wreg(adev, 8947U, ((adev->gfx.config.sc_prim_fifo_size_frontend | (adev->gfx.config.sc_prim_fifo_size_backend << 6)) | (adev->gfx.config.sc_hiz_tile_fifo_size << 15)) | (adev->gfx.config.sc_earlyz_tile_fifo_size << 23),
                 0);
  mutex_unlock(& adev->grbm_idx_mutex);
  return;
}
}
static void gfx_v8_0_wait_for_rlc_serdes(struct amdgpu_device *adev )
{
  u32 i ;
  u32 j ;
  u32 k ;
  u32 mask ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0U;
  goto ldv_49523;
  ldv_49522:
  j = 0U;
  goto ldv_49520;
  ldv_49519:
  gfx_v8_0_select_se_sh(adev, i, j);
  k = 0U;
  goto ldv_49518;
  ldv_49517:
  tmp = amdgpu_mm_rreg(adev, 60513U, 0);
  if (tmp == 0U) {
    goto ldv_49516;
  } else {
  }
  __const_udelay(4295UL);
  k = k + 1U;
  ldv_49518: ;
  if ((u32 )adev->usec_timeout > k) {
    goto ldv_49517;
  } else {
  }
  ldv_49516:
  j = j + 1U;
  ldv_49520: ;
  if (adev->gfx.config.max_sh_per_se > j) {
    goto ldv_49519;
  } else {
  }
  i = i + 1U;
  ldv_49523: ;
  if (adev->gfx.config.max_shader_engines > i) {
    goto ldv_49522;
  } else {
  }
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  mutex_unlock(& adev->grbm_idx_mutex);
  mask = 917503U;
  k = 0U;
  goto ldv_49527;
  ldv_49526:
  tmp___0 = amdgpu_mm_rreg(adev, 60514U, 0);
  if ((tmp___0 & mask) == 0U) {
    goto ldv_49525;
  } else {
  }
  __const_udelay(4295UL);
  k = k + 1U;
  ldv_49527: ;
  if ((u32 )adev->usec_timeout > k) {
    goto ldv_49526;
  } else {
  }
  ldv_49525: ;
  return;
}
}
static void gfx_v8_0_enable_gui_idle_interrupt(struct amdgpu_device *adev , bool enable )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 12394U, 0);
  tmp = tmp___0;
  if ((int )enable) {
    tmp = tmp | 524288U;
    tmp = tmp | 1048576U;
    tmp = tmp | 262144U;
    tmp = tmp | 2097152U;
  } else {
    tmp = tmp & 4294443007U;
    tmp = tmp & 4293918719U;
    tmp = tmp & 4294705151U;
    tmp = tmp & 4292870143U;
  }
  amdgpu_mm_wreg(adev, 12394U, tmp, 0);
  return;
}
}
void gfx_v8_0_rlc_stop(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 60416U, 0);
  tmp = tmp___0;
  tmp = tmp & 4294967294U;
  amdgpu_mm_wreg(adev, 60416U, tmp, 0);
  gfx_v8_0_enable_gui_idle_interrupt(adev, 0);
  gfx_v8_0_wait_for_rlc_serdes(adev);
  return;
}
}
static void gfx_v8_0_rlc_reset(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 8200U, 0);
  tmp = tmp___0;
  tmp = tmp | 4U;
  amdgpu_mm_wreg(adev, 8200U, tmp, 0);
  __const_udelay(214750UL);
  tmp = tmp & 4294967291U;
  amdgpu_mm_wreg(adev, 8200U, tmp, 0);
  __const_udelay(214750UL);
  return;
}
}
static void gfx_v8_0_rlc_start(struct amdgpu_device *adev )
{
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 60416U, 0);
  tmp = tmp___0;
  tmp = tmp | 1U;
  amdgpu_mm_wreg(adev, 60416U, tmp, 0);
  if ((unsigned int )adev->asic_type != 7U) {
    gfx_v8_0_enable_gui_idle_interrupt(adev, 1);
  } else {
  }
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v8_0_rlc_load_microcode(struct amdgpu_device *adev )
{
  struct rlc_firmware_header_v2_0 const *hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  {
  if ((unsigned long )adev->gfx.rlc_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct rlc_firmware_header_v2_0 const *)(adev->gfx.rlc_fw)->data;
  amdgpu_ucode_print_rlc_hdr(& hdr->header);
  adev->gfx.rlc_fw_version = hdr->header.ucode_version;
  fw_data = (__le32 const *)(adev->gfx.rlc_fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 63548U, 0U, 0);
  i = 0U;
  goto ldv_49553;
  ldv_49552:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, 63549U, tmp___0, 0);
  i = i + 1U;
  ldv_49553: ;
  if (i < fw_size) {
    goto ldv_49552;
  } else {
  }
  amdgpu_mm_wreg(adev, 63548U, adev->gfx.rlc_fw_version, 0);
  return (0);
}
}
static int gfx_v8_0_rlc_resume(struct amdgpu_device *adev )
{
  int r ;
  {
  gfx_v8_0_rlc_stop(adev);
  amdgpu_mm_wreg(adev, 60489U, 0U, 0);
  amdgpu_mm_wreg(adev, 60483U, 0U, 0);
  gfx_v8_0_rlc_reset(adev);
  if (! adev->firmware.smu_load) {
    r = gfx_v8_0_rlc_load_microcode(adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 7U);
    if (r != 0) {
      return (-22);
    } else {
    }
  }
  gfx_v8_0_rlc_start(adev);
  return (0);
}
}
static void gfx_v8_0_cp_gfx_enable(struct amdgpu_device *adev , bool enable )
{
  int i ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  tmp___0 = amdgpu_mm_rreg(adev, 8630U, 0);
  tmp = tmp___0;
  if ((int )enable) {
    tmp = tmp & 4026531839U;
    tmp = tmp & 4227858431U;
    tmp = tmp & 4278190079U;
  } else {
    tmp = tmp | 268435456U;
    tmp = tmp | 67108864U;
    tmp = tmp | 16777216U;
    i = 0;
    goto ldv_49566;
    ldv_49565:
    adev->gfx.gfx_ring[i].ready = 0;
    i = i + 1;
    ldv_49566: ;
    if ((unsigned int )i < adev->gfx.num_gfx_rings) {
      goto ldv_49565;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 8630U, tmp, 0);
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v8_0_cp_gfx_load_microcode(struct amdgpu_device *adev )
{
  struct gfx_firmware_header_v1_0 const *pfp_hdr ;
  struct gfx_firmware_header_v1_0 const *ce_hdr ;
  struct gfx_firmware_header_v1_0 const *me_hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  __le32 const *tmp___1 ;
  __u32 tmp___2 ;
  __le32 const *tmp___3 ;
  __u32 tmp___4 ;
  {
  if (((unsigned long )adev->gfx.me_fw == (unsigned long )((struct firmware const *)0) || (unsigned long )adev->gfx.pfp_fw == (unsigned long )((struct firmware const *)0)) || (unsigned long )adev->gfx.ce_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  pfp_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.pfp_fw)->data;
  ce_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.ce_fw)->data;
  me_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.me_fw)->data;
  amdgpu_ucode_print_gfx_hdr(& pfp_hdr->header);
  amdgpu_ucode_print_gfx_hdr(& ce_hdr->header);
  amdgpu_ucode_print_gfx_hdr(& me_hdr->header);
  adev->gfx.pfp_fw_version = pfp_hdr->header.ucode_version;
  adev->gfx.ce_fw_version = ce_hdr->header.ucode_version;
  adev->gfx.me_fw_version = me_hdr->header.ucode_version;
  adev->gfx.me_feature_version = me_hdr->ucode_feature_version;
  adev->gfx.ce_feature_version = ce_hdr->ucode_feature_version;
  adev->gfx.pfp_feature_version = pfp_hdr->ucode_feature_version;
  gfx_v8_0_cp_gfx_enable(adev, 0);
  fw_data = (__le32 const *)(adev->gfx.pfp_fw)->data + (unsigned long )pfp_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )pfp_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 63508U, 0U, 0);
  i = 0U;
  goto ldv_49578;
  ldv_49577:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, 63509U, tmp___0, 0);
  i = i + 1U;
  ldv_49578: ;
  if (i < fw_size) {
    goto ldv_49577;
  } else {
  }
  amdgpu_mm_wreg(adev, 63508U, adev->gfx.pfp_fw_version, 0);
  fw_data = (__le32 const *)(adev->gfx.ce_fw)->data + (unsigned long )ce_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )ce_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 63512U, 0U, 0);
  i = 0U;
  goto ldv_49581;
  ldv_49580:
  tmp___1 = fw_data;
  fw_data = fw_data + 1;
  tmp___2 = __le32_to_cpup(tmp___1);
  amdgpu_mm_wreg(adev, 63513U, tmp___2, 0);
  i = i + 1U;
  ldv_49581: ;
  if (i < fw_size) {
    goto ldv_49580;
  } else {
  }
  amdgpu_mm_wreg(adev, 63512U, adev->gfx.ce_fw_version, 0);
  fw_data = (__le32 const *)(adev->gfx.me_fw)->data + (unsigned long )me_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )me_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 63510U, 0U, 0);
  i = 0U;
  goto ldv_49584;
  ldv_49583:
  tmp___3 = fw_data;
  fw_data = fw_data + 1;
  tmp___4 = __le32_to_cpup(tmp___3);
  amdgpu_mm_wreg(adev, 63511U, tmp___4, 0);
  i = i + 1U;
  ldv_49584: ;
  if (i < fw_size) {
    goto ldv_49583;
  } else {
  }
  amdgpu_mm_wreg(adev, 63510U, adev->gfx.me_fw_version, 0);
  return (0);
}
}
static u32 gfx_v8_0_get_csb_size(struct amdgpu_device *adev )
{
  u32 count ;
  struct cs_section_def const *sect ;
  struct cs_extent_def const *ext ;
  {
  count = 0U;
  sect = (struct cs_section_def const *)0;
  ext = (struct cs_extent_def const *)0;
  count = count + 2U;
  count = count + 3U;
  sect = (struct cs_section_def const *)(& vi_cs_data);
  goto ldv_49596;
  ldv_49595:
  ext = sect->section;
  goto ldv_49593;
  ldv_49592: ;
  if ((unsigned int )sect->id == 1U) {
    count = ((u32 )ext->reg_count + count) + 2U;
  } else {
    return (0U);
  }
  ext = ext + 1;
  ldv_49593: ;
  if ((unsigned long )ext->extent != (unsigned long )((unsigned int const * )0U)) {
    goto ldv_49592;
  } else {
  }
  sect = sect + 1;
  ldv_49596: ;
  if ((unsigned long )sect->section != (unsigned long )((struct cs_extent_def const * )0)) {
    goto ldv_49595;
  } else {
  }
  count = count + 4U;
  count = count + 2U;
  count = count + 2U;
  return (count);
}
}
static int gfx_v8_0_cp_gfx_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  struct cs_section_def const *sect ;
  struct cs_extent_def const *ext ;
  int r ;
  int i ;
  u32 tmp ;
  {
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring);
  sect = (struct cs_section_def const *)0;
  ext = (struct cs_extent_def const *)0;
  amdgpu_mm_wreg(adev, 12462U, adev->gfx.config.max_hw_contexts - 1U, 0);
  amdgpu_mm_wreg(adev, 12368U, 0U, 0);
  amdgpu_mm_wreg(adev, 12363U, 1U, 0);
  gfx_v8_0_cp_gfx_enable(adev, 1);
  tmp = gfx_v8_0_get_csb_size(adev);
  r = amdgpu_ring_lock(ring, tmp + 4U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring (%d).\n", r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 3221244416U);
  amdgpu_ring_write(ring, 536870912U);
  amdgpu_ring_write(ring, 3221301248U);
  amdgpu_ring_write(ring, 2147483648U);
  amdgpu_ring_write(ring, 2147483648U);
  sect = (struct cs_section_def const *)(& vi_cs_data);
  goto ldv_49613;
  ldv_49612:
  ext = sect->section;
  goto ldv_49610;
  ldv_49609: ;
  if ((unsigned int )sect->id == 1U) {
    amdgpu_ring_write(ring, (((unsigned int )ext->reg_count & 16383U) << 16) | 3221252352U);
    amdgpu_ring_write(ring, (unsigned int )ext->reg_index - 40960U);
    i = 0;
    goto ldv_49607;
    ldv_49606:
    amdgpu_ring_write(ring, *(ext->extent + (unsigned long )i));
    i = i + 1;
    ldv_49607: ;
    if ((unsigned int )i < (unsigned int )ext->reg_count) {
      goto ldv_49606;
    } else {
    }
  } else {
  }
  ext = ext + 1;
  ldv_49610: ;
  if ((unsigned long )ext->extent != (unsigned long )((unsigned int const * )0U)) {
    goto ldv_49609;
  } else {
  }
  sect = sect + 1;
  ldv_49613: ;
  if ((unsigned long )sect->section != (unsigned long )((struct cs_extent_def const * )0)) {
    goto ldv_49612;
  } else {
  }
  amdgpu_ring_write(ring, 3221383424U);
  amdgpu_ring_write(ring, 212U);
  switch ((unsigned int )adev->asic_type) {
  case 6U:
  amdgpu_ring_write(ring, 369098770U);
  amdgpu_ring_write(ring, 42U);
  goto ldv_49616;
  case 5U: ;
  case 7U:
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, 0U);
  goto ldv_49616;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/gfx_v8_0.c"),
                       "i" (2493), "i" (12UL));
  ldv_49620: ;
  goto ldv_49620;
  }
  ldv_49616:
  amdgpu_ring_write(ring, 3221244416U);
  amdgpu_ring_write(ring, 805306368U);
  amdgpu_ring_write(ring, 3221230080U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 3221360896U);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_write(ring, 32768U);
  amdgpu_ring_write(ring, 32768U);
  amdgpu_ring_unlock_commit(ring);
  return (0);
}
}
static int gfx_v8_0_cp_gfx_resume(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 tmp ;
  u32 rb_bufsz ;
  u64 rb_addr ;
  u64 rptr_addr ;
  int r ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  unsigned long __ms ;
  unsigned long tmp___2 ;
  {
  amdgpu_mm_wreg(adev, 8641U, 0U, 0);
  amdgpu_mm_wreg(adev, 12369U, 0U, 0);
  ring = (struct amdgpu_ring *)(& adev->gfx.gfx_ring);
  tmp___0 = __roundup_pow_of_two((unsigned long )(ring->ring_size / 8U));
  tmp___1 = __ilog2_u64((u64 )tmp___0);
  rb_bufsz = (u32 )tmp___1;
  tmp = rb_bufsz & 63U;
  tmp = (tmp & 4294951167U) | (((rb_bufsz - 2U) << 8) & 16128U);
  tmp = tmp | 98304U;
  tmp = (tmp & 4282384383U) | 4194304U;
  amdgpu_mm_wreg(adev, 12353U, tmp, 0);
  amdgpu_mm_wreg(adev, 12353U, tmp | 2147483648U, 0);
  ring->wptr = 0U;
  amdgpu_mm_wreg(adev, 12357U, ring->wptr, 0);
  rptr_addr = adev->wb.gpu_addr + (uint64_t )(ring->rptr_offs * 4U);
  amdgpu_mm_wreg(adev, 12355U, (unsigned int )rptr_addr, 0);
  amdgpu_mm_wreg(adev, 12356U, (unsigned int )(rptr_addr >> 32ULL) & 255U, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_49632;
    ldv_49631:
    __const_udelay(4295000UL);
    ldv_49632:
    tmp___2 = __ms;
    __ms = __ms - 1UL;
    if (tmp___2 != 0UL) {
      goto ldv_49631;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 12353U, tmp, 0);
  rb_addr = ring->gpu_addr >> 8;
  amdgpu_mm_wreg(adev, 12352U, (u32 )rb_addr, 0);
  amdgpu_mm_wreg(adev, 12465U, (unsigned int )(rb_addr >> 32ULL), 0);
  if ((unsigned int )adev->asic_type != 5U) {
    tmp = amdgpu_mm_rreg(adev, 12377U, 0);
    if ((int )ring->use_doorbell) {
      tmp = (tmp & 4286578691U) | ((ring->doorbell_index << 2) & 8388604U);
      tmp = tmp | 1073741824U;
    } else {
      tmp = tmp & 3221225471U;
    }
    amdgpu_mm_wreg(adev, 12377U, tmp, 0);
    if ((unsigned int )adev->asic_type == 6U) {
      tmp = 128U;
      amdgpu_mm_wreg(adev, 12378U, tmp, 0);
      amdgpu_mm_wreg(adev, 12379U, 8388604U, 0);
    } else {
    }
  } else {
  }
  gfx_v8_0_cp_gfx_start(adev);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  return (0);
}
}
static void gfx_v8_0_cp_compute_enable(struct amdgpu_device *adev , bool enable )
{
  int i ;
  {
  if ((int )enable) {
    amdgpu_mm_wreg(adev, 8333U, 0U, 0);
  } else {
    amdgpu_mm_wreg(adev, 8333U, 1342177280U, 0);
    i = 0;
    goto ldv_49640;
    ldv_49639:
    adev->gfx.compute_ring[i].ready = 0;
    i = i + 1;
    ldv_49640: ;
    if ((unsigned int )i < adev->gfx.num_compute_rings) {
      goto ldv_49639;
    } else {
    }
  }
  __const_udelay(214750UL);
  return;
}
}
static int gfx_v8_0_cp_compute_start(struct amdgpu_device *adev )
{
  {
  gfx_v8_0_cp_compute_enable(adev, 1);
  return (0);
}
}
static int gfx_v8_0_cp_compute_load_microcode(struct amdgpu_device *adev )
{
  struct gfx_firmware_header_v1_0 const *mec_hdr ;
  __le32 const *fw_data ;
  unsigned int i ;
  unsigned int fw_size ;
  __u32 tmp ;
  struct gfx_firmware_header_v1_0 const *mec2_hdr ;
  __u32 tmp___0 ;
  {
  if ((unsigned long )adev->gfx.mec_fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  gfx_v8_0_cp_compute_enable(adev, 0);
  mec_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec_fw)->data;
  amdgpu_ucode_print_gfx_hdr(& mec_hdr->header);
  adev->gfx.mec_fw_version = mec_hdr->header.ucode_version;
  fw_data = (__le32 const *)(adev->gfx.mec_fw)->data + (unsigned long )mec_hdr->header.ucode_array_offset_bytes;
  fw_size = (unsigned int )mec_hdr->header.ucode_size_bytes / 4U;
  amdgpu_mm_wreg(adev, 63514U, 0U, 0);
  i = 0U;
  goto ldv_49653;
  ldv_49652:
  tmp = __le32_to_cpup(fw_data + (unsigned long )i);
  amdgpu_mm_wreg(adev, 63515U, tmp, 0);
  i = i + 1U;
  ldv_49653: ;
  if (i < fw_size) {
    goto ldv_49652;
  } else {
  }
  amdgpu_mm_wreg(adev, 63514U, adev->gfx.mec_fw_version, 0);
  if ((unsigned long )adev->gfx.mec2_fw != (unsigned long )((struct firmware const *)0)) {
    mec2_hdr = (struct gfx_firmware_header_v1_0 const *)(adev->gfx.mec2_fw)->data;
    amdgpu_ucode_print_gfx_hdr(& mec2_hdr->header);
    adev->gfx.mec2_fw_version = mec2_hdr->header.ucode_version;
    fw_data = (__le32 const *)(adev->gfx.mec2_fw)->data + (unsigned long )mec2_hdr->header.ucode_array_offset_bytes;
    fw_size = (unsigned int )mec2_hdr->header.ucode_size_bytes / 4U;
    amdgpu_mm_wreg(adev, 63516U, 0U, 0);
    i = 0U;
    goto ldv_49657;
    ldv_49656:
    tmp___0 = __le32_to_cpup(fw_data + (unsigned long )i);
    amdgpu_mm_wreg(adev, 63517U, tmp___0, 0);
    i = i + 1U;
    ldv_49657: ;
    if (i < fw_size) {
      goto ldv_49656;
    } else {
    }
    amdgpu_mm_wreg(adev, 63516U, adev->gfx.mec2_fw_version, 0);
  } else {
  }
  return (0);
}
}
static void gfx_v8_0_cp_compute_fini(struct amdgpu_device *adev )
{
  int i ;
  int r ;
  struct amdgpu_ring *ring ;
  long tmp ;
  {
  i = 0;
  goto ldv_49924;
  ldv_49923:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if ((unsigned long )ring->mqd_obj != (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_reserve(ring->mqd_obj, 0);
    tmp = ldv__builtin_expect(r != 0, 0L);
    if (tmp != 0L) {
      dev_warn((struct device const *)adev->dev, "(%d) reserve MQD bo failed\n",
               r);
    } else {
    }
    amdgpu_bo_unpin(ring->mqd_obj);
    amdgpu_bo_unreserve(ring->mqd_obj);
    amdgpu_bo_unref(& ring->mqd_obj);
    ring->mqd_obj = (struct amdgpu_bo *)0;
  } else {
  }
  i = i + 1;
  ldv_49924: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_49923;
  } else {
  }
  return;
}
}
static int gfx_v8_0_cp_compute_resume(struct amdgpu_device *adev )
{
  int r ;
  int i ;
  int j ;
  u32 tmp ;
  bool use_doorbell ;
  u64 hqd_gpu_addr ;
  u64 mqd_gpu_addr ;
  u64 eop_gpu_addr ;
  u64 wb_gpu_addr ;
  u32 *buf ;
  struct vi_mqd *mqd ;
  int me ;
  int pipe ;
  struct amdgpu_ring *ring ;
  long tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  unsigned long tmp___3 ;
  int tmp___4 ;
  struct amdgpu_ring *ring___0 ;
  {
  use_doorbell = 1;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_49943;
  ldv_49942:
  me = i <= 3 ? 1 : 2;
  pipe = i > 3 ? i + -4 : i;
  eop_gpu_addr = adev->gfx.mec.hpd_eop_gpu_addr + (u64 )(i * 2048);
  eop_gpu_addr = eop_gpu_addr >> 8;
  vi_srbm_select(adev, (u32 )me, (u32 )pipe, 0U, 0U);
  amdgpu_mm_wreg(adev, 12906U, (u32 )eop_gpu_addr, 0);
  amdgpu_mm_wreg(adev, 12907U, (unsigned int )(eop_gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 12872U, 0U, 0);
  tmp = amdgpu_mm_rreg(adev, 12908U, 0);
  tmp = (tmp & 4294967232U) | 8U;
  amdgpu_mm_wreg(adev, 12908U, tmp, 0);
  i = i + 1;
  ldv_49943: ;
  if ((u32 )i < adev->gfx.mec.num_pipe * adev->gfx.mec.num_mec) {
    goto ldv_49942;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  i = 0;
  goto ldv_49950;
  ldv_49949:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if ((unsigned long )ring->mqd_obj == (unsigned long )((struct amdgpu_bo *)0)) {
    r = amdgpu_bo_create(adev, 2048UL, 4096, 1, 2U, 0ULL, (struct sg_table *)0, & ring->mqd_obj);
    if (r != 0) {
      dev_warn((struct device const *)adev->dev, "(%d) create MQD bo failed\n",
               r);
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_reserve(ring->mqd_obj, 0);
  tmp___0 = ldv__builtin_expect(r != 0, 0L);
  if (tmp___0 != 0L) {
    gfx_v8_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(ring->mqd_obj, 2U, & mqd_gpu_addr);
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) pin MQD bo failed\n", r);
    gfx_v8_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(ring->mqd_obj, (void **)(& buf));
  if (r != 0) {
    dev_warn((struct device const *)adev->dev, "(%d) map MQD bo failed\n", r);
    gfx_v8_0_cp_compute_fini(adev);
    return (r);
  } else {
  }
  memset((void *)buf, 0, 2048UL);
  mqd = (struct vi_mqd *)buf;
  mqd->header = 3224438784U;
  mqd->compute_pipelinestat_enable = 1U;
  mqd->compute_static_thread_mgmt_se0 = 4294967295U;
  mqd->compute_static_thread_mgmt_se1 = 4294967295U;
  mqd->compute_static_thread_mgmt_se2 = 4294967295U;
  mqd->compute_static_thread_mgmt_se3 = 4294967295U;
  mqd->compute_misc_reserved = 3U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  vi_srbm_select(adev, ring->me, ring->pipe, ring->queue, 0U);
  tmp = amdgpu_mm_rreg(adev, 12419U, 0);
  tmp = tmp & 2147483647U;
  amdgpu_mm_wreg(adev, 12419U, tmp, 0);
  mqd->cp_hqd_eop_base_addr_lo = amdgpu_mm_rreg(adev, 12906U, 0);
  mqd->cp_hqd_eop_base_addr_hi = amdgpu_mm_rreg(adev, 12907U, 0);
  tmp = amdgpu_mm_rreg(adev, 12884U, 0);
  if ((int )use_doorbell) {
    tmp = tmp | 1073741824U;
  } else {
    tmp = tmp & 3221225471U;
  }
  amdgpu_mm_wreg(adev, 12884U, tmp, 0);
  mqd->cp_hqd_pq_doorbell_control = tmp;
  mqd->cp_hqd_dequeue_request = 0U;
  mqd->cp_hqd_pq_rptr = 0U;
  mqd->cp_hqd_pq_wptr = 0U;
  tmp___2 = amdgpu_mm_rreg(adev, 12871U, 0);
  if ((int )tmp___2 & 1) {
    amdgpu_mm_wreg(adev, 12893U, 1U, 0);
    j = 0;
    goto ldv_49948;
    ldv_49947:
    tmp___1 = amdgpu_mm_rreg(adev, 12871U, 0);
    if ((tmp___1 & 1U) == 0U) {
      goto ldv_49946;
    } else {
    }
    __const_udelay(4295UL);
    j = j + 1;
    ldv_49948: ;
    if (adev->usec_timeout > j) {
      goto ldv_49947;
    } else {
    }
    ldv_49946:
    amdgpu_mm_wreg(adev, 12893U, mqd->cp_hqd_dequeue_request, 0);
    amdgpu_mm_wreg(adev, 12879U, mqd->cp_hqd_pq_rptr, 0);
    amdgpu_mm_wreg(adev, 12885U, mqd->cp_hqd_pq_wptr, 0);
  } else {
  }
  mqd->cp_mqd_base_addr_lo = (u32 )mqd_gpu_addr & 4294967292U;
  mqd->cp_mqd_base_addr_hi = (unsigned int )(mqd_gpu_addr >> 32ULL);
  amdgpu_mm_wreg(adev, 12869U, mqd->cp_mqd_base_addr_lo, 0);
  amdgpu_mm_wreg(adev, 12870U, mqd->cp_mqd_base_addr_hi, 0);
  tmp = amdgpu_mm_rreg(adev, 12903U, 0);
  tmp = tmp & 4294967280U;
  amdgpu_mm_wreg(adev, 12903U, tmp, 0);
  mqd->cp_mqd_control = tmp;
  hqd_gpu_addr = ring->gpu_addr >> 8;
  mqd->cp_hqd_pq_base_lo = (u32 )hqd_gpu_addr;
  mqd->cp_hqd_pq_base_hi = (unsigned int )(hqd_gpu_addr >> 32ULL);
  amdgpu_mm_wreg(adev, 12877U, mqd->cp_hqd_pq_base_lo, 0);
  amdgpu_mm_wreg(adev, 12878U, mqd->cp_hqd_pq_base_hi, 0);
  tmp = amdgpu_mm_rreg(adev, 12886U, 0);
  tmp___3 = __roundup_pow_of_two((unsigned long )(ring->ring_size / 4U));
  tmp___4 = __ilog2_u64((u64 )tmp___3);
  tmp = (tmp & 4294967232U) | ((u32 )(tmp___4 + -1) & 63U);
  tmp = tmp & 4294951167U;
  tmp = tmp & 4026531839U;
  tmp = tmp & 3758096383U;
  tmp = tmp | 1073741824U;
  tmp = tmp | 2147483648U;
  amdgpu_mm_wreg(adev, 12886U, tmp, 0);
  mqd->cp_hqd_pq_control = tmp;
  wb_gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->rptr_offs * 4U);
  mqd->cp_hqd_pq_rptr_report_addr_lo = (u32 )wb_gpu_addr & 4294967292U;
  mqd->cp_hqd_pq_rptr_report_addr_hi = (unsigned int )(wb_gpu_addr >> 32ULL) & 65535U;
  amdgpu_mm_wreg(adev, 12880U, mqd->cp_hqd_pq_rptr_report_addr_lo, 0);
  amdgpu_mm_wreg(adev, 12881U, mqd->cp_hqd_pq_rptr_report_addr_hi, 0);
  wb_gpu_addr = adev->wb.gpu_addr + (uint64_t )(ring->wptr_offs * 4U);
  mqd->cp_hqd_pq_wptr_poll_addr = (u32 )wb_gpu_addr & 4294967292U;
  mqd->cp_hqd_pq_wptr_poll_addr_hi = (unsigned int )(wb_gpu_addr >> 32ULL) & 65535U;
  amdgpu_mm_wreg(adev, 12882U, mqd->cp_hqd_pq_wptr_poll_addr, 0);
  amdgpu_mm_wreg(adev, 12883U, mqd->cp_hqd_pq_wptr_poll_addr_hi, 0);
  if ((int )use_doorbell) {
    if ((unsigned int )adev->asic_type == 7U) {
      amdgpu_mm_wreg(adev, 12380U, 0U, 0);
      amdgpu_mm_wreg(adev, 12381U, 2097148U, 0);
    } else {
    }
    tmp = amdgpu_mm_rreg(adev, 12884U, 0);
    tmp = (tmp & 4286578691U) | ((ring->doorbell_index << 2) & 8388604U);
    tmp = tmp | 1073741824U;
    tmp = tmp & 4026531839U;
    tmp = tmp & 2147483647U;
    mqd->cp_hqd_pq_doorbell_control = tmp;
  } else {
    mqd->cp_hqd_pq_doorbell_control = 0U;
  }
  amdgpu_mm_wreg(adev, 12884U, mqd->cp_hqd_pq_doorbell_control, 0);
  ring->wptr = 0U;
  mqd->cp_hqd_pq_wptr = ring->wptr;
  amdgpu_mm_wreg(adev, 12885U, mqd->cp_hqd_pq_wptr, 0);
  mqd->cp_hqd_pq_rptr = amdgpu_mm_rreg(adev, 12879U, 0);
  mqd->cp_hqd_vmid = 0U;
  amdgpu_mm_wreg(adev, 12872U, mqd->cp_hqd_vmid, 0);
  tmp = amdgpu_mm_rreg(adev, 12873U, 0);
  tmp = (tmp & 4294705407U) | 21248U;
  amdgpu_mm_wreg(adev, 12873U, tmp, 0);
  mqd->cp_hqd_persistent_state = tmp;
  mqd->cp_hqd_active = 1U;
  amdgpu_mm_wreg(adev, 12871U, mqd->cp_hqd_active, 0);
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  amdgpu_bo_kunmap(ring->mqd_obj);
  amdgpu_bo_unreserve(ring->mqd_obj);
  i = i + 1;
  ldv_49950: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_49949;
  } else {
  }
  if ((int )use_doorbell) {
    tmp = amdgpu_mm_rreg(adev, 12472U, 0);
    tmp = tmp | 2U;
    amdgpu_mm_wreg(adev, 12472U, tmp, 0);
  } else {
  }
  r = gfx_v8_0_cp_compute_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  i = 0;
  goto ldv_49954;
  ldv_49953:
  ring___0 = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  ring___0->ready = 1;
  r = (*((ring___0->funcs)->test_ring))(ring___0);
  if (r != 0) {
    ring___0->ready = 0;
  } else {
  }
  i = i + 1;
  ldv_49954: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_49953;
  } else {
  }
  return (0);
}
}
static int gfx_v8_0_cp_resume(struct amdgpu_device *adev )
{
  int r ;
  {
  if ((unsigned int )adev->asic_type != 7U) {
    gfx_v8_0_enable_gui_idle_interrupt(adev, 0);
  } else {
  }
  if (! adev->firmware.smu_load) {
    r = gfx_v8_0_cp_gfx_load_microcode(adev);
    if (r != 0) {
      return (r);
    } else {
    }
    r = gfx_v8_0_cp_compute_load_microcode(adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 2U);
    if (r != 0) {
      return (-22);
    } else {
    }
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 3U);
    if (r != 0) {
      return (-22);
    } else {
    }
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 4U);
    if (r != 0) {
      return (-22);
    } else {
    }
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 5U);
    if (r != 0) {
      return (-22);
    } else {
    }
  }
  r = gfx_v8_0_cp_gfx_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v8_0_cp_compute_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  gfx_v8_0_enable_gui_idle_interrupt(adev, 1);
  return (0);
}
}
static void gfx_v8_0_cp_enable(struct amdgpu_device *adev , bool enable )
{
  {
  gfx_v8_0_cp_gfx_enable(adev, (int )enable);
  gfx_v8_0_cp_compute_enable(adev, (int )enable);
  return;
}
}
static int gfx_v8_0_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gfx_v8_0_init_golden_registers(adev);
  gfx_v8_0_gpu_init(adev);
  r = gfx_v8_0_rlc_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = gfx_v8_0_cp_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int gfx_v8_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  gfx_v8_0_cp_enable(adev, 0);
  gfx_v8_0_rlc_stop(adev);
  gfx_v8_0_cp_compute_fini(adev);
  return (0);
}
}
static int gfx_v8_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = gfx_v8_0_hw_fini((void *)adev);
  return (tmp);
}
}
static int gfx_v8_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = gfx_v8_0_hw_init((void *)adev);
  return (tmp);
}
}
static bool gfx_v8_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((int )tmp < 0) {
    return (0);
  } else {
    return (1);
  }
}
}
static int gfx_v8_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_49992;
  ldv_49991:
  tmp___0 = amdgpu_mm_rreg(adev, 8196U, 0);
  tmp = tmp___0 & 2147483648U;
  if ((int )tmp >= 0) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_49992: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_49991;
  } else {
  }
  return (-110);
}
}
static void gfx_v8_0_print_status(void *handle )
{
  int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  u32 tmp___46 ;
  u32 tmp___47 ;
  u32 tmp___48 ;
  u32 tmp___49 ;
  u32 tmp___50 ;
  u32 tmp___51 ;
  u32 tmp___52 ;
  u32 tmp___53 ;
  u32 tmp___54 ;
  u32 tmp___55 ;
  u32 tmp___56 ;
  u32 tmp___57 ;
  u32 tmp___58 ;
  u32 tmp___59 ;
  u32 tmp___60 ;
  u32 tmp___61 ;
  u32 tmp___62 ;
  u32 tmp___63 ;
  u32 tmp___64 ;
  u32 tmp___65 ;
  u32 tmp___66 ;
  u32 tmp___67 ;
  u32 tmp___68 ;
  u32 tmp___69 ;
  u32 tmp___70 ;
  u32 tmp___71 ;
  u32 tmp___72 ;
  u32 tmp___73 ;
  u32 tmp___74 ;
  u32 tmp___75 ;
  u32 tmp___76 ;
  u32 tmp___77 ;
  u32 tmp___78 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "GFX 8.x registers\n");
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 8194U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS2=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 8197U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE0=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 8198U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE1=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 8206U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE2=0x%08X\n", tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 8207U, 0);
  _dev_info((struct device const *)adev->dev, "  GRBM_STATUS_SE3=0x%08X\n", tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 8608U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STAT = 0x%08x\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 8605U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT1 = 0x%08x\n", tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 8606U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT2 = 0x%08x\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 8604U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_STALLED_STAT3 = 0x%08x\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 8328U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_BUSY_STAT = 0x%08x\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 8329U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STALLED_STAT1 = 0x%08x\n",
            tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 8327U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_STATUS = 0x%08x\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 8325U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_BUSY_STAT = 0x%08x\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 8326U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STALLED_STAT1 = 0x%08x\n",
            tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 8324U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPC_STATUS = 0x%08x\n", tmp___14);
  i = 0;
  goto ldv_50000;
  ldv_49999:
  tmp___15 = amdgpu_mm_rreg(adev, (u32 )((i + 2449) * 4), 0);
  _dev_info((struct device const *)adev->dev, "  GB_TILE_MODE%d=0x%08X\n", i, tmp___15);
  i = i + 1;
  ldv_50000: ;
  if (i <= 31) {
    goto ldv_49999;
  } else {
  }
  i = 0;
  goto ldv_50003;
  ldv_50002:
  tmp___16 = amdgpu_mm_rreg(adev, (u32 )((i + 2457) * 4), 0);
  _dev_info((struct device const *)adev->dev, "  GB_MACROTILE_MODE%d=0x%08X\n",
            i, tmp___16);
  i = i + 1;
  ldv_50003: ;
  if (i <= 15) {
    goto ldv_50002;
  } else {
  }
  i = 0;
  goto ldv_50006;
  ldv_50005:
  _dev_info((struct device const *)adev->dev, "  se: %d\n", i);
  gfx_v8_0_select_se_sh(adev, (u32 )i, 4294967295U);
  tmp___17 = amdgpu_mm_rreg(adev, 41172U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_RASTER_CONFIG=0x%08X\n",
            tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 41173U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_RASTER_CONFIG_1=0x%08X\n",
            tmp___18);
  i = i + 1;
  ldv_50006: ;
  if ((unsigned int )i < adev->gfx.config.max_shader_engines) {
    goto ldv_50005;
  } else {
  }
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  tmp___19 = amdgpu_mm_rreg(adev, 9790U, 0);
  _dev_info((struct device const *)adev->dev, "  GB_ADDR_CONFIG=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 3026U, 0);
  _dev_info((struct device const *)adev->dev, "  HDP_ADDR_CONFIG=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 771U, 0);
  _dev_info((struct device const *)adev->dev, "  DMIF_ADDR_CALC=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 13318U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA0_TILING_CONFIG=0x%08X\n",
            tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 13830U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA1_TILING_CONFIG=0x%08X\n",
            tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 15315U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_ADDR_CONFIG=0x%08X\n",
            tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 15316U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DB_ADDR_CONFIG=0x%08X\n",
            tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 15317U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DBW_ADDR_CONFIG=0x%08X\n",
            tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 8665U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MEQ_THRESHOLDS=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 9240U, 0);
  _dev_info((struct device const *)adev->dev, "  SX_DEBUG_1=0x%08X\n", tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 9538U, 0);
  _dev_info((struct device const *)adev->dev, "  TA_CNTL_AUX=0x%08X\n", tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 9280U, 0);
  _dev_info((struct device const *)adev->dev, "  SPI_CONFIG_CNTL=0x%08X\n", tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 8960U, 0);
  _dev_info((struct device const *)adev->dev, "  SQ_CONFIG=0x%08X\n", tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 9740U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG=0x%08X\n", tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 9741U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG2=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 9742U, 0);
  _dev_info((struct device const *)adev->dev, "  DB_DEBUG3=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 9860U, 0);
  _dev_info((struct device const *)adev->dev, "  CB_HW_CONTROL=0x%08X\n", tmp___35);
  tmp___36 = amdgpu_mm_rreg(adev, 9295U, 0);
  _dev_info((struct device const *)adev->dev, "  SPI_CONFIG_CNTL_1=0x%08X\n", tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, 8947U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_FIFO_SIZE=0x%08X\n", tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, 49741U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_NUM_INSTANCES=0x%08X\n", tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, 55304U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_PERFMON_CNTL=0x%08X\n", tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, 8905U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_FORCE_EOV_MAX_CNTS=0x%08X\n",
            tmp___40);
  tmp___41 = amdgpu_mm_rreg(adev, 8753U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_CACHE_INVALIDATION=0x%08X\n",
            tmp___41);
  tmp___42 = amdgpu_mm_rreg(adev, 8757U, 0);
  _dev_info((struct device const *)adev->dev, "  VGT_GS_VERTEX_REUSE=0x%08X\n",
            tmp___42);
  tmp___43 = amdgpu_mm_rreg(adev, 49793U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_LINE_STIPPLE_STATE=0x%08X\n",
            tmp___43);
  tmp___44 = amdgpu_mm_rreg(adev, 8837U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_CL_ENHANCE=0x%08X\n", tmp___44);
  tmp___45 = amdgpu_mm_rreg(adev, 8956U, 0);
  _dev_info((struct device const *)adev->dev, "  PA_SC_ENHANCE=0x%08X\n", tmp___45);
  tmp___46 = amdgpu_mm_rreg(adev, 8630U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_ME_CNTL=0x%08X\n", tmp___46);
  tmp___47 = amdgpu_mm_rreg(adev, 12462U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MAX_CONTEXT=0x%08X\n", tmp___47);
  tmp___48 = amdgpu_mm_rreg(adev, 12368U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_ENDIAN_SWAP=0x%08X\n", tmp___48);
  tmp___49 = amdgpu_mm_rreg(adev, 12363U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_DEVICE_ID=0x%08X\n", tmp___49);
  tmp___50 = amdgpu_mm_rreg(adev, 49263U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_SEM_WAIT_TIMER=0x%08X\n", tmp___50);
  tmp___51 = amdgpu_mm_rreg(adev, 8641U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB_WPTR_DELAY=0x%08X\n", tmp___51);
  tmp___52 = amdgpu_mm_rreg(adev, 12369U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB_VMID=0x%08X\n", tmp___52);
  tmp___53 = amdgpu_mm_rreg(adev, 12353U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_CNTL=0x%08X\n", tmp___53);
  tmp___54 = amdgpu_mm_rreg(adev, 12357U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_WPTR=0x%08X\n", tmp___54);
  tmp___55 = amdgpu_mm_rreg(adev, 12355U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_RPTR_ADDR=0x%08X\n", tmp___55);
  tmp___56 = amdgpu_mm_rreg(adev, 12356U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_RPTR_ADDR_HI=0x%08X\n",
            tmp___56);
  tmp___57 = amdgpu_mm_rreg(adev, 12353U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_CNTL=0x%08X\n", tmp___57);
  tmp___58 = amdgpu_mm_rreg(adev, 12352U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_BASE=0x%08X\n", tmp___58);
  tmp___59 = amdgpu_mm_rreg(adev, 12465U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_RB0_BASE_HI=0x%08X\n", tmp___59);
  tmp___60 = amdgpu_mm_rreg(adev, 8333U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_MEC_CNTL=0x%08X\n", tmp___60);
  tmp___61 = amdgpu_mm_rreg(adev, 12416U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_CPF_DEBUG=0x%08X\n", tmp___61);
  tmp___62 = amdgpu_mm_rreg(adev, 49233U, 0);
  _dev_info((struct device const *)adev->dev, "  SCRATCH_ADDR=0x%08X\n", tmp___62);
  tmp___63 = amdgpu_mm_rreg(adev, 49232U, 0);
  _dev_info((struct device const *)adev->dev, "  SCRATCH_UMSK=0x%08X\n", tmp___63);
  tmp___64 = amdgpu_mm_rreg(adev, 12394U, 0);
  _dev_info((struct device const *)adev->dev, "  CP_INT_CNTL_RING0=0x%08X\n", tmp___64);
  tmp___65 = amdgpu_mm_rreg(adev, 60441U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTL=0x%08X\n", tmp___65);
  tmp___66 = amdgpu_mm_rreg(adev, 60416U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_CNTL=0x%08X\n", tmp___66);
  tmp___67 = amdgpu_mm_rreg(adev, 60489U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_CGCG_CGLS_CTRL=0x%08X\n", tmp___67);
  tmp___68 = amdgpu_mm_rreg(adev, 60443U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTR_INIT=0x%08X\n", tmp___68);
  tmp___69 = amdgpu_mm_rreg(adev, 60434U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTR_MAX=0x%08X\n", tmp___69);
  tmp___70 = amdgpu_mm_rreg(adev, 60495U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_INIT_CU_MASK=0x%08X\n",
            tmp___70);
  tmp___71 = amdgpu_mm_rreg(adev, 60497U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_PARAMS=0x%08X\n", tmp___71);
  tmp___72 = amdgpu_mm_rreg(adev, 60441U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_LB_CNTL=0x%08X\n", tmp___72);
  tmp___73 = amdgpu_mm_rreg(adev, 60419U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_MC_CNTL=0x%08X\n", tmp___73);
  tmp___74 = amdgpu_mm_rreg(adev, 60455U, 0);
  _dev_info((struct device const *)adev->dev, "  RLC_UCODE_CNTL=0x%08X\n", tmp___74);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  i = 0;
  goto ldv_50009;
  ldv_50008:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )i);
  _dev_info((struct device const *)adev->dev, "  VM %d:\n", i);
  tmp___75 = amdgpu_mm_rreg(adev, 8973U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_CONFIG=0x%08X\n", tmp___75);
  tmp___76 = amdgpu_mm_rreg(adev, 8971U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_APE1_BASE=0x%08X\n", tmp___76);
  tmp___77 = amdgpu_mm_rreg(adev, 8972U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_APE1_LIMIT=0x%08X\n", tmp___77);
  tmp___78 = amdgpu_mm_rreg(adev, 8970U, 0);
  _dev_info((struct device const *)adev->dev, "  SH_MEM_BASES=0x%08X\n", tmp___78);
  i = i + 1;
  ldv_50009: ;
  if (i <= 15) {
    goto ldv_50008;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  return;
}
}
static int gfx_v8_0_soft_reset(void *handle )
{
  u32 grbm_soft_reset ;
  u32 srbm_soft_reset ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  {
  grbm_soft_reset = 0U;
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 8196U, 0);
  if ((tmp & 1205780480U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    grbm_soft_reset = grbm_soft_reset | 65536U;
  } else {
  }
  if ((tmp & 805306368U) != 0U) {
    grbm_soft_reset = grbm_soft_reset | 1U;
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 8194U, 0);
  if ((tmp & 16777216U) >> 24 != 0U) {
    grbm_soft_reset = grbm_soft_reset | 4U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 32U) >> 5 != 0U) {
    srbm_soft_reset = srbm_soft_reset | 256U;
  } else {
  }
  if (grbm_soft_reset != 0U || srbm_soft_reset != 0U) {
    gfx_v8_0_print_status((void *)adev);
    gfx_v8_0_rlc_stop(adev);
    gfx_v8_0_cp_gfx_enable(adev, 0);
    if (grbm_soft_reset != 0U) {
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
      tmp = tmp | grbm_soft_reset;
      _dev_info((struct device const *)adev->dev, "GRBM_SOFT_RESET=0x%08X\n", tmp);
      amdgpu_mm_wreg(adev, 8200U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
      __const_udelay(214750UL);
      tmp = ~ grbm_soft_reset & tmp;
      amdgpu_mm_wreg(adev, 8200U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 8200U, 0);
    } else {
    }
    if (srbm_soft_reset != 0U) {
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
      tmp = tmp | srbm_soft_reset;
      _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
      amdgpu_mm_wreg(adev, 920U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
      __const_udelay(214750UL);
      tmp = ~ srbm_soft_reset & tmp;
      amdgpu_mm_wreg(adev, 920U, tmp, 0);
      tmp = amdgpu_mm_rreg(adev, 920U, 0);
    } else {
    }
    __const_udelay(214750UL);
    gfx_v8_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
uint64_t gfx_v8_0_get_gpu_clock_counter(struct amdgpu_device *adev )
{
  uint64_t clock ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  mutex_lock_nested(& adev->gfx.gpu_clock_mutex, 0U);
  amdgpu_mm_wreg(adev, 60454U, 1U, 0);
  tmp = amdgpu_mm_rreg(adev, 60452U, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 60453U, 0);
  clock = (unsigned long long )tmp | ((unsigned long long )tmp___0 << 32);
  mutex_unlock(& adev->gfx.gpu_clock_mutex);
  return (clock);
}
}
static void gfx_v8_0_ring_emit_gds_switch(struct amdgpu_ring *ring , u32 vmid , u32 gds_base ,
                                          u32 gds_size , u32 gws_base , u32 gws_size ,
                                          u32 oa_base , u32 oa_size )
{
  {
  gds_base = gds_base >> 2;
  gds_size = gds_size >> 2;
  gws_base = gws_base >> 12;
  gws_size = gws_size >> 12;
  oa_base = oa_base >> 12;
  oa_size = oa_size >> 12;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset___0[vmid].mem_base);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, gds_base);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset___0[vmid].mem_size);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, gds_size);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset___0[vmid].gws);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (gws_size << 16) | gws_base);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, amdgpu_gds_reg_offset___0[vmid].oa);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )((1 << (int )(oa_size + oa_base)) - (1 << (int )oa_base)));
  return;
}
}
static int gfx_v8_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  adev->gfx.num_gfx_rings = 1U;
  adev->gfx.num_compute_rings = 8U;
  gfx_v8_0_set_ring_funcs(adev);
  gfx_v8_0_set_irq_funcs(adev);
  gfx_v8_0_set_gds_init(adev);
  return (0);
}
}
static int gfx_v8_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
static int gfx_v8_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static u32 gfx_v8_0_ring_get_rptr_gfx(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs);
  return (rptr);
}
}
static u32 gfx_v8_0_ring_get_wptr_gfx(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 wptr ;
  {
  adev = ring->adev;
  if ((int )ring->use_doorbell) {
    wptr = *((ring->adev)->wb.wb + (unsigned long )ring->wptr_offs);
  } else {
    wptr = amdgpu_mm_rreg(adev, 12357U, 0);
  }
  return (wptr);
}
}
static void gfx_v8_0_ring_set_wptr_gfx(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  if ((int )ring->use_doorbell) {
    *(adev->wb.wb + (unsigned long )ring->wptr_offs) = ring->wptr;
    amdgpu_mm_wdoorbell(adev, ring->doorbell_index, ring->wptr);
  } else {
    amdgpu_mm_wreg(adev, 12357U, ring->wptr, 0);
    amdgpu_mm_rreg(adev, 12357U, 0);
  }
  return;
}
}
static void gfx_v8_0_ring_emit_hdp_flush(struct amdgpu_ring *ring )
{
  u32 ref_and_mask ;
  u32 reg_mem_engine ;
  {
  if ((unsigned int )ring->type == 1U) {
    switch (ring->me) {
    case 1U:
    ref_and_mask = (u32 )(4 << (int )ring->pipe);
    goto ldv_50063;
    case 2U:
    ref_and_mask = (u32 )(64 << (int )ring->pipe);
    goto ldv_50063;
    default: ;
    return;
    }
    ldv_50063:
    reg_mem_engine = 0U;
  } else {
    ref_and_mask = 1U;
    reg_mem_engine = 256U;
  }
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, reg_mem_engine | 67U);
  amdgpu_ring_write(ring, 5431U);
  amdgpu_ring_write(ring, 5432U);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, 32U);
  return;
}
}
static void gfx_v8_0_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  bool need_ctx_switch ;
  u32 header ;
  u32 control ;
  u32 next_rptr ;
  {
  need_ctx_switch = (unsigned long )ring->current_ctx != (unsigned long )ib->ctx;
  control = 0U;
  next_rptr = ring->wptr + 5U;
  if (((unsigned int )ring->type == 0U && (ib->flags & 2U) != 0U) && ! need_ctx_switch) {
    return;
  } else {
  }
  if ((unsigned int )ring->type == 1U) {
    control = control | 8388608U;
  } else {
  }
  if ((int )need_ctx_switch && (unsigned int )ring->type == 0U) {
    next_rptr = next_rptr + 2U;
  } else {
  }
  next_rptr = next_rptr + 4U;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 1049856U);
  amdgpu_ring_write(ring, (u32 )ring->next_rptr_gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ring->next_rptr_gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, next_rptr);
  if ((int )need_ctx_switch && (unsigned int )ring->type == 0U) {
    amdgpu_ring_write(ring, 3221261056U);
    amdgpu_ring_write(ring, 0U);
  } else {
  }
  if ((int )ib->flags & 1) {
    header = 3221369600U;
  } else {
    header = 3221372672U;
  }
  control = (ib->length_dw | ((unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0) ? (ib->vm)->ids[ring->idx].id << 24 : 0U)) | control;
  amdgpu_ring_write(ring, header);
  amdgpu_ring_write(ring, (u32 )ib->gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL) & 65535U);
  amdgpu_ring_write(ring, control);
  return;
}
}
static void gfx_v8_0_ring_emit_fence_gfx(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                         unsigned int flags )
{
  bool write64bit ;
  bool int_sel ;
  {
  write64bit = (flags & 1U) != 0U;
  int_sel = (flags & 2U) != 0U;
  amdgpu_ring_write(ring, 3221505792U);
  amdgpu_ring_write(ring, 197908U);
  amdgpu_ring_write(ring, (u32 )addr & 4294967292U);
  amdgpu_ring_write(ring, (((unsigned int )(addr >> 32ULL) & 65535U) | ((int )write64bit ? 1073741824U : 536870912U)) | ((int )int_sel ? 33554432U : 0U));
  amdgpu_ring_write(ring, (unsigned int )seq);
  amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  return;
}
}
static bool gfx_v8_0_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  uint64_t addr ;
  unsigned int sel ;
  {
  addr = semaphore->gpu_addr;
  sel = (int )emit_wait ? 3758096384U : 3221225472U;
  if ((unsigned int )(ring->adev)->asic_type == 5U || (unsigned int )(ring->adev)->asic_type == 6U) {
    return (0);
  } else {
    amdgpu_ring_write(ring, 3221371136U);
    amdgpu_ring_write(ring, (unsigned int )addr);
    amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
    amdgpu_ring_write(ring, sel);
  }
  if ((int )emit_wait && (unsigned int )ring->type == 0U) {
    amdgpu_ring_write(ring, 3221242368U);
    amdgpu_ring_write(ring, 0U);
  } else {
  }
  return (1);
}
}
static void gfx_v8_0_ce_sync_me(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(adev->gfx.ce_sync_offs * 4U);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 1280U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, 531U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 4294967295U);
  amdgpu_ring_write(ring, 4U);
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 2148533504U);
  amdgpu_ring_write(ring, (u32 )gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static void gfx_v8_0_ring_emit_vm_flush(struct amdgpu_ring *ring , unsigned int vm_id ,
                                        uint64_t pd_addr )
{
  int usepfp ;
  {
  usepfp = (unsigned int )ring->type == 0U;
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, (u32 )(usepfp << 30));
  if (vm_id <= 7U) {
    amdgpu_ring_write(ring, vm_id + 1359U);
  } else {
    amdgpu_ring_write(ring, vm_id + 1286U);
  }
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )(pd_addr >> 12));
  amdgpu_ring_write(ring, 3221436160U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (u32 )(1 << (int )vm_id));
  amdgpu_ring_write(ring, 3221568512U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 32U);
  if (usepfp != 0) {
    amdgpu_ring_write(ring, 3221242368U);
    amdgpu_ring_write(ring, 0U);
    gfx_v8_0_ce_sync_me(ring);
  } else {
  }
  return;
}
}
static bool gfx_v8_0_ring_is_lockup(struct amdgpu_ring *ring )
{
  bool tmp ;
  bool tmp___0 ;
  {
  tmp = gfx_v8_0_is_idle((void *)ring->adev);
  if ((int )tmp) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___0 = amdgpu_ring_test_lockup(ring);
  return (tmp___0);
}
}
static u32 gfx_v8_0_ring_get_rptr_compute(struct amdgpu_ring *ring )
{
  {
  return ((u32 )*((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs));
}
}
static u32 gfx_v8_0_ring_get_wptr_compute(struct amdgpu_ring *ring )
{
  {
  return ((u32 )*((ring->adev)->wb.wb + (unsigned long )ring->wptr_offs));
}
}
static void gfx_v8_0_ring_set_wptr_compute(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  *(adev->wb.wb + (unsigned long )ring->wptr_offs) = ring->wptr;
  amdgpu_mm_wdoorbell(adev, ring->doorbell_index, ring->wptr);
  return;
}
}
static void gfx_v8_0_ring_emit_fence_compute(struct amdgpu_ring *ring , u64 addr ,
                                             u64 seq , unsigned int flags )
{
  bool write64bit ;
  bool int_sel ;
  {
  write64bit = (flags & 1U) != 0U;
  int_sel = (flags & 2U) != 0U;
  amdgpu_ring_write(ring, 3221571840U);
  amdgpu_ring_write(ring, 197908U);
  amdgpu_ring_write(ring, (u32 )(((int )write64bit ? 1073741824 : 536870912) | ((int )int_sel ? 33554432 : 0)));
  amdgpu_ring_write(ring, (u32 )addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )seq);
  amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  return;
}
}
static void gfx_v8_0_set_gfx_eop_interrupt_state(struct amdgpu_device *adev , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4227858431U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50127;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl | 67108864U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50127;
  default: ;
  goto ldv_50127;
  }
  ldv_50127: ;
  return;
}
}
static void gfx_v8_0_set_compute_eop_interrupt_state(struct amdgpu_device *adev ,
                                                     int me , int pipe , enum amdgpu_interrupt_state state )
{
  u32 mec_int_cntl ;
  u32 mec_int_cntl_reg ;
  long tmp ;
  long tmp___0 ;
  {
  if (me == 1) {
    switch (pipe) {
    case 0:
    mec_int_cntl_reg = 12421U;
    goto ldv_50139;
    default:
    tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp != 0L) {
      drm_ut_debug_printk("gfx_v8_0_set_compute_eop_interrupt_state", "invalid pipe %d\n",
                          pipe);
    } else {
    }
    return;
    }
    ldv_50139: ;
  } else {
    tmp___0 = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("gfx_v8_0_set_compute_eop_interrupt_state", "invalid me %d\n",
                          me);
    } else {
    }
    return;
  }
  switch ((unsigned int )state) {
  case 0U:
  mec_int_cntl = amdgpu_mm_rreg(adev, mec_int_cntl_reg, 0);
  mec_int_cntl = mec_int_cntl & 4227858431U;
  amdgpu_mm_wreg(adev, mec_int_cntl_reg, mec_int_cntl, 0);
  goto ldv_50143;
  case 1U:
  mec_int_cntl = amdgpu_mm_rreg(adev, mec_int_cntl_reg, 0);
  mec_int_cntl = mec_int_cntl | 67108864U;
  amdgpu_mm_wreg(adev, mec_int_cntl_reg, mec_int_cntl, 0);
  goto ldv_50143;
  default: ;
  goto ldv_50143;
  }
  ldv_50143: ;
  return;
}
}
static int gfx_v8_0_set_priv_reg_fault_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                             unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4286578687U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50154;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4286578687U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50154;
  default: ;
  goto ldv_50154;
  }
  ldv_50154: ;
  return (0);
}
}
static int gfx_v8_0_set_priv_inst_fault_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                              unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 cp_int_cntl ;
  {
  switch ((unsigned int )state) {
  case 0U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl & 4290772991U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50165;
  case 1U:
  cp_int_cntl = amdgpu_mm_rreg(adev, 12394U, 0);
  cp_int_cntl = cp_int_cntl | 4194304U;
  amdgpu_mm_wreg(adev, 12394U, cp_int_cntl, 0);
  goto ldv_50165;
  default: ;
  goto ldv_50165;
  }
  ldv_50165: ;
  return (0);
}
}
static int gfx_v8_0_set_eop_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                            unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  switch (type) {
  case 0U:
  gfx_v8_0_set_gfx_eop_interrupt_state(adev, state);
  goto ldv_50175;
  case 1U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 1, 0, state);
  goto ldv_50175;
  case 2U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 1, 1, state);
  goto ldv_50175;
  case 3U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 1, 2, state);
  goto ldv_50175;
  case 4U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 1, 3, state);
  goto ldv_50175;
  case 5U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 2, 0, state);
  goto ldv_50175;
  case 6U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 2, 1, state);
  goto ldv_50175;
  case 7U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 2, 2, state);
  goto ldv_50175;
  case 8U:
  gfx_v8_0_set_compute_eop_interrupt_state(adev, 2, 3, state);
  goto ldv_50175;
  default: ;
  goto ldv_50175;
  }
  ldv_50175: ;
  return (0);
}
}
static int gfx_v8_0_eop_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                            struct amdgpu_iv_entry *entry )
{
  int i ;
  u8 me_id ;
  u8 pipe_id ;
  u8 queue_id ;
  struct amdgpu_ring *ring ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("gfx_v8_0_eop_irq", "IH: CP EOP\n");
  } else {
  }
  me_id = (u8 )((entry->ring_id & 12U) >> 2);
  pipe_id = (unsigned int )((u8 )entry->ring_id) & 3U;
  queue_id = (u8 )((entry->ring_id & 112U) >> 4);
  switch ((int )me_id) {
  case 0:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->gfx.gfx_ring));
  goto ldv_50197;
  case 1: ;
  case 2:
  i = 0;
  goto ldv_50201;
  ldv_50200:
  ring = (struct amdgpu_ring *)(& adev->gfx.compute_ring) + (unsigned long )i;
  if ((ring->me == (u32 )me_id && ring->pipe == (u32 )pipe_id) && ring->queue == (u32 )queue_id) {
    amdgpu_fence_process(ring);
  } else {
  }
  i = i + 1;
  ldv_50201: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_50200;
  } else {
  }
  goto ldv_50197;
  }
  ldv_50197: ;
  return (0);
}
}
static int gfx_v8_0_priv_reg_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                 struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal register access in command stream\n");
  schedule_work___6(& adev->reset_work);
  return (0);
}
}
static int gfx_v8_0_priv_inst_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                  struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal instruction in command stream\n");
  schedule_work___6(& adev->reset_work);
  return (0);
}
}
struct amd_ip_funcs const gfx_v8_0_ip_funcs =
     {& gfx_v8_0_early_init, (int (*)(void * ))0, & gfx_v8_0_sw_init, & gfx_v8_0_sw_fini,
    & gfx_v8_0_hw_init, & gfx_v8_0_hw_fini, & gfx_v8_0_suspend, & gfx_v8_0_resume,
    & gfx_v8_0_is_idle, & gfx_v8_0_wait_for_idle, & gfx_v8_0_soft_reset, & gfx_v8_0_print_status,
    & gfx_v8_0_set_clockgating_state, & gfx_v8_0_set_powergating_state};
static struct amdgpu_ring_funcs const gfx_v8_0_ring_funcs_gfx =
     {& gfx_v8_0_ring_get_rptr_gfx, & gfx_v8_0_ring_get_wptr_gfx, & gfx_v8_0_ring_set_wptr_gfx,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & gfx_v8_0_ring_emit_ib, & gfx_v8_0_ring_emit_fence_gfx,
    & gfx_v8_0_ring_emit_semaphore, & gfx_v8_0_ring_emit_vm_flush, & gfx_v8_0_ring_emit_hdp_flush,
    & gfx_v8_0_ring_emit_gds_switch, & gfx_v8_0_ring_test_ring, & gfx_v8_0_ring_test_ib,
    & gfx_v8_0_ring_is_lockup};
static struct amdgpu_ring_funcs const gfx_v8_0_ring_funcs_compute =
     {& gfx_v8_0_ring_get_rptr_compute, & gfx_v8_0_ring_get_wptr_compute, & gfx_v8_0_ring_set_wptr_compute,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & gfx_v8_0_ring_emit_ib, & gfx_v8_0_ring_emit_fence_compute,
    & gfx_v8_0_ring_emit_semaphore, & gfx_v8_0_ring_emit_vm_flush, & gfx_v8_0_ring_emit_hdp_flush,
    & gfx_v8_0_ring_emit_gds_switch, & gfx_v8_0_ring_test_ring, & gfx_v8_0_ring_test_ib,
    & gfx_v8_0_ring_is_lockup};
static void gfx_v8_0_set_ring_funcs(struct amdgpu_device *adev )
{
  int i ;
  {
  i = 0;
  goto ldv_50221;
  ldv_50220:
  adev->gfx.gfx_ring[i].funcs = & gfx_v8_0_ring_funcs_gfx;
  i = i + 1;
  ldv_50221: ;
  if ((unsigned int )i < adev->gfx.num_gfx_rings) {
    goto ldv_50220;
  } else {
  }
  i = 0;
  goto ldv_50224;
  ldv_50223:
  adev->gfx.compute_ring[i].funcs = & gfx_v8_0_ring_funcs_compute;
  i = i + 1;
  ldv_50224: ;
  if ((unsigned int )i < adev->gfx.num_compute_rings) {
    goto ldv_50223;
  } else {
  }
  return;
}
}
static struct amdgpu_irq_src_funcs const gfx_v8_0_eop_irq_funcs = {& gfx_v8_0_set_eop_interrupt_state, & gfx_v8_0_eop_irq};
static struct amdgpu_irq_src_funcs const gfx_v8_0_priv_reg_irq_funcs = {& gfx_v8_0_set_priv_reg_fault_state, & gfx_v8_0_priv_reg_irq};
static struct amdgpu_irq_src_funcs const gfx_v8_0_priv_inst_irq_funcs = {& gfx_v8_0_set_priv_inst_fault_state, & gfx_v8_0_priv_inst_irq};
static void gfx_v8_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->gfx.eop_irq.num_types = 9U;
  adev->gfx.eop_irq.funcs = & gfx_v8_0_eop_irq_funcs;
  adev->gfx.priv_reg_irq.num_types = 1U;
  adev->gfx.priv_reg_irq.funcs = & gfx_v8_0_priv_reg_irq_funcs;
  adev->gfx.priv_inst_irq.num_types = 1U;
  adev->gfx.priv_inst_irq.funcs = & gfx_v8_0_priv_inst_irq_funcs;
  return;
}
}
static void gfx_v8_0_set_gds_init(struct amdgpu_device *adev )
{
  {
  adev->gds.mem.total_size = amdgpu_mm_rreg(adev, 13057U, 0);
  adev->gds.gws.total_size = 64U;
  adev->gds.oa.total_size = 16U;
  if (adev->gds.mem.total_size == 65536U) {
    adev->gds.mem.gfx_partition_size = 4096U;
    adev->gds.mem.cs_partition_size = 4096U;
    adev->gds.gws.gfx_partition_size = 4U;
    adev->gds.gws.cs_partition_size = 4U;
    adev->gds.oa.gfx_partition_size = 4U;
    adev->gds.oa.cs_partition_size = 1U;
  } else {
    adev->gds.mem.gfx_partition_size = 1024U;
    adev->gds.mem.cs_partition_size = 1024U;
    adev->gds.gws.gfx_partition_size = 16U;
    adev->gds.gws.cs_partition_size = 16U;
    adev->gds.oa.gfx_partition_size = 4U;
    adev->gds.oa.cs_partition_size = 4U;
  }
  return;
}
}
static u32 gfx_v8_0_get_cu_active_bitmap(struct amdgpu_device *adev , u32 se , u32 sh )
{
  u32 mask ;
  u32 tmp ;
  u32 tmp1 ;
  int i ;
  {
  mask = 0U;
  gfx_v8_0_select_se_sh(adev, se, sh);
  tmp = amdgpu_mm_rreg(adev, 8815U, 0);
  tmp1 = amdgpu_mm_rreg(adev, 8816U, 0);
  gfx_v8_0_select_se_sh(adev, 4294967295U, 4294967295U);
  tmp = tmp & 4294901760U;
  tmp = tmp | tmp1;
  tmp = tmp >> 16;
  i = 0;
  goto ldv_50245;
  ldv_50244:
  mask = mask << 1;
  mask = mask | 1U;
  i = i + 1;
  ldv_50245: ;
  if ((unsigned int )i < adev->gfx.config.max_cu_per_sh) {
    goto ldv_50244;
  } else {
  }
  return (~ tmp & mask);
}
}
int gfx_v8_0_get_cu_info(struct amdgpu_device *adev , struct amdgpu_cu_info *cu_info )
{
  int i ;
  int j ;
  int k ;
  int counter ;
  int active_cu_number ;
  u32 mask ;
  u32 bitmap ;
  u32 ao_bitmap ;
  u32 ao_cu_mask ;
  {
  active_cu_number = 0;
  ao_cu_mask = 0U;
  if ((unsigned long )adev == (unsigned long )((struct amdgpu_device *)0) || (unsigned long )cu_info == (unsigned long )((struct amdgpu_cu_info *)0)) {
    return (-22);
  } else {
  }
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  i = 0;
  goto ldv_50267;
  ldv_50266:
  j = 0;
  goto ldv_50264;
  ldv_50263:
  mask = 1U;
  ao_bitmap = 0U;
  counter = 0;
  bitmap = gfx_v8_0_get_cu_active_bitmap(adev, (u32 )i, (u32 )j);
  cu_info->bitmap[i][j] = bitmap;
  k = 0;
  goto ldv_50261;
  ldv_50260: ;
  if ((bitmap & mask) != 0U) {
    if (counter <= 1) {
      ao_bitmap = ao_bitmap | mask;
    } else {
    }
    counter = counter + 1;
  } else {
  }
  mask = mask << 1;
  k = k + 1;
  ldv_50261: ;
  if ((unsigned int )k < adev->gfx.config.max_cu_per_sh) {
    goto ldv_50260;
  } else {
  }
  active_cu_number = active_cu_number + counter;
  ao_cu_mask = (ao_bitmap << (i * 2 + j) * 8) | ao_cu_mask;
  j = j + 1;
  ldv_50264: ;
  if ((unsigned int )j < adev->gfx.config.max_sh_per_se) {
    goto ldv_50263;
  } else {
  }
  i = i + 1;
  ldv_50267: ;
  if ((unsigned int )i < adev->gfx.config.max_shader_engines) {
    goto ldv_50266;
  } else {
  }
  cu_info->number = (u32 )active_cu_number;
  cu_info->ao_cu_mask = ao_cu_mask;
  mutex_unlock(& adev->grbm_idx_mutex);
  return (0);
}
}
int ldv_retval_61 ;
extern int ldv_release_39(void) ;
extern int ldv_probe_39(void) ;
int ldv_retval_62 ;
void ldv_initialize_amdgpu_irq_src_funcs_36(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v8_0_eop_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v8_0_eop_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_35(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v8_0_priv_reg_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v8_0_priv_reg_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_38(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  gfx_v8_0_ring_funcs_gfx_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_37(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  gfx_v8_0_ring_funcs_compute_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_34(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  gfx_v8_0_priv_inst_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  gfx_v8_0_priv_inst_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_38(void)
{
  u32 ldvarg1092 ;
  u32 ldvarg1090 ;
  u32 ldvarg1089 ;
  struct amdgpu_semaphore *ldvarg1088 ;
  void *tmp ;
  u32 ldvarg1091 ;
  uint64_t ldvarg1097 ;
  bool ldvarg1087 ;
  u32 ldvarg1093 ;
  struct amdgpu_ib *ldvarg1096 ;
  void *tmp___0 ;
  unsigned int ldvarg1099 ;
  unsigned int ldvarg1086 ;
  u32 ldvarg1094 ;
  uint64_t ldvarg1098 ;
  uint64_t ldvarg1085 ;
  u32 ldvarg1095 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg1088 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg1096 = (struct amdgpu_ib *)tmp___0;
  ldv_memset((void *)(& ldvarg1092), 0, 4UL);
  ldv_memset((void *)(& ldvarg1090), 0, 4UL);
  ldv_memset((void *)(& ldvarg1089), 0, 4UL);
  ldv_memset((void *)(& ldvarg1091), 0, 4UL);
  ldv_memset((void *)(& ldvarg1097), 0, 8UL);
  ldv_memset((void *)(& ldvarg1087), 0, 1UL);
  ldv_memset((void *)(& ldvarg1093), 0, 4UL);
  ldv_memset((void *)(& ldvarg1099), 0, 4UL);
  ldv_memset((void *)(& ldvarg1086), 0, 4UL);
  ldv_memset((void *)(& ldvarg1094), 0, 4UL);
  ldv_memset((void *)(& ldvarg1098), 0, 8UL);
  ldv_memset((void *)(& ldvarg1085), 0, 8UL);
  ldv_memset((void *)(& ldvarg1095), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_fence_gfx(gfx_v8_0_ring_funcs_gfx_group0, ldvarg1098, ldvarg1097,
                                 ldvarg1099);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 1: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_get_rptr_gfx(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 2: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_test_ring(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 3: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_hdp_flush(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 4: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_set_wptr_gfx(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 5: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_get_wptr_gfx(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 6: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_ib(gfx_v8_0_ring_funcs_gfx_group0, ldvarg1096);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 7: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_gds_switch(gfx_v8_0_ring_funcs_gfx_group0, ldvarg1092, ldvarg1090,
                                  ldvarg1094, ldvarg1095, ldvarg1089, ldvarg1091,
                                  ldvarg1093);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 8: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_semaphore(gfx_v8_0_ring_funcs_gfx_group0, ldvarg1088, (int )ldvarg1087);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 9: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_test_ib(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 10: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_is_lockup(gfx_v8_0_ring_funcs_gfx_group0);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  case 11: ;
  if (ldv_state_variable_38 == 1) {
    gfx_v8_0_ring_emit_vm_flush(gfx_v8_0_ring_funcs_gfx_group0, ldvarg1086, ldvarg1085);
    ldv_state_variable_38 = 1;
  } else {
  }
  goto ldv_50309;
  default:
  ldv_stop();
  }
  ldv_50309: ;
  return;
}
}
void ldv_main_exported_35(void)
{
  struct amdgpu_iv_entry *ldvarg119 ;
  void *tmp ;
  unsigned int ldvarg120 ;
  enum amdgpu_interrupt_state ldvarg121 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg119 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg120), 0, 4UL);
  ldv_memset((void *)(& ldvarg121), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_35 == 1) {
    gfx_v8_0_set_priv_reg_fault_state(gfx_v8_0_priv_reg_irq_funcs_group0, gfx_v8_0_priv_reg_irq_funcs_group1,
                                      ldvarg120, ldvarg121);
    ldv_state_variable_35 = 1;
  } else {
  }
  goto ldv_50329;
  case 1: ;
  if (ldv_state_variable_35 == 1) {
    gfx_v8_0_priv_reg_irq(gfx_v8_0_priv_reg_irq_funcs_group0, gfx_v8_0_priv_reg_irq_funcs_group1,
                          ldvarg119);
    ldv_state_variable_35 = 1;
  } else {
  }
  goto ldv_50329;
  default:
  ldv_stop();
  }
  ldv_50329: ;
  return;
}
}
void ldv_main_exported_39(void)
{
  enum amd_clockgating_state ldvarg754 ;
  void *ldvarg759 ;
  void *tmp ;
  void *ldvarg750 ;
  void *tmp___0 ;
  void *ldvarg762 ;
  void *tmp___1 ;
  void *ldvarg764 ;
  void *tmp___2 ;
  void *ldvarg753 ;
  void *tmp___3 ;
  void *ldvarg755 ;
  void *tmp___4 ;
  void *ldvarg752 ;
  void *tmp___5 ;
  void *ldvarg763 ;
  void *tmp___6 ;
  void *ldvarg760 ;
  void *tmp___7 ;
  void *ldvarg756 ;
  void *tmp___8 ;
  void *ldvarg757 ;
  void *tmp___9 ;
  void *ldvarg751 ;
  void *tmp___10 ;
  void *ldvarg761 ;
  void *tmp___11 ;
  enum amd_powergating_state ldvarg758 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg759 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg750 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg762 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg764 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg753 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg755 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg752 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg763 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg760 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg756 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg757 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg751 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg761 = tmp___11;
  ldv_memset((void *)(& ldvarg754), 0, 4UL);
  ldv_memset((void *)(& ldvarg758), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_hw_fini(ldvarg764);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_hw_fini(ldvarg764);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_hw_fini(ldvarg764);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 1: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_print_status(ldvarg763);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_print_status(ldvarg763);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_print_status(ldvarg763);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 2: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_early_init(ldvarg762);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_early_init(ldvarg762);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_early_init(ldvarg762);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 3: ;
  if (ldv_state_variable_39 == 2) {
    ldv_retval_62 = gfx_v8_0_suspend(ldvarg761);
    if (ldv_retval_62 == 0) {
      ldv_state_variable_39 = 3;
    } else {
    }
  } else {
  }
  goto ldv_50351;
  case 4: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_sw_init(ldvarg760);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_sw_init(ldvarg760);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_sw_init(ldvarg760);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 5: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_set_powergating_state(ldvarg759, ldvarg758);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_set_powergating_state(ldvarg759, ldvarg758);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_set_powergating_state(ldvarg759, ldvarg758);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 6: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_wait_for_idle(ldvarg757);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_wait_for_idle(ldvarg757);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_wait_for_idle(ldvarg757);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 7: ;
  if (ldv_state_variable_39 == 3) {
    ldv_retval_61 = gfx_v8_0_resume(ldvarg756);
    if (ldv_retval_61 == 0) {
      ldv_state_variable_39 = 2;
    } else {
    }
  } else {
  }
  goto ldv_50351;
  case 8: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_set_clockgating_state(ldvarg755, ldvarg754);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_set_clockgating_state(ldvarg755, ldvarg754);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_set_clockgating_state(ldvarg755, ldvarg754);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 9: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_hw_init(ldvarg753);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_hw_init(ldvarg753);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_hw_init(ldvarg753);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 10: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_soft_reset(ldvarg752);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_soft_reset(ldvarg752);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_soft_reset(ldvarg752);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 11: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_sw_fini(ldvarg751);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_sw_fini(ldvarg751);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_sw_fini(ldvarg751);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 12: ;
  if (ldv_state_variable_39 == 2) {
    gfx_v8_0_is_idle(ldvarg750);
    ldv_state_variable_39 = 2;
  } else {
  }
  if (ldv_state_variable_39 == 1) {
    gfx_v8_0_is_idle(ldvarg750);
    ldv_state_variable_39 = 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    gfx_v8_0_is_idle(ldvarg750);
    ldv_state_variable_39 = 3;
  } else {
  }
  goto ldv_50351;
  case 13: ;
  if (ldv_state_variable_39 == 2) {
    ldv_release_39();
    ldv_state_variable_39 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_39 == 3) {
    ldv_release_39();
    ldv_state_variable_39 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50351;
  case 14: ;
  if (ldv_state_variable_39 == 1) {
    ldv_probe_39();
    ldv_state_variable_39 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50351;
  default:
  ldv_stop();
  }
  ldv_50351: ;
  return;
}
}
void ldv_main_exported_34(void)
{
  struct amdgpu_iv_entry *ldvarg1100 ;
  void *tmp ;
  unsigned int ldvarg1101 ;
  enum amdgpu_interrupt_state ldvarg1102 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1100 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1101), 0, 4UL);
  ldv_memset((void *)(& ldvarg1102), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_34 == 1) {
    gfx_v8_0_set_priv_inst_fault_state(gfx_v8_0_priv_inst_irq_funcs_group0, gfx_v8_0_priv_inst_irq_funcs_group1,
                                       ldvarg1101, ldvarg1102);
    ldv_state_variable_34 = 1;
  } else {
  }
  goto ldv_50374;
  case 1: ;
  if (ldv_state_variable_34 == 1) {
    gfx_v8_0_priv_inst_irq(gfx_v8_0_priv_inst_irq_funcs_group0, gfx_v8_0_priv_inst_irq_funcs_group1,
                           ldvarg1100);
    ldv_state_variable_34 = 1;
  } else {
  }
  goto ldv_50374;
  default:
  ldv_stop();
  }
  ldv_50374: ;
  return;
}
}
void ldv_main_exported_36(void)
{
  enum amdgpu_interrupt_state ldvarg1059 ;
  struct amdgpu_iv_entry *ldvarg1057 ;
  void *tmp ;
  unsigned int ldvarg1058 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1057 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1059), 0, 4UL);
  ldv_memset((void *)(& ldvarg1058), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_36 == 1) {
    gfx_v8_0_set_eop_interrupt_state(gfx_v8_0_eop_irq_funcs_group0, gfx_v8_0_eop_irq_funcs_group1,
                                     ldvarg1058, ldvarg1059);
    ldv_state_variable_36 = 1;
  } else {
  }
  goto ldv_50384;
  case 1: ;
  if (ldv_state_variable_36 == 1) {
    gfx_v8_0_eop_irq(gfx_v8_0_eop_irq_funcs_group0, gfx_v8_0_eop_irq_funcs_group1,
                     ldvarg1057);
    ldv_state_variable_36 = 1;
  } else {
  }
  goto ldv_50384;
  default:
  ldv_stop();
  }
  ldv_50384: ;
  return;
}
}
void ldv_main_exported_37(void)
{
  u32 ldvarg466 ;
  bool ldvarg462 ;
  unsigned int ldvarg461 ;
  u32 ldvarg470 ;
  uint64_t ldvarg472 ;
  u32 ldvarg464 ;
  u32 ldvarg467 ;
  u32 ldvarg465 ;
  uint64_t ldvarg473 ;
  u32 ldvarg468 ;
  struct amdgpu_ib *ldvarg471 ;
  void *tmp ;
  unsigned int ldvarg474 ;
  struct amdgpu_semaphore *ldvarg463 ;
  void *tmp___0 ;
  uint64_t ldvarg460 ;
  u32 ldvarg469 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(272UL);
  ldvarg471 = (struct amdgpu_ib *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg463 = (struct amdgpu_semaphore *)tmp___0;
  ldv_memset((void *)(& ldvarg466), 0, 4UL);
  ldv_memset((void *)(& ldvarg462), 0, 1UL);
  ldv_memset((void *)(& ldvarg461), 0, 4UL);
  ldv_memset((void *)(& ldvarg470), 0, 4UL);
  ldv_memset((void *)(& ldvarg472), 0, 8UL);
  ldv_memset((void *)(& ldvarg464), 0, 4UL);
  ldv_memset((void *)(& ldvarg467), 0, 4UL);
  ldv_memset((void *)(& ldvarg465), 0, 4UL);
  ldv_memset((void *)(& ldvarg473), 0, 8UL);
  ldv_memset((void *)(& ldvarg468), 0, 4UL);
  ldv_memset((void *)(& ldvarg474), 0, 4UL);
  ldv_memset((void *)(& ldvarg460), 0, 8UL);
  ldv_memset((void *)(& ldvarg469), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_fence_compute(gfx_v8_0_ring_funcs_compute_group0, ldvarg473,
                                     ldvarg472, ldvarg474);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 1: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_get_rptr_compute(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 2: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_test_ring(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 3: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_hdp_flush(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 4: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_set_wptr_compute(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 5: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_get_wptr_compute(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 6: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_ib(gfx_v8_0_ring_funcs_compute_group0, ldvarg471);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 7: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_gds_switch(gfx_v8_0_ring_funcs_compute_group0, ldvarg467, ldvarg465,
                                  ldvarg469, ldvarg470, ldvarg464, ldvarg466, ldvarg468);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 8: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_semaphore(gfx_v8_0_ring_funcs_compute_group0, ldvarg463, (int )ldvarg462);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 9: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_test_ib(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 10: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_is_lockup(gfx_v8_0_ring_funcs_compute_group0);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  case 11: ;
  if (ldv_state_variable_37 == 1) {
    gfx_v8_0_ring_emit_vm_flush(gfx_v8_0_ring_funcs_compute_group0, ldvarg461, ldvarg460);
    ldv_state_variable_37 = 1;
  } else {
  }
  goto ldv_50406;
  default:
  ldv_stop();
  }
  ldv_50406: ;
  return;
}
}
bool ldv_queue_work_on_939(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_940(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_941(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_942(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_943(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_953(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_955(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_954(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_957(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_956(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___8(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_953(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___7(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___8(system_wq, work);
  return (tmp);
}
}
static void sdma_v2_4_set_ring_funcs(struct amdgpu_device *adev ) ;
static void sdma_v2_4_set_buffer_funcs(struct amdgpu_device *adev ) ;
static void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev ) ;
static void sdma_v2_4_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const sdma_offsets___0[2U] = { 0U, 512U};
static u32 const golden_settings_iceland_a11___1[12U] =
  { 13317U, 4237361159U, 8454151U, 13315U,
        4278194175U, 0U, 13829U, 4237361159U,
        8454151U, 13827U, 4278194175U, 0U};
static u32 const iceland_mgcg_cgcg_init___2[6U] = { 13315U, 4278194160U, 256U, 13827U,
        4278194160U, 256U};
static void sdma_v2_4_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& iceland_mgcg_cgcg_init___2),
                                   6U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_iceland_a11___1),
                                   12U);
  goto ldv_49932;
  default: ;
  goto ldv_49932;
  }
  ldv_49932: ;
  return;
}
}
static int sdma_v2_4_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  int i ;
  struct amdgpu_firmware_info *info ;
  struct common_firmware_header const *header ;
  long tmp ;
  {
  info = (struct amdgpu_firmware_info *)0;
  header = (struct common_firmware_header const *)0;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("sdma_v2_4_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 5U:
  chip_name = "topaz";
  goto ldv_49945;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/sdma_v2_4.c"),
                       "i" (131), "i" (12UL));
  ldv_49947: ;
  goto ldv_49947;
  }
  ldv_49945:
  i = 0;
  goto ldv_49950;
  ldv_49949: ;
  if (i == 0) {
    snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_sdma.bin", chip_name);
  } else {
    snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_sdma1.bin", chip_name);
  }
  err = request_firmware(& adev->sdma[i].fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->sdma[i].fw);
  if (err != 0) {
    goto out;
  } else {
  }
  if ((int )adev->firmware.smu_load) {
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )i;
    info->ucode_id = (enum AMDGPU_UCODE_ID )i;
    info->fw = adev->sdma[i].fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
  } else {
  }
  i = i + 1;
  ldv_49950: ;
  if (i <= 1) {
    goto ldv_49949;
  } else {
  }
  out: ;
  if (err != 0) {
    printk("\vsdma_v2_4: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    i = 0;
    goto ldv_49953;
    ldv_49952:
    release_firmware(adev->sdma[i].fw);
    adev->sdma[i].fw = (struct firmware const *)0;
    i = i + 1;
    ldv_49953: ;
    if (i <= 1) {
      goto ldv_49952;
    } else {
    }
  } else {
  }
  return (err);
}
}
static u32 sdma_v2_4_ring_get_rptr(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs) >> 2;
  return (rptr);
}
}
static u32 sdma_v2_4_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  int me ;
  u32 wptr ;
  u32 tmp ;
  {
  adev = ring->adev;
  me = (unsigned long )(& (ring->adev)->sdma[0].ring) != (unsigned long )ring;
  tmp = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[me] + 13444U, 0);
  wptr = tmp >> 2;
  return (wptr);
}
}
static void sdma_v2_4_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  int me ;
  {
  adev = ring->adev;
  me = (unsigned long )(& (ring->adev)->sdma[0].ring) != (unsigned long )ring;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[me] + 13444U, ring->wptr << 2,
                 0);
  return;
}
}
static void sdma_v2_4_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  u32 vmid ;
  u32 next_rptr ;
  {
  vmid = (unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0) ? (ib->vm)->ids[ring->idx].id & 15U : 0U;
  next_rptr = ring->wptr + 5U;
  goto ldv_49977;
  ldv_49976:
  next_rptr = next_rptr + 1U;
  ldv_49977: ;
  if ((next_rptr & 7U) != 2U) {
    goto ldv_49976;
  } else {
  }
  next_rptr = next_rptr + 6U;
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )ring->next_rptr_gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ring->next_rptr_gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, next_rptr);
  goto ldv_49980;
  ldv_49979:
  amdgpu_ring_write(ring, 0U);
  ldv_49980: ;
  if ((ring->wptr & 7U) != 2U) {
    goto ldv_49979;
  } else {
  }
  amdgpu_ring_write(ring, ((vmid & 15U) << 16) | 4U);
  amdgpu_ring_write(ring, (unsigned int )ib->gpu_addr & 4294967264U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, ib->length_dw);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static void sdma_v2_4_ring_emit_hdp_flush(struct amdgpu_ring *ring )
{
  u32 ref_and_mask ;
  {
  ref_and_mask = 0U;
  if ((unsigned long )(& (ring->adev)->sdma[0].ring) == (unsigned long )ring) {
    ref_and_mask = ref_and_mask | 1024U;
  } else {
    ref_and_mask = ref_and_mask | 2048U;
  }
  amdgpu_ring_write(ring, 872415240U);
  amdgpu_ring_write(ring, 21728U);
  amdgpu_ring_write(ring, 21724U);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static void sdma_v2_4_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                      unsigned int flags )
{
  bool write64bit ;
  {
  write64bit = (flags & 1U) != 0U;
  amdgpu_ring_write(ring, 5U);
  amdgpu_ring_write(ring, (unsigned int )addr);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )seq);
  if ((int )write64bit) {
    addr = addr + 4ULL;
    amdgpu_ring_write(ring, 5U);
    amdgpu_ring_write(ring, (unsigned int )addr);
    amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
    amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  } else {
  }
  amdgpu_ring_write(ring, 6U);
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static bool sdma_v2_4_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                          bool emit_wait )
{
  u64 addr ;
  u32 sig ;
  {
  addr = semaphore->gpu_addr;
  sig = (int )emit_wait ? 0U : 1U;
  amdgpu_ring_write(ring, ((sig & 1U) << 30) | 7U);
  amdgpu_ring_write(ring, (unsigned int )addr & 4294967288U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  return (1);
}
}
static void sdma_v2_4_gfx_stop(struct amdgpu_device *adev )
{
  struct amdgpu_ring *sdma0 ;
  struct amdgpu_ring *sdma1 ;
  u32 rb_cntl ;
  u32 ib_cntl ;
  int i ;
  {
  sdma0 = & adev->sdma[0].ring;
  sdma1 = & adev->sdma[1].ring;
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma0 || (unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma1) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.visible_vram_size);
  } else {
  }
  i = 0;
  goto ldv_50009;
  ldv_50008:
  rb_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, 0);
  rb_cntl = rb_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, rb_cntl, 0);
  ib_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13450U, 0);
  ib_cntl = ib_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13450U, ib_cntl, 0);
  i = i + 1;
  ldv_50009: ;
  if (i <= 1) {
    goto ldv_50008;
  } else {
  }
  sdma0->ready = 0;
  sdma1->ready = 0;
  return;
}
}
static void sdma_v2_4_rlc_stop(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void sdma_v2_4_enable(struct amdgpu_device *adev , bool enable )
{
  u32 f32_cntl ;
  int i ;
  {
  if (! enable) {
    sdma_v2_4_gfx_stop(adev);
    sdma_v2_4_rlc_stop(adev);
  } else {
  }
  i = 0;
  goto ldv_50021;
  ldv_50020:
  f32_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13330U, 0);
  if ((int )enable) {
    f32_cntl = f32_cntl & 4294967294U;
  } else {
    f32_cntl = f32_cntl | 1U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13330U, f32_cntl, 0);
  i = i + 1;
  ldv_50021: ;
  if (i <= 1) {
    goto ldv_50020;
  } else {
  }
  return;
}
}
static int sdma_v2_4_gfx_resume(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_cntl ;
  u32 ib_cntl ;
  u32 rb_bufsz ;
  u32 wb_offset ;
  int i ;
  int j ;
  int r ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  i = 0;
  goto ldv_50038;
  ldv_50037:
  ring = & adev->sdma[i].ring;
  wb_offset = ring->rptr_offs * 4U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_50035;
  ldv_50034:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13479U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13480U, 0U, 0);
  j = j + 1;
  ldv_50035: ;
  if (j <= 15) {
    goto ldv_50034;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13321U, 0U, 0);
  tmp = __roundup_pow_of_two((unsigned long )(ring->ring_size / 4U));
  tmp___0 = __ilog2_u64((u64 )tmp);
  rb_bufsz = (u32 )tmp___0;
  rb_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, 0);
  rb_cntl = (rb_cntl & 4294967233U) | ((rb_bufsz << 1) & 62U);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, rb_cntl, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13443U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13444U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13448U, (unsigned int )((adev->wb.gpu_addr + (uint64_t )wb_offset) >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13449U, ((unsigned int )adev->wb.gpu_addr + wb_offset) & 4294967292U,
                 0);
  rb_cntl = rb_cntl | 4096U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13441U, (u32 )(ring->gpu_addr >> 8),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13442U, (u32 )(ring->gpu_addr >> 40),
                 0);
  ring->wptr = 0U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13444U, ring->wptr << 2,
                 0);
  rb_cntl = rb_cntl | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, rb_cntl, 0);
  ib_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13450U, 0);
  ib_cntl = ib_cntl | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13450U, ib_cntl, 0);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )ring) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.real_vram_size);
  } else {
  }
  i = i + 1;
  ldv_50038: ;
  if (i <= 1) {
    goto ldv_50037;
  } else {
  }
  return (0);
}
}
static int sdma_v2_4_rlc_resume(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static int sdma_v2_4_load_microcode(struct amdgpu_device *adev )
{
  struct sdma_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  u32 fw_size ;
  int i ;
  int j ;
  bool smc_loads_fw ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  {
  smc_loads_fw = 0;
  if ((unsigned long )adev->sdma[0].fw == (unsigned long )((struct firmware const *)0) || (unsigned long )adev->sdma[1].fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  sdma_v2_4_enable(adev, 0);
  if ((int )smc_loads_fw) {
  } else {
    i = 0;
    goto ldv_50056;
    ldv_50055:
    hdr = (struct sdma_firmware_header_v1_0 const *)(adev->sdma[i].fw)->data;
    amdgpu_ucode_print_sdma_hdr(& hdr->header);
    fw_size = (unsigned int )hdr->header.ucode_size_bytes / 4U;
    adev->sdma[i].fw_version = hdr->header.ucode_version;
    fw_data = (__le32 const *)(adev->sdma[i].fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
    amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13312U, 0U, 0);
    j = 0;
    goto ldv_50053;
    ldv_50052:
    tmp = fw_data;
    fw_data = fw_data + 1;
    tmp___0 = __le32_to_cpup(tmp);
    amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13313U, tmp___0, 0);
    j = j + 1;
    ldv_50053: ;
    if ((u32 )j < fw_size) {
      goto ldv_50052;
    } else {
    }
    amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___0[i] + 13312U, adev->sdma[i].fw_version,
                   0);
    i = i + 1;
    ldv_50056: ;
    if (i <= 1) {
      goto ldv_50055;
    } else {
    }
  }
  return (0);
}
}
static int sdma_v2_4_start(struct amdgpu_device *adev )
{
  int r ;
  {
  if (! adev->firmware.smu_load) {
    r = sdma_v2_4_load_microcode(adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 0U);
    if (r != 0) {
      return (-22);
    } else {
    }
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 1U);
    if (r != 0) {
      return (-22);
    } else {
    }
  }
  sdma_v2_4_enable(adev, 1);
  r = sdma_v2_4_gfx_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = sdma_v2_4_rlc_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int sdma_v2_4_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ring_lock(ring, 5U);
  if (r != 0) {
    drm_err("amdgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);
    amdgpu_wb_free(adev, index);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )gpu_addr);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_50073;
  ldv_50072:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_50071;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50073: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50072;
  } else {
  }
  ldv_50071: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static int sdma_v2_4_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ib ib ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 256U, & ib);
  if (r != 0) {
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    return (r);
  } else {
  }
  *(ib.ptr) = 2U;
  *(ib.ptr + 1UL) = (unsigned int )gpu_addr;
  *(ib.ptr + 2UL) = (unsigned int )(gpu_addr >> 32ULL);
  *(ib.ptr + 3UL) = 1U;
  *(ib.ptr + 4UL) = 3735928559U;
  *(ib.ptr + 5UL) = 0U;
  *(ib.ptr + 6UL) = 0U;
  *(ib.ptr + 7UL) = 0U;
  ib.length_dw = 8U;
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_fence_wait(ib.fence, 0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_50086;
  ldv_50085:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_50084;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50086: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50085;
  } else {
  }
  ldv_50084: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ib test on ring %d succeeded in %u usecs\n", ((ib.fence)->ring)->idx,
           i);
  } else {
    drm_err("amdgpu: ib test failed (0x%08X)\n", tmp);
    r = -22;
  }
  amdgpu_ib_free(adev, & ib);
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static void sdma_v2_4_vm_copy_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t src ,
                                  unsigned int count )
{
  unsigned int bytes ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  {
  goto ldv_50095;
  ldv_50094:
  bytes = count * 8U;
  if (bytes > 2097144U) {
    bytes = 2097144U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 1U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = bytes;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = 0U;
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = (unsigned int )src;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (unsigned int )(src >> 32ULL);
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )pe;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(pe >> 32ULL);
  pe = (uint64_t )bytes + pe;
  src = (uint64_t )bytes + src;
  count = count - bytes / 8U;
  ldv_50095: ;
  if (count != 0U) {
    goto ldv_50094;
  } else {
  }
  return;
}
}
static void sdma_v2_4_vm_write_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                   unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  goto ldv_50111;
  ldv_50110:
  ndw = count * 2U;
  if (ndw > 1048574U) {
    ndw = 1048574U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 2U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = ndw;
  goto ldv_50108;
  ldv_50107: ;
  if ((flags & 2U) != 0U) {
    value = amdgpu_vm_map_gart((ib->ring)->adev, addr);
    value = value & 0xfffffffffffff000ULL;
  } else
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  addr = (uint64_t )incr + addr;
  value = (uint64_t )flags | value;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (u32 )value;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )(value >> 32ULL);
  ndw = ndw - 2U;
  count = count - 1U;
  pe = pe + 8ULL;
  ldv_50108: ;
  if (ndw != 0U) {
    goto ldv_50107;
  } else {
  }
  ldv_50111: ;
  if (count != 0U) {
    goto ldv_50110;
  } else {
  }
  return;
}
}
static void sdma_v2_4_vm_set_pte_pde(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                     unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  {
  goto ldv_50124;
  ldv_50123:
  ndw = count;
  if (ndw > 524287U) {
    ndw = 524287U;
  } else {
  }
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 12U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = flags;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = 0U;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (u32 )value;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(value >> 32ULL);
  tmp___6 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___6) = incr;
  tmp___7 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___7) = 0U;
  tmp___8 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___8) = ndw;
  pe = (uint64_t )(ndw * 8U) + pe;
  addr = (uint64_t )(ndw * incr) + addr;
  count = count - ndw;
  ldv_50124: ;
  if (count != 0U) {
    goto ldv_50123;
  } else {
  }
  return;
}
}
static void sdma_v2_4_vm_pad_ib(struct amdgpu_ib *ib )
{
  u32 tmp ;
  {
  goto ldv_50130;
  ldv_50129:
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 0U;
  ldv_50130: ;
  if ((ib->length_dw & 7U) != 0U) {
    goto ldv_50129;
  } else {
  }
  return;
}
}
static void sdma_v2_4_ring_emit_vm_flush(struct amdgpu_ring *ring , unsigned int vm_id ,
                                         uint64_t pd_addr )
{
  {
  amdgpu_ring_write(ring, 4026531854U);
  if (vm_id <= 7U) {
    amdgpu_ring_write(ring, vm_id + 1359U);
  } else {
    amdgpu_ring_write(ring, vm_id + 1286U);
  }
  amdgpu_ring_write(ring, (u32 )(pd_addr >> 12));
  amdgpu_ring_write(ring, 4026531854U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, (u32 )(1 << (int )vm_id));
  amdgpu_ring_write(ring, 8U);
  amdgpu_ring_write(ring, 5240U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static int sdma_v2_4_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v2_4_set_ring_funcs(adev);
  sdma_v2_4_set_buffer_funcs(adev);
  sdma_v2_4_set_vm_pte_funcs(adev);
  sdma_v2_4_set_irq_funcs(adev);
  return (0);
}
}
static int sdma_v2_4_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 224U, & adev->sdma_trap_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 241U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 247U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = sdma_v2_4_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load sdma firmware!\n");
    return (r);
  } else {
  }
  ring = & adev->sdma[0].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 0;
  ring = & adev->sdma[1].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 0;
  ring = & adev->sdma[0].ring;
  sprintf((char *)(& ring->name), "sdma0");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 0U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->sdma[1].ring;
  sprintf((char *)(& ring->name), "sdma1");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 1U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int sdma_v2_4_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_ring_fini(& adev->sdma[0].ring);
  amdgpu_ring_fini(& adev->sdma[1].ring);
  return (0);
}
}
static int sdma_v2_4_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v2_4_init_golden_registers(adev);
  r = sdma_v2_4_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int sdma_v2_4_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v2_4_enable(adev, 0);
  return (0);
}
}
static int sdma_v2_4_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = sdma_v2_4_hw_fini((void *)adev);
  return (tmp);
}
}
static int sdma_v2_4_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = sdma_v2_4_hw_init((void *)adev);
  return (tmp);
}
}
static bool sdma_v2_4_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 96U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int sdma_v2_4_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_50180;
  ldv_50179:
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0 & 96U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50180: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50179;
  } else {
  }
  return (-110);
}
}
static void sdma_v2_4_print_status(void *handle )
{
  int i ;
  int j ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "VI SDMA registers\n");
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp);
  i = 0;
  goto ldv_50192;
  ldv_50191:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13325U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_STATUS_REG=0x%08X\n", i,
            tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13330U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_F32_CNTL=0x%08X\n", i, tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13316U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_CNTL=0x%08X\n", i, tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13321U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_SEM_WAIT_FAIL_TIMER_CNTL=0x%08X\n",
            i, tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13450U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_IB_CNTL=0x%08X\n", i,
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13440U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_CNTL=0x%08X\n", i,
            tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13443U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR=0x%08X\n", i,
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13444U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_WPTR=0x%08X\n", i,
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13448U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_HI=0x%08X\n",
            i, tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13449U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_LO=0x%08X\n",
            i, tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13441U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE=0x%08X\n", i,
            tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13442U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE_HI=0x%08X\n",
            i, tmp___11);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_50189;
  ldv_50188:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  _dev_info((struct device const *)adev->dev, "  VM %d:\n", j);
  tmp___12 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13479U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_VIRTUAL_ADDR=0x%08X\n",
            i, tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___0[i] + 13480U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_APE1_CNTL=0x%08X\n",
            i, tmp___13);
  j = j + 1;
  ldv_50189: ;
  if (j <= 15) {
    goto ldv_50188;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  i = i + 1;
  ldv_50192: ;
  if (i <= 1) {
    goto ldv_50191;
  } else {
  }
  return;
}
}
static int sdma_v2_4_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 32U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13330U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 13330U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 1048576U;
  } else {
  }
  if ((tmp & 64U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13842U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 13842U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 64U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    sdma_v2_4_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    sdma_v2_4_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int sdma_v2_4_set_trap_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *src ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 sdma_cntl ;
  {
  switch (type) {
  case 0U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_50209;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_50209;
  default: ;
  goto ldv_50209;
  }
  ldv_50209: ;
  goto ldv_50212;
  case 1U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_50215;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_50215;
  default: ;
  goto ldv_50215;
  }
  ldv_50215: ;
  goto ldv_50212;
  default: ;
  goto ldv_50212;
  }
  ldv_50212: ;
  return (0);
}
}
static int sdma_v2_4_process_trap_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  u8 instance_id ;
  u8 queue_id ;
  long tmp ;
  {
  instance_id = (unsigned int )((u8 )entry->ring_id) & 3U;
  queue_id = (u8 )((entry->ring_id & 12U) >> 2);
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("sdma_v2_4_process_trap_irq", "IH: SDMA trap\n");
  } else {
  }
  switch ((int )instance_id) {
  case 0: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[0].ring);
  goto ldv_50229;
  case 1: ;
  goto ldv_50229;
  case 2: ;
  goto ldv_50229;
  }
  ldv_50229: ;
  goto ldv_50232;
  case 1: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[1].ring);
  goto ldv_50235;
  case 1: ;
  goto ldv_50235;
  case 2: ;
  goto ldv_50235;
  }
  ldv_50235: ;
  goto ldv_50232;
  }
  ldv_50232: ;
  return (0);
}
}
static int sdma_v2_4_process_illegal_inst_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                              struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal instruction in SDMA command stream\n");
  schedule_work___7(& adev->reset_work);
  return (0);
}
}
static int sdma_v2_4_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int sdma_v2_4_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const sdma_v2_4_ip_funcs =
     {& sdma_v2_4_early_init, (int (*)(void * ))0, & sdma_v2_4_sw_init, & sdma_v2_4_sw_fini,
    & sdma_v2_4_hw_init, & sdma_v2_4_hw_fini, & sdma_v2_4_suspend, & sdma_v2_4_resume,
    & sdma_v2_4_is_idle, & sdma_v2_4_wait_for_idle, & sdma_v2_4_soft_reset, & sdma_v2_4_print_status,
    & sdma_v2_4_set_clockgating_state, & sdma_v2_4_set_powergating_state};
static bool sdma_v2_4_ring_is_lockup(struct amdgpu_ring *ring )
{
  bool tmp ;
  bool tmp___0 ;
  {
  tmp = sdma_v2_4_is_idle((void *)ring->adev);
  if ((int )tmp) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___0 = amdgpu_ring_test_lockup(ring);
  return (tmp___0);
}
}
static struct amdgpu_ring_funcs const sdma_v2_4_ring_funcs =
     {& sdma_v2_4_ring_get_rptr, & sdma_v2_4_ring_get_wptr, & sdma_v2_4_ring_set_wptr,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & sdma_v2_4_ring_emit_ib, & sdma_v2_4_ring_emit_fence,
    & sdma_v2_4_ring_emit_semaphore, & sdma_v2_4_ring_emit_vm_flush, & sdma_v2_4_ring_emit_hdp_flush,
    0, & sdma_v2_4_ring_test_ring, & sdma_v2_4_ring_test_ib, & sdma_v2_4_ring_is_lockup};
static void sdma_v2_4_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma[0].ring.funcs = & sdma_v2_4_ring_funcs;
  adev->sdma[1].ring.funcs = & sdma_v2_4_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const sdma_v2_4_trap_irq_funcs = {& sdma_v2_4_set_trap_irq_state, & sdma_v2_4_process_trap_irq};
static struct amdgpu_irq_src_funcs const sdma_v2_4_illegal_inst_irq_funcs = {0, & sdma_v2_4_process_illegal_inst_irq};
static void sdma_v2_4_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma_trap_irq.num_types = 2U;
  adev->sdma_trap_irq.funcs = & sdma_v2_4_trap_irq_funcs;
  adev->sdma_illegal_inst_irq.funcs = & sdma_v2_4_illegal_inst_irq_funcs;
  return;
}
}
static void sdma_v2_4_emit_copy_buffer(struct amdgpu_ring *ring , uint64_t src_offset ,
                                       uint64_t dst_offset , u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, byte_count);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (unsigned int )src_offset);
  amdgpu_ring_write(ring, (unsigned int )(src_offset >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  return;
}
}
static void sdma_v2_4_emit_fill_buffer(struct amdgpu_ring *ring , u32 src_data , uint64_t dst_offset ,
                                       u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 11U);
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  amdgpu_ring_write(ring, src_data);
  amdgpu_ring_write(ring, byte_count);
  return;
}
}
static struct amdgpu_buffer_funcs const sdma_v2_4_buffer_funcs = {2097151U, 7U, & sdma_v2_4_emit_copy_buffer, 2097151U, 7U, & sdma_v2_4_emit_fill_buffer};
static void sdma_v2_4_set_buffer_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mman.buffer_funcs == (unsigned long )((struct amdgpu_buffer_funcs const *)0)) {
    adev->mman.buffer_funcs = & sdma_v2_4_buffer_funcs;
    adev->mman.buffer_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
static struct amdgpu_vm_pte_funcs const sdma_v2_4_vm_pte_funcs = {& sdma_v2_4_vm_copy_pte, & sdma_v2_4_vm_write_pte, & sdma_v2_4_vm_set_pte_pde,
    & sdma_v2_4_vm_pad_ib};
static void sdma_v2_4_set_vm_pte_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->vm_manager.vm_pte_funcs == (unsigned long )((struct amdgpu_vm_pte_funcs const *)0)) {
    adev->vm_manager.vm_pte_funcs = & sdma_v2_4_vm_pte_funcs;
    adev->vm_manager.vm_pte_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
extern int ldv_release_33(void) ;
int ldv_retval_23 ;
extern int ldv_probe_33(void) ;
extern int ldv_release_28(void) ;
int ldv_retval_24 ;
extern int ldv_probe_28(void) ;
void ldv_initialize_amdgpu_ring_funcs_32(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  sdma_v2_4_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_buffer_funcs_29(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  sdma_v2_4_buffer_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_31(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  sdma_v2_4_trap_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  sdma_v2_4_trap_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_vm_pte_funcs_28(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(272UL);
  sdma_v2_4_vm_pte_funcs_group0 = (struct amdgpu_ib *)tmp;
  return;
}
}
void ldv_main_exported_33(void)
{
  void *ldvarg505 ;
  void *tmp ;
  void *ldvarg519 ;
  void *tmp___0 ;
  enum amd_powergating_state ldvarg513 ;
  void *ldvarg516 ;
  void *tmp___1 ;
  void *ldvarg512 ;
  void *tmp___2 ;
  void *ldvarg518 ;
  void *tmp___3 ;
  void *ldvarg511 ;
  void *tmp___4 ;
  void *ldvarg517 ;
  void *tmp___5 ;
  void *ldvarg507 ;
  void *tmp___6 ;
  void *ldvarg510 ;
  void *tmp___7 ;
  void *ldvarg514 ;
  void *tmp___8 ;
  void *ldvarg508 ;
  void *tmp___9 ;
  enum amd_clockgating_state ldvarg509 ;
  void *ldvarg506 ;
  void *tmp___10 ;
  void *ldvarg515 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg505 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg519 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg516 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg512 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg518 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg511 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg517 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg507 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg510 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg514 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg508 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg506 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg515 = tmp___11;
  ldv_memset((void *)(& ldvarg513), 0, 4UL);
  ldv_memset((void *)(& ldvarg509), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_hw_fini(ldvarg519);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_hw_fini(ldvarg519);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_hw_fini(ldvarg519);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 1: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_print_status(ldvarg518);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_print_status(ldvarg518);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_print_status(ldvarg518);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 2: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_early_init(ldvarg517);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_early_init(ldvarg517);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_early_init(ldvarg517);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 3: ;
  if (ldv_state_variable_33 == 2) {
    ldv_retval_24 = sdma_v2_4_suspend(ldvarg516);
    if (ldv_retval_24 == 0) {
      ldv_state_variable_33 = 3;
    } else {
    }
  } else {
  }
  goto ldv_50325;
  case 4: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_sw_init(ldvarg515);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_sw_init(ldvarg515);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_sw_init(ldvarg515);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 5: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_set_powergating_state(ldvarg514, ldvarg513);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_set_powergating_state(ldvarg514, ldvarg513);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_set_powergating_state(ldvarg514, ldvarg513);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 6: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_wait_for_idle(ldvarg512);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_wait_for_idle(ldvarg512);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_wait_for_idle(ldvarg512);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 7: ;
  if (ldv_state_variable_33 == 3) {
    ldv_retval_23 = sdma_v2_4_resume(ldvarg511);
    if (ldv_retval_23 == 0) {
      ldv_state_variable_33 = 2;
    } else {
    }
  } else {
  }
  goto ldv_50325;
  case 8: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_set_clockgating_state(ldvarg510, ldvarg509);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_set_clockgating_state(ldvarg510, ldvarg509);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_set_clockgating_state(ldvarg510, ldvarg509);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 9: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_hw_init(ldvarg508);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_hw_init(ldvarg508);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_hw_init(ldvarg508);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 10: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_soft_reset(ldvarg507);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_soft_reset(ldvarg507);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_soft_reset(ldvarg507);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 11: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_sw_fini(ldvarg506);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_sw_fini(ldvarg506);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_sw_fini(ldvarg506);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 12: ;
  if (ldv_state_variable_33 == 1) {
    sdma_v2_4_is_idle(ldvarg505);
    ldv_state_variable_33 = 1;
  } else {
  }
  if (ldv_state_variable_33 == 3) {
    sdma_v2_4_is_idle(ldvarg505);
    ldv_state_variable_33 = 3;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    sdma_v2_4_is_idle(ldvarg505);
    ldv_state_variable_33 = 2;
  } else {
  }
  goto ldv_50325;
  case 13: ;
  if (ldv_state_variable_33 == 3) {
    ldv_release_33();
    ldv_state_variable_33 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_33 == 2) {
    ldv_release_33();
    ldv_state_variable_33 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50325;
  case 14: ;
  if (ldv_state_variable_33 == 1) {
    ldv_probe_33();
    ldv_state_variable_33 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50325;
  default:
  ldv_stop();
  }
  ldv_50325: ;
  return;
}
}
void ldv_main_exported_32(void)
{
  uint64_t ldvarg2 ;
  uint64_t ldvarg9 ;
  bool ldvarg4 ;
  uint64_t ldvarg8 ;
  struct amdgpu_ib *ldvarg6 ;
  void *tmp ;
  struct amdgpu_semaphore *ldvarg5 ;
  void *tmp___0 ;
  unsigned int ldvarg3 ;
  unsigned int ldvarg7 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(272UL);
  ldvarg6 = (struct amdgpu_ib *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg5 = (struct amdgpu_semaphore *)tmp___0;
  ldv_memset((void *)(& ldvarg2), 0, 8UL);
  ldv_memset((void *)(& ldvarg9), 0, 8UL);
  ldv_memset((void *)(& ldvarg4), 0, 1UL);
  ldv_memset((void *)(& ldvarg8), 0, 8UL);
  ldv_memset((void *)(& ldvarg3), 0, 4UL);
  ldv_memset((void *)(& ldvarg7), 0, 4UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_emit_fence(sdma_v2_4_ring_funcs_group0, ldvarg9, ldvarg8, ldvarg7);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 1: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_get_rptr(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 2: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_test_ring(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 3: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_emit_hdp_flush(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 4: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_set_wptr(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 5: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_get_wptr(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 6: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_emit_ib(sdma_v2_4_ring_funcs_group0, ldvarg6);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 7: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_emit_semaphore(sdma_v2_4_ring_funcs_group0, ldvarg5, (int )ldvarg4);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 8: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_test_ib(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 9: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_is_lockup(sdma_v2_4_ring_funcs_group0);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  case 10: ;
  if (ldv_state_variable_32 == 1) {
    sdma_v2_4_ring_emit_vm_flush(sdma_v2_4_ring_funcs_group0, ldvarg3, ldvarg2);
    ldv_state_variable_32 = 1;
  } else {
  }
  goto ldv_50353;
  default:
  ldv_stop();
  }
  ldv_50353: ;
  return;
}
}
void ldv_main_exported_28(void)
{
  unsigned int ldvarg905 ;
  uint64_t ldvarg911 ;
  u32 ldvarg914 ;
  uint64_t ldvarg912 ;
  u32 ldvarg902 ;
  uint64_t ldvarg908 ;
  unsigned int ldvarg913 ;
  uint64_t ldvarg903 ;
  uint64_t ldvarg904 ;
  uint64_t ldvarg907 ;
  u32 ldvarg910 ;
  u32 ldvarg906 ;
  unsigned int ldvarg909 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg905), 0, 4UL);
  ldv_memset((void *)(& ldvarg911), 0, 8UL);
  ldv_memset((void *)(& ldvarg914), 0, 4UL);
  ldv_memset((void *)(& ldvarg912), 0, 8UL);
  ldv_memset((void *)(& ldvarg902), 0, 4UL);
  ldv_memset((void *)(& ldvarg908), 0, 8UL);
  ldv_memset((void *)(& ldvarg913), 0, 4UL);
  ldv_memset((void *)(& ldvarg903), 0, 8UL);
  ldv_memset((void *)(& ldvarg904), 0, 8UL);
  ldv_memset((void *)(& ldvarg907), 0, 8UL);
  ldv_memset((void *)(& ldvarg910), 0, 4UL);
  ldv_memset((void *)(& ldvarg906), 0, 4UL);
  ldv_memset((void *)(& ldvarg909), 0, 4UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_28 == 2) {
    sdma_v2_4_vm_write_pte(sdma_v2_4_vm_pte_funcs_group0, ldvarg912, ldvarg911, ldvarg913,
                           ldvarg914, ldvarg910);
    ldv_state_variable_28 = 2;
  } else {
  }
  goto ldv_50382;
  case 1: ;
  if (ldv_state_variable_28 == 2) {
    sdma_v2_4_vm_copy_pte(sdma_v2_4_vm_pte_funcs_group0, ldvarg908, ldvarg907, ldvarg909);
    ldv_state_variable_28 = 2;
  } else {
  }
  if (ldv_state_variable_28 == 1) {
    sdma_v2_4_vm_copy_pte(sdma_v2_4_vm_pte_funcs_group0, ldvarg908, ldvarg907, ldvarg909);
    ldv_state_variable_28 = 1;
  } else {
  }
  goto ldv_50382;
  case 2: ;
  if (ldv_state_variable_28 == 2) {
    sdma_v2_4_vm_pad_ib(sdma_v2_4_vm_pte_funcs_group0);
    ldv_state_variable_28 = 2;
  } else {
  }
  if (ldv_state_variable_28 == 1) {
    sdma_v2_4_vm_pad_ib(sdma_v2_4_vm_pte_funcs_group0);
    ldv_state_variable_28 = 1;
  } else {
  }
  goto ldv_50382;
  case 3: ;
  if (ldv_state_variable_28 == 2) {
    sdma_v2_4_vm_set_pte_pde(sdma_v2_4_vm_pte_funcs_group0, ldvarg904, ldvarg903,
                             ldvarg905, ldvarg906, ldvarg902);
    ldv_state_variable_28 = 2;
  } else {
  }
  if (ldv_state_variable_28 == 1) {
    sdma_v2_4_vm_set_pte_pde(sdma_v2_4_vm_pte_funcs_group0, ldvarg904, ldvarg903,
                             ldvarg905, ldvarg906, ldvarg902);
    ldv_state_variable_28 = 1;
  } else {
  }
  goto ldv_50382;
  case 4: ;
  if (ldv_state_variable_28 == 2) {
    ldv_release_28();
    ldv_state_variable_28 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50382;
  case 5: ;
  if (ldv_state_variable_28 == 1) {
    ldv_probe_28();
    ldv_state_variable_28 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50382;
  default:
  ldv_stop();
  }
  ldv_50382: ;
  return;
}
}
void ldv_main_exported_30(void)
{
  struct amdgpu_irq_src *ldvarg892 ;
  void *tmp ;
  struct amdgpu_iv_entry *ldvarg891 ;
  void *tmp___0 ;
  struct amdgpu_device *ldvarg893 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg892 = (struct amdgpu_irq_src *)tmp;
  tmp___0 = ldv_init_zalloc(20UL);
  ldvarg891 = (struct amdgpu_iv_entry *)tmp___0;
  tmp___1 = ldv_init_zalloc(23352UL);
  ldvarg893 = (struct amdgpu_device *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_30 == 1) {
    sdma_v2_4_process_illegal_inst_irq(ldvarg893, ldvarg892, ldvarg891);
    ldv_state_variable_30 = 1;
  } else {
  }
  goto ldv_50396;
  default:
  ldv_stop();
  }
  ldv_50396: ;
  return;
}
}
void ldv_main_exported_31(void)
{
  enum amdgpu_interrupt_state ldvarg118 ;
  unsigned int ldvarg117 ;
  struct amdgpu_iv_entry *ldvarg116 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg116 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg118), 0, 4UL);
  ldv_memset((void *)(& ldvarg117), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_31 == 1) {
    sdma_v2_4_set_trap_irq_state(sdma_v2_4_trap_irq_funcs_group0, sdma_v2_4_trap_irq_funcs_group1,
                                 ldvarg117, ldvarg118);
    ldv_state_variable_31 = 1;
  } else {
  }
  goto ldv_50405;
  case 1: ;
  if (ldv_state_variable_31 == 1) {
    sdma_v2_4_process_trap_irq(sdma_v2_4_trap_irq_funcs_group0, sdma_v2_4_trap_irq_funcs_group1,
                               ldvarg116);
    ldv_state_variable_31 = 1;
  } else {
  }
  goto ldv_50405;
  default:
  ldv_stop();
  }
  ldv_50405: ;
  return;
}
}
void ldv_main_exported_29(void)
{
  uint64_t ldvarg161 ;
  uint64_t ldvarg162 ;
  u32 ldvarg166 ;
  u32 ldvarg163 ;
  u32 ldvarg165 ;
  uint64_t ldvarg164 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg161), 0, 8UL);
  ldv_memset((void *)(& ldvarg162), 0, 8UL);
  ldv_memset((void *)(& ldvarg166), 0, 4UL);
  ldv_memset((void *)(& ldvarg163), 0, 4UL);
  ldv_memset((void *)(& ldvarg165), 0, 4UL);
  ldv_memset((void *)(& ldvarg164), 0, 8UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_29 == 1) {
    sdma_v2_4_emit_fill_buffer(sdma_v2_4_buffer_funcs_group0, ldvarg165, ldvarg164,
                               ldvarg166);
    ldv_state_variable_29 = 1;
  } else {
  }
  goto ldv_50418;
  case 1: ;
  if (ldv_state_variable_29 == 1) {
    sdma_v2_4_emit_copy_buffer(sdma_v2_4_buffer_funcs_group0, ldvarg162, ldvarg161,
                               ldvarg163);
    ldv_state_variable_29 = 1;
  } else {
  }
  goto ldv_50418;
  default:
  ldv_stop();
  }
  ldv_50418: ;
  return;
}
}
bool ldv_queue_work_on_953(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_954(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_955(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_956(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_957(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_967(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_969(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_968(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_971(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_970(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___9(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_967(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___8(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___9(system_wq, work);
  return (tmp);
}
}
static void sdma_v3_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void sdma_v3_0_set_buffer_funcs(struct amdgpu_device *adev ) ;
static void sdma_v3_0_set_vm_pte_funcs(struct amdgpu_device *adev ) ;
static void sdma_v3_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 const sdma_offsets___1[2U] = { 0U, 512U};
static u32 const golden_settings_tonga_a11___2[30U] =
  { 13317U, 4237361159U, 8454151U, 13315U,
        4278194175U, 0U, 13450U, 2148466961U,
        256U, 13578U, 2148466961U, 256U,
        13706U, 2148466961U, 256U, 13829U,
        4237361159U, 8454151U, 13827U, 4278194175U,
        0U, 13962U, 2148466961U, 256U,
        14090U, 2148466961U, 256U, 14218U,
        2148466961U, 256U};
static u32 const tonga_mgcg_cgcg_init___3[6U] = { 13315U, 4278194160U, 256U, 13827U,
        4278194160U, 256U};
static u32 const cz_golden_settings_a11___1[36U] =
  { 13317U, 4237361159U, 8454151U, 13315U,
        4278194175U, 0U, 13450U, 256U,
        256U, 13314U, 2048U, 247808U,
        13578U, 256U, 256U, 13706U,
        256U, 256U, 13829U, 4237361159U,
        8454151U, 13827U, 4278194175U, 0U,
        13962U, 256U, 256U, 13826U,
        2048U, 247808U, 14090U, 256U,
        256U, 14218U, 256U, 256U};
static u32 const cz_mgcg_cgcg_init___3[6U] = { 13315U, 4278194160U, 256U, 13827U,
        4278194160U, 256U};
static void sdma_v3_0_init_golden_registers(struct amdgpu_device *adev )
{
  {
  switch ((unsigned int )adev->asic_type) {
  case 6U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& tonga_mgcg_cgcg_init___3),
                                   6U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& golden_settings_tonga_a11___2),
                                   30U);
  goto ldv_49936;
  case 7U:
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_mgcg_cgcg_init___3),
                                   6U);
  amdgpu_program_register_sequence(adev, (u32 const *)(& cz_golden_settings_a11___1),
                                   36U);
  goto ldv_49936;
  default: ;
  goto ldv_49936;
  }
  ldv_49936: ;
  return;
}
}
static int sdma_v3_0_init_microcode(struct amdgpu_device *adev )
{
  char const *chip_name ;
  char fw_name[30U] ;
  int err ;
  int i ;
  struct amdgpu_firmware_info *info ;
  struct common_firmware_header const *header ;
  long tmp ;
  {
  info = (struct amdgpu_firmware_info *)0;
  header = (struct common_firmware_header const *)0;
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("sdma_v3_0_init_microcode", "\n");
  } else {
  }
  switch ((unsigned int )adev->asic_type) {
  case 6U:
  chip_name = "tonga";
  goto ldv_49954;
  case 7U:
  chip_name = "carrizo";
  goto ldv_49954;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/sdma_v3_0.c"),
                       "i" (172), "i" (12UL));
  ldv_49957: ;
  goto ldv_49957;
  }
  ldv_49954:
  i = 0;
  goto ldv_49960;
  ldv_49959: ;
  if (i == 0) {
    snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_sdma.bin", chip_name);
  } else {
    snprintf((char *)(& fw_name), 30UL, "amdgpu/%s_sdma1.bin", chip_name);
  }
  err = request_firmware(& adev->sdma[i].fw, (char const *)(& fw_name), adev->dev);
  if (err != 0) {
    goto out;
  } else {
  }
  err = amdgpu_ucode_validate(adev->sdma[i].fw);
  if (err != 0) {
    goto out;
  } else {
  }
  if ((int )adev->firmware.smu_load) {
    info = (struct amdgpu_firmware_info *)(& adev->firmware.ucode) + (unsigned long )i;
    info->ucode_id = (enum AMDGPU_UCODE_ID )i;
    info->fw = adev->sdma[i].fw;
    header = (struct common_firmware_header const *)(info->fw)->data;
    adev->firmware.fw_size = adev->firmware.fw_size + (((unsigned int )header->ucode_size_bytes + 4095U) & 4294963200U);
  } else {
  }
  i = i + 1;
  ldv_49960: ;
  if (i <= 1) {
    goto ldv_49959;
  } else {
  }
  out: ;
  if (err != 0) {
    printk("\vsdma_v3_0: Failed to load firmware \"%s\"\n", (char *)(& fw_name));
    i = 0;
    goto ldv_49963;
    ldv_49962:
    release_firmware(adev->sdma[i].fw);
    adev->sdma[i].fw = (struct firmware const *)0;
    i = i + 1;
    ldv_49963: ;
    if (i <= 1) {
      goto ldv_49962;
    } else {
    }
  } else {
  }
  return (err);
}
}
static u32 sdma_v3_0_ring_get_rptr(struct amdgpu_ring *ring )
{
  u32 rptr ;
  {
  rptr = *((ring->adev)->wb.wb + (unsigned long )ring->rptr_offs) >> 2;
  return (rptr);
}
}
static u32 sdma_v3_0_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 wptr ;
  int me ;
  u32 tmp ;
  {
  adev = ring->adev;
  if ((int )ring->use_doorbell) {
    wptr = *((ring->adev)->wb.wb + (unsigned long )ring->wptr_offs) >> 2;
  } else {
    me = (unsigned long )(& (ring->adev)->sdma[0].ring) != (unsigned long )ring;
    tmp = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[me] + 13444U, 0);
    wptr = tmp >> 2;
  }
  return (wptr);
}
}
static void sdma_v3_0_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  int me ;
  {
  adev = ring->adev;
  if ((int )ring->use_doorbell) {
    *(adev->wb.wb + (unsigned long )ring->wptr_offs) = ring->wptr << 2;
    amdgpu_mm_wdoorbell(adev, ring->doorbell_index, ring->wptr << 2);
  } else {
    me = (unsigned long )(& (ring->adev)->sdma[0].ring) != (unsigned long )ring;
    amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[me] + 13444U, ring->wptr << 2,
                   0);
  }
  return;
}
}
static void sdma_v3_0_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  u32 vmid ;
  u32 next_rptr ;
  {
  vmid = (unsigned long )ib->vm != (unsigned long )((struct amdgpu_vm *)0) ? (ib->vm)->ids[ring->idx].id & 15U : 0U;
  next_rptr = ring->wptr + 5U;
  goto ldv_49987;
  ldv_49986:
  next_rptr = next_rptr + 1U;
  ldv_49987: ;
  if ((next_rptr & 7U) != 2U) {
    goto ldv_49986;
  } else {
  }
  next_rptr = next_rptr + 6U;
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )ring->next_rptr_gpu_addr & 4294967292U);
  amdgpu_ring_write(ring, (unsigned int )(ring->next_rptr_gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, next_rptr);
  goto ldv_49990;
  ldv_49989:
  amdgpu_ring_write(ring, 0U);
  ldv_49990: ;
  if ((ring->wptr & 7U) != 2U) {
    goto ldv_49989;
  } else {
  }
  amdgpu_ring_write(ring, ((vmid & 15U) << 16) | 4U);
  amdgpu_ring_write(ring, (unsigned int )ib->gpu_addr & 4294967264U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, ib->length_dw);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static void sdma_v3_0_ring_emit_hdp_flush(struct amdgpu_ring *ring )
{
  u32 ref_and_mask ;
  {
  ref_and_mask = 0U;
  if ((unsigned long )(& (ring->adev)->sdma[0].ring) == (unsigned long )ring) {
    ref_and_mask = ref_and_mask | 1024U;
  } else {
    ref_and_mask = ref_and_mask | 2048U;
  }
  amdgpu_ring_write(ring, 872415240U);
  amdgpu_ring_write(ring, 21728U);
  amdgpu_ring_write(ring, 21724U);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, ref_and_mask);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static void sdma_v3_0_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                      unsigned int flags )
{
  bool write64bit ;
  {
  write64bit = (flags & 1U) != 0U;
  amdgpu_ring_write(ring, 5U);
  amdgpu_ring_write(ring, (unsigned int )addr);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )seq);
  if ((int )write64bit) {
    addr = addr + 4ULL;
    amdgpu_ring_write(ring, 5U);
    amdgpu_ring_write(ring, (unsigned int )addr);
    amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
    amdgpu_ring_write(ring, (unsigned int )(seq >> 32ULL));
  } else {
  }
  amdgpu_ring_write(ring, 6U);
  amdgpu_ring_write(ring, 0U);
  return;
}
}
static bool sdma_v3_0_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                          bool emit_wait )
{
  u64 addr ;
  u32 sig ;
  {
  addr = semaphore->gpu_addr;
  sig = (int )emit_wait ? 0U : 1U;
  amdgpu_ring_write(ring, ((sig & 1U) << 30) | 7U);
  amdgpu_ring_write(ring, (unsigned int )addr & 4294967288U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  return (1);
}
}
static void sdma_v3_0_gfx_stop(struct amdgpu_device *adev )
{
  struct amdgpu_ring *sdma0 ;
  struct amdgpu_ring *sdma1 ;
  u32 rb_cntl ;
  u32 ib_cntl ;
  int i ;
  {
  sdma0 = & adev->sdma[0].ring;
  sdma1 = & adev->sdma[1].ring;
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma0 || (unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )sdma1) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.visible_vram_size);
  } else {
  }
  i = 0;
  goto ldv_50019;
  ldv_50018:
  rb_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, 0);
  rb_cntl = rb_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, rb_cntl, 0);
  ib_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13450U, 0);
  ib_cntl = ib_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13450U, ib_cntl, 0);
  i = i + 1;
  ldv_50019: ;
  if (i <= 1) {
    goto ldv_50018;
  } else {
  }
  sdma0->ready = 0;
  sdma1->ready = 0;
  return;
}
}
static void sdma_v3_0_rlc_stop(struct amdgpu_device *adev )
{
  {
  return;
}
}
static void sdma_v3_0_ctx_switch_enable(struct amdgpu_device *adev , bool enable )
{
  u32 f32_cntl ;
  int i ;
  {
  i = 0;
  goto ldv_50031;
  ldv_50030:
  f32_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13316U, 0);
  if ((int )enable) {
    f32_cntl = f32_cntl | 262144U;
  } else {
    f32_cntl = f32_cntl & 4294705151U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13316U, f32_cntl, 0);
  i = i + 1;
  ldv_50031: ;
  if (i <= 1) {
    goto ldv_50030;
  } else {
  }
  return;
}
}
static void sdma_v3_0_enable(struct amdgpu_device *adev , bool enable )
{
  u32 f32_cntl ;
  int i ;
  {
  if (! enable) {
    sdma_v3_0_gfx_stop(adev);
    sdma_v3_0_rlc_stop(adev);
  } else {
  }
  i = 0;
  goto ldv_50040;
  ldv_50039:
  f32_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13330U, 0);
  if ((int )enable) {
    f32_cntl = f32_cntl & 4294967294U;
  } else {
    f32_cntl = f32_cntl | 1U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13330U, f32_cntl, 0);
  i = i + 1;
  ldv_50040: ;
  if (i <= 1) {
    goto ldv_50039;
  } else {
  }
  return;
}
}
static int sdma_v3_0_gfx_resume(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_cntl ;
  u32 ib_cntl ;
  u32 rb_bufsz ;
  u32 wb_offset ;
  u32 doorbell ;
  int i ;
  int j ;
  int r ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  i = 0;
  goto ldv_50058;
  ldv_50057:
  ring = & adev->sdma[i].ring;
  wb_offset = ring->rptr_offs * 4U;
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_50055;
  ldv_50054:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13479U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13480U, 0U, 0);
  j = j + 1;
  ldv_50055: ;
  if (j <= 15) {
    goto ldv_50054;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13321U, 0U, 0);
  tmp = __roundup_pow_of_two((unsigned long )(ring->ring_size / 4U));
  tmp___0 = __ilog2_u64((u64 )tmp);
  rb_bufsz = (u32 )tmp___0;
  rb_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, 0);
  rb_cntl = (rb_cntl & 4294967233U) | ((rb_bufsz << 1) & 62U);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, rb_cntl, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13443U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13444U, 0U, 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13448U, (unsigned int )((adev->wb.gpu_addr + (uint64_t )wb_offset) >> 32ULL),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13449U, ((unsigned int )adev->wb.gpu_addr + wb_offset) & 4294967292U,
                 0);
  rb_cntl = rb_cntl | 4096U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13441U, (u32 )(ring->gpu_addr >> 8),
                 0);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13442U, (u32 )(ring->gpu_addr >> 40),
                 0);
  ring->wptr = 0U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13444U, ring->wptr << 2,
                 0);
  doorbell = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13458U, 0);
  if ((int )ring->use_doorbell) {
    doorbell = (doorbell & 4292870144U) | (ring->doorbell_index & 2097151U);
    doorbell = doorbell | 268435456U;
  } else {
    doorbell = doorbell & 4026531839U;
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13458U, doorbell, 0);
  rb_cntl = rb_cntl | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, rb_cntl, 0);
  ib_cntl = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13450U, 0);
  ib_cntl = ib_cntl | 1U;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13450U, ib_cntl, 0);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  if ((unsigned long )adev->mman.buffer_funcs_ring == (unsigned long )ring) {
    amdgpu_ttm_set_active_vram_size(adev, adev->mc.real_vram_size);
  } else {
  }
  i = i + 1;
  ldv_50058: ;
  if (i <= 1) {
    goto ldv_50057;
  } else {
  }
  return (0);
}
}
static int sdma_v3_0_rlc_resume(struct amdgpu_device *adev )
{
  {
  return (0);
}
}
static int sdma_v3_0_load_microcode(struct amdgpu_device *adev )
{
  struct sdma_firmware_header_v1_0 const *hdr ;
  __le32 const *fw_data ;
  u32 fw_size ;
  int i ;
  int j ;
  __le32 const *tmp ;
  __u32 tmp___0 ;
  {
  if ((unsigned long )adev->sdma[0].fw == (unsigned long )((struct firmware const *)0) || (unsigned long )adev->sdma[1].fw == (unsigned long )((struct firmware const *)0)) {
    return (-22);
  } else {
  }
  sdma_v3_0_enable(adev, 0);
  i = 0;
  goto ldv_50075;
  ldv_50074:
  hdr = (struct sdma_firmware_header_v1_0 const *)(adev->sdma[i].fw)->data;
  amdgpu_ucode_print_sdma_hdr(& hdr->header);
  fw_size = (unsigned int )hdr->header.ucode_size_bytes / 4U;
  adev->sdma[i].fw_version = hdr->header.ucode_version;
  fw_data = (__le32 const *)(adev->sdma[i].fw)->data + (unsigned long )hdr->header.ucode_array_offset_bytes;
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13312U, 0U, 0);
  j = 0;
  goto ldv_50072;
  ldv_50071:
  tmp = fw_data;
  fw_data = fw_data + 1;
  tmp___0 = __le32_to_cpup(tmp);
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13313U, tmp___0, 0);
  j = j + 1;
  ldv_50072: ;
  if ((u32 )j < fw_size) {
    goto ldv_50071;
  } else {
  }
  amdgpu_mm_wreg(adev, (unsigned int )sdma_offsets___1[i] + 13312U, adev->sdma[i].fw_version,
                 0);
  i = i + 1;
  ldv_50075: ;
  if (i <= 1) {
    goto ldv_50074;
  } else {
  }
  return (0);
}
}
static int sdma_v3_0_start(struct amdgpu_device *adev )
{
  int r ;
  {
  if (! adev->firmware.smu_load) {
    r = sdma_v3_0_load_microcode(adev);
    if (r != 0) {
      return (r);
    } else {
    }
  } else {
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 0U);
    if (r != 0) {
      return (-22);
    } else {
    }
    r = (*((adev->smu.smumgr_funcs)->check_fw_load_finish))(adev, 1U);
    if (r != 0) {
      return (-22);
    } else {
    }
  }
  sdma_v3_0_enable(adev, 1);
  sdma_v3_0_ctx_switch_enable(adev, 1);
  r = sdma_v3_0_gfx_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = sdma_v3_0_rlc_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int sdma_v3_0_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ring_lock(ring, 5U);
  if (r != 0) {
    drm_err("amdgpu: dma failed to lock ring %d (%d).\n", ring->idx, r);
    amdgpu_wb_free(adev, index);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )gpu_addr);
  amdgpu_ring_write(ring, (unsigned int )(gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_50092;
  ldv_50091:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_50090;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50092: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50091;
  } else {
  }
  ldv_50090: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static int sdma_v3_0_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ib ib ;
  unsigned int i ;
  unsigned int index ;
  int r ;
  u32 tmp ;
  u64 gpu_addr ;
  {
  adev = ring->adev;
  tmp = 0U;
  r = amdgpu_wb_get(adev, & index);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate wb slot\n",
            r);
    return (r);
  } else {
  }
  gpu_addr = adev->wb.gpu_addr + (uint64_t )(index * 4U);
  tmp = 3405700781U;
  *(adev->wb.wb + (unsigned long )index) = tmp;
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 256U, & ib);
  if (r != 0) {
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    return (r);
  } else {
  }
  *(ib.ptr) = 2U;
  *(ib.ptr + 1UL) = (unsigned int )gpu_addr;
  *(ib.ptr + 2UL) = (unsigned int )(gpu_addr >> 32ULL);
  *(ib.ptr + 3UL) = 1U;
  *(ib.ptr + 4UL) = 3735928559U;
  *(ib.ptr + 5UL) = 0U;
  *(ib.ptr + 6UL) = 0U;
  *(ib.ptr + 7UL) = 0U;
  ib.length_dw = 8U;
  r = amdgpu_ib_schedule(adev, 1U, & ib, (void *)0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_fence_wait(ib.fence, 0);
  if (r != 0) {
    amdgpu_ib_free(adev, & ib);
    amdgpu_wb_free(adev, index);
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    return (r);
  } else {
  }
  i = 0U;
  goto ldv_50105;
  ldv_50104:
  tmp = *(adev->wb.wb + (unsigned long )index);
  if (tmp == 3735928559U) {
    goto ldv_50103;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50105: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50104;
  } else {
  }
  ldv_50103: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ib test on ring %d succeeded in %u usecs\n", ((ib.fence)->ring)->idx,
           i);
  } else {
    drm_err("amdgpu: ib test failed (0x%08X)\n", tmp);
    r = -22;
  }
  amdgpu_ib_free(adev, & ib);
  amdgpu_wb_free(adev, index);
  return (r);
}
}
static void sdma_v3_0_vm_copy_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t src ,
                                  unsigned int count )
{
  unsigned int bytes ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  {
  goto ldv_50114;
  ldv_50113:
  bytes = count * 8U;
  if (bytes > 2097144U) {
    bytes = 2097144U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 1U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = bytes;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = 0U;
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = (unsigned int )src;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (unsigned int )(src >> 32ULL);
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )pe;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(pe >> 32ULL);
  pe = (uint64_t )bytes + pe;
  src = (uint64_t )bytes + src;
  count = count - bytes / 8U;
  ldv_50114: ;
  if (count != 0U) {
    goto ldv_50113;
  } else {
  }
  return;
}
}
static void sdma_v3_0_vm_write_pte(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                   unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  {
  goto ldv_50130;
  ldv_50129:
  ndw = count * 2U;
  if (ndw > 1048574U) {
    ndw = 1048574U;
  } else {
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 2U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = ndw;
  goto ldv_50127;
  ldv_50126: ;
  if ((flags & 2U) != 0U) {
    value = amdgpu_vm_map_gart((ib->ring)->adev, addr);
    value = value & 0xfffffffffffff000ULL;
  } else
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  addr = (uint64_t )incr + addr;
  value = (uint64_t )flags | value;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = (u32 )value;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (unsigned int )(value >> 32ULL);
  ndw = ndw - 2U;
  count = count - 1U;
  pe = pe + 8ULL;
  ldv_50127: ;
  if (ndw != 0U) {
    goto ldv_50126;
  } else {
  }
  ldv_50130: ;
  if (count != 0U) {
    goto ldv_50129;
  } else {
  }
  return;
}
}
static void sdma_v3_0_vm_set_pte_pde(struct amdgpu_ib *ib , uint64_t pe , uint64_t addr ,
                                     unsigned int count , u32 incr , u32 flags )
{
  uint64_t value ;
  unsigned int ndw ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  {
  goto ldv_50143;
  ldv_50142:
  ndw = count;
  if (ndw > 524287U) {
    ndw = 524287U;
  } else {
  }
  if ((int )flags & 1) {
    value = addr;
  } else {
    value = 0ULL;
  }
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 12U;
  tmp___0 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___0) = (u32 )pe;
  tmp___1 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___1) = (unsigned int )(pe >> 32ULL);
  tmp___2 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___2) = flags;
  tmp___3 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___3) = 0U;
  tmp___4 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___4) = (u32 )value;
  tmp___5 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___5) = (unsigned int )(value >> 32ULL);
  tmp___6 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___6) = incr;
  tmp___7 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___7) = 0U;
  tmp___8 = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp___8) = ndw;
  pe = (uint64_t )(ndw * 8U) + pe;
  addr = (uint64_t )(ndw * incr) + addr;
  count = count - ndw;
  ldv_50143: ;
  if (count != 0U) {
    goto ldv_50142;
  } else {
  }
  return;
}
}
static void sdma_v3_0_vm_pad_ib(struct amdgpu_ib *ib )
{
  u32 tmp ;
  {
  goto ldv_50149;
  ldv_50148:
  tmp = ib->length_dw;
  ib->length_dw = ib->length_dw + 1U;
  *(ib->ptr + (unsigned long )tmp) = 0U;
  ldv_50149: ;
  if ((ib->length_dw & 7U) != 0U) {
    goto ldv_50148;
  } else {
  }
  return;
}
}
static void sdma_v3_0_ring_emit_vm_flush(struct amdgpu_ring *ring , unsigned int vm_id ,
                                         uint64_t pd_addr )
{
  {
  amdgpu_ring_write(ring, 4026531854U);
  if (vm_id <= 7U) {
    amdgpu_ring_write(ring, vm_id + 1359U);
  } else {
    amdgpu_ring_write(ring, vm_id + 1286U);
  }
  amdgpu_ring_write(ring, (u32 )(pd_addr >> 12));
  amdgpu_ring_write(ring, 4026531854U);
  amdgpu_ring_write(ring, 1310U);
  amdgpu_ring_write(ring, (u32 )(1 << (int )vm_id));
  amdgpu_ring_write(ring, 8U);
  amdgpu_ring_write(ring, 5240U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 268369930U);
  return;
}
}
static int sdma_v3_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v3_0_set_ring_funcs(adev);
  sdma_v3_0_set_buffer_funcs(adev);
  sdma_v3_0_set_vm_pte_funcs(adev);
  sdma_v3_0_set_irq_funcs(adev);
  return (0);
}
}
static int sdma_v3_0_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 224U, & adev->sdma_trap_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 241U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_irq_add_id(adev, 247U, & adev->sdma_illegal_inst_irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = sdma_v3_0_init_microcode(adev);
  if (r != 0) {
    drm_err("Failed to load sdma firmware!\n");
    return (r);
  } else {
  }
  ring = & adev->sdma[0].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 1;
  ring->doorbell_index = 480U;
  ring = & adev->sdma[1].ring;
  ring->ring_obj = (struct amdgpu_bo *)0;
  ring->use_doorbell = 1;
  ring->doorbell_index = 481U;
  ring = & adev->sdma[0].ring;
  sprintf((char *)(& ring->name), "sdma0");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 0U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->sdma[1].ring;
  sprintf((char *)(& ring->name), "sdma1");
  r = amdgpu_ring_init(adev, ring, 262144U, 0U, 15U, & adev->sdma_trap_irq, 1U, 2);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int sdma_v3_0_sw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  amdgpu_ring_fini(& adev->sdma[0].ring);
  amdgpu_ring_fini(& adev->sdma[1].ring);
  return (0);
}
}
static int sdma_v3_0_hw_init(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v3_0_init_golden_registers(adev);
  r = sdma_v3_0_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int sdma_v3_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  sdma_v3_0_ctx_switch_enable(adev, 0);
  sdma_v3_0_enable(adev, 0);
  return (0);
}
}
static int sdma_v3_0_suspend(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = sdma_v3_0_hw_fini((void *)adev);
  return (tmp);
}
}
static int sdma_v3_0_resume(void *handle )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = sdma_v3_0_hw_init((void *)adev);
  return (tmp);
}
}
static bool sdma_v3_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 96U) != 0U) {
    return (0);
  } else {
  }
  return (1);
}
}
static int sdma_v3_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  u32 tmp ;
  struct amdgpu_device *adev ;
  u32 tmp___0 ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_50199;
  ldv_50198:
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0 & 96U;
  if (tmp == 0U) {
    return (0);
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_50199: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_50198;
  } else {
  }
  return (-110);
}
}
static void sdma_v3_0_print_status(void *handle )
{
  int i ;
  int j ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "VI SDMA registers\n");
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  _dev_info((struct device const *)adev->dev, "  SRBM_STATUS2=0x%08X\n", tmp);
  i = 0;
  goto ldv_50211;
  ldv_50210:
  tmp___0 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13325U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_STATUS_REG=0x%08X\n", i,
            tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13330U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_F32_CNTL=0x%08X\n", i, tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13316U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_CNTL=0x%08X\n", i, tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13321U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_SEM_WAIT_FAIL_TIMER_CNTL=0x%08X\n",
            i, tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13450U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_IB_CNTL=0x%08X\n", i,
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13440U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_CNTL=0x%08X\n", i,
            tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13443U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR=0x%08X\n", i,
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13444U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_WPTR=0x%08X\n", i,
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13448U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_HI=0x%08X\n",
            i, tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13449U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_RPTR_ADDR_LO=0x%08X\n",
            i, tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13441U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE=0x%08X\n", i,
            tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13442U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_RB_BASE_HI=0x%08X\n",
            i, tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13458U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_DOORBELL=0x%08X\n",
            i, tmp___12);
  mutex_lock_nested(& adev->srbm_mutex, 0U);
  j = 0;
  goto ldv_50208;
  ldv_50207:
  vi_srbm_select(adev, 0U, 0U, 0U, (u32 )j);
  _dev_info((struct device const *)adev->dev, "  VM %d:\n", j);
  tmp___13 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13479U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_VIRTUAL_ADDR=0x%08X\n",
            i, tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, (unsigned int )sdma_offsets___1[i] + 13480U, 0);
  _dev_info((struct device const *)adev->dev, "  SDMA%d_GFX_APE1_CNTL=0x%08X\n",
            i, tmp___14);
  j = j + 1;
  ldv_50208: ;
  if (j <= 15) {
    goto ldv_50207;
  } else {
  }
  vi_srbm_select(adev, 0U, 0U, 0U, 0U);
  mutex_unlock(& adev->srbm_mutex);
  i = i + 1;
  ldv_50211: ;
  if (i <= 1) {
    goto ldv_50210;
  } else {
  }
  return;
}
}
static int sdma_v3_0_soft_reset(void *handle )
{
  u32 srbm_soft_reset ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  srbm_soft_reset = 0U;
  adev = (struct amdgpu_device *)handle;
  tmp___0 = amdgpu_mm_rreg(adev, 915U, 0);
  tmp = tmp___0;
  if ((tmp & 32U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13330U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 13330U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 1048576U;
  } else {
  }
  if ((tmp & 64U) != 0U) {
    tmp = amdgpu_mm_rreg(adev, 13842U, 0);
    tmp = tmp & 4294967294U;
    amdgpu_mm_wreg(adev, 13842U, tmp, 0);
    srbm_soft_reset = srbm_soft_reset | 64U;
  } else {
  }
  if (srbm_soft_reset != 0U) {
    sdma_v3_0_print_status((void *)adev);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    tmp = tmp | srbm_soft_reset;
    _dev_info((struct device const *)adev->dev, "SRBM_SOFT_RESET=0x%08X\n", tmp);
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    tmp = ~ srbm_soft_reset & tmp;
    amdgpu_mm_wreg(adev, 920U, tmp, 0);
    tmp = amdgpu_mm_rreg(adev, 920U, 0);
    __const_udelay(214750UL);
    sdma_v3_0_print_status((void *)adev);
  } else {
  }
  return (0);
}
}
static int sdma_v3_0_set_trap_irq_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 sdma_cntl ;
  {
  switch (type) {
  case 0U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_50228;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13316U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13316U, sdma_cntl, 0);
  goto ldv_50228;
  default: ;
  goto ldv_50228;
  }
  ldv_50228: ;
  goto ldv_50231;
  case 1U: ;
  switch ((unsigned int )state) {
  case 0U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl & 4294967294U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_50234;
  case 1U:
  sdma_cntl = amdgpu_mm_rreg(adev, 13828U, 0);
  sdma_cntl = sdma_cntl | 1U;
  amdgpu_mm_wreg(adev, 13828U, sdma_cntl, 0);
  goto ldv_50234;
  default: ;
  goto ldv_50234;
  }
  ldv_50234: ;
  goto ldv_50231;
  default: ;
  goto ldv_50231;
  }
  ldv_50231: ;
  return (0);
}
}
static int sdma_v3_0_process_trap_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  u8 instance_id ;
  u8 queue_id ;
  long tmp ;
  {
  instance_id = (unsigned int )((u8 )entry->ring_id) & 3U;
  queue_id = (u8 )((entry->ring_id & 12U) >> 2);
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("sdma_v3_0_process_trap_irq", "IH: SDMA trap\n");
  } else {
  }
  switch ((int )instance_id) {
  case 0: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[0].ring);
  goto ldv_50248;
  case 1: ;
  goto ldv_50248;
  case 2: ;
  goto ldv_50248;
  }
  ldv_50248: ;
  goto ldv_50251;
  case 1: ;
  switch ((int )queue_id) {
  case 0:
  amdgpu_fence_process(& adev->sdma[1].ring);
  goto ldv_50254;
  case 1: ;
  goto ldv_50254;
  case 2: ;
  goto ldv_50254;
  }
  ldv_50254: ;
  goto ldv_50251;
  }
  ldv_50251: ;
  return (0);
}
}
static int sdma_v3_0_process_illegal_inst_irq(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                              struct amdgpu_iv_entry *entry )
{
  {
  drm_err("Illegal instruction in SDMA command stream\n");
  schedule_work___8(& adev->reset_work);
  return (0);
}
}
static int sdma_v3_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int sdma_v3_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  {
  return (0);
}
}
struct amd_ip_funcs const sdma_v3_0_ip_funcs =
     {& sdma_v3_0_early_init, (int (*)(void * ))0, & sdma_v3_0_sw_init, & sdma_v3_0_sw_fini,
    & sdma_v3_0_hw_init, & sdma_v3_0_hw_fini, & sdma_v3_0_suspend, & sdma_v3_0_resume,
    & sdma_v3_0_is_idle, & sdma_v3_0_wait_for_idle, & sdma_v3_0_soft_reset, & sdma_v3_0_print_status,
    & sdma_v3_0_set_clockgating_state, & sdma_v3_0_set_powergating_state};
static bool sdma_v3_0_ring_is_lockup(struct amdgpu_ring *ring )
{
  bool tmp ;
  bool tmp___0 ;
  {
  tmp = sdma_v3_0_is_idle((void *)ring->adev);
  if ((int )tmp) {
    amdgpu_ring_lockup_update(ring);
    return (0);
  } else {
  }
  tmp___0 = amdgpu_ring_test_lockup(ring);
  return (tmp___0);
}
}
static struct amdgpu_ring_funcs const sdma_v3_0_ring_funcs =
     {& sdma_v3_0_ring_get_rptr, & sdma_v3_0_ring_get_wptr, & sdma_v3_0_ring_set_wptr,
    (int (*)(struct amdgpu_cs_parser * , u32 ))0, & sdma_v3_0_ring_emit_ib, & sdma_v3_0_ring_emit_fence,
    & sdma_v3_0_ring_emit_semaphore, & sdma_v3_0_ring_emit_vm_flush, & sdma_v3_0_ring_emit_hdp_flush,
    0, & sdma_v3_0_ring_test_ring, & sdma_v3_0_ring_test_ib, & sdma_v3_0_ring_is_lockup};
static void sdma_v3_0_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma[0].ring.funcs = & sdma_v3_0_ring_funcs;
  adev->sdma[1].ring.funcs = & sdma_v3_0_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const sdma_v3_0_trap_irq_funcs = {& sdma_v3_0_set_trap_irq_state, & sdma_v3_0_process_trap_irq};
static struct amdgpu_irq_src_funcs const sdma_v3_0_illegal_inst_irq_funcs = {0, & sdma_v3_0_process_illegal_inst_irq};
static void sdma_v3_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->sdma_trap_irq.num_types = 2U;
  adev->sdma_trap_irq.funcs = & sdma_v3_0_trap_irq_funcs;
  adev->sdma_illegal_inst_irq.funcs = & sdma_v3_0_illegal_inst_irq_funcs;
  return;
}
}
static void sdma_v3_0_emit_copy_buffer(struct amdgpu_ring *ring , uint64_t src_offset ,
                                       uint64_t dst_offset , u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_write(ring, byte_count);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, (unsigned int )src_offset);
  amdgpu_ring_write(ring, (unsigned int )(src_offset >> 32ULL));
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  return;
}
}
static void sdma_v3_0_emit_fill_buffer(struct amdgpu_ring *ring , u32 src_data , uint64_t dst_offset ,
                                       u32 byte_count )
{
  {
  amdgpu_ring_write(ring, 11U);
  amdgpu_ring_write(ring, (unsigned int )dst_offset);
  amdgpu_ring_write(ring, (unsigned int )(dst_offset >> 32ULL));
  amdgpu_ring_write(ring, src_data);
  amdgpu_ring_write(ring, byte_count);
  return;
}
}
static struct amdgpu_buffer_funcs const sdma_v3_0_buffer_funcs = {2097151U, 7U, & sdma_v3_0_emit_copy_buffer, 2097151U, 5U, & sdma_v3_0_emit_fill_buffer};
static void sdma_v3_0_set_buffer_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->mman.buffer_funcs == (unsigned long )((struct amdgpu_buffer_funcs const *)0)) {
    adev->mman.buffer_funcs = & sdma_v3_0_buffer_funcs;
    adev->mman.buffer_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
static struct amdgpu_vm_pte_funcs const sdma_v3_0_vm_pte_funcs = {& sdma_v3_0_vm_copy_pte, & sdma_v3_0_vm_write_pte, & sdma_v3_0_vm_set_pte_pde,
    & sdma_v3_0_vm_pad_ib};
static void sdma_v3_0_set_vm_pte_funcs(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->vm_manager.vm_pte_funcs == (unsigned long )((struct amdgpu_vm_pte_funcs const *)0)) {
    adev->vm_manager.vm_pte_funcs = & sdma_v3_0_vm_pte_funcs;
    adev->vm_manager.vm_pte_funcs_ring = & adev->sdma[0].ring;
  } else {
  }
  return;
}
}
extern int ldv_release_27(void) ;
extern int ldv_probe_22(void) ;
extern int ldv_release_22(void) ;
extern int ldv_probe_27(void) ;
int ldv_retval_3 ;
int ldv_retval_2 ;
void ldv_initialize_amdgpu_irq_src_funcs_25(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  sdma_v3_0_trap_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  sdma_v3_0_trap_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_26(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  sdma_v3_0_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_vm_pte_funcs_22(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(272UL);
  sdma_v3_0_vm_pte_funcs_group0 = (struct amdgpu_ib *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_buffer_funcs_23(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  sdma_v3_0_buffer_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_main_exported_27(void)
{
  void *ldvarg55 ;
  void *tmp ;
  enum amd_powergating_state ldvarg64 ;
  void *ldvarg66 ;
  void *tmp___0 ;
  void *ldvarg61 ;
  void *tmp___1 ;
  void *ldvarg58 ;
  void *tmp___2 ;
  void *ldvarg57 ;
  void *tmp___3 ;
  void *ldvarg65 ;
  void *tmp___4 ;
  void *ldvarg62 ;
  void *tmp___5 ;
  void *ldvarg56 ;
  void *tmp___6 ;
  void *ldvarg69 ;
  void *tmp___7 ;
  void *ldvarg59 ;
  void *tmp___8 ;
  void *ldvarg68 ;
  void *tmp___9 ;
  enum amd_clockgating_state ldvarg60 ;
  void *ldvarg67 ;
  void *tmp___10 ;
  void *ldvarg63 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg55 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg66 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg61 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg58 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg57 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg65 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg62 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg56 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg69 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg59 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg68 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg67 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg63 = tmp___11;
  ldv_memset((void *)(& ldvarg64), 0, 4UL);
  ldv_memset((void *)(& ldvarg60), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_hw_fini(ldvarg69);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_hw_fini(ldvarg69);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_hw_fini(ldvarg69);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 1: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_print_status(ldvarg68);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_print_status(ldvarg68);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_print_status(ldvarg68);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 2: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_early_init(ldvarg67);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_early_init(ldvarg67);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_early_init(ldvarg67);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 3: ;
  if (ldv_state_variable_27 == 2) {
    ldv_retval_3 = sdma_v3_0_suspend(ldvarg66);
    if (ldv_retval_3 == 0) {
      ldv_state_variable_27 = 3;
    } else {
    }
  } else {
  }
  goto ldv_50344;
  case 4: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_sw_init(ldvarg65);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_sw_init(ldvarg65);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_sw_init(ldvarg65);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 5: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_set_powergating_state(ldvarg63, ldvarg64);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_set_powergating_state(ldvarg63, ldvarg64);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_set_powergating_state(ldvarg63, ldvarg64);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 6: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_wait_for_idle(ldvarg62);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_wait_for_idle(ldvarg62);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_wait_for_idle(ldvarg62);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 7: ;
  if (ldv_state_variable_27 == 3) {
    ldv_retval_2 = sdma_v3_0_resume(ldvarg61);
    if (ldv_retval_2 == 0) {
      ldv_state_variable_27 = 2;
    } else {
    }
  } else {
  }
  goto ldv_50344;
  case 8: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_set_clockgating_state(ldvarg59, ldvarg60);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_set_clockgating_state(ldvarg59, ldvarg60);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_set_clockgating_state(ldvarg59, ldvarg60);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 9: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_hw_init(ldvarg58);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_hw_init(ldvarg58);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_hw_init(ldvarg58);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 10: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_soft_reset(ldvarg57);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_soft_reset(ldvarg57);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_soft_reset(ldvarg57);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 11: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_sw_fini(ldvarg56);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_sw_fini(ldvarg56);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_sw_fini(ldvarg56);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 12: ;
  if (ldv_state_variable_27 == 1) {
    sdma_v3_0_is_idle(ldvarg55);
    ldv_state_variable_27 = 1;
  } else {
  }
  if (ldv_state_variable_27 == 3) {
    sdma_v3_0_is_idle(ldvarg55);
    ldv_state_variable_27 = 3;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    sdma_v3_0_is_idle(ldvarg55);
    ldv_state_variable_27 = 2;
  } else {
  }
  goto ldv_50344;
  case 13: ;
  if (ldv_state_variable_27 == 3) {
    ldv_release_27();
    ldv_state_variable_27 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_27 == 2) {
    ldv_release_27();
    ldv_state_variable_27 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50344;
  case 14: ;
  if (ldv_state_variable_27 == 1) {
    ldv_probe_27();
    ldv_state_variable_27 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50344;
  default:
  ldv_stop();
  }
  ldv_50344: ;
  return;
}
}
void ldv_main_exported_25(void)
{
  enum amdgpu_interrupt_state ldvarg899 ;
  struct amdgpu_iv_entry *ldvarg897 ;
  void *tmp ;
  unsigned int ldvarg898 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg897 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg899), 0, 4UL);
  ldv_memset((void *)(& ldvarg898), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_25 == 1) {
    sdma_v3_0_set_trap_irq_state(sdma_v3_0_trap_irq_funcs_group0, sdma_v3_0_trap_irq_funcs_group1,
                                 ldvarg898, ldvarg899);
    ldv_state_variable_25 = 1;
  } else {
  }
  goto ldv_50367;
  case 1: ;
  if (ldv_state_variable_25 == 1) {
    sdma_v3_0_process_trap_irq(sdma_v3_0_trap_irq_funcs_group0, sdma_v3_0_trap_irq_funcs_group1,
                               ldvarg897);
    ldv_state_variable_25 = 1;
  } else {
  }
  goto ldv_50367;
  default:
  ldv_stop();
  }
  ldv_50367: ;
  return;
}
}
void ldv_main_exported_22(void)
{
  u32 ldvarg1023 ;
  u32 ldvarg1019 ;
  u32 ldvarg1031 ;
  unsigned int ldvarg1022 ;
  uint64_t ldvarg1028 ;
  unsigned int ldvarg1026 ;
  uint64_t ldvarg1020 ;
  uint64_t ldvarg1025 ;
  uint64_t ldvarg1029 ;
  uint64_t ldvarg1024 ;
  u32 ldvarg1027 ;
  uint64_t ldvarg1021 ;
  unsigned int ldvarg1030 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg1023), 0, 4UL);
  ldv_memset((void *)(& ldvarg1019), 0, 4UL);
  ldv_memset((void *)(& ldvarg1031), 0, 4UL);
  ldv_memset((void *)(& ldvarg1022), 0, 4UL);
  ldv_memset((void *)(& ldvarg1028), 0, 8UL);
  ldv_memset((void *)(& ldvarg1026), 0, 4UL);
  ldv_memset((void *)(& ldvarg1020), 0, 8UL);
  ldv_memset((void *)(& ldvarg1025), 0, 8UL);
  ldv_memset((void *)(& ldvarg1029), 0, 8UL);
  ldv_memset((void *)(& ldvarg1024), 0, 8UL);
  ldv_memset((void *)(& ldvarg1027), 0, 4UL);
  ldv_memset((void *)(& ldvarg1021), 0, 8UL);
  ldv_memset((void *)(& ldvarg1030), 0, 4UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_22 == 2) {
    sdma_v3_0_vm_write_pte(sdma_v3_0_vm_pte_funcs_group0, ldvarg1029, ldvarg1028,
                           ldvarg1030, ldvarg1031, ldvarg1027);
    ldv_state_variable_22 = 2;
  } else {
  }
  goto ldv_50387;
  case 1: ;
  if (ldv_state_variable_22 == 2) {
    sdma_v3_0_vm_copy_pte(sdma_v3_0_vm_pte_funcs_group0, ldvarg1025, ldvarg1024, ldvarg1026);
    ldv_state_variable_22 = 2;
  } else {
  }
  if (ldv_state_variable_22 == 1) {
    sdma_v3_0_vm_copy_pte(sdma_v3_0_vm_pte_funcs_group0, ldvarg1025, ldvarg1024, ldvarg1026);
    ldv_state_variable_22 = 1;
  } else {
  }
  goto ldv_50387;
  case 2: ;
  if (ldv_state_variable_22 == 2) {
    sdma_v3_0_vm_pad_ib(sdma_v3_0_vm_pte_funcs_group0);
    ldv_state_variable_22 = 2;
  } else {
  }
  if (ldv_state_variable_22 == 1) {
    sdma_v3_0_vm_pad_ib(sdma_v3_0_vm_pte_funcs_group0);
    ldv_state_variable_22 = 1;
  } else {
  }
  goto ldv_50387;
  case 3: ;
  if (ldv_state_variable_22 == 2) {
    sdma_v3_0_vm_set_pte_pde(sdma_v3_0_vm_pte_funcs_group0, ldvarg1021, ldvarg1020,
                             ldvarg1022, ldvarg1023, ldvarg1019);
    ldv_state_variable_22 = 2;
  } else {
  }
  if (ldv_state_variable_22 == 1) {
    sdma_v3_0_vm_set_pte_pde(sdma_v3_0_vm_pte_funcs_group0, ldvarg1021, ldvarg1020,
                             ldvarg1022, ldvarg1023, ldvarg1019);
    ldv_state_variable_22 = 1;
  } else {
  }
  goto ldv_50387;
  case 4: ;
  if (ldv_state_variable_22 == 2) {
    ldv_release_22();
    ldv_state_variable_22 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_50387;
  case 5: ;
  if (ldv_state_variable_22 == 1) {
    ldv_probe_22();
    ldv_state_variable_22 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_50387;
  default:
  ldv_stop();
  }
  ldv_50387: ;
  return;
}
}
void ldv_main_exported_24(void)
{
  struct amdgpu_iv_entry *ldvarg386 ;
  void *tmp ;
  struct amdgpu_irq_src *ldvarg387 ;
  void *tmp___0 ;
  struct amdgpu_device *ldvarg388 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg386 = (struct amdgpu_iv_entry *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg387 = (struct amdgpu_irq_src *)tmp___0;
  tmp___1 = ldv_init_zalloc(23352UL);
  ldvarg388 = (struct amdgpu_device *)tmp___1;
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_24 == 1) {
    sdma_v3_0_process_illegal_inst_irq(ldvarg388, ldvarg387, ldvarg386);
    ldv_state_variable_24 = 1;
  } else {
  }
  goto ldv_50401;
  default:
  ldv_stop();
  }
  ldv_50401: ;
  return;
}
}
void ldv_main_exported_26(void)
{
  uint64_t ldvarg537 ;
  struct amdgpu_semaphore *ldvarg540 ;
  void *tmp ;
  struct amdgpu_ib *ldvarg541 ;
  void *tmp___0 ;
  uint64_t ldvarg543 ;
  unsigned int ldvarg538 ;
  unsigned int ldvarg544 ;
  uint64_t ldvarg542 ;
  bool ldvarg539 ;
  int tmp___1 ;
  {
  tmp = ldv_init_zalloc(24UL);
  ldvarg540 = (struct amdgpu_semaphore *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg541 = (struct amdgpu_ib *)tmp___0;
  ldv_memset((void *)(& ldvarg537), 0, 8UL);
  ldv_memset((void *)(& ldvarg543), 0, 8UL);
  ldv_memset((void *)(& ldvarg538), 0, 4UL);
  ldv_memset((void *)(& ldvarg544), 0, 4UL);
  ldv_memset((void *)(& ldvarg542), 0, 8UL);
  ldv_memset((void *)(& ldvarg539), 0, 1UL);
  tmp___1 = __VERIFIER_nondet_int();
  switch (tmp___1) {
  case 0: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_emit_fence(sdma_v3_0_ring_funcs_group0, ldvarg543, ldvarg542, ldvarg544);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 1: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_get_rptr(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 2: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_test_ring(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 3: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_emit_hdp_flush(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 4: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_set_wptr(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 5: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_get_wptr(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 6: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_emit_ib(sdma_v3_0_ring_funcs_group0, ldvarg541);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 7: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_emit_semaphore(sdma_v3_0_ring_funcs_group0, ldvarg540, (int )ldvarg539);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 8: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_test_ib(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 9: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_is_lockup(sdma_v3_0_ring_funcs_group0);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  case 10: ;
  if (ldv_state_variable_26 == 1) {
    sdma_v3_0_ring_emit_vm_flush(sdma_v3_0_ring_funcs_group0, ldvarg538, ldvarg537);
    ldv_state_variable_26 = 1;
  } else {
  }
  goto ldv_50415;
  default:
  ldv_stop();
  }
  ldv_50415: ;
  return;
}
}
void ldv_main_exported_23(void)
{
  uint64_t ldvarg423 ;
  u32 ldvarg427 ;
  u32 ldvarg425 ;
  u32 ldvarg428 ;
  uint64_t ldvarg424 ;
  uint64_t ldvarg426 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg423), 0, 8UL);
  ldv_memset((void *)(& ldvarg427), 0, 4UL);
  ldv_memset((void *)(& ldvarg425), 0, 4UL);
  ldv_memset((void *)(& ldvarg428), 0, 4UL);
  ldv_memset((void *)(& ldvarg424), 0, 8UL);
  ldv_memset((void *)(& ldvarg426), 0, 8UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_23 == 1) {
    sdma_v3_0_emit_fill_buffer(sdma_v3_0_buffer_funcs_group0, ldvarg427, ldvarg426,
                               ldvarg428);
    ldv_state_variable_23 = 1;
  } else {
  }
  goto ldv_50437;
  case 1: ;
  if (ldv_state_variable_23 == 1) {
    sdma_v3_0_emit_copy_buffer(sdma_v3_0_buffer_funcs_group0, ldvarg424, ldvarg423,
                               ldvarg425);
    ldv_state_variable_23 = 1;
  } else {
  }
  goto ldv_50437;
  default:
  ldv_stop();
  }
  ldv_50437: ;
  return;
}
}
bool ldv_queue_work_on_967(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_968(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_969(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_970(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_971(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
extern unsigned long __msecs_to_jiffies(unsigned int const ) ;
__inline static unsigned long msecs_to_jiffies(unsigned int const m )
{
  unsigned long tmp___0 ;
  {
  tmp___0 = __msecs_to_jiffies(m);
  return (tmp___0);
}
}
bool ldv_queue_work_on_981(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_983(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_982(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_985(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_984(struct workqueue_struct *ldv_func_arg1 ) ;
extern bool cancel_delayed_work_sync(struct delayed_work * ) ;
bool ldv_cancel_delayed_work_sync_986(struct delayed_work *ldv_func_arg1 ) ;
__inline static bool queue_delayed_work___0(struct workqueue_struct *wq , struct delayed_work *dwork ,
                                            unsigned long delay )
{
  bool tmp ;
  {
  tmp = ldv_queue_delayed_work_on_982(8192, wq, dwork, delay);
  return (tmp);
}
}
__inline static bool schedule_delayed_work(struct delayed_work *dwork , unsigned long delay )
{
  bool tmp ;
  {
  tmp = queue_delayed_work___0(system_wq, dwork, delay);
  return (tmp);
}
}
void invoke_work_8(void) ;
void disable_work_8(struct work_struct *work ) ;
void activate_work_8(struct work_struct *work , int state ) ;
void call_and_disable_work_8(struct work_struct *work ) ;
void call_and_disable_all_8(int state ) ;
__inline static u32 amdgpu_get_ib_value(struct amdgpu_cs_parser *p , u32 ib_idx ,
                                        int idx )
{
  {
  return (*((p->ibs + (unsigned long )ib_idx)->ptr + (unsigned long )idx));
}
}
static void amdgpu_uvd_note_usage(struct amdgpu_device *adev ) ;
static void amdgpu_uvd_idle_work_handler(struct work_struct *work ) ;
int amdgpu_uvd_sw_init(struct amdgpu_device *adev )
{
  unsigned long bo_size ;
  char const *fw_name ;
  struct common_firmware_header const *hdr ;
  unsigned int version_major ;
  unsigned int version_minor ;
  unsigned int family_id ;
  int i ;
  int r ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  int tmp ;
  {
  __init_work(& adev->uvd.idle_work.work, 0);
  __constr_expr_0.counter = 137438953408L;
  adev->uvd.idle_work.work.data = __constr_expr_0;
  lockdep_init_map(& adev->uvd.idle_work.work.lockdep_map, "(&(&adev->uvd.idle_work)->work)",
                   & __key, 0);
  INIT_LIST_HEAD(& adev->uvd.idle_work.work.entry);
  adev->uvd.idle_work.work.func = & amdgpu_uvd_idle_work_handler;
  init_timer_key(& adev->uvd.idle_work.timer, 2097152U, "(&(&adev->uvd.idle_work)->timer)",
                 & __key___0);
  adev->uvd.idle_work.timer.function = & delayed_work_timer_fn;
  adev->uvd.idle_work.timer.data = (unsigned long )(& adev->uvd.idle_work);
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  fw_name = "radeon/bonaire_uvd.bin";
  goto ldv_43749;
  case 2U:
  fw_name = "radeon/kabini_uvd.bin";
  goto ldv_43749;
  case 1U:
  fw_name = "radeon/kaveri_uvd.bin";
  goto ldv_43749;
  case 3U:
  fw_name = "radeon/hawaii_uvd.bin";
  goto ldv_43749;
  case 4U:
  fw_name = "radeon/mullins_uvd.bin";
  goto ldv_43749;
  case 6U:
  fw_name = "amdgpu/tonga_uvd.bin";
  goto ldv_43749;
  case 7U:
  fw_name = "amdgpu/carrizo_uvd.bin";
  goto ldv_43749;
  default: ;
  return (-22);
  }
  ldv_43749:
  r = request_firmware(& adev->uvd.fw, fw_name, adev->dev);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "amdgpu_uvd: Can\'t load firmware \"%s\"\n",
            fw_name);
    return (r);
  } else {
  }
  r = amdgpu_ucode_validate(adev->uvd.fw);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "amdgpu_uvd: Can\'t validate firmware \"%s\"\n",
            fw_name);
    release_firmware(adev->uvd.fw);
    adev->uvd.fw = (struct firmware const *)0;
    return (r);
  } else {
  }
  hdr = (struct common_firmware_header const *)(adev->uvd.fw)->data;
  family_id = (unsigned int )hdr->ucode_version & 255U;
  version_major = (unsigned int )(hdr->ucode_version >> 24);
  version_minor = (unsigned int )(hdr->ucode_version >> 8) & 255U;
  printk("\016[drm] Found UVD firmware Version: %hu.%hu Family ID: %hu\n", version_major,
         version_minor, family_id);
  bo_size = (unsigned long )((((unsigned int )hdr->ucode_size_bytes + 4103U) & 4294963200U) + 2097152U);
  r = amdgpu_bo_create(adev, bo_size, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & adev->uvd.vcpu_bo);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate UVD bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_reserve(adev->uvd.vcpu_bo, 0);
  if (r != 0) {
    amdgpu_bo_unref(& adev->uvd.vcpu_bo);
    dev_err((struct device const *)adev->dev, "(%d) failed to reserve UVD bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->uvd.vcpu_bo, 4U, & adev->uvd.gpu_addr);
  if (r != 0) {
    amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
    amdgpu_bo_unref(& adev->uvd.vcpu_bo);
    dev_err((struct device const *)adev->dev, "(%d) UVD bo pin failed\n", r);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->uvd.vcpu_bo, & adev->uvd.cpu_addr);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) UVD map failed\n", r);
    return (r);
  } else {
  }
  amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
  i = 0;
  goto ldv_43758;
  ldv_43757:
  atomic_set((atomic_t *)(& adev->uvd.handles) + (unsigned long )i, 0);
  adev->uvd.filp[i] = (struct drm_file *)0;
  i = i + 1;
  ldv_43758: ;
  if (i <= 9) {
    goto ldv_43757;
  } else {
  }
  tmp = amdgpu_ip_block_version_cmp(adev, 7, 5U, 0U);
  if (tmp == 0) {
    adev->uvd.address_64_bit = 1;
  } else {
  }
  return (0);
}
}
int amdgpu_uvd_sw_fini(struct amdgpu_device *adev )
{
  int r ;
  {
  if ((unsigned long )adev->uvd.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (0);
  } else {
  }
  r = amdgpu_bo_reserve(adev->uvd.vcpu_bo, 0);
  if (r == 0) {
    amdgpu_bo_kunmap(adev->uvd.vcpu_bo);
    amdgpu_bo_unpin(adev->uvd.vcpu_bo);
    amdgpu_bo_unreserve(adev->uvd.vcpu_bo);
  } else {
  }
  amdgpu_bo_unref(& adev->uvd.vcpu_bo);
  amdgpu_ring_fini(& adev->uvd.ring);
  release_firmware(adev->uvd.fw);
  return (0);
}
}
int amdgpu_uvd_suspend(struct amdgpu_device *adev )
{
  unsigned int size ;
  void *ptr ;
  struct common_firmware_header const *hdr ;
  int i ;
  int tmp ;
  unsigned long tmp___0 ;
  {
  if ((unsigned long )adev->uvd.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (0);
  } else {
  }
  i = 0;
  goto ldv_43773;
  ldv_43772:
  tmp = atomic_read((atomic_t const *)(& adev->uvd.handles) + (unsigned long )i);
  if (tmp != 0) {
    goto ldv_43771;
  } else {
  }
  i = i + 1;
  ldv_43773: ;
  if (i <= 9) {
    goto ldv_43772;
  } else {
  }
  ldv_43771: ;
  if (i == 10) {
    return (0);
  } else {
  }
  hdr = (struct common_firmware_header const *)(adev->uvd.fw)->data;
  tmp___0 = amdgpu_bo_size(adev->uvd.vcpu_bo);
  size = (unsigned int )tmp___0;
  size = size - (unsigned int )hdr->ucode_size_bytes;
  ptr = adev->uvd.cpu_addr;
  ptr = ptr + (unsigned long )hdr->ucode_size_bytes;
  adev->uvd.saved_bo = kmalloc((size_t )size, 208U);
  memcpy(adev->uvd.saved_bo, (void const *)ptr, (size_t )size);
  return (0);
}
}
int amdgpu_uvd_resume(struct amdgpu_device *adev )
{
  unsigned int size ;
  void *ptr ;
  struct common_firmware_header const *hdr ;
  unsigned int offset ;
  unsigned long tmp ;
  {
  if ((unsigned long )adev->uvd.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (-22);
  } else {
  }
  hdr = (struct common_firmware_header const *)(adev->uvd.fw)->data;
  offset = hdr->ucode_array_offset_bytes;
  memcpy(adev->uvd.cpu_addr, (void const *)(adev->uvd.fw)->data + (unsigned long )offset,
           (unsigned long )(adev->uvd.fw)->size - (unsigned long )offset);
  tmp = amdgpu_bo_size(adev->uvd.vcpu_bo);
  size = (unsigned int )tmp;
  size = size - (unsigned int )hdr->ucode_size_bytes;
  ptr = adev->uvd.cpu_addr;
  ptr = ptr + (unsigned long )hdr->ucode_size_bytes;
  if ((unsigned long )adev->uvd.saved_bo != (unsigned long )((void *)0)) {
    memcpy(ptr, (void const *)adev->uvd.saved_bo, (size_t )size);
    kfree((void const *)adev->uvd.saved_bo);
    adev->uvd.saved_bo = (void *)0;
  } else {
    memset(ptr, 0, (size_t )size);
  }
  return (0);
}
}
void amdgpu_uvd_free_handles(struct amdgpu_device *adev , struct drm_file *filp )
{
  struct amdgpu_ring *ring ;
  int i ;
  int r ;
  u32 handle ;
  int tmp ;
  struct amdgpu_fence *fence ;
  {
  ring = & adev->uvd.ring;
  i = 0;
  goto ldv_43792;
  ldv_43791:
  tmp = atomic_read((atomic_t const *)(& adev->uvd.handles) + (unsigned long )i);
  handle = (u32 )tmp;
  if (handle != 0U && (unsigned long )adev->uvd.filp[i] == (unsigned long )filp) {
    amdgpu_uvd_note_usage(adev);
    r = amdgpu_uvd_get_destroy_msg(ring, handle, & fence);
    if (r != 0) {
      drm_err("Error destroying UVD (%d)!\n", r);
      goto ldv_43790;
    } else {
    }
    amdgpu_fence_wait(fence, 0);
    amdgpu_fence_unref(& fence);
    adev->uvd.filp[i] = (struct drm_file *)0;
    atomic_set((atomic_t *)(& adev->uvd.handles) + (unsigned long )i, 0);
  } else {
  }
  ldv_43790:
  i = i + 1;
  ldv_43792: ;
  if (i <= 9) {
    goto ldv_43791;
  } else {
  }
  return;
}
}
static void amdgpu_uvd_force_into_uvd_segment(struct amdgpu_bo *rbo )
{
  int i ;
  {
  i = 0;
  goto ldv_43799;
  ldv_43798:
  rbo->placements[i].fpfn = 0U;
  rbo->placements[i].lpfn = 65536U;
  i = i + 1;
  ldv_43799: ;
  if ((unsigned int )i < rbo->placement.num_placement) {
    goto ldv_43798;
  } else {
  }
  return;
}
}
static int amdgpu_uvd_cs_pass1(struct amdgpu_uvd_cs_ctx *ctx )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_bo *bo ;
  u32 cmd ;
  u32 lo ;
  u32 hi ;
  uint64_t addr ;
  int r ;
  u32 tmp ;
  u32 domain ;
  {
  r = 0;
  lo = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->data0);
  hi = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->data1);
  addr = (unsigned long long )lo | ((unsigned long long )hi << 32);
  mapping = amdgpu_cs_find_mapping(ctx->parser, addr, & bo);
  if ((unsigned long )mapping == (unsigned long )((struct amdgpu_bo_va_mapping *)0)) {
    drm_err("Can\'t find BO for addr 0x%08Lx\n", addr);
    return (-22);
  } else {
  }
  if (! ((ctx->parser)->adev)->uvd.address_64_bit) {
    tmp = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->idx);
    cmd = tmp >> 1;
    if (cmd == 0U || cmd == 3U) {
      domain = 4U;
      amdgpu_ttm_placement_from_domain(bo, domain);
    } else {
    }
    amdgpu_uvd_force_into_uvd_segment(bo);
    r = ttm_bo_validate(& bo->tbo, & bo->placement, 0, 0);
  } else {
  }
  return (r);
}
}
static int amdgpu_uvd_cs_msg_decode(u32 *msg , unsigned int *buf_sizes )
{
  unsigned int stream_type ;
  unsigned int width ;
  unsigned int height ;
  unsigned int dpb_size ;
  unsigned int pitch ;
  unsigned int level ;
  unsigned int width_in_mb ;
  unsigned int height_in_mb ;
  unsigned int fs_in_mb ;
  unsigned int image_size ;
  unsigned int tmp ;
  unsigned int min_dpb_size ;
  unsigned int num_dpb_buffer ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  {
  stream_type = *(msg + 4UL);
  width = *(msg + 6UL);
  height = *(msg + 7UL);
  dpb_size = *(msg + 9UL);
  pitch = *(msg + 28UL);
  level = *(msg + 57UL);
  width_in_mb = width / 16U;
  height_in_mb = (height / 16U + 1U) & 4294967294U;
  fs_in_mb = width_in_mb * height_in_mb;
  image_size = width * height;
  image_size = image_size / 2U + image_size;
  image_size = (image_size + 1023U) & 4294966272U;
  switch (stream_type) {
  case 0U: ;
  case 7U: ;
  switch (level) {
  case 30U:
  num_dpb_buffer = 8100U / fs_in_mb;
  goto ldv_43832;
  case 31U:
  num_dpb_buffer = 18000U / fs_in_mb;
  goto ldv_43832;
  case 32U:
  num_dpb_buffer = 20480U / fs_in_mb;
  goto ldv_43832;
  case 41U:
  num_dpb_buffer = 32768U / fs_in_mb;
  goto ldv_43832;
  case 42U:
  num_dpb_buffer = 34816U / fs_in_mb;
  goto ldv_43832;
  case 50U:
  num_dpb_buffer = 110400U / fs_in_mb;
  goto ldv_43832;
  case 51U:
  num_dpb_buffer = 184320U / fs_in_mb;
  goto ldv_43832;
  default:
  num_dpb_buffer = 184320U / fs_in_mb;
  goto ldv_43832;
  }
  ldv_43832:
  num_dpb_buffer = num_dpb_buffer + 1U;
  if (num_dpb_buffer > 17U) {
    num_dpb_buffer = 17U;
  } else {
  }
  min_dpb_size = image_size * num_dpb_buffer;
  min_dpb_size = ((width_in_mb * height_in_mb) * num_dpb_buffer) * 192U + min_dpb_size;
  min_dpb_size = (width_in_mb * height_in_mb) * 32U + min_dpb_size;
  goto ldv_43840;
  case 1U:
  min_dpb_size = image_size * 3U;
  min_dpb_size = (width_in_mb * height_in_mb) * 128U + min_dpb_size;
  min_dpb_size = width_in_mb * 64U + min_dpb_size;
  min_dpb_size = width_in_mb * 128U + min_dpb_size;
  _max1 = width_in_mb;
  _max2 = height_in_mb;
  tmp = _max1 > _max2 ? _max1 : _max2;
  min_dpb_size = ((tmp * 112U + 63U) & 4294967232U) + min_dpb_size;
  goto ldv_43840;
  case 3U:
  min_dpb_size = image_size * 3U;
  goto ldv_43840;
  case 4U:
  min_dpb_size = image_size * 3U;
  min_dpb_size = (width_in_mb * height_in_mb) * 64U + min_dpb_size;
  min_dpb_size = (((width_in_mb * height_in_mb) * 32U + 63U) & 4294967232U) + min_dpb_size;
  goto ldv_43840;
  case 16U:
  image_size = ((((width + 15U) & 4294967280U) * ((height + 15U) & 4294967280U)) * 3U) / 2U;
  image_size = (image_size + 255U) & 4294967040U;
  num_dpb_buffer = (*(msg + 59UL) & 255U) + 2U;
  min_dpb_size = image_size * num_dpb_buffer;
  goto ldv_43840;
  default:
  drm_err("UVD codec not handled %d!\n", stream_type);
  return (-22);
  }
  ldv_43840: ;
  if (width > pitch) {
    drm_err("Invalid UVD decoding target pitch!\n");
    return (-22);
  } else {
  }
  if (dpb_size < min_dpb_size) {
    drm_err("Invalid dpb_size in UVD message (%d / %d)!\n", dpb_size, min_dpb_size);
    return (-22);
  } else {
  }
  *(buf_sizes + 1UL) = dpb_size;
  *(buf_sizes + 2UL) = image_size;
  return (0);
}
}
static int amdgpu_uvd_cs_msg(struct amdgpu_uvd_cs_ctx *ctx , struct amdgpu_bo *bo ,
                             unsigned int offset )
{
  struct amdgpu_device *adev ;
  int32_t *msg ;
  int32_t msg_type ;
  int32_t handle ;
  struct fence *f ;
  void *ptr ;
  int i ;
  int r ;
  int tmp ;
  int tmp___0 ;
  {
  adev = (ctx->parser)->adev;
  if ((offset & 63U) != 0U) {
    drm_err("UVD messages must be 64 byte aligned!\n");
    return (-22);
  } else {
  }
  f = reservation_object_get_excl(bo->tbo.resv);
  if ((unsigned long )f != (unsigned long )((struct fence *)0)) {
    r = amdgpu_fence_wait((struct amdgpu_fence *)f, 0);
    if (r != 0) {
      drm_err("Failed waiting for UVD message (%d)!\n", r);
      return (r);
    } else {
    }
  } else {
  }
  r = amdgpu_bo_kmap(bo, & ptr);
  if (r != 0) {
    drm_err("Failed mapping the UVD message (%d)!\n", r);
    return (r);
  } else {
  }
  msg = (int32_t *)ptr + (unsigned long )offset;
  msg_type = *(msg + 1UL);
  handle = *(msg + 2UL);
  if (handle == 0) {
    drm_err("Invalid UVD handle!\n");
    return (-22);
  } else {
  }
  if (msg_type == 1) {
    r = amdgpu_uvd_cs_msg_decode((u32 *)msg, ctx->buf_sizes);
    amdgpu_bo_kunmap(bo);
    if (r != 0) {
      return (r);
    } else {
    }
  } else
  if (msg_type == 2) {
    i = 0;
    goto ldv_43863;
    ldv_43862:
    atomic_cmpxchg((atomic_t *)(& adev->uvd.handles) + (unsigned long )i, handle,
                   0);
    i = i + 1;
    ldv_43863: ;
    if (i <= 9) {
      goto ldv_43862;
    } else {
    }
    amdgpu_bo_kunmap(bo);
    return (0);
  } else {
    amdgpu_bo_kunmap(bo);
    if (msg_type != 0) {
      drm_err("Illegal UVD message type (%d)!\n", msg_type);
      return (-22);
    } else {
    }
  }
  i = 0;
  goto ldv_43866;
  ldv_43865:
  tmp = atomic_read((atomic_t const *)(& adev->uvd.handles) + (unsigned long )i);
  if (tmp == handle) {
    return (0);
  } else {
  }
  i = i + 1;
  ldv_43866: ;
  if (i <= 9) {
    goto ldv_43865;
  } else {
  }
  i = 0;
  goto ldv_43869;
  ldv_43868:
  tmp___0 = atomic_cmpxchg((atomic_t *)(& adev->uvd.handles) + (unsigned long )i,
                           0, handle);
  if (tmp___0 == 0) {
    adev->uvd.filp[i] = (ctx->parser)->filp;
    return (0);
  } else {
  }
  i = i + 1;
  ldv_43869: ;
  if (i <= 9) {
    goto ldv_43868;
  } else {
  }
  drm_err("No more free UVD handles!\n");
  return (-22);
}
}
static int amdgpu_uvd_cs_pass2(struct amdgpu_uvd_cs_ctx *ctx )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_bo *bo ;
  struct amdgpu_ib *ib ;
  u32 cmd ;
  u32 lo ;
  u32 hi ;
  uint64_t start ;
  uint64_t end ;
  uint64_t addr ;
  int r ;
  u32 tmp ;
  {
  lo = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->data0);
  hi = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->data1);
  addr = (unsigned long long )lo | ((unsigned long long )hi << 32);
  mapping = amdgpu_cs_find_mapping(ctx->parser, addr, & bo);
  if ((unsigned long )mapping == (unsigned long )((struct amdgpu_bo_va_mapping *)0)) {
    return (-22);
  } else {
  }
  start = amdgpu_bo_gpu_offset(bo);
  end = (uint64_t )((mapping->it.last - mapping->it.start) + 1UL);
  end = end * 4096ULL + start;
  addr = addr - (unsigned long long )mapping->it.start * 4096ULL;
  start = start + addr;
  ib = (ctx->parser)->ibs + (unsigned long )ctx->ib_idx;
  *(ib->ptr + (unsigned long )ctx->data0) = (u32 )start;
  *(ib->ptr + (unsigned long )ctx->data1) = (u32 )(start >> 32);
  tmp = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->idx);
  cmd = tmp >> 1;
  if (cmd <= 3U) {
    if (end - start < (uint64_t )*(ctx->buf_sizes + (unsigned long )cmd)) {
      drm_err("buffer (%d) to small (%d / %d)!\n", cmd, (unsigned int )end - (unsigned int )start,
              *(ctx->buf_sizes + (unsigned long )cmd));
      return (-22);
    } else {
    }
  } else
  if (cmd != 256U && cmd != 516U) {
    drm_err("invalid UVD command %X!\n", cmd);
    return (-22);
  } else {
  }
  if (! ((ctx->parser)->adev)->uvd.address_64_bit) {
    if (start >> 28 != (end - 1ULL) >> 28) {
      drm_err("reloc %LX-%LX crossing 256MB boundary!\n", start, end);
      return (-22);
    } else {
    }
    if ((cmd == 0U || cmd == 3U) && start >> 28 != ((ctx->parser)->adev)->uvd.gpu_addr >> 28) {
      drm_err("msg/fb buffer %LX-%LX out of 256MB segment!\n", start, end);
      return (-22);
    } else {
    }
  } else {
  }
  if (cmd == 0U) {
    ctx->has_msg_cmd = 1;
    r = amdgpu_uvd_cs_msg(ctx, bo, (unsigned int )addr);
    if (r != 0) {
      return (r);
    } else {
    }
  } else
  if (! ctx->has_msg_cmd) {
    drm_err("Message needed before other commands are send!\n");
    return (-22);
  } else {
  }
  return (0);
}
}
static int amdgpu_uvd_cs_reg(struct amdgpu_uvd_cs_ctx *ctx , int (*cb)(struct amdgpu_uvd_cs_ctx * ) )
{
  struct amdgpu_ib *ib ;
  int i ;
  int r ;
  unsigned int reg ;
  {
  ib = (ctx->parser)->ibs + (unsigned long )ctx->ib_idx;
  ctx->idx = ctx->idx + 1U;
  i = 0;
  goto ldv_43900;
  ldv_43899:
  reg = ctx->reg + (unsigned int )i;
  if (ctx->idx >= ib->length_dw) {
    drm_err("Register command after end of CS!\n");
    return (-22);
  } else {
  }
  switch (reg) {
  case 15300U:
  ctx->data0 = ctx->idx;
  goto ldv_43894;
  case 15301U:
  ctx->data1 = ctx->idx;
  goto ldv_43894;
  case 15299U:
  r = (*cb)(ctx);
  if (r != 0) {
    return (r);
  } else {
  }
  goto ldv_43894;
  case 15302U: ;
  goto ldv_43894;
  default:
  drm_err("Invalid reg 0x%X!\n", reg);
  return (-22);
  }
  ldv_43894:
  ctx->idx = ctx->idx + 1U;
  i = i + 1;
  ldv_43900: ;
  if ((unsigned int )i <= ctx->count) {
    goto ldv_43899;
  } else {
  }
  return (0);
}
}
static int amdgpu_uvd_cs_packets(struct amdgpu_uvd_cs_ctx *ctx , int (*cb)(struct amdgpu_uvd_cs_ctx * ) )
{
  struct amdgpu_ib *ib ;
  int r ;
  u32 cmd ;
  u32 tmp ;
  unsigned int type ;
  {
  ib = (ctx->parser)->ibs + (unsigned long )ctx->ib_idx;
  ctx->idx = 0U;
  goto ldv_43916;
  ldv_43915:
  tmp = amdgpu_get_ib_value(ctx->parser, ctx->ib_idx, (int )ctx->idx);
  cmd = tmp;
  type = cmd >> 30;
  switch (type) {
  case 0U:
  ctx->reg = cmd & 65535U;
  ctx->count = (cmd >> 16) & 16383U;
  r = amdgpu_uvd_cs_reg(ctx, cb);
  if (r != 0) {
    return (r);
  } else {
  }
  goto ldv_43912;
  case 2U:
  ctx->idx = ctx->idx + 1U;
  goto ldv_43912;
  default:
  drm_err("Unknown packet type %d !\n", type);
  return (-22);
  }
  ldv_43912: ;
  ldv_43916: ;
  if (ctx->idx < ib->length_dw) {
    goto ldv_43915;
  } else {
  }
  return (0);
}
}
int amdgpu_uvd_ring_parse_cs(struct amdgpu_cs_parser *parser , u32 ib_idx )
{
  struct amdgpu_uvd_cs_ctx ctx ;
  unsigned int buf_sizes[4U] ;
  struct amdgpu_ib *ib ;
  int r ;
  {
  ctx.parser = 0;
  ctx.reg = 0U;
  ctx.count = 0U;
  ctx.data0 = 0U;
  ctx.data1 = 0U;
  ctx.idx = 0U;
  ctx.ib_idx = 0U;
  ctx.has_msg_cmd = (_Bool)0;
  ctx.buf_sizes = 0;
  buf_sizes[0] = 2048U;
  buf_sizes[1] = 33554432U;
  buf_sizes[2] = 7077888U;
  buf_sizes[3] = 2048U;
  ib = parser->ibs + (unsigned long )ib_idx;
  if ((ib->length_dw & 15U) != 0U) {
    drm_err("UVD IB length (%d) not 16 dwords aligned!\n", ib->length_dw);
    return (-22);
  } else {
  }
  ctx.parser = parser;
  ctx.buf_sizes = (unsigned int *)(& buf_sizes);
  ctx.ib_idx = ib_idx;
  r = amdgpu_uvd_cs_packets(& ctx, & amdgpu_uvd_cs_pass1);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_cs_packets(& ctx, & amdgpu_uvd_cs_pass2);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! ctx.has_msg_cmd) {
    drm_err("UVD-IBs need a msg command!\n");
    return (-22);
  } else {
  }
  amdgpu_uvd_note_usage((ctx.parser)->adev);
  return (0);
}
}
static int amdgpu_uvd_send_msg(struct amdgpu_ring *ring , struct amdgpu_bo *bo , struct amdgpu_fence **fence )
{
  struct ttm_validate_buffer tv ;
  struct ww_acquire_ctx ticket ;
  struct list_head head ;
  struct amdgpu_ib ib ;
  uint64_t addr ;
  int i ;
  int r ;
  {
  memset((void *)(& tv), 0, 32UL);
  tv.bo = & bo->tbo;
  INIT_LIST_HEAD(& head);
  list_add(& tv.head, & head);
  r = ttm_eu_reserve_buffers(& ticket, & head, 1, (struct list_head *)0);
  if (r != 0) {
    return (r);
  } else {
  }
  if (! (bo->adev)->uvd.address_64_bit) {
    amdgpu_ttm_placement_from_domain(bo, 4U);
    amdgpu_uvd_force_into_uvd_segment(bo);
  } else {
  }
  r = ttm_bo_validate(& bo->tbo, & bo->placement, 1, 0);
  if (r != 0) {
    goto err;
  } else {
  }
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, 64U, & ib);
  if (r != 0) {
    goto err;
  } else {
  }
  addr = amdgpu_bo_gpu_offset(bo);
  *(ib.ptr) = 15300U;
  *(ib.ptr + 1UL) = (u32 )addr;
  *(ib.ptr + 2UL) = 15301U;
  *(ib.ptr + 3UL) = (u32 )(addr >> 32);
  *(ib.ptr + 4UL) = 15299U;
  *(ib.ptr + 5UL) = 0U;
  i = 6;
  goto ldv_43940;
  ldv_43939:
  *(ib.ptr + (unsigned long )i) = 2147483648U;
  i = i + 1;
  ldv_43940: ;
  if (i <= 15) {
    goto ldv_43939;
  } else {
  }
  ib.length_dw = 16U;
  r = amdgpu_ib_schedule(ring->adev, 1U, & ib, (void *)0);
  if (r != 0) {
    goto err;
  } else {
  }
  ttm_eu_fence_buffer_objects(& ticket, & head, & (ib.fence)->base);
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence **)0)) {
    *fence = amdgpu_fence_ref(ib.fence);
  } else {
  }
  amdgpu_ib_free(ring->adev, & ib);
  amdgpu_bo_unref(& bo);
  return (0);
  err:
  ttm_eu_backoff_reservation(& ticket, & head);
  return (r);
}
}
int amdgpu_uvd_get_create_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence )
{
  struct amdgpu_device *adev ;
  struct amdgpu_bo *bo ;
  u32 *msg ;
  int r ;
  int i ;
  int tmp ;
  {
  adev = ring->adev;
  r = amdgpu_bo_create(adev, 1024UL, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_reserve(bo, 0);
  if (r != 0) {
    amdgpu_bo_unref(& bo);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(bo, (void **)(& msg));
  if (r != 0) {
    amdgpu_bo_unreserve(bo);
    amdgpu_bo_unref(& bo);
    return (r);
  } else {
  }
  *msg = 3556U;
  *(msg + 1UL) = 0U;
  *(msg + 2UL) = handle;
  *(msg + 3UL) = 0U;
  *(msg + 4UL) = 0U;
  *(msg + 5UL) = 0U;
  *(msg + 6UL) = 0U;
  *(msg + 7UL) = 1920U;
  *(msg + 8UL) = 1088U;
  *(msg + 9UL) = 0U;
  *(msg + 10UL) = 28536832U;
  i = 11;
  goto ldv_43953;
  ldv_43952:
  *(msg + (unsigned long )i) = 0U;
  i = i + 1;
  ldv_43953: ;
  if (i <= 1023) {
    goto ldv_43952;
  } else {
  }
  amdgpu_bo_kunmap(bo);
  amdgpu_bo_unreserve(bo);
  tmp = amdgpu_uvd_send_msg(ring, bo, fence);
  return (tmp);
}
}
int amdgpu_uvd_get_destroy_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence )
{
  struct amdgpu_device *adev ;
  struct amdgpu_bo *bo ;
  u32 *msg ;
  int r ;
  int i ;
  int tmp ;
  {
  adev = ring->adev;
  r = amdgpu_bo_create(adev, 1024UL, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & bo);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_bo_reserve(bo, 0);
  if (r != 0) {
    amdgpu_bo_unref(& bo);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(bo, (void **)(& msg));
  if (r != 0) {
    amdgpu_bo_unreserve(bo);
    amdgpu_bo_unref(& bo);
    return (r);
  } else {
  }
  *msg = 3556U;
  *(msg + 1UL) = 2U;
  *(msg + 2UL) = handle;
  *(msg + 3UL) = 0U;
  i = 4;
  goto ldv_43966;
  ldv_43965:
  *(msg + (unsigned long )i) = 0U;
  i = i + 1;
  ldv_43966: ;
  if (i <= 1023) {
    goto ldv_43965;
  } else {
  }
  amdgpu_bo_kunmap(bo);
  amdgpu_bo_unreserve(bo);
  tmp = amdgpu_uvd_send_msg(ring, bo, fence);
  return (tmp);
}
}
static void amdgpu_uvd_idle_work_handler(struct work_struct *work )
{
  struct amdgpu_device *adev ;
  struct work_struct const *__mptr ;
  unsigned int i ;
  unsigned int fences ;
  unsigned int handles ;
  int tmp ;
  unsigned long tmp___0 ;
  {
  __mptr = (struct work_struct const *)work;
  adev = (struct amdgpu_device *)__mptr + 0xffffffffffffb570UL;
  handles = 0U;
  fences = amdgpu_fence_count_emitted(& adev->uvd.ring);
  i = 0U;
  goto ldv_43978;
  ldv_43977:
  tmp = atomic_read((atomic_t const *)(& adev->uvd.handles) + (unsigned long )i);
  if (tmp != 0) {
    handles = handles + 1U;
  } else {
  }
  i = i + 1U;
  ldv_43978: ;
  if (i <= 9U) {
    goto ldv_43977;
  } else {
  }
  if (fences == 0U && handles == 0U) {
    if ((int )adev->pm.dpm_enabled) {
      amdgpu_dpm_enable_uvd(adev, 0);
    } else {
      (*((adev->asic_funcs)->set_uvd_clocks))(adev, 0U, 0U);
    }
  } else {
    tmp___0 = msecs_to_jiffies(1000U);
    schedule_delayed_work(& adev->uvd.idle_work, tmp___0);
  }
  return;
}
}
static void amdgpu_uvd_note_usage(struct amdgpu_device *adev )
{
  bool set_clocks ;
  bool tmp ;
  int tmp___0 ;
  unsigned long tmp___1 ;
  bool tmp___2 ;
  {
  tmp = ldv_cancel_delayed_work_sync_986(& adev->uvd.idle_work);
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  set_clocks = (bool )tmp___0;
  tmp___1 = msecs_to_jiffies(1000U);
  tmp___2 = schedule_delayed_work(& adev->uvd.idle_work, tmp___1);
  set_clocks = ((int )set_clocks & (int )tmp___2) != 0;
  if ((int )set_clocks) {
    if ((int )adev->pm.dpm_enabled) {
      amdgpu_dpm_enable_uvd(adev, 1);
    } else {
      (*((adev->asic_funcs)->set_uvd_clocks))(adev, 53300U, 40000U);
    }
  } else {
  }
  return;
}
}
void invoke_work_8(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_8_0 == 2 || ldv_work_8_0 == 3) {
    ldv_work_8_0 = 4;
    amdgpu_uvd_idle_work_handler(ldv_work_struct_8_0);
    ldv_work_8_0 = 1;
  } else {
  }
  goto ldv_43990;
  case 1: ;
  if (ldv_work_8_1 == 2 || ldv_work_8_1 == 3) {
    ldv_work_8_1 = 4;
    amdgpu_uvd_idle_work_handler(ldv_work_struct_8_0);
    ldv_work_8_1 = 1;
  } else {
  }
  goto ldv_43990;
  case 2: ;
  if (ldv_work_8_2 == 2 || ldv_work_8_2 == 3) {
    ldv_work_8_2 = 4;
    amdgpu_uvd_idle_work_handler(ldv_work_struct_8_0);
    ldv_work_8_2 = 1;
  } else {
  }
  goto ldv_43990;
  case 3: ;
  if (ldv_work_8_3 == 2 || ldv_work_8_3 == 3) {
    ldv_work_8_3 = 4;
    amdgpu_uvd_idle_work_handler(ldv_work_struct_8_0);
    ldv_work_8_3 = 1;
  } else {
  }
  goto ldv_43990;
  default:
  ldv_stop();
  }
  ldv_43990: ;
  return;
}
}
void disable_work_8(struct work_struct *work )
{
  {
  if ((ldv_work_8_0 == 3 || ldv_work_8_0 == 2) && (unsigned long )ldv_work_struct_8_0 == (unsigned long )work) {
    ldv_work_8_0 = 1;
  } else {
  }
  if ((ldv_work_8_1 == 3 || ldv_work_8_1 == 2) && (unsigned long )ldv_work_struct_8_1 == (unsigned long )work) {
    ldv_work_8_1 = 1;
  } else {
  }
  if ((ldv_work_8_2 == 3 || ldv_work_8_2 == 2) && (unsigned long )ldv_work_struct_8_2 == (unsigned long )work) {
    ldv_work_8_2 = 1;
  } else {
  }
  if ((ldv_work_8_3 == 3 || ldv_work_8_3 == 2) && (unsigned long )ldv_work_struct_8_3 == (unsigned long )work) {
    ldv_work_8_3 = 1;
  } else {
  }
  return;
}
}
void activate_work_8(struct work_struct *work , int state )
{
  {
  if (ldv_work_8_0 == 0) {
    ldv_work_struct_8_0 = work;
    ldv_work_8_0 = state;
    return;
  } else {
  }
  if (ldv_work_8_1 == 0) {
    ldv_work_struct_8_1 = work;
    ldv_work_8_1 = state;
    return;
  } else {
  }
  if (ldv_work_8_2 == 0) {
    ldv_work_struct_8_2 = work;
    ldv_work_8_2 = state;
    return;
  } else {
  }
  if (ldv_work_8_3 == 0) {
    ldv_work_struct_8_3 = work;
    ldv_work_8_3 = state;
    return;
  } else {
  }
  return;
}
}
void work_init_8(void)
{
  {
  ldv_work_8_0 = 0;
  ldv_work_8_1 = 0;
  ldv_work_8_2 = 0;
  ldv_work_8_3 = 0;
  return;
}
}
void call_and_disable_work_8(struct work_struct *work )
{
  {
  if ((ldv_work_8_0 == 2 || ldv_work_8_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_8_0) {
    amdgpu_uvd_idle_work_handler(work);
    ldv_work_8_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_8_1 == 2 || ldv_work_8_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_8_1) {
    amdgpu_uvd_idle_work_handler(work);
    ldv_work_8_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_8_2 == 2 || ldv_work_8_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_8_2) {
    amdgpu_uvd_idle_work_handler(work);
    ldv_work_8_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_8_3 == 2 || ldv_work_8_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_8_3) {
    amdgpu_uvd_idle_work_handler(work);
    ldv_work_8_3 = 1;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_8(int state )
{
  {
  if (ldv_work_8_0 == state) {
    call_and_disable_work_8(ldv_work_struct_8_0);
  } else {
  }
  if (ldv_work_8_1 == state) {
    call_and_disable_work_8(ldv_work_struct_8_1);
  } else {
  }
  if (ldv_work_8_2 == state) {
    call_and_disable_work_8(ldv_work_struct_8_2);
  } else {
  }
  if (ldv_work_8_3 == state) {
    call_and_disable_work_8(ldv_work_struct_8_3);
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_981(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_982(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_983(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_984(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_985(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_cancel_delayed_work_sync_986(struct delayed_work *ldv_func_arg1 )
{
  ldv_func_ret_type___3 ldv_func_res ;
  bool tmp ;
  {
  tmp = cancel_delayed_work_sync(ldv_func_arg1);
  ldv_func_res = tmp;
  disable_work_2(& ldv_func_arg1->work);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_997(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_999(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_998(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1001(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1000(struct workqueue_struct *ldv_func_arg1 ) ;
static void uvd_v5_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void uvd_v5_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static int uvd_v5_0_start(struct amdgpu_device *adev ) ;
static void uvd_v5_0_stop(struct amdgpu_device *adev ) ;
static u32 uvd_v5_0_ring_get_rptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15780U, 0);
  return (tmp);
}
}
static u32 uvd_v5_0_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15781U, 0);
  return (tmp);
}
}
static void uvd_v5_0_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  return;
}
}
static int uvd_v5_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v5_0_set_ring_funcs(adev);
  uvd_v5_0_set_irq_funcs(adev);
  return (0);
}
}
static int uvd_v5_0_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  struct amdgpu_device *adev ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 124U, & adev->uvd.irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->uvd.ring;
  sprintf((char *)(& ring->name), "uvd");
  r = amdgpu_ring_init(adev, ring, 4096U, 2147483648U, 15U, & adev->uvd.irq, 0U, 3);
  return (r);
}
}
static int uvd_v5_0_sw_fini(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_fini(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v5_0_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  u32 tmp ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 53300U, 40000U);
  r = uvd_v5_0_start(adev);
  if (r != 0) {
    goto done;
  } else {
  }
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    goto done;
  } else {
  }
  r = amdgpu_ring_lock(ring, 10U);
  if (r != 0) {
    drm_err("amdgpu: ring failed to lock UVD ring (%d).\n", r);
    goto done;
  } else {
  }
  tmp = 15794U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15793U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15795U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  amdgpu_ring_write(ring, 15792U);
  amdgpu_ring_write(ring, 8U);
  amdgpu_ring_write(ring, 15616U);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_unlock_commit(ring);
  done:
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 0U, 0U);
  if (r == 0) {
    printk("\016[drm] UVD initialized successfully.\n");
  } else {
  }
  return (r);
}
}
static int uvd_v5_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  uvd_v5_0_stop(adev);
  ring->ready = 0;
  return (0);
}
}
static int uvd_v5_0_suspend(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = uvd_v5_0_hw_fini((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v5_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = uvd_v5_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static void uvd_v5_0_mc_resume(struct amdgpu_device *adev )
{
  uint64_t offset ;
  u32 size ;
  {
  amdgpu_mm_wreg(adev, 15455U, (unsigned int )adev->uvd.gpu_addr, 0);
  amdgpu_mm_wreg(adev, 15454U, (unsigned int )(adev->uvd.gpu_addr >> 32ULL), 0);
  offset = 256ULL;
  size = ((u32 )(adev->uvd.fw)->size + 4099U) & 4294963200U;
  amdgpu_mm_wreg(adev, 15746U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15747U, size, 0);
  offset = (uint64_t )size + offset;
  size = 1048576U;
  amdgpu_mm_wreg(adev, 15748U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15749U, size, 0);
  offset = (uint64_t )size + offset;
  size = 1048576U;
  amdgpu_mm_wreg(adev, 15750U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15751U, size, 0);
  return;
}
}
static int uvd_v5_0_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_bufsz ;
  u32 tmp ;
  u32 lmi_swap_cntl ;
  u32 mp_swap_cntl ;
  int i ;
  int j ;
  int r ;
  u32 tmp_ ;
  u32 tmp___0 ;
  u32 tmp____0 ;
  u32 tmp___1 ;
  u32 tmp____1 ;
  u32 tmp___2 ;
  unsigned long __ms ;
  unsigned long tmp___3 ;
  unsigned long __ms___0 ;
  unsigned long tmp___4 ;
  u32 tmp____2 ;
  u32 tmp___5 ;
  unsigned long __ms___1 ;
  unsigned long tmp___6 ;
  unsigned long __ms___2 ;
  unsigned long tmp___7 ;
  u32 tmp____3 ;
  u32 tmp___8 ;
  unsigned long __ms___3 ;
  unsigned long tmp___9 ;
  u32 status ;
  unsigned long __ms___4 ;
  unsigned long tmp___10 ;
  u32 tmp____4 ;
  u32 tmp___11 ;
  unsigned long __ms___5 ;
  unsigned long tmp___12 ;
  u32 tmp____5 ;
  u32 tmp___13 ;
  unsigned long __ms___6 ;
  unsigned long tmp___14 ;
  u32 tmp____6 ;
  u32 tmp___15 ;
  u32 tmp____7 ;
  u32 tmp___16 ;
  unsigned long tmp___17 ;
  int tmp___18 ;
  u32 tmp____8 ;
  u32 tmp___19 ;
  {
  ring = & adev->uvd.ring;
  tmp___0 = amdgpu_mm_rreg(adev, 14532U, 0);
  tmp_ = tmp___0;
  tmp_ = tmp_ & 4294967291U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 14532U, tmp_, 0);
  lmi_swap_cntl = 0U;
  mp_swap_cntl = 0U;
  uvd_v5_0_mc_resume(adev);
  amdgpu_mm_wreg(adev, 15658U, 0U, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp____0 = tmp___1;
  tmp____0 = tmp____0 & 4294967293U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 15680U, tmp____0, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____1 = tmp___2;
  tmp____1 = tmp____1 & 4294967039U;
  tmp____1 = tmp____1 | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp____1, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43769;
    ldv_43768:
    __const_udelay(4295000UL);
    ldv_43769:
    tmp___3 = __ms;
    __ms = __ms - 1UL;
    if (tmp___3 != 0UL) {
      goto ldv_43768;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8431U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43773;
    ldv_43772:
    __const_udelay(4295000UL);
    ldv_43773:
    tmp___4 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___4 != 0UL) {
      goto ldv_43772;
    } else {
    }
  }
  tmp___5 = amdgpu_mm_rreg(adev, 920U, 0);
  tmp____2 = tmp___5;
  tmp____2 = tmp____2 & 4294705151U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 920U, tmp____2, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___1 = 5UL;
    goto ldv_43778;
    ldv_43777:
    __const_udelay(4295000UL);
    ldv_43778:
    tmp___6 = __ms___1;
    __ms___1 = __ms___1 - 1UL;
    if (tmp___6 != 0UL) {
      goto ldv_43777;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15718U, 3154752U, 0);
  amdgpu_mm_wreg(adev, 15725U, lmi_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15727U, mp_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15737U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15738U, 0U, 0);
  amdgpu_mm_wreg(adev, 15739U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15740U, 0U, 0);
  amdgpu_mm_wreg(adev, 15742U, 0U, 0);
  amdgpu_mm_wreg(adev, 15741U, 136U, 0);
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___2 = 5UL;
    goto ldv_43782;
    ldv_43781:
    __const_udelay(4295000UL);
    ldv_43782:
    tmp___7 = __ms___2;
    __ms___2 = __ms___2 - 1UL;
    if (tmp___7 != 0UL) {
      goto ldv_43781;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 512U, 0);
  tmp___8 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____3 = tmp___8;
  tmp____3 = tmp____3 & 4294967039U;
  tmp____3 = tmp____3;
  amdgpu_mm_wreg(adev, 15677U, tmp____3, 0);
  amdgpu_mm_wreg(adev, 15776U, 0U, 0);
  __ms___3 = 10UL;
  goto ldv_43787;
  ldv_43786:
  __const_udelay(4295000UL);
  ldv_43787:
  tmp___9 = __ms___3;
  __ms___3 = __ms___3 - 1UL;
  if (tmp___9 != 0UL) {
    goto ldv_43786;
  } else {
  }
  i = 0;
  goto ldv_43809;
  ldv_43808:
  j = 0;
  goto ldv_43796;
  ldv_43795:
  status = amdgpu_mm_rreg(adev, 15791U, 0);
  if ((status & 2U) != 0U) {
    goto ldv_43790;
  } else {
  }
  __ms___4 = 10UL;
  goto ldv_43793;
  ldv_43792:
  __const_udelay(4295000UL);
  ldv_43793:
  tmp___10 = __ms___4;
  __ms___4 = __ms___4 - 1UL;
  if (tmp___10 != 0UL) {
    goto ldv_43792;
  } else {
  }
  j = j + 1;
  ldv_43796: ;
  if (j <= 99) {
    goto ldv_43795;
  } else {
  }
  ldv_43790:
  r = 0;
  if ((status & 2U) != 0U) {
    goto ldv_43797;
  } else {
  }
  drm_err("UVD not responding, trying to reset the VCPU!!!\n");
  tmp___11 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____4 = tmp___11;
  tmp____4 = tmp____4 & 4294967287U;
  tmp____4 = tmp____4 | 8U;
  amdgpu_mm_wreg(adev, 15776U, tmp____4, 0);
  __ms___5 = 10UL;
  goto ldv_43801;
  ldv_43800:
  __const_udelay(4295000UL);
  ldv_43801:
  tmp___12 = __ms___5;
  __ms___5 = __ms___5 - 1UL;
  if (tmp___12 != 0UL) {
    goto ldv_43800;
  } else {
  }
  tmp___13 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____5 = tmp___13;
  tmp____5 = tmp____5 & 4294967287U;
  tmp____5 = tmp____5;
  amdgpu_mm_wreg(adev, 15776U, tmp____5, 0);
  __ms___6 = 10UL;
  goto ldv_43806;
  ldv_43805:
  __const_udelay(4295000UL);
  ldv_43806:
  tmp___14 = __ms___6;
  __ms___6 = __ms___6 - 1UL;
  if (tmp___14 != 0UL) {
    goto ldv_43805;
  } else {
  }
  r = -1;
  i = i + 1;
  ldv_43809: ;
  if (i <= 9) {
    goto ldv_43808;
  } else {
  }
  ldv_43797: ;
  if (r != 0) {
    drm_err("UVD not responding, giving up!!!\n");
    return (r);
  } else {
  }
  tmp___15 = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp____6 = tmp___15;
  tmp____6 = tmp____6 & 4294967289U;
  tmp____6 = tmp____6 | 6U;
  amdgpu_mm_wreg(adev, 15680U, tmp____6, 0);
  tmp___16 = amdgpu_mm_rreg(adev, 15791U, 0);
  tmp____7 = tmp___16;
  tmp____7 = tmp____7 & 4294967291U;
  tmp____7 = tmp____7;
  amdgpu_mm_wreg(adev, 15791U, tmp____7, 0);
  tmp___17 = __roundup_pow_of_two((unsigned long )ring->ring_size);
  tmp___18 = __ilog2_u64((u64 )tmp___17);
  rb_bufsz = (u32 )tmp___18;
  tmp = 0U;
  tmp = (tmp & 4294967264U) | (rb_bufsz & 31U);
  tmp = (tmp & 4294959359U) | 256U;
  tmp = tmp | 65536U;
  tmp = tmp & 4293918719U;
  tmp = tmp | 16777216U;
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, 15785U, tmp, 0);
  amdgpu_mm_wreg(adev, 15782U, 0U, 0);
  amdgpu_mm_wreg(adev, 15786U, (unsigned int )(ring->gpu_addr >> 32ULL) >> 2, 0);
  amdgpu_mm_wreg(adev, 15465U, (unsigned int )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 15464U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 15780U, 0U, 0);
  ring->wptr = amdgpu_mm_rreg(adev, 15780U, 0);
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  tmp___19 = amdgpu_mm_rreg(adev, 15785U, 0);
  tmp____8 = tmp___19;
  tmp____8 = tmp____8 & 4294901759U;
  tmp____8 = tmp____8;
  amdgpu_mm_wreg(adev, 15785U, tmp____8, 0);
  return (0);
}
}
static void uvd_v5_0_stop(struct amdgpu_device *adev )
{
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  unsigned long __ms___0 ;
  unsigned long tmp___1 ;
  u32 tmp____0 ;
  u32 tmp___2 ;
  {
  amdgpu_mm_wreg(adev, 15785U, 285278465U, 0);
  tmp = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967039U;
  tmp_ = tmp_ | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp_, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43819;
    ldv_43818:
    __const_udelay(4295000UL);
    ldv_43819:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43818;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43823;
    ldv_43822:
    __const_udelay(4295000UL);
    ldv_43823:
    tmp___1 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_43822;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 0U, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____0 = tmp___2;
  tmp____0 = tmp____0 & 4294967039U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 15677U, tmp____0, 0);
  return;
}
}
static void uvd_v5_0_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                     unsigned int flags )
{
  int __ret_warn_on ;
  long tmp ;
  {
  __ret_warn_on = (int )flags & 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/uvd_v5_0.c",
                       466);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, (u32 )seq);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, (u32 )addr);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL) & 255U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 2U);
  return;
}
}
static bool uvd_v5_0_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  uint64_t addr ;
  {
  addr = semaphore->gpu_addr;
  amdgpu_ring_write(ring, 15296U);
  amdgpu_ring_write(ring, (u32 )(addr >> 3) & 1048575U);
  amdgpu_ring_write(ring, 15297U);
  amdgpu_ring_write(ring, (u32 )(addr >> 23) & 1048575U);
  amdgpu_ring_write(ring, 15298U);
  amdgpu_ring_write(ring, (u32 )((int )emit_wait | 128));
  return (1);
}
}
static int uvd_v5_0_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  amdgpu_mm_wreg(adev, 15805U, 3405700781U, 0);
  r = amdgpu_ring_lock(ring, 3U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring %d (%d).\n", ring->idx, r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_43849;
  ldv_43848:
  tmp = amdgpu_mm_rreg(adev, 15805U, 0);
  if (tmp == 3735928559U) {
    goto ldv_43847;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43849: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43848;
  } else {
  }
  ldv_43847: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  return (r);
}
}
static void uvd_v5_0_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  {
  amdgpu_ring_write(ring, 15463U);
  amdgpu_ring_write(ring, (unsigned int )ib->gpu_addr);
  amdgpu_ring_write(ring, 15462U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 15778U);
  amdgpu_ring_write(ring, ib->length_dw);
  return;
}
}
static int uvd_v5_0_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  struct amdgpu_fence *fence ;
  int r ;
  {
  adev = ring->adev;
  fence = (struct amdgpu_fence *)0;
  r = (*((adev->asic_funcs)->set_uvd_clocks))(adev, 53300U, 40000U);
  if (r != 0) {
    drm_err("amdgpu: failed to raise UVD clocks (%d).\n", r);
    return (r);
  } else {
  }
  r = amdgpu_uvd_get_create_msg(ring, 1U, (struct amdgpu_fence **)0);
  if (r != 0) {
    drm_err("amdgpu: failed to get create msg (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_uvd_get_destroy_msg(ring, 1U, & fence);
  if (r != 0) {
    drm_err("amdgpu: failed to get destroy ib (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    goto error;
  } else {
  }
  printk("\016[drm] ib test on ring %d succeeded\n", ring->idx);
  error:
  amdgpu_fence_unref(& fence);
  (*((adev->asic_funcs)->set_uvd_clocks))(adev, 0U, 0U);
  return (r);
}
}
static bool uvd_v5_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  return ((tmp & 524288U) == 0U);
}
}
static int uvd_v5_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43871;
  ldv_43870:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 524288U) == 0U) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_43871: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43870;
  } else {
  }
  return (-110);
}
}
static int uvd_v5_0_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v5_0_stop(adev);
  tmp = amdgpu_mm_rreg(adev, 920U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294705151U;
  tmp_ = tmp_ | 262144U;
  amdgpu_mm_wreg(adev, 920U, tmp_, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms = 5UL;
    goto ldv_43880;
    ldv_43879:
    __const_udelay(4295000UL);
    ldv_43880:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43879;
    } else {
    }
  }
  tmp___1 = uvd_v5_0_start(adev);
  return (tmp___1);
}
}
static void uvd_v5_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  u32 tmp___46 ;
  u32 tmp___47 ;
  u32 tmp___48 ;
  u32 tmp___49 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "UVD 5.0 registers\n");
  tmp = amdgpu_mm_rreg(adev, 15296U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_LOW=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 15297U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_HIGH=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 15298U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CMD=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 15299U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_CMD=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 15300U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA0=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 15301U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 15302U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_ENGINE_CNTL=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 15315U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_ADDR_CONFIG=0x%08X\n",
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 15316U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DB_ADDR_CONFIG=0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 15317U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DBW_ADDR_CONFIG=0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 15616U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CNTL=0x%08X\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 15654U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_EXT40_ADDR=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 15656U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_INDEX=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 15657U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_DATA=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 15658U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_GATE=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 15660U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_CTRL=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 15677U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL2=0x%08X\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 15680U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MASTINT_EN=0x%08X\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 15717U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_ADDR_EXT=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 15718U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 15725U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_SWAP_CNTL=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 15727U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MP_SWAP_CNTL=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 15737U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA0=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 15738U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA1=0x%08X\n", tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 15739U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB0=0x%08X\n", tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 15740U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB1=0x%08X\n", tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 15741U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUX=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 15742U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_ALU=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 15746U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET0=0x%08X\n",
            tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 15747U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE0=0x%08X\n",
            tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 15748U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET1=0x%08X\n",
            tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 15749U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE1=0x%08X\n",
            tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 15750U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET2=0x%08X\n",
            tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 15751U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE2=0x%08X\n",
            tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 15768U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CNTL=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 15776U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SOFT_RESET=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 15463U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_RBC_IB_64BIT_BAR_LOW=0x%08X\n",
            tmp___35);
  tmp___36 = amdgpu_mm_rreg(adev, 15462U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_RBC_IB_64BIT_BAR_HIGH=0x%08X\n",
            tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, 15778U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_IB_SIZE=0x%08X\n", tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, 15465U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_RBC_RB_64BIT_BAR_LOW=0x%08X\n",
            tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, 15464U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_RBC_RB_64BIT_BAR_HIGH=0x%08X\n",
            tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, 15780U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_RPTR=0x%08X\n", tmp___40);
  tmp___41 = amdgpu_mm_rreg(adev, 15781U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR=0x%08X\n", tmp___41);
  tmp___42 = amdgpu_mm_rreg(adev, 15782U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR_CNTL=0x%08X\n",
            tmp___42);
  tmp___43 = amdgpu_mm_rreg(adev, 15785U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_CNTL=0x%08X\n", tmp___43);
  tmp___44 = amdgpu_mm_rreg(adev, 15791U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_STATUS=0x%08X\n", tmp___44);
  tmp___45 = amdgpu_mm_rreg(adev, 15792U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_TIMEOUT_STATUS=0x%08X\n",
            tmp___45);
  tmp___46 = amdgpu_mm_rreg(adev, 15793U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___46);
  tmp___47 = amdgpu_mm_rreg(adev, 15794U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_FAULT_TIMEOUT_CNTL=0x%08X\n",
            tmp___47);
  tmp___48 = amdgpu_mm_rreg(adev, 15795U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_SIGNAL_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___48);
  tmp___49 = amdgpu_mm_rreg(adev, 15805U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CONTEXT_ID=0x%08X\n", tmp___49);
  return;
}
}
static int uvd_v5_0_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  return (0);
}
}
static int uvd_v5_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("uvd_v5_0_process_interrupt", "IH: UVD TRAP\n");
  } else {
  }
  amdgpu_fence_process(& adev->uvd.ring);
  return (0);
}
}
static int uvd_v5_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int uvd_v5_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    uvd_v5_0_stop(adev);
    return (0);
  } else {
    tmp = uvd_v5_0_start(adev);
    return (tmp);
  }
}
}
struct amd_ip_funcs const uvd_v5_0_ip_funcs =
     {& uvd_v5_0_early_init, (int (*)(void * ))0, & uvd_v5_0_sw_init, & uvd_v5_0_sw_fini,
    & uvd_v5_0_hw_init, & uvd_v5_0_hw_fini, & uvd_v5_0_suspend, & uvd_v5_0_resume,
    & uvd_v5_0_is_idle, & uvd_v5_0_wait_for_idle, & uvd_v5_0_soft_reset, & uvd_v5_0_print_status,
    & uvd_v5_0_set_clockgating_state, & uvd_v5_0_set_powergating_state};
static struct amdgpu_ring_funcs const uvd_v5_0_ring_funcs =
     {& uvd_v5_0_ring_get_rptr, & uvd_v5_0_ring_get_wptr, & uvd_v5_0_ring_set_wptr,
    & amdgpu_uvd_ring_parse_cs, & uvd_v5_0_ring_emit_ib, & uvd_v5_0_ring_emit_fence,
    & uvd_v5_0_ring_emit_semaphore, 0, 0, 0, & uvd_v5_0_ring_test_ring, & uvd_v5_0_ring_test_ib,
    & amdgpu_ring_test_lockup};
static void uvd_v5_0_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.ring.funcs = & uvd_v5_0_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const uvd_v5_0_irq_funcs = {& uvd_v5_0_set_interrupt_state, & uvd_v5_0_process_interrupt};
static void uvd_v5_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.irq.num_types = 1U;
  adev->uvd.irq.funcs = & uvd_v5_0_irq_funcs;
  return;
}
}
int ldv_retval_26 ;
int ldv_retval_25 ;
extern int ldv_probe_21(void) ;
extern int ldv_release_21(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_19(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  uvd_v5_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  uvd_v5_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_20(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  uvd_v5_0_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_main_exported_21(void)
{
  void *ldvarg525 ;
  void *tmp ;
  void *ldvarg530 ;
  void *tmp___0 ;
  void *ldvarg526 ;
  void *tmp___1 ;
  enum amd_clockgating_state ldvarg524 ;
  void *ldvarg529 ;
  void *tmp___2 ;
  void *ldvarg531 ;
  void *tmp___3 ;
  void *ldvarg523 ;
  void *tmp___4 ;
  void *ldvarg522 ;
  void *tmp___5 ;
  void *ldvarg527 ;
  void *tmp___6 ;
  void *ldvarg520 ;
  void *tmp___7 ;
  void *ldvarg521 ;
  void *tmp___8 ;
  void *ldvarg532 ;
  void *tmp___9 ;
  enum amd_powergating_state ldvarg528 ;
  void *ldvarg533 ;
  void *tmp___10 ;
  void *ldvarg534 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg525 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg530 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg526 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg529 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg531 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg523 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg522 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg527 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg520 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg521 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg532 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg533 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg534 = tmp___11;
  ldv_memset((void *)(& ldvarg524), 0, 4UL);
  ldv_memset((void *)(& ldvarg528), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_hw_fini(ldvarg534);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_hw_fini(ldvarg534);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_hw_fini(ldvarg534);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 1: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_print_status(ldvarg533);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_print_status(ldvarg533);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_print_status(ldvarg533);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 2: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_early_init(ldvarg532);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_early_init(ldvarg532);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_early_init(ldvarg532);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 3: ;
  if (ldv_state_variable_21 == 2) {
    ldv_retval_26 = uvd_v5_0_suspend(ldvarg531);
    if (ldv_retval_26 == 0) {
      ldv_state_variable_21 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43947;
  case 4: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_sw_init(ldvarg530);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_sw_init(ldvarg530);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_sw_init(ldvarg530);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 5: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_set_powergating_state(ldvarg529, ldvarg528);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_set_powergating_state(ldvarg529, ldvarg528);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_set_powergating_state(ldvarg529, ldvarg528);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 6: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_wait_for_idle(ldvarg527);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_wait_for_idle(ldvarg527);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_wait_for_idle(ldvarg527);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 7: ;
  if (ldv_state_variable_21 == 3) {
    ldv_retval_25 = uvd_v5_0_resume(ldvarg526);
    if (ldv_retval_25 == 0) {
      ldv_state_variable_21 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43947;
  case 8: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_set_clockgating_state(ldvarg525, ldvarg524);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_set_clockgating_state(ldvarg525, ldvarg524);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_set_clockgating_state(ldvarg525, ldvarg524);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 9: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_hw_init(ldvarg523);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_hw_init(ldvarg523);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_hw_init(ldvarg523);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 10: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_soft_reset(ldvarg522);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_soft_reset(ldvarg522);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_soft_reset(ldvarg522);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 11: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_sw_fini(ldvarg521);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_sw_fini(ldvarg521);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_sw_fini(ldvarg521);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 12: ;
  if (ldv_state_variable_21 == 1) {
    uvd_v5_0_is_idle(ldvarg520);
    ldv_state_variable_21 = 1;
  } else {
  }
  if (ldv_state_variable_21 == 3) {
    uvd_v5_0_is_idle(ldvarg520);
    ldv_state_variable_21 = 3;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    uvd_v5_0_is_idle(ldvarg520);
    ldv_state_variable_21 = 2;
  } else {
  }
  goto ldv_43947;
  case 13: ;
  if (ldv_state_variable_21 == 3) {
    ldv_release_21();
    ldv_state_variable_21 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_21 == 2) {
    ldv_release_21();
    ldv_state_variable_21 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43947;
  case 14: ;
  if (ldv_state_variable_21 == 1) {
    ldv_probe_21();
    ldv_state_variable_21 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43947;
  default:
  ldv_stop();
  }
  ldv_43947: ;
  return;
}
}
void ldv_main_exported_19(void)
{
  unsigned int ldvarg818 ;
  enum amdgpu_interrupt_state ldvarg819 ;
  struct amdgpu_iv_entry *ldvarg817 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg817 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg818), 0, 4UL);
  ldv_memset((void *)(& ldvarg819), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_19 == 1) {
    uvd_v5_0_set_interrupt_state(uvd_v5_0_irq_funcs_group0, uvd_v5_0_irq_funcs_group1,
                                 ldvarg818, ldvarg819);
    ldv_state_variable_19 = 1;
  } else {
  }
  goto ldv_43970;
  case 1: ;
  if (ldv_state_variable_19 == 1) {
    uvd_v5_0_process_interrupt(uvd_v5_0_irq_funcs_group0, uvd_v5_0_irq_funcs_group1,
                               ldvarg817);
    ldv_state_variable_19 = 1;
  } else {
  }
  goto ldv_43970;
  default:
  ldv_stop();
  }
  ldv_43970: ;
  return;
}
}
void ldv_main_exported_20(void)
{
  uint64_t ldvarg99 ;
  struct amdgpu_ib *ldvarg96 ;
  void *tmp ;
  u32 ldvarg97 ;
  struct amdgpu_cs_parser *ldvarg98 ;
  void *tmp___0 ;
  uint64_t ldvarg100 ;
  bool ldvarg94 ;
  unsigned int ldvarg101 ;
  struct amdgpu_semaphore *ldvarg95 ;
  void *tmp___1 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(272UL);
  ldvarg96 = (struct amdgpu_ib *)tmp;
  tmp___0 = ldv_init_zalloc(200UL);
  ldvarg98 = (struct amdgpu_cs_parser *)tmp___0;
  tmp___1 = ldv_init_zalloc(24UL);
  ldvarg95 = (struct amdgpu_semaphore *)tmp___1;
  ldv_memset((void *)(& ldvarg99), 0, 8UL);
  ldv_memset((void *)(& ldvarg97), 0, 4UL);
  ldv_memset((void *)(& ldvarg100), 0, 8UL);
  ldv_memset((void *)(& ldvarg94), 0, 1UL);
  ldv_memset((void *)(& ldvarg101), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_emit_fence(uvd_v5_0_ring_funcs_group0, ldvarg100, ldvarg99, ldvarg101);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 1: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_get_rptr(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 2: ;
  if (ldv_state_variable_20 == 1) {
    amdgpu_uvd_ring_parse_cs(ldvarg98, ldvarg97);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 3: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_test_ring(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 4: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_set_wptr(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 5: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_get_wptr(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 6: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_emit_ib(uvd_v5_0_ring_funcs_group0, ldvarg96);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 7: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_emit_semaphore(uvd_v5_0_ring_funcs_group0, ldvarg95, (int )ldvarg94);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 8: ;
  if (ldv_state_variable_20 == 1) {
    uvd_v5_0_ring_test_ib(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  case 9: ;
  if (ldv_state_variable_20 == 1) {
    amdgpu_ring_test_lockup(uvd_v5_0_ring_funcs_group0);
    ldv_state_variable_20 = 1;
  } else {
  }
  goto ldv_43985;
  default:
  ldv_stop();
  }
  ldv_43985: ;
  return;
}
}
bool ldv_queue_work_on_997(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_998(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                   struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_999(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                           struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1000(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1001(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_1011(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1013(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1012(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1015(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1014(struct workqueue_struct *ldv_func_arg1 ) ;
static void uvd_v6_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void uvd_v6_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static int uvd_v6_0_start(struct amdgpu_device *adev ) ;
static void uvd_v6_0_stop(struct amdgpu_device *adev ) ;
static u32 uvd_v6_0_ring_get_rptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15780U, 0);
  return (tmp);
}
}
static u32 uvd_v6_0_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = ring->adev;
  tmp = amdgpu_mm_rreg(adev, 15781U, 0);
  return (tmp);
}
}
static void uvd_v6_0_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  return;
}
}
static int uvd_v6_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v6_0_set_ring_funcs(adev);
  uvd_v6_0_set_irq_funcs(adev);
  return (0);
}
}
static int uvd_v6_0_sw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 124U, & adev->uvd.irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_init(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = & adev->uvd.ring;
  sprintf((char *)(& ring->name), "uvd");
  r = amdgpu_ring_init(adev, ring, 4096U, 2147483648U, 15U, & adev->uvd.irq, 0U, 3);
  return (r);
}
}
static int uvd_v6_0_sw_fini(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_sw_fini(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v6_0_hw_init(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  u32 tmp ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  r = uvd_v6_0_start(adev);
  if (r != 0) {
    goto done;
  } else {
  }
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    goto done;
  } else {
  }
  r = amdgpu_ring_lock(ring, 10U);
  if (r != 0) {
    drm_err("amdgpu: ring failed to lock UVD ring (%d).\n", r);
    goto done;
  } else {
  }
  tmp = 15794U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15793U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  tmp = 15795U;
  amdgpu_ring_write(ring, tmp);
  amdgpu_ring_write(ring, 1048575U);
  amdgpu_ring_write(ring, 15792U);
  amdgpu_ring_write(ring, 8U);
  amdgpu_ring_write(ring, 15616U);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_unlock_commit(ring);
  done: ;
  if (r == 0) {
    printk("\016[drm] UVD initialized successfully.\n");
  } else {
  }
  return (r);
}
}
static int uvd_v6_0_hw_fini(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  {
  adev = (struct amdgpu_device *)handle;
  ring = & adev->uvd.ring;
  uvd_v6_0_stop(adev);
  ring->ready = 0;
  return (0);
}
}
static int uvd_v6_0_suspend(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = uvd_v6_0_hw_fini((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_uvd_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int uvd_v6_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_uvd_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = uvd_v6_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static void uvd_v6_0_mc_resume(struct amdgpu_device *adev )
{
  uint64_t offset ;
  u32 size ;
  {
  amdgpu_mm_wreg(adev, 15455U, (unsigned int )adev->uvd.gpu_addr, 0);
  amdgpu_mm_wreg(adev, 15454U, (unsigned int )(adev->uvd.gpu_addr >> 32ULL), 0);
  offset = 256ULL;
  size = ((u32 )(adev->uvd.fw)->size + 4099U) & 4294963200U;
  amdgpu_mm_wreg(adev, 15746U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15747U, size, 0);
  offset = (uint64_t )size + offset;
  size = 1048576U;
  amdgpu_mm_wreg(adev, 15748U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15749U, size, 0);
  offset = (uint64_t )size + offset;
  size = 1048576U;
  amdgpu_mm_wreg(adev, 15750U, (u32 )(offset >> 3), 0);
  amdgpu_mm_wreg(adev, 15751U, size, 0);
  return;
}
}
static int uvd_v6_0_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  u32 rb_bufsz ;
  u32 tmp ;
  u32 lmi_swap_cntl ;
  u32 mp_swap_cntl ;
  int i ;
  int j ;
  int r ;
  u32 tmp_ ;
  u32 tmp___0 ;
  u32 tmp____0 ;
  u32 tmp___1 ;
  u32 tmp____1 ;
  u32 tmp___2 ;
  unsigned long __ms ;
  unsigned long tmp___3 ;
  unsigned long __ms___0 ;
  unsigned long tmp___4 ;
  u32 tmp____2 ;
  u32 tmp___5 ;
  unsigned long __ms___1 ;
  unsigned long tmp___6 ;
  unsigned long __ms___2 ;
  unsigned long tmp___7 ;
  u32 tmp____3 ;
  u32 tmp___8 ;
  unsigned long __ms___3 ;
  unsigned long tmp___9 ;
  u32 status ;
  unsigned long __ms___4 ;
  unsigned long tmp___10 ;
  u32 tmp____4 ;
  u32 tmp___11 ;
  unsigned long __ms___5 ;
  unsigned long tmp___12 ;
  u32 tmp____5 ;
  u32 tmp___13 ;
  unsigned long __ms___6 ;
  unsigned long tmp___14 ;
  u32 tmp____6 ;
  u32 tmp___15 ;
  u32 tmp____7 ;
  u32 tmp___16 ;
  unsigned long tmp___17 ;
  int tmp___18 ;
  u32 tmp____8 ;
  u32 tmp___19 ;
  {
  ring = & adev->uvd.ring;
  tmp___0 = amdgpu_mm_rreg(adev, 14532U, 0);
  tmp_ = tmp___0;
  tmp_ = tmp_ & 4294967291U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 14532U, tmp_, 0);
  lmi_swap_cntl = 0U;
  mp_swap_cntl = 0U;
  uvd_v6_0_mc_resume(adev);
  amdgpu_mm_wreg(adev, 15658U, 0U, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp____0 = tmp___1;
  tmp____0 = tmp____0 & 4294967293U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 15680U, tmp____0, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____1 = tmp___2;
  tmp____1 = tmp____1 & 4294967039U;
  tmp____1 = tmp____1 | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp____1, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43769;
    ldv_43768:
    __const_udelay(4295000UL);
    ldv_43769:
    tmp___3 = __ms;
    __ms = __ms - 1UL;
    if (tmp___3 != 0UL) {
      goto ldv_43768;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8431U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43773;
    ldv_43772:
    __const_udelay(4295000UL);
    ldv_43773:
    tmp___4 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___4 != 0UL) {
      goto ldv_43772;
    } else {
    }
  }
  tmp___5 = amdgpu_mm_rreg(adev, 920U, 0);
  tmp____2 = tmp___5;
  tmp____2 = tmp____2 & 4294705151U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 920U, tmp____2, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___1 = 5UL;
    goto ldv_43778;
    ldv_43777:
    __const_udelay(4295000UL);
    ldv_43778:
    tmp___6 = __ms___1;
    __ms___1 = __ms___1 - 1UL;
    if (tmp___6 != 0UL) {
      goto ldv_43777;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15718U, 3154752U, 0);
  amdgpu_mm_wreg(adev, 15725U, lmi_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15727U, mp_swap_cntl, 0);
  amdgpu_mm_wreg(adev, 15737U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15738U, 0U, 0);
  amdgpu_mm_wreg(adev, 15739U, 67903552U, 0);
  amdgpu_mm_wreg(adev, 15740U, 0U, 0);
  amdgpu_mm_wreg(adev, 15742U, 0U, 0);
  amdgpu_mm_wreg(adev, 15741U, 136U, 0);
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___2 = 5UL;
    goto ldv_43782;
    ldv_43781:
    __const_udelay(4295000UL);
    ldv_43782:
    tmp___7 = __ms___2;
    __ms___2 = __ms___2 - 1UL;
    if (tmp___7 != 0UL) {
      goto ldv_43781;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 512U, 0);
  tmp___8 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____3 = tmp___8;
  tmp____3 = tmp____3 & 4294967039U;
  tmp____3 = tmp____3;
  amdgpu_mm_wreg(adev, 15677U, tmp____3, 0);
  amdgpu_mm_wreg(adev, 15776U, 0U, 0);
  __ms___3 = 10UL;
  goto ldv_43787;
  ldv_43786:
  __const_udelay(4295000UL);
  ldv_43787:
  tmp___9 = __ms___3;
  __ms___3 = __ms___3 - 1UL;
  if (tmp___9 != 0UL) {
    goto ldv_43786;
  } else {
  }
  i = 0;
  goto ldv_43809;
  ldv_43808:
  j = 0;
  goto ldv_43796;
  ldv_43795:
  status = amdgpu_mm_rreg(adev, 15791U, 0);
  if ((status & 2U) != 0U) {
    goto ldv_43790;
  } else {
  }
  __ms___4 = 10UL;
  goto ldv_43793;
  ldv_43792:
  __const_udelay(4295000UL);
  ldv_43793:
  tmp___10 = __ms___4;
  __ms___4 = __ms___4 - 1UL;
  if (tmp___10 != 0UL) {
    goto ldv_43792;
  } else {
  }
  j = j + 1;
  ldv_43796: ;
  if (j <= 99) {
    goto ldv_43795;
  } else {
  }
  ldv_43790:
  r = 0;
  if ((status & 2U) != 0U) {
    goto ldv_43797;
  } else {
  }
  drm_err("UVD not responding, trying to reset the VCPU!!!\n");
  tmp___11 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____4 = tmp___11;
  tmp____4 = tmp____4 & 4294967287U;
  tmp____4 = tmp____4 | 8U;
  amdgpu_mm_wreg(adev, 15776U, tmp____4, 0);
  __ms___5 = 10UL;
  goto ldv_43801;
  ldv_43800:
  __const_udelay(4295000UL);
  ldv_43801:
  tmp___12 = __ms___5;
  __ms___5 = __ms___5 - 1UL;
  if (tmp___12 != 0UL) {
    goto ldv_43800;
  } else {
  }
  tmp___13 = amdgpu_mm_rreg(adev, 15776U, 0);
  tmp____5 = tmp___13;
  tmp____5 = tmp____5 & 4294967287U;
  tmp____5 = tmp____5;
  amdgpu_mm_wreg(adev, 15776U, tmp____5, 0);
  __ms___6 = 10UL;
  goto ldv_43806;
  ldv_43805:
  __const_udelay(4295000UL);
  ldv_43806:
  tmp___14 = __ms___6;
  __ms___6 = __ms___6 - 1UL;
  if (tmp___14 != 0UL) {
    goto ldv_43805;
  } else {
  }
  r = -1;
  i = i + 1;
  ldv_43809: ;
  if (i <= 9) {
    goto ldv_43808;
  } else {
  }
  ldv_43797: ;
  if (r != 0) {
    drm_err("UVD not responding, giving up!!!\n");
    return (r);
  } else {
  }
  tmp___15 = amdgpu_mm_rreg(adev, 15680U, 0);
  tmp____6 = tmp___15;
  tmp____6 = tmp____6 & 4294967289U;
  tmp____6 = tmp____6 | 6U;
  amdgpu_mm_wreg(adev, 15680U, tmp____6, 0);
  tmp___16 = amdgpu_mm_rreg(adev, 15791U, 0);
  tmp____7 = tmp___16;
  tmp____7 = tmp____7 & 4294967291U;
  tmp____7 = tmp____7;
  amdgpu_mm_wreg(adev, 15791U, tmp____7, 0);
  tmp___17 = __roundup_pow_of_two((unsigned long )ring->ring_size);
  tmp___18 = __ilog2_u64((u64 )tmp___17);
  rb_bufsz = (u32 )tmp___18;
  tmp = 0U;
  tmp = (tmp & 4294967264U) | (rb_bufsz & 31U);
  tmp = (tmp & 4294959359U) | 256U;
  tmp = tmp | 65536U;
  tmp = tmp & 4293918719U;
  tmp = tmp | 16777216U;
  tmp = tmp | 268435456U;
  amdgpu_mm_wreg(adev, 15785U, tmp, 0);
  amdgpu_mm_wreg(adev, 15782U, 0U, 0);
  amdgpu_mm_wreg(adev, 15786U, (unsigned int )(ring->gpu_addr >> 32ULL) >> 2, 0);
  amdgpu_mm_wreg(adev, 15465U, (unsigned int )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 15464U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 15780U, 0U, 0);
  ring->wptr = amdgpu_mm_rreg(adev, 15780U, 0);
  amdgpu_mm_wreg(adev, 15781U, ring->wptr, 0);
  tmp___19 = amdgpu_mm_rreg(adev, 15785U, 0);
  tmp____8 = tmp___19;
  tmp____8 = tmp____8 & 4294901759U;
  tmp____8 = tmp____8;
  amdgpu_mm_wreg(adev, 15785U, tmp____8, 0);
  return (0);
}
}
static void uvd_v6_0_stop(struct amdgpu_device *adev )
{
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  unsigned long __ms___0 ;
  unsigned long tmp___1 ;
  u32 tmp____0 ;
  u32 tmp___2 ;
  {
  amdgpu_mm_wreg(adev, 15785U, 285278465U, 0);
  tmp = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967039U;
  tmp_ = tmp_ | 256U;
  amdgpu_mm_wreg(adev, 15677U, tmp_, 0);
  if (1) {
    __const_udelay(4295000UL);
  } else {
    __ms = 1UL;
    goto ldv_43819;
    ldv_43818:
    __const_udelay(4295000UL);
    ldv_43819:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43818;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15776U, 8U, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms___0 = 5UL;
    goto ldv_43823;
    ldv_43822:
    __const_udelay(4295000UL);
    ldv_43823:
    tmp___1 = __ms___0;
    __ms___0 = __ms___0 - 1UL;
    if (tmp___1 != 0UL) {
      goto ldv_43822;
    } else {
    }
  }
  amdgpu_mm_wreg(adev, 15768U, 0U, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 15677U, 0);
  tmp____0 = tmp___2;
  tmp____0 = tmp____0 & 4294967039U;
  tmp____0 = tmp____0;
  amdgpu_mm_wreg(adev, 15677U, tmp____0, 0);
  return;
}
}
static void uvd_v6_0_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq ,
                                     unsigned int flags )
{
  int __ret_warn_on ;
  long tmp ;
  {
  __ret_warn_on = (int )flags & 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/uvd_v6_0.c",
                       462);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, (u32 )seq);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, (u32 )addr);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL) & 255U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15300U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15301U);
  amdgpu_ring_write(ring, 0U);
  amdgpu_ring_write(ring, 15299U);
  amdgpu_ring_write(ring, 2U);
  return;
}
}
static bool uvd_v6_0_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                         bool emit_wait )
{
  uint64_t addr ;
  {
  addr = semaphore->gpu_addr;
  amdgpu_ring_write(ring, 15296U);
  amdgpu_ring_write(ring, (u32 )(addr >> 3) & 1048575U);
  amdgpu_ring_write(ring, 15297U);
  amdgpu_ring_write(ring, (u32 )(addr >> 23) & 1048575U);
  amdgpu_ring_write(ring, 15298U);
  amdgpu_ring_write(ring, (u32 )((int )emit_wait | 128));
  return (1);
}
}
static int uvd_v6_0_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  {
  adev = ring->adev;
  tmp = 0U;
  amdgpu_mm_wreg(adev, 15805U, 3405700781U, 0);
  r = amdgpu_ring_lock(ring, 3U);
  if (r != 0) {
    drm_err("amdgpu: cp failed to lock ring %d (%d).\n", ring->idx, r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 15805U);
  amdgpu_ring_write(ring, 3735928559U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_43849;
  ldv_43848:
  tmp = amdgpu_mm_rreg(adev, 15805U, 0);
  if (tmp == 3735928559U) {
    goto ldv_43847;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43849: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43848;
  } else {
  }
  ldv_43847: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed (0x%08X)\n", ring->idx, tmp);
    r = -22;
  }
  return (r);
}
}
static void uvd_v6_0_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  {
  amdgpu_ring_write(ring, 15463U);
  amdgpu_ring_write(ring, (unsigned int )ib->gpu_addr);
  amdgpu_ring_write(ring, 15462U);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, 15778U);
  amdgpu_ring_write(ring, ib->length_dw);
  return;
}
}
static int uvd_v6_0_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_fence *fence ;
  int r ;
  {
  fence = (struct amdgpu_fence *)0;
  r = amdgpu_uvd_get_create_msg(ring, 1U, (struct amdgpu_fence **)0);
  if (r != 0) {
    drm_err("amdgpu: failed to get create msg (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_uvd_get_destroy_msg(ring, 1U, & fence);
  if (r != 0) {
    drm_err("amdgpu: failed to get destroy ib (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
    goto error;
  } else {
  }
  printk("\016[drm] ib test on ring %d succeeded\n", ring->idx);
  error:
  amdgpu_fence_unref(& fence);
  return (r);
}
}
static bool uvd_v6_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  return ((tmp & 524288U) == 0U);
}
}
static int uvd_v6_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43870;
  ldv_43869:
  tmp = amdgpu_mm_rreg(adev, 916U, 0);
  if ((tmp & 524288U) == 0U) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_43870: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43869;
  } else {
  }
  return (-110);
}
}
static int uvd_v6_0_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  uvd_v6_0_stop(adev);
  tmp = amdgpu_mm_rreg(adev, 920U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294705151U;
  tmp_ = tmp_ | 262144U;
  amdgpu_mm_wreg(adev, 920U, tmp_, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms = 5UL;
    goto ldv_43879;
    ldv_43878:
    __const_udelay(4295000UL);
    ldv_43879:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43878;
    } else {
    }
  }
  tmp___1 = uvd_v6_0_start(adev);
  return (tmp___1);
}
}
static void uvd_v6_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  u32 tmp___29 ;
  u32 tmp___30 ;
  u32 tmp___31 ;
  u32 tmp___32 ;
  u32 tmp___33 ;
  u32 tmp___34 ;
  u32 tmp___35 ;
  u32 tmp___36 ;
  u32 tmp___37 ;
  u32 tmp___38 ;
  u32 tmp___39 ;
  u32 tmp___40 ;
  u32 tmp___41 ;
  u32 tmp___42 ;
  u32 tmp___43 ;
  u32 tmp___44 ;
  u32 tmp___45 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "UVD 6.0 registers\n");
  tmp = amdgpu_mm_rreg(adev, 15296U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_LOW=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 15297U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_ADDR_HIGH=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 15298U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CMD=0x%08X\n", tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 15299U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_CMD=0x%08X\n", tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 15300U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA0=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 15301U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_GPCOM_VCPU_DATA1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 15302U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_ENGINE_CNTL=0x%08X\n", tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 15315U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_ADDR_CONFIG=0x%08X\n",
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 15316U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DB_ADDR_CONFIG=0x%08X\n",
            tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 15317U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_UDEC_DBW_ADDR_CONFIG=0x%08X\n",
            tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 15616U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_CNTL=0x%08X\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 15654U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_EXT40_ADDR=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 15656U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_INDEX=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 15657U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CTX_DATA=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 15658U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_GATE=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 15660U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CGC_CTRL=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 15677U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL2=0x%08X\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 15680U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MASTINT_EN=0x%08X\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 15717U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_ADDR_EXT=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 15718U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_CTRL=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 15725U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_LMI_SWAP_CNTL=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 15727U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MP_SWAP_CNTL=0x%08X\n", tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 15737U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA0=0x%08X\n", tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 15738U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXA1=0x%08X\n", tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 15739U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB0=0x%08X\n", tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 15740U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUXB1=0x%08X\n", tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 15741U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_MUX=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 15742U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_MPC_SET_ALU=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 15746U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET0=0x%08X\n",
            tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 15747U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE0=0x%08X\n",
            tmp___28);
  tmp___29 = amdgpu_mm_rreg(adev, 15748U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET1=0x%08X\n",
            tmp___29);
  tmp___30 = amdgpu_mm_rreg(adev, 15749U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE1=0x%08X\n",
            tmp___30);
  tmp___31 = amdgpu_mm_rreg(adev, 15750U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_OFFSET2=0x%08X\n",
            tmp___31);
  tmp___32 = amdgpu_mm_rreg(adev, 15751U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CACHE_SIZE2=0x%08X\n",
            tmp___32);
  tmp___33 = amdgpu_mm_rreg(adev, 15768U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_VCPU_CNTL=0x%08X\n", tmp___33);
  tmp___34 = amdgpu_mm_rreg(adev, 15776U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SOFT_RESET=0x%08X\n", tmp___34);
  tmp___35 = amdgpu_mm_rreg(adev, 15778U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_IB_SIZE=0x%08X\n", tmp___35);
  tmp___36 = amdgpu_mm_rreg(adev, 15780U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_RPTR=0x%08X\n", tmp___36);
  tmp___37 = amdgpu_mm_rreg(adev, 15781U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR=0x%08X\n", tmp___37);
  tmp___38 = amdgpu_mm_rreg(adev, 15782U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_WPTR_CNTL=0x%08X\n",
            tmp___38);
  tmp___39 = amdgpu_mm_rreg(adev, 15785U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_RBC_RB_CNTL=0x%08X\n", tmp___39);
  tmp___40 = amdgpu_mm_rreg(adev, 15791U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_STATUS=0x%08X\n", tmp___40);
  tmp___41 = amdgpu_mm_rreg(adev, 15792U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_TIMEOUT_STATUS=0x%08X\n",
            tmp___41);
  tmp___42 = amdgpu_mm_rreg(adev, 15793U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___42);
  tmp___43 = amdgpu_mm_rreg(adev, 15794U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_WAIT_FAULT_TIMEOUT_CNTL=0x%08X\n",
            tmp___43);
  tmp___44 = amdgpu_mm_rreg(adev, 15795U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_SEMA_SIGNAL_INCOMPLETE_TIMEOUT_CNTL=0x%08X\n",
            tmp___44);
  tmp___45 = amdgpu_mm_rreg(adev, 15805U, 0);
  _dev_info((struct device const *)adev->dev, "  UVD_CONTEXT_ID=0x%08X\n", tmp___45);
  return;
}
}
static int uvd_v6_0_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  {
  return (0);
}
}
static int uvd_v6_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("uvd_v6_0_process_interrupt", "IH: UVD TRAP\n");
  } else {
  }
  amdgpu_fence_process(& adev->uvd.ring);
  return (0);
}
}
static int uvd_v6_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int uvd_v6_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    uvd_v6_0_stop(adev);
    return (0);
  } else {
    tmp = uvd_v6_0_start(adev);
    return (tmp);
  }
}
}
struct amd_ip_funcs const uvd_v6_0_ip_funcs =
     {& uvd_v6_0_early_init, (int (*)(void * ))0, & uvd_v6_0_sw_init, & uvd_v6_0_sw_fini,
    & uvd_v6_0_hw_init, & uvd_v6_0_hw_fini, & uvd_v6_0_suspend, & uvd_v6_0_resume,
    & uvd_v6_0_is_idle, & uvd_v6_0_wait_for_idle, & uvd_v6_0_soft_reset, & uvd_v6_0_print_status,
    & uvd_v6_0_set_clockgating_state, & uvd_v6_0_set_powergating_state};
static struct amdgpu_ring_funcs const uvd_v6_0_ring_funcs =
     {& uvd_v6_0_ring_get_rptr, & uvd_v6_0_ring_get_wptr, & uvd_v6_0_ring_set_wptr,
    & amdgpu_uvd_ring_parse_cs, & uvd_v6_0_ring_emit_ib, & uvd_v6_0_ring_emit_fence,
    & uvd_v6_0_ring_emit_semaphore, 0, 0, 0, & uvd_v6_0_ring_test_ring, & uvd_v6_0_ring_test_ib,
    & amdgpu_ring_test_lockup};
static void uvd_v6_0_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.ring.funcs = & uvd_v6_0_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const uvd_v6_0_irq_funcs = {& uvd_v6_0_set_interrupt_state, & uvd_v6_0_process_interrupt};
static void uvd_v6_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->uvd.irq.num_types = 1U;
  adev->uvd.irq.funcs = & uvd_v6_0_irq_funcs;
  return;
}
}
int ldv_retval_0 ;
int ldv_retval_1 ;
extern int ldv_release_18(void) ;
extern int ldv_probe_18(void) ;
void ldv_initialize_amdgpu_irq_src_funcs_16(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  uvd_v6_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  uvd_v6_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_initialize_amdgpu_ring_funcs_17(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  uvd_v6_0_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_main_exported_18(void)
{
  void *ldvarg28 ;
  void *tmp ;
  enum amd_powergating_state ldvarg34 ;
  void *ldvarg29 ;
  void *tmp___0 ;
  void *ldvarg36 ;
  void *tmp___1 ;
  void *ldvarg25 ;
  void *tmp___2 ;
  void *ldvarg37 ;
  void *tmp___3 ;
  enum amd_clockgating_state ldvarg30 ;
  void *ldvarg33 ;
  void *tmp___4 ;
  void *ldvarg38 ;
  void *tmp___5 ;
  void *ldvarg31 ;
  void *tmp___6 ;
  void *ldvarg32 ;
  void *tmp___7 ;
  void *ldvarg35 ;
  void *tmp___8 ;
  void *ldvarg26 ;
  void *tmp___9 ;
  void *ldvarg27 ;
  void *tmp___10 ;
  void *ldvarg39 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg28 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg29 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg36 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg25 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg37 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg33 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg38 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg31 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg32 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg35 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg26 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg27 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg39 = tmp___11;
  ldv_memset((void *)(& ldvarg34), 0, 4UL);
  ldv_memset((void *)(& ldvarg30), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_hw_fini(ldvarg39);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_hw_fini(ldvarg39);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_hw_fini(ldvarg39);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 1: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_print_status(ldvarg38);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_print_status(ldvarg38);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_print_status(ldvarg38);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 2: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_early_init(ldvarg37);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_early_init(ldvarg37);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_early_init(ldvarg37);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 3: ;
  if (ldv_state_variable_18 == 2) {
    ldv_retval_1 = uvd_v6_0_suspend(ldvarg36);
    if (ldv_retval_1 == 0) {
      ldv_state_variable_18 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43946;
  case 4: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_sw_init(ldvarg35);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_sw_init(ldvarg35);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_sw_init(ldvarg35);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 5: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_set_powergating_state(ldvarg33, ldvarg34);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_set_powergating_state(ldvarg33, ldvarg34);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_set_powergating_state(ldvarg33, ldvarg34);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 6: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_wait_for_idle(ldvarg32);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_wait_for_idle(ldvarg32);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_wait_for_idle(ldvarg32);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 7: ;
  if (ldv_state_variable_18 == 3) {
    ldv_retval_0 = uvd_v6_0_resume(ldvarg31);
    if (ldv_retval_0 == 0) {
      ldv_state_variable_18 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43946;
  case 8: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_set_clockgating_state(ldvarg29, ldvarg30);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_set_clockgating_state(ldvarg29, ldvarg30);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_set_clockgating_state(ldvarg29, ldvarg30);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 9: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_hw_init(ldvarg28);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_hw_init(ldvarg28);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_hw_init(ldvarg28);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 10: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_soft_reset(ldvarg27);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_soft_reset(ldvarg27);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_soft_reset(ldvarg27);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 11: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_sw_fini(ldvarg26);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_sw_fini(ldvarg26);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_sw_fini(ldvarg26);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 12: ;
  if (ldv_state_variable_18 == 1) {
    uvd_v6_0_is_idle(ldvarg25);
    ldv_state_variable_18 = 1;
  } else {
  }
  if (ldv_state_variable_18 == 3) {
    uvd_v6_0_is_idle(ldvarg25);
    ldv_state_variable_18 = 3;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    uvd_v6_0_is_idle(ldvarg25);
    ldv_state_variable_18 = 2;
  } else {
  }
  goto ldv_43946;
  case 13: ;
  if (ldv_state_variable_18 == 3) {
    ldv_release_18();
    ldv_state_variable_18 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_18 == 2) {
    ldv_release_18();
    ldv_state_variable_18 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43946;
  case 14: ;
  if (ldv_state_variable_18 == 1) {
    ldv_probe_18();
    ldv_state_variable_18 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43946;
  default:
  ldv_stop();
  }
  ldv_43946: ;
  return;
}
}
void ldv_main_exported_16(void)
{
  unsigned int ldvarg44 ;
  struct amdgpu_iv_entry *ldvarg42 ;
  void *tmp ;
  enum amdgpu_interrupt_state ldvarg43 ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg42 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg44), 0, 4UL);
  ldv_memset((void *)(& ldvarg43), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_16 == 1) {
    uvd_v6_0_set_interrupt_state(uvd_v6_0_irq_funcs_group0, uvd_v6_0_irq_funcs_group1,
                                 ldvarg44, ldvarg43);
    ldv_state_variable_16 = 1;
  } else {
  }
  goto ldv_43969;
  case 1: ;
  if (ldv_state_variable_16 == 1) {
    uvd_v6_0_process_interrupt(uvd_v6_0_irq_funcs_group0, uvd_v6_0_irq_funcs_group1,
                               ldvarg42);
    ldv_state_variable_16 = 1;
  } else {
  }
  goto ldv_43969;
  default:
  ldv_stop();
  }
  ldv_43969: ;
  return;
}
}
void ldv_main_exported_17(void)
{
  unsigned int ldvarg309 ;
  struct amdgpu_cs_parser *ldvarg306 ;
  void *tmp ;
  struct amdgpu_ib *ldvarg304 ;
  void *tmp___0 ;
  struct amdgpu_semaphore *ldvarg303 ;
  void *tmp___1 ;
  bool ldvarg302 ;
  uint64_t ldvarg307 ;
  u32 ldvarg305 ;
  uint64_t ldvarg308 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(200UL);
  ldvarg306 = (struct amdgpu_cs_parser *)tmp;
  tmp___0 = ldv_init_zalloc(272UL);
  ldvarg304 = (struct amdgpu_ib *)tmp___0;
  tmp___1 = ldv_init_zalloc(24UL);
  ldvarg303 = (struct amdgpu_semaphore *)tmp___1;
  ldv_memset((void *)(& ldvarg309), 0, 4UL);
  ldv_memset((void *)(& ldvarg302), 0, 1UL);
  ldv_memset((void *)(& ldvarg307), 0, 8UL);
  ldv_memset((void *)(& ldvarg305), 0, 4UL);
  ldv_memset((void *)(& ldvarg308), 0, 8UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_emit_fence(uvd_v6_0_ring_funcs_group0, ldvarg308, ldvarg307, ldvarg309);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 1: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_get_rptr(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 2: ;
  if (ldv_state_variable_17 == 1) {
    amdgpu_uvd_ring_parse_cs(ldvarg306, ldvarg305);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 3: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_test_ring(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 4: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_set_wptr(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 5: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_get_wptr(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 6: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_emit_ib(uvd_v6_0_ring_funcs_group0, ldvarg304);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 7: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_emit_semaphore(uvd_v6_0_ring_funcs_group0, ldvarg303, (int )ldvarg302);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 8: ;
  if (ldv_state_variable_17 == 1) {
    uvd_v6_0_ring_test_ib(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  case 9: ;
  if (ldv_state_variable_17 == 1) {
    amdgpu_ring_test_lockup(uvd_v6_0_ring_funcs_group0);
    ldv_state_variable_17 = 1;
  } else {
  }
  goto ldv_43984;
  default:
  ldv_stop();
  }
  ldv_43984: ;
  return;
}
}
bool ldv_queue_work_on_1011(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1012(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1013(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1014(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1015(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_1025(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1027(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1026(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1029(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1028(struct workqueue_struct *ldv_func_arg1 ) ;
bool ldv_cancel_delayed_work_sync_1030(struct delayed_work *ldv_func_arg1 ) ;
__inline static bool queue_delayed_work___1(struct workqueue_struct *wq , struct delayed_work *dwork ,
                                            unsigned long delay )
{
  bool tmp ;
  {
  tmp = ldv_queue_delayed_work_on_1026(8192, wq, dwork, delay);
  return (tmp);
}
}
__inline static bool schedule_delayed_work___0(struct delayed_work *dwork , unsigned long delay )
{
  bool tmp ;
  {
  tmp = queue_delayed_work___1(system_wq, dwork, delay);
  return (tmp);
}
}
void call_and_disable_all_9(int state ) ;
void call_and_disable_work_9(struct work_struct *work ) ;
void invoke_work_9(void) ;
void activate_work_9(struct work_struct *work , int state ) ;
void disable_work_9(struct work_struct *work ) ;
static void amdgpu_vce_idle_work_handler(struct work_struct *work ) ;
int amdgpu_vce_sw_init(struct amdgpu_device *adev , unsigned long size )
{
  char const *fw_name ;
  struct common_firmware_header const *hdr ;
  unsigned int ucode_version ;
  unsigned int version_major ;
  unsigned int version_minor ;
  unsigned int binary_id ;
  int i ;
  int r ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  {
  __init_work(& adev->vce.idle_work.work, 0);
  __constr_expr_0.counter = 137438953408L;
  adev->vce.idle_work.work.data = __constr_expr_0;
  lockdep_init_map(& adev->vce.idle_work.work.lockdep_map, "(&(&adev->vce.idle_work)->work)",
                   & __key, 0);
  INIT_LIST_HEAD(& adev->vce.idle_work.work.entry);
  adev->vce.idle_work.work.func = & amdgpu_vce_idle_work_handler;
  init_timer_key(& adev->vce.idle_work.timer, 2097152U, "(&(&adev->vce.idle_work)->timer)",
                 & __key___0);
  adev->vce.idle_work.timer.function = & delayed_work_timer_fn;
  adev->vce.idle_work.timer.data = (unsigned long )(& adev->vce.idle_work);
  switch ((unsigned int )adev->asic_type) {
  case 0U:
  fw_name = "radeon/bonaire_vce.bin";
  goto ldv_43755;
  case 1U:
  fw_name = "radeon/kaveri_vce.bin";
  goto ldv_43755;
  case 2U:
  fw_name = "radeon/kabini_vce.bin";
  goto ldv_43755;
  case 3U:
  fw_name = "radeon/hawaii_vce.bin";
  goto ldv_43755;
  case 4U:
  fw_name = "radeon/mullins_vce.bin";
  goto ldv_43755;
  case 6U:
  fw_name = "amdgpu/tonga_vce.bin";
  goto ldv_43755;
  case 7U:
  fw_name = "amdgpu/carrizo_vce.bin";
  goto ldv_43755;
  default: ;
  return (-22);
  }
  ldv_43755:
  r = request_firmware(& adev->vce.fw, fw_name, adev->dev);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "amdgpu_vce: Can\'t load firmware \"%s\"\n",
            fw_name);
    return (r);
  } else {
  }
  r = amdgpu_ucode_validate(adev->vce.fw);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "amdgpu_vce: Can\'t validate firmware \"%s\"\n",
            fw_name);
    release_firmware(adev->vce.fw);
    adev->vce.fw = (struct firmware const *)0;
    return (r);
  } else {
  }
  hdr = (struct common_firmware_header const *)(adev->vce.fw)->data;
  ucode_version = hdr->ucode_version;
  version_major = ucode_version >> 20;
  version_minor = (ucode_version >> 8) & 4095U;
  binary_id = ucode_version & 255U;
  printk("\016[drm] Found VCE firmware Version: %hhd.%hhd Binary ID: %hhd\n", version_major,
         version_minor, binary_id);
  adev->vce.fw_version = ((version_major << 24) | (version_minor << 16)) | (binary_id << 8);
  r = amdgpu_bo_create(adev, size, 4096, 1, 4U, 0ULL, (struct sg_table *)0, & adev->vce.vcpu_bo);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to allocate VCE bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_reserve(adev->vce.vcpu_bo, 0);
  if (r != 0) {
    amdgpu_bo_unref(& adev->vce.vcpu_bo);
    dev_err((struct device const *)adev->dev, "(%d) failed to reserve VCE bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_pin(adev->vce.vcpu_bo, 4U, & adev->vce.gpu_addr);
  amdgpu_bo_unreserve(adev->vce.vcpu_bo);
  if (r != 0) {
    amdgpu_bo_unref(& adev->vce.vcpu_bo);
    dev_err((struct device const *)adev->dev, "(%d) VCE bo pin failed\n", r);
    return (r);
  } else {
  }
  i = 0;
  goto ldv_43764;
  ldv_43763:
  atomic_set((atomic_t *)(& adev->vce.handles) + (unsigned long )i, 0);
  adev->vce.filp[i] = (struct drm_file *)0;
  i = i + 1;
  ldv_43764: ;
  if (i <= 15) {
    goto ldv_43763;
  } else {
  }
  return (0);
}
}
int amdgpu_vce_sw_fini(struct amdgpu_device *adev )
{
  {
  if ((unsigned long )adev->vce.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (0);
  } else {
  }
  amdgpu_bo_unref(& adev->vce.vcpu_bo);
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->vce.ring));
  amdgpu_ring_fini((struct amdgpu_ring *)(& adev->vce.ring) + 1UL);
  release_firmware(adev->vce.fw);
  return (0);
}
}
int amdgpu_vce_suspend(struct amdgpu_device *adev )
{
  int i ;
  int tmp ;
  {
  if ((unsigned long )adev->vce.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (0);
  } else {
  }
  i = 0;
  goto ldv_43775;
  ldv_43774:
  tmp = atomic_read((atomic_t const *)(& adev->vce.handles) + (unsigned long )i);
  if (tmp != 0) {
    goto ldv_43773;
  } else {
  }
  i = i + 1;
  ldv_43775: ;
  if (i <= 15) {
    goto ldv_43774;
  } else {
  }
  ldv_43773: ;
  if (i == 16) {
    return (0);
  } else {
  }
  return (-22);
}
}
int amdgpu_vce_resume(struct amdgpu_device *adev )
{
  void *cpu_addr ;
  struct common_firmware_header const *hdr ;
  unsigned int offset ;
  int r ;
  {
  if ((unsigned long )adev->vce.vcpu_bo == (unsigned long )((struct amdgpu_bo *)0)) {
    return (-22);
  } else {
  }
  r = amdgpu_bo_reserve(adev->vce.vcpu_bo, 0);
  if (r != 0) {
    dev_err((struct device const *)adev->dev, "(%d) failed to reserve VCE bo\n",
            r);
    return (r);
  } else {
  }
  r = amdgpu_bo_kmap(adev->vce.vcpu_bo, & cpu_addr);
  if (r != 0) {
    amdgpu_bo_unreserve(adev->vce.vcpu_bo);
    dev_err((struct device const *)adev->dev, "(%d) VCE map failed\n", r);
    return (r);
  } else {
  }
  hdr = (struct common_firmware_header const *)(adev->vce.fw)->data;
  offset = hdr->ucode_array_offset_bytes;
  memcpy(cpu_addr, (void const *)(adev->vce.fw)->data + (unsigned long )offset,
           (unsigned long )(adev->vce.fw)->size - (unsigned long )offset);
  amdgpu_bo_kunmap(adev->vce.vcpu_bo);
  amdgpu_bo_unreserve(adev->vce.vcpu_bo);
  return (0);
}
}
static void amdgpu_vce_idle_work_handler(struct work_struct *work )
{
  struct amdgpu_device *adev ;
  struct work_struct const *__mptr ;
  unsigned long tmp ;
  unsigned int tmp___0 ;
  unsigned int tmp___1 ;
  {
  __mptr = (struct work_struct const *)work;
  adev = (struct amdgpu_device *)__mptr + 0xffffffffffffb0d8UL;
  tmp___0 = amdgpu_fence_count_emitted((struct amdgpu_ring *)(& adev->vce.ring));
  if (tmp___0 == 0U) {
    tmp___1 = amdgpu_fence_count_emitted((struct amdgpu_ring *)(& adev->vce.ring) + 1UL);
    if (tmp___1 == 0U) {
      if ((int )adev->pm.dpm_enabled) {
        amdgpu_dpm_enable_vce(adev, 0);
      } else {
        (*((adev->asic_funcs)->set_vce_clocks))(adev, 0U, 0U);
      }
    } else {
      tmp = msecs_to_jiffies(1000U);
      schedule_delayed_work___0(& adev->vce.idle_work, tmp);
    }
  } else {
    tmp = msecs_to_jiffies(1000U);
    schedule_delayed_work___0(& adev->vce.idle_work, tmp);
  }
  return;
}
}
static void amdgpu_vce_note_usage(struct amdgpu_device *adev )
{
  bool streams_changed ;
  bool set_clocks ;
  bool tmp ;
  int tmp___0 ;
  unsigned long tmp___1 ;
  bool tmp___2 ;
  {
  streams_changed = 0;
  tmp = ldv_cancel_delayed_work_sync_1030(& adev->vce.idle_work);
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  set_clocks = (bool )tmp___0;
  tmp___1 = msecs_to_jiffies(1000U);
  tmp___2 = schedule_delayed_work___0(& adev->vce.idle_work, tmp___1);
  set_clocks = ((int )set_clocks & (int )tmp___2) != 0;
  if ((int )adev->pm.dpm_enabled) {
    streams_changed = 0;
  } else {
  }
  if ((int )set_clocks || (int )streams_changed) {
    if ((int )adev->pm.dpm_enabled) {
      amdgpu_dpm_enable_vce(adev, 1);
    } else {
      (*((adev->asic_funcs)->set_vce_clocks))(adev, 53300U, 40000U);
    }
  } else {
  }
  return;
}
}
void amdgpu_vce_free_handles(struct amdgpu_device *adev , struct drm_file *filp )
{
  struct amdgpu_ring *ring ;
  int i ;
  int r ;
  u32 handle ;
  int tmp ;
  {
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  i = 0;
  goto ldv_43804;
  ldv_43803:
  tmp = atomic_read((atomic_t const *)(& adev->vce.handles) + (unsigned long )i);
  handle = (u32 )tmp;
  if (handle == 0U || (unsigned long )adev->vce.filp[i] != (unsigned long )filp) {
    goto ldv_43802;
  } else {
  }
  amdgpu_vce_note_usage(adev);
  r = amdgpu_vce_get_destroy_msg(ring, handle, (struct amdgpu_fence **)0);
  if (r != 0) {
    drm_err("Error destroying VCE handle (%d)!\n", r);
  } else {
  }
  adev->vce.filp[i] = (struct drm_file *)0;
  atomic_set((atomic_t *)(& adev->vce.handles) + (unsigned long )i, 0);
  ldv_43802:
  i = i + 1;
  ldv_43804: ;
  if (i <= 15) {
    goto ldv_43803;
  } else {
  }
  return;
}
}
int amdgpu_vce_get_create_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence )
{
  unsigned int ib_size_dw ;
  struct amdgpu_ib ib ;
  uint64_t dummy ;
  int i ;
  int r ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  {
  ib_size_dw = 1024U;
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, ib_size_dw * 4U, & ib);
  if (r != 0) {
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    return (r);
  } else {
  }
  dummy = ib.gpu_addr + 1024ULL;
  ib.length_dw = 0U;
  tmp = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp) = 12U;
  tmp___0 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___0) = 1U;
  tmp___1 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___1) = handle;
  tmp___2 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___2) = 48U;
  tmp___3 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___3) = 16777217U;
  tmp___4 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___4) = 0U;
  tmp___5 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___5) = 66U;
  tmp___6 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___6) = 10U;
  tmp___7 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___7) = 1U;
  tmp___8 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___8) = 128U;
  tmp___9 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___9) = 96U;
  tmp___10 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___10) = 256U;
  tmp___11 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___11) = 256U;
  tmp___12 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___12) = 12U;
  tmp___13 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___13) = 0U;
  tmp___14 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___14) = 20U;
  tmp___15 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___15) = 83886085U;
  tmp___16 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___16) = (unsigned int )(dummy >> 32ULL);
  tmp___17 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___17) = (u32 )dummy;
  tmp___18 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___18) = 1U;
  i = (int )ib.length_dw;
  goto ldv_43817;
  ldv_43816:
  *(ib.ptr + (unsigned long )i) = 0U;
  i = i + 1;
  ldv_43817: ;
  if ((unsigned int )i < ib_size_dw) {
    goto ldv_43816;
  } else {
  }
  r = amdgpu_ib_schedule(ring->adev, 1U, & ib, (void *)0);
  if (r != 0) {
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
  } else {
  }
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence **)0)) {
    *fence = amdgpu_fence_ref(ib.fence);
  } else {
  }
  amdgpu_ib_free(ring->adev, & ib);
  return (r);
}
}
int amdgpu_vce_get_destroy_msg(struct amdgpu_ring *ring , u32 handle , struct amdgpu_fence **fence )
{
  unsigned int ib_size_dw ;
  struct amdgpu_ib ib ;
  uint64_t dummy ;
  int i ;
  int r ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  {
  ib_size_dw = 1024U;
  r = amdgpu_ib_get(ring, (struct amdgpu_vm *)0, ib_size_dw * 4U, & ib);
  if (r != 0) {
    drm_err("amdgpu: failed to get ib (%d).\n", r);
    return (r);
  } else {
  }
  dummy = ib.gpu_addr + 1024ULL;
  ib.length_dw = 0U;
  tmp = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp) = 12U;
  tmp___0 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___0) = 1U;
  tmp___1 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___1) = handle;
  tmp___2 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___2) = 20U;
  tmp___3 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___3) = 83886085U;
  tmp___4 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___4) = (unsigned int )(dummy >> 32ULL);
  tmp___5 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___5) = (u32 )dummy;
  tmp___6 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___6) = 1U;
  tmp___7 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___7) = 8U;
  tmp___8 = ib.length_dw;
  ib.length_dw = ib.length_dw + 1U;
  *(ib.ptr + (unsigned long )tmp___8) = 33554433U;
  i = (int )ib.length_dw;
  goto ldv_43830;
  ldv_43829:
  *(ib.ptr + (unsigned long )i) = 0U;
  i = i + 1;
  ldv_43830: ;
  if ((unsigned int )i < ib_size_dw) {
    goto ldv_43829;
  } else {
  }
  r = amdgpu_ib_schedule(ring->adev, 1U, & ib, (void *)0);
  if (r != 0) {
    drm_err("amdgpu: failed to schedule ib (%d).\n", r);
  } else {
  }
  if ((unsigned long )fence != (unsigned long )((struct amdgpu_fence **)0)) {
    *fence = amdgpu_fence_ref(ib.fence);
  } else {
  }
  amdgpu_ib_free(ring->adev, & ib);
  return (r);
}
}
static int amdgpu_vce_cs_reloc(struct amdgpu_cs_parser *p , u32 ib_idx , int lo ,
                               int hi , unsigned int size , u32 index )
{
  struct amdgpu_bo_va_mapping *mapping ;
  struct amdgpu_ib *ib ;
  struct amdgpu_bo *bo ;
  uint64_t addr ;
  u32 tmp ;
  u32 tmp___0 ;
  u64 tmp___1 ;
  {
  ib = p->ibs + (unsigned long )ib_idx;
  if (index == 4294967295U) {
    index = 0U;
  } else {
  }
  tmp = amdgpu_get_ib_value(p, ib_idx, lo);
  tmp___0 = amdgpu_get_ib_value(p, ib_idx, hi);
  addr = (unsigned long long )tmp | ((unsigned long long )tmp___0 << 32);
  addr = (unsigned long long )size * (unsigned long long )index + addr;
  mapping = amdgpu_cs_find_mapping(p, addr, & bo);
  if ((unsigned long )mapping == (unsigned long )((struct amdgpu_bo_va_mapping *)0)) {
    drm_err("Can\'t find BO for addr 0x%010Lx %d %d %d %d\n", addr, lo, hi, size,
            index);
    return (-22);
  } else {
  }
  if ((unsigned long long )size + addr > ((unsigned long long )mapping->it.last + 1ULL) * 4096ULL) {
    drm_err("BO to small for addr 0x%010Lx %d %d\n", addr, lo, hi);
    return (-22);
  } else {
  }
  addr = addr - (unsigned long long )mapping->it.start * 4096ULL;
  tmp___1 = amdgpu_bo_gpu_offset(bo);
  addr = tmp___1 + addr;
  addr = addr - (unsigned long long )size * (unsigned long long )index;
  *(ib->ptr + (unsigned long )lo) = (u32 )addr;
  *(ib->ptr + (unsigned long )hi) = (u32 )(addr >> 32);
  return (0);
}
}
static int amdgpu_vce_validate_handle(struct amdgpu_cs_parser *p , u32 handle , bool *allocated )
{
  unsigned int i ;
  int tmp ;
  int tmp___0 ;
  {
  *allocated = 0;
  i = 0U;
  goto ldv_43851;
  ldv_43850:
  tmp = atomic_read((atomic_t const *)(& (p->adev)->vce.handles) + (unsigned long )i);
  if ((u32 )tmp == handle) {
    if ((unsigned long )(p->adev)->vce.filp[i] != (unsigned long )p->filp) {
      drm_err("VCE handle collision detected!\n");
      return (-22);
    } else {
    }
    return ((int )i);
  } else {
  }
  i = i + 1U;
  ldv_43851: ;
  if (i <= 15U) {
    goto ldv_43850;
  } else {
  }
  i = 0U;
  goto ldv_43854;
  ldv_43853:
  tmp___0 = atomic_cmpxchg((atomic_t *)(& (p->adev)->vce.handles) + (unsigned long )i,
                           0, (int )handle);
  if (tmp___0 == 0) {
    (p->adev)->vce.filp[i] = p->filp;
    (p->adev)->vce.img_size[i] = 0U;
    *allocated = 1;
    return ((int )i);
  } else {
  }
  i = i + 1U;
  ldv_43854: ;
  if (i <= 15U) {
    goto ldv_43853;
  } else {
  }
  drm_err("No more free VCE handles!\n");
  return (-22);
}
}
int amdgpu_vce_ring_parse_cs(struct amdgpu_cs_parser *p , u32 ib_idx )
{
  struct amdgpu_ib *ib ;
  unsigned int fb_idx ;
  unsigned int bs_idx ;
  int session_idx ;
  bool destroyed ;
  bool created ;
  bool allocated ;
  u32 tmp ;
  u32 handle ;
  u32 *size ;
  int i ;
  int r ;
  int idx ;
  u32 len ;
  u32 tmp___0 ;
  u32 cmd ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  {
  ib = p->ibs + (unsigned long )ib_idx;
  fb_idx = 0U;
  bs_idx = 0U;
  session_idx = -1;
  destroyed = 0;
  created = 0;
  allocated = 0;
  handle = 0U;
  size = & tmp;
  r = 0;
  idx = 0;
  amdgpu_vce_note_usage(p->adev);
  goto ldv_43894;
  ldv_43893:
  tmp___0 = amdgpu_get_ib_value(p, ib_idx, idx);
  len = tmp___0;
  tmp___1 = amdgpu_get_ib_value(p, ib_idx, idx + 1);
  cmd = tmp___1;
  if (len <= 7U || (len & 3U) != 0U) {
    drm_err("invalid VCE command length (%d)!\n", len);
    r = -22;
    goto out;
  } else {
  }
  if ((int )destroyed) {
    drm_err("No other command allowed after destroy!\n");
    r = -22;
    goto out;
  } else {
  }
  switch (cmd) {
  case 1U:
  handle = amdgpu_get_ib_value(p, ib_idx, idx + 2);
  session_idx = amdgpu_vce_validate_handle(p, handle, & allocated);
  if (session_idx < 0) {
    return (session_idx);
  } else {
  }
  size = (u32 *)(& (p->adev)->vce.img_size) + (unsigned long )session_idx;
  goto ldv_43877;
  case 2U:
  fb_idx = amdgpu_get_ib_value(p, ib_idx, idx + 6);
  bs_idx = amdgpu_get_ib_value(p, ib_idx, idx + 7);
  goto ldv_43877;
  case 16777217U:
  created = 1;
  if (! allocated) {
    drm_err("Handle already in use!\n");
    r = -22;
    goto out;
  } else {
  }
  tmp___2 = amdgpu_get_ib_value(p, ib_idx, idx + 8);
  tmp___3 = amdgpu_get_ib_value(p, ib_idx, idx + 10);
  *size = ((tmp___2 * tmp___3) * 24U) / 2U;
  goto ldv_43877;
  case 67108865U: ;
  case 67108866U: ;
  case 67108869U: ;
  case 67108871U: ;
  case 67108872U: ;
  case 67108873U: ;
  case 83886082U: ;
  goto ldv_43877;
  case 50331649U:
  r = amdgpu_vce_cs_reloc(p, ib_idx, idx + 10, idx + 9, *size, 0U);
  if (r != 0) {
    goto out;
  } else {
  }
  r = amdgpu_vce_cs_reloc(p, ib_idx, idx + 12, idx + 11, *size / 3U, 0U);
  if (r != 0) {
    goto out;
  } else {
  }
  goto ldv_43877;
  case 33554433U:
  destroyed = 1;
  goto ldv_43877;
  case 83886081U:
  r = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2, *size * 2U, 0U);
  if (r != 0) {
    goto out;
  } else {
  }
  goto ldv_43877;
  case 83886084U:
  tmp = amdgpu_get_ib_value(p, ib_idx, idx + 4);
  r = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2, tmp, bs_idx);
  if (r != 0) {
    goto out;
  } else {
  }
  goto ldv_43877;
  case 83886085U:
  r = amdgpu_vce_cs_reloc(p, ib_idx, idx + 3, idx + 2, 4096U, fb_idx);
  if (r != 0) {
    goto out;
  } else {
  }
  goto ldv_43877;
  default:
  drm_err("invalid VCE command (0x%x)!\n", cmd);
  r = -22;
  goto out;
  }
  ldv_43877: ;
  if (session_idx == -1) {
    drm_err("no session command at start of IB\n");
    r = -22;
    goto out;
  } else {
  }
  idx = (int )(len / 4U + (u32 )idx);
  ldv_43894: ;
  if ((u32 )idx < ib->length_dw) {
    goto ldv_43893;
  } else {
  }
  if ((int )allocated && ! created) {
    drm_err("New session without create command!\n");
    r = -2;
  } else {
  }
  out: ;
  if ((r == 0 && (int )destroyed) || (r != 0 && (int )allocated)) {
    i = 0;
    goto ldv_43897;
    ldv_43896:
    atomic_cmpxchg((atomic_t *)(& (p->adev)->vce.handles) + (unsigned long )i, (int )handle,
                   0);
    i = i + 1;
    ldv_43897: ;
    if (i <= 15) {
      goto ldv_43896;
    } else {
    }
  } else {
  }
  return (r);
}
}
bool amdgpu_vce_ring_emit_semaphore(struct amdgpu_ring *ring , struct amdgpu_semaphore *semaphore ,
                                    bool emit_wait )
{
  uint64_t addr ;
  {
  addr = semaphore->gpu_addr;
  amdgpu_ring_write(ring, 6U);
  amdgpu_ring_write(ring, (u32 )(addr >> 3) & 1048575U);
  amdgpu_ring_write(ring, (u32 )(addr >> 23) & 1048575U);
  amdgpu_ring_write(ring, (u32 )((int )emit_wait | 16789504));
  if (! emit_wait) {
    amdgpu_ring_write(ring, 1U);
  } else {
  }
  return (1);
}
}
void amdgpu_vce_ring_emit_ib(struct amdgpu_ring *ring , struct amdgpu_ib *ib )
{
  {
  amdgpu_ring_write(ring, 2U);
  amdgpu_ring_write(ring, (unsigned int )ib->gpu_addr);
  amdgpu_ring_write(ring, (unsigned int )(ib->gpu_addr >> 32ULL));
  amdgpu_ring_write(ring, ib->length_dw);
  return;
}
}
void amdgpu_vce_ring_emit_fence(struct amdgpu_ring *ring , u64 addr , u64 seq , unsigned int flags )
{
  int __ret_warn_on ;
  long tmp ;
  {
  __ret_warn_on = (int )flags & 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/work/ldvuser/mutilin/launch/work/current--X--drivers/--X--defaultlinux-4.2-rc1.tar.xz--X--08_1a--X--cpachecker/linux-4.2-rc1.tar.xz/csd_deg_dscv/10451/dscv_tempdir/dscv/ri/08_1a/drivers/gpu/drm/amd/amdgpu/amdgpu_vce.c",
                       745);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  amdgpu_ring_write(ring, 3U);
  amdgpu_ring_write(ring, (u32 )addr);
  amdgpu_ring_write(ring, (unsigned int )(addr >> 32ULL));
  amdgpu_ring_write(ring, (u32 )seq);
  amdgpu_ring_write(ring, 4U);
  amdgpu_ring_write(ring, 1U);
  return;
}
}
int amdgpu_vce_ring_test_ring(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 rptr ;
  u32 tmp ;
  unsigned int i ;
  int r ;
  u32 tmp___0 ;
  {
  adev = ring->adev;
  tmp = (*((ring->funcs)->get_rptr))(ring);
  rptr = tmp;
  r = amdgpu_ring_lock(ring, 16U);
  if (r != 0) {
    drm_err("amdgpu: vce failed to lock ring %d (%d).\n", ring->idx, r);
    return (r);
  } else {
  }
  amdgpu_ring_write(ring, 1U);
  amdgpu_ring_unlock_commit(ring);
  i = 0U;
  goto ldv_43926;
  ldv_43925:
  tmp___0 = (*((ring->funcs)->get_rptr))(ring);
  if (tmp___0 != rptr) {
    goto ldv_43924;
  } else {
  }
  __const_udelay(4295UL);
  i = i + 1U;
  ldv_43926: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43925;
  } else {
  }
  ldv_43924: ;
  if ((unsigned int )adev->usec_timeout > i) {
    printk("\016[drm] ring test on %d succeeded in %d usecs\n", ring->idx, i);
  } else {
    drm_err("amdgpu: ring %d test failed\n", ring->idx);
    r = -110;
  }
  return (r);
}
}
int amdgpu_vce_ring_test_ib(struct amdgpu_ring *ring )
{
  struct amdgpu_fence *fence ;
  int r ;
  {
  fence = (struct amdgpu_fence *)0;
  r = amdgpu_vce_get_create_msg(ring, 1U, (struct amdgpu_fence **)0);
  if (r != 0) {
    drm_err("amdgpu: failed to get create msg (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_vce_get_destroy_msg(ring, 1U, & fence);
  if (r != 0) {
    drm_err("amdgpu: failed to get destroy ib (%d).\n", r);
    goto error;
  } else {
  }
  r = amdgpu_fence_wait(fence, 0);
  if (r != 0) {
    drm_err("amdgpu: fence wait failed (%d).\n", r);
  } else {
    printk("\016[drm] ib test on ring %d succeeded\n", ring->idx);
  }
  error:
  amdgpu_fence_unref(& fence);
  return (r);
}
}
void call_and_disable_all_9(int state )
{
  {
  if (ldv_work_9_0 == state) {
    call_and_disable_work_9(ldv_work_struct_9_0);
  } else {
  }
  if (ldv_work_9_1 == state) {
    call_and_disable_work_9(ldv_work_struct_9_1);
  } else {
  }
  if (ldv_work_9_2 == state) {
    call_and_disable_work_9(ldv_work_struct_9_2);
  } else {
  }
  if (ldv_work_9_3 == state) {
    call_and_disable_work_9(ldv_work_struct_9_3);
  } else {
  }
  return;
}
}
void call_and_disable_work_9(struct work_struct *work )
{
  {
  if ((ldv_work_9_0 == 2 || ldv_work_9_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_9_0) {
    amdgpu_vce_idle_work_handler(work);
    ldv_work_9_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_9_1 == 2 || ldv_work_9_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_9_1) {
    amdgpu_vce_idle_work_handler(work);
    ldv_work_9_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_9_2 == 2 || ldv_work_9_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_9_2) {
    amdgpu_vce_idle_work_handler(work);
    ldv_work_9_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_9_3 == 2 || ldv_work_9_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_9_3) {
    amdgpu_vce_idle_work_handler(work);
    ldv_work_9_3 = 1;
    return;
  } else {
  }
  return;
}
}
void work_init_9(void)
{
  {
  ldv_work_9_0 = 0;
  ldv_work_9_1 = 0;
  ldv_work_9_2 = 0;
  ldv_work_9_3 = 0;
  return;
}
}
void invoke_work_9(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_9_0 == 2 || ldv_work_9_0 == 3) {
    ldv_work_9_0 = 4;
    amdgpu_vce_idle_work_handler(ldv_work_struct_9_0);
    ldv_work_9_0 = 1;
  } else {
  }
  goto ldv_43950;
  case 1: ;
  if (ldv_work_9_1 == 2 || ldv_work_9_1 == 3) {
    ldv_work_9_1 = 4;
    amdgpu_vce_idle_work_handler(ldv_work_struct_9_0);
    ldv_work_9_1 = 1;
  } else {
  }
  goto ldv_43950;
  case 2: ;
  if (ldv_work_9_2 == 2 || ldv_work_9_2 == 3) {
    ldv_work_9_2 = 4;
    amdgpu_vce_idle_work_handler(ldv_work_struct_9_0);
    ldv_work_9_2 = 1;
  } else {
  }
  goto ldv_43950;
  case 3: ;
  if (ldv_work_9_3 == 2 || ldv_work_9_3 == 3) {
    ldv_work_9_3 = 4;
    amdgpu_vce_idle_work_handler(ldv_work_struct_9_0);
    ldv_work_9_3 = 1;
  } else {
  }
  goto ldv_43950;
  default:
  ldv_stop();
  }
  ldv_43950: ;
  return;
}
}
void activate_work_9(struct work_struct *work , int state )
{
  {
  if (ldv_work_9_0 == 0) {
    ldv_work_struct_9_0 = work;
    ldv_work_9_0 = state;
    return;
  } else {
  }
  if (ldv_work_9_1 == 0) {
    ldv_work_struct_9_1 = work;
    ldv_work_9_1 = state;
    return;
  } else {
  }
  if (ldv_work_9_2 == 0) {
    ldv_work_struct_9_2 = work;
    ldv_work_9_2 = state;
    return;
  } else {
  }
  if (ldv_work_9_3 == 0) {
    ldv_work_struct_9_3 = work;
    ldv_work_9_3 = state;
    return;
  } else {
  }
  return;
}
}
void disable_work_9(struct work_struct *work )
{
  {
  if ((ldv_work_9_0 == 3 || ldv_work_9_0 == 2) && (unsigned long )ldv_work_struct_9_0 == (unsigned long )work) {
    ldv_work_9_0 = 1;
  } else {
  }
  if ((ldv_work_9_1 == 3 || ldv_work_9_1 == 2) && (unsigned long )ldv_work_struct_9_1 == (unsigned long )work) {
    ldv_work_9_1 = 1;
  } else {
  }
  if ((ldv_work_9_2 == 3 || ldv_work_9_2 == 2) && (unsigned long )ldv_work_struct_9_2 == (unsigned long )work) {
    ldv_work_9_2 = 1;
  } else {
  }
  if ((ldv_work_9_3 == 3 || ldv_work_9_3 == 2) && (unsigned long )ldv_work_struct_9_3 == (unsigned long )work) {
    ldv_work_9_3 = 1;
  } else {
  }
  return;
}
}
bool ldv_queue_work_on_1025(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1026(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1027(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1028(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1029(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_cancel_delayed_work_sync_1030(struct delayed_work *ldv_func_arg1 )
{
  ldv_func_ret_type___3 ldv_func_res ;
  bool tmp ;
  {
  tmp = cancel_delayed_work_sync(ldv_func_arg1);
  ldv_func_res = tmp;
  disable_work_2(& ldv_func_arg1->work);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
bool ldv_queue_work_on_1041(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1043(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1042(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1045(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1044(struct workqueue_struct *ldv_func_arg1 ) ;
static void vce_v3_0_mc_resume(struct amdgpu_device *adev , int idx ) ;
static void vce_v3_0_set_ring_funcs(struct amdgpu_device *adev ) ;
static void vce_v3_0_set_irq_funcs(struct amdgpu_device *adev ) ;
static u32 vce_v3_0_ring_get_rptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    tmp = amdgpu_mm_rreg(adev, 32867U, 0);
    return (tmp);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 32862U, 0);
    return (tmp___0);
  }
}
}
static u32 vce_v3_0_ring_get_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    tmp = amdgpu_mm_rreg(adev, 32868U, 0);
    return (tmp);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 32863U, 0);
    return (tmp___0);
  }
}
}
static void vce_v3_0_ring_set_wptr(struct amdgpu_ring *ring )
{
  struct amdgpu_device *adev ;
  {
  adev = ring->adev;
  if ((unsigned long )((struct amdgpu_ring *)(& adev->vce.ring)) == (unsigned long )ring) {
    amdgpu_mm_wreg(adev, 32868U, ring->wptr, 0);
  } else {
    amdgpu_mm_wreg(adev, 32863U, ring->wptr, 0);
  }
  return;
}
}
static int vce_v3_0_start(struct amdgpu_device *adev )
{
  struct amdgpu_ring *ring ;
  int idx ;
  int i ;
  int j ;
  int r ;
  u32 tmp_ ;
  u32 tmp ;
  u32 tmp____0 ;
  u32 tmp___0 ;
  u32 tmp____1 ;
  u32 tmp___1 ;
  u32 tmp____2 ;
  u32 tmp___2 ;
  u32 tmp____3 ;
  u32 tmp___3 ;
  unsigned long __ms ;
  unsigned long tmp___4 ;
  u32 tmp____4 ;
  u32 tmp___5 ;
  u32 status ;
  unsigned long __ms___0 ;
  unsigned long tmp___6 ;
  u32 tmp____5 ;
  u32 tmp___7 ;
  unsigned long __ms___1 ;
  unsigned long tmp___8 ;
  u32 tmp____6 ;
  u32 tmp___9 ;
  unsigned long __ms___2 ;
  unsigned long tmp___10 ;
  u32 tmp____7 ;
  u32 tmp___11 ;
  u32 tmp____8 ;
  u32 tmp___12 ;
  {
  mutex_lock_nested(& adev->grbm_idx_mutex, 0U);
  idx = 0;
  goto ldv_43767;
  ldv_43766: ;
  if (idx == 0) {
    tmp = amdgpu_mm_rreg(adev, 49664U, 0);
    tmp_ = tmp;
    tmp_ = tmp_ & 4294967279U;
    tmp_ = tmp_;
    amdgpu_mm_wreg(adev, 49664U, tmp_, 0);
  } else {
    tmp___0 = amdgpu_mm_rreg(adev, 49664U, 0);
    tmp____0 = tmp___0;
    tmp____0 = tmp____0 & 4294967279U;
    tmp____0 = tmp____0 | 16U;
    amdgpu_mm_wreg(adev, 49664U, tmp____0, 0);
  }
  vce_v3_0_mc_resume(adev, idx);
  tmp___1 = amdgpu_mm_rreg(adev, 32769U, 0);
  tmp____1 = tmp___1;
  tmp____1 = tmp____1 & 4294967294U;
  tmp____1 = tmp____1 | 1U;
  amdgpu_mm_wreg(adev, 32769U, tmp____1, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 32773U, 0);
  tmp____2 = tmp___2;
  tmp____2 = tmp____2 & 4294967294U;
  tmp____2 = tmp____2 | 1U;
  amdgpu_mm_wreg(adev, 32773U, tmp____2, 0);
  tmp___3 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____3 = tmp___3;
  tmp____3 = tmp____3 & 4294967294U;
  tmp____3 = tmp____3 | 1U;
  amdgpu_mm_wreg(adev, 32840U, tmp____3, 0);
  __ms = 100UL;
  goto ldv_43741;
  ldv_43740:
  __const_udelay(4295000UL);
  ldv_43741:
  tmp___4 = __ms;
  __ms = __ms - 1UL;
  if (tmp___4 != 0UL) {
    goto ldv_43740;
  } else {
  }
  tmp___5 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____4 = tmp___5;
  tmp____4 = tmp____4 & 4294967294U;
  tmp____4 = tmp____4;
  amdgpu_mm_wreg(adev, 32840U, tmp____4, 0);
  i = 0;
  goto ldv_43764;
  ldv_43763:
  j = 0;
  goto ldv_43751;
  ldv_43750:
  status = amdgpu_mm_rreg(adev, 32769U, 0);
  if ((status & 2U) != 0U) {
    goto ldv_43745;
  } else {
  }
  __ms___0 = 10UL;
  goto ldv_43748;
  ldv_43747:
  __const_udelay(4295000UL);
  ldv_43748:
  tmp___6 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___6 != 0UL) {
    goto ldv_43747;
  } else {
  }
  j = j + 1;
  ldv_43751: ;
  if (j <= 99) {
    goto ldv_43750;
  } else {
  }
  ldv_43745:
  r = 0;
  if ((status & 2U) != 0U) {
    goto ldv_43752;
  } else {
  }
  drm_err("VCE not responding, trying to reset the ECPU!!!\n");
  tmp___7 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____5 = tmp___7;
  tmp____5 = tmp____5 & 4294967294U;
  tmp____5 = tmp____5 | 1U;
  amdgpu_mm_wreg(adev, 32840U, tmp____5, 0);
  __ms___1 = 10UL;
  goto ldv_43756;
  ldv_43755:
  __const_udelay(4295000UL);
  ldv_43756:
  tmp___8 = __ms___1;
  __ms___1 = __ms___1 - 1UL;
  if (tmp___8 != 0UL) {
    goto ldv_43755;
  } else {
  }
  tmp___9 = amdgpu_mm_rreg(adev, 32840U, 0);
  tmp____6 = tmp___9;
  tmp____6 = tmp____6 & 4294967294U;
  tmp____6 = tmp____6;
  amdgpu_mm_wreg(adev, 32840U, tmp____6, 0);
  __ms___2 = 10UL;
  goto ldv_43761;
  ldv_43760:
  __const_udelay(4295000UL);
  ldv_43761:
  tmp___10 = __ms___2;
  __ms___2 = __ms___2 - 1UL;
  if (tmp___10 != 0UL) {
    goto ldv_43760;
  } else {
  }
  r = -1;
  i = i + 1;
  ldv_43764: ;
  if (i <= 9) {
    goto ldv_43763;
  } else {
  }
  ldv_43752:
  tmp___11 = amdgpu_mm_rreg(adev, 32769U, 0);
  tmp____7 = tmp___11;
  tmp____7 = tmp____7 & 4294967294U;
  tmp____7 = tmp____7;
  amdgpu_mm_wreg(adev, 32769U, tmp____7, 0);
  if (r != 0) {
    drm_err("VCE not responding, giving up!!!\n");
    mutex_unlock(& adev->grbm_idx_mutex);
    return (r);
  } else {
  }
  idx = idx + 1;
  ldv_43767: ;
  if (idx <= 1) {
    goto ldv_43766;
  } else {
  }
  tmp___12 = amdgpu_mm_rreg(adev, 49664U, 0);
  tmp____8 = tmp___12;
  tmp____8 = tmp____8 & 4294967279U;
  tmp____8 = tmp____8;
  amdgpu_mm_wreg(adev, 49664U, tmp____8, 0);
  mutex_unlock(& adev->grbm_idx_mutex);
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  amdgpu_mm_wreg(adev, 32867U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32868U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32864U, (u32 )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 32865U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 32866U, ring->ring_size / 4U, 0);
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  amdgpu_mm_wreg(adev, 32862U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32863U, ring->wptr, 0);
  amdgpu_mm_wreg(adev, 32859U, (u32 )ring->gpu_addr, 0);
  amdgpu_mm_wreg(adev, 32860U, (unsigned int )(ring->gpu_addr >> 32ULL), 0);
  amdgpu_mm_wreg(adev, 32861U, ring->ring_size / 4U, 0);
  return (0);
}
}
static int vce_v3_0_early_init(void *handle )
{
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  vce_v3_0_set_ring_funcs(adev);
  vce_v3_0_set_irq_funcs(adev);
  return (0);
}
}
static int vce_v3_0_sw_init(void *handle )
{
  struct amdgpu_device *adev ;
  struct amdgpu_ring *ring ;
  int r ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_irq_add_id(adev, 167U, & adev->vce.irq);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_sw_init(adev, 1155072UL);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  sprintf((char *)(& ring->name), "vce0");
  r = amdgpu_ring_init(adev, ring, 4096U, 0U, 15U, & adev->vce.irq, 0U, 4);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  sprintf((char *)(& ring->name), "vce1");
  r = amdgpu_ring_init(adev, ring, 4096U, 0U, 15U, & adev->vce.irq, 0U, 4);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v3_0_sw_fini(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_vce_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_sw_fini(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v3_0_hw_init(void *handle )
{
  struct amdgpu_ring *ring ;
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = vce_v3_0_start(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring);
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  ring = (struct amdgpu_ring *)(& adev->vce.ring) + 1UL;
  ring->ready = 1;
  r = (*((ring->funcs)->test_ring))(ring);
  if (r != 0) {
    ring->ready = 0;
    return (r);
  } else {
  }
  printk("\016[drm] VCE initialized successfully.\n");
  return (0);
}
}
static int vce_v3_0_hw_fini(void *handle )
{
  {
  return (0);
}
}
static int vce_v3_0_suspend(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = vce_v3_0_hw_fini((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_vce_suspend(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static int vce_v3_0_resume(void *handle )
{
  int r ;
  struct amdgpu_device *adev ;
  {
  adev = (struct amdgpu_device *)handle;
  r = amdgpu_vce_resume(adev);
  if (r != 0) {
    return (r);
  } else {
  }
  r = vce_v3_0_hw_init((void *)adev);
  if (r != 0) {
    return (r);
  } else {
  }
  return (r);
}
}
static void vce_v3_0_mc_resume(struct amdgpu_device *adev , int idx )
{
  u32 offset ;
  u32 size ;
  u32 tmp_ ;
  u32 tmp ;
  u32 tmp____0 ;
  u32 tmp___0 ;
  u32 tmp____1 ;
  u32 tmp___1 ;
  u32 tmp____2 ;
  u32 tmp___2 ;
  u32 tmp____3 ;
  u32 tmp___3 ;
  u32 tmp____4 ;
  u32 tmp___4 ;
  {
  tmp = amdgpu_mm_rreg(adev, 32958U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294901759U;
  tmp_ = tmp_;
  amdgpu_mm_wreg(adev, 32958U, tmp_, 0);
  tmp___0 = amdgpu_mm_rreg(adev, 33263U, 0);
  tmp____0 = tmp___0;
  tmp____0 = tmp____0 & 6295551U;
  tmp____0 = tmp____0 | 2093056U;
  amdgpu_mm_wreg(adev, 33263U, tmp____0, 0);
  tmp___1 = amdgpu_mm_rreg(adev, 33264U, 0);
  tmp____1 = tmp___1;
  tmp____1 = tmp____1 & 4294967232U;
  tmp____1 = tmp____1 | 63U;
  amdgpu_mm_wreg(adev, 33264U, tmp____1, 0);
  amdgpu_mm_wreg(adev, 32959U, 247U, 0);
  amdgpu_mm_wreg(adev, 34214U, 3768320U, 0);
  tmp___2 = amdgpu_mm_rreg(adev, 34237U, 0);
  tmp____2 = tmp___2;
  tmp____2 = tmp____2 & 4294967294U;
  tmp____2 = tmp____2;
  amdgpu_mm_wreg(adev, 34237U, tmp____2, 0);
  amdgpu_mm_wreg(adev, 34221U, 0U, 0);
  amdgpu_mm_wreg(adev, 34222U, 0U, 0);
  amdgpu_mm_wreg(adev, 34216U, 0U, 0);
  amdgpu_mm_wreg(adev, 34199U, (u32 )(adev->vce.gpu_addr >> 8), 0);
  offset = 256U;
  size = 393216U;
  amdgpu_mm_wreg(adev, 32777U, offset & 2147483647U, 0);
  amdgpu_mm_wreg(adev, 32778U, size, 0);
  if (idx == 0) {
    offset = offset + size;
    size = 65536U;
    amdgpu_mm_wreg(adev, 32779U, offset & 2147483647U, 0);
    amdgpu_mm_wreg(adev, 32780U, size, 0);
    offset = offset + size;
    size = 315392U;
    amdgpu_mm_wreg(adev, 32781U, offset & 2147483647U, 0);
    amdgpu_mm_wreg(adev, 32782U, size, 0);
  } else {
    offset = (size + offset) + 380928U;
    size = 65536U;
    amdgpu_mm_wreg(adev, 32779U, offset & 268435455U, 0);
    amdgpu_mm_wreg(adev, 32780U, size, 0);
    offset = offset + size;
    size = 315392U;
    amdgpu_mm_wreg(adev, 32781U, offset & 268435455U, 0);
    amdgpu_mm_wreg(adev, 32782U, size, 0);
  }
  tmp___3 = amdgpu_mm_rreg(adev, 34205U, 0);
  tmp____3 = tmp___3;
  tmp____3 = tmp____3 & 4294967039U;
  tmp____3 = tmp____3;
  amdgpu_mm_wreg(adev, 34205U, tmp____3, 0);
  tmp___4 = amdgpu_mm_rreg(adev, 34112U, 0);
  tmp____4 = tmp___4;
  tmp____4 = tmp____4 & 4294967287U;
  tmp____4 = tmp____4 | 8U;
  amdgpu_mm_wreg(adev, 34112U, tmp____4, 0);
  return;
}
}
static bool vce_v3_0_is_idle(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  return ((tmp & 128U) == 0U);
}
}
static int vce_v3_0_wait_for_idle(void *handle )
{
  unsigned int i ;
  struct amdgpu_device *adev ;
  u32 tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  i = 0U;
  goto ldv_43826;
  ldv_43825:
  tmp = amdgpu_mm_rreg(adev, 915U, 0);
  if ((tmp & 128U) == 0U) {
    return (0);
  } else {
  }
  i = i + 1U;
  ldv_43826: ;
  if ((unsigned int )adev->usec_timeout > i) {
    goto ldv_43825;
  } else {
  }
  return (-110);
}
}
static int vce_v3_0_soft_reset(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp_ ;
  u32 tmp ;
  unsigned long __ms ;
  unsigned long tmp___0 ;
  int tmp___1 ;
  {
  adev = (struct amdgpu_device *)handle;
  tmp = amdgpu_mm_rreg(adev, 920U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4278190079U;
  tmp_ = tmp_ | 16777216U;
  amdgpu_mm_wreg(adev, 920U, tmp_, 0);
  if (1) {
    __const_udelay(21475000UL);
  } else {
    __ms = 5UL;
    goto ldv_43835;
    ldv_43834:
    __const_udelay(4295000UL);
    ldv_43835:
    tmp___0 = __ms;
    __ms = __ms - 1UL;
    if (tmp___0 != 0UL) {
      goto ldv_43834;
    } else {
    }
  }
  tmp___1 = vce_v3_0_start(adev);
  return (tmp___1);
}
}
static void vce_v3_0_print_status(void *handle )
{
  struct amdgpu_device *adev ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  u32 tmp___3 ;
  u32 tmp___4 ;
  u32 tmp___5 ;
  u32 tmp___6 ;
  u32 tmp___7 ;
  u32 tmp___8 ;
  u32 tmp___9 ;
  u32 tmp___10 ;
  u32 tmp___11 ;
  u32 tmp___12 ;
  u32 tmp___13 ;
  u32 tmp___14 ;
  u32 tmp___15 ;
  u32 tmp___16 ;
  u32 tmp___17 ;
  u32 tmp___18 ;
  u32 tmp___19 ;
  u32 tmp___20 ;
  u32 tmp___21 ;
  u32 tmp___22 ;
  u32 tmp___23 ;
  u32 tmp___24 ;
  u32 tmp___25 ;
  u32 tmp___26 ;
  u32 tmp___27 ;
  u32 tmp___28 ;
  {
  adev = (struct amdgpu_device *)handle;
  _dev_info((struct device const *)adev->dev, "VCE 3.0 registers\n");
  tmp = amdgpu_mm_rreg(adev, 32769U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_STATUS=0x%08X\n", tmp);
  tmp___0 = amdgpu_mm_rreg(adev, 32773U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CNTL=0x%08X\n", tmp___0);
  tmp___1 = amdgpu_mm_rreg(adev, 32777U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET0=0x%08X\n",
            tmp___1);
  tmp___2 = amdgpu_mm_rreg(adev, 32778U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE0=0x%08X\n",
            tmp___2);
  tmp___3 = amdgpu_mm_rreg(adev, 32779U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET1=0x%08X\n",
            tmp___3);
  tmp___4 = amdgpu_mm_rreg(adev, 32780U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE1=0x%08X\n",
            tmp___4);
  tmp___5 = amdgpu_mm_rreg(adev, 32781U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_OFFSET2=0x%08X\n",
            tmp___5);
  tmp___6 = amdgpu_mm_rreg(adev, 32782U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_VCPU_CACHE_SIZE2=0x%08X\n",
            tmp___6);
  tmp___7 = amdgpu_mm_rreg(adev, 32840U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_SOFT_RESET=0x%08X\n", tmp___7);
  tmp___8 = amdgpu_mm_rreg(adev, 32859U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_LO2=0x%08X\n", tmp___8);
  tmp___9 = amdgpu_mm_rreg(adev, 32860U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_HI2=0x%08X\n", tmp___9);
  tmp___10 = amdgpu_mm_rreg(adev, 32861U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_SIZE2=0x%08X\n", tmp___10);
  tmp___11 = amdgpu_mm_rreg(adev, 32862U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_RPTR2=0x%08X\n", tmp___11);
  tmp___12 = amdgpu_mm_rreg(adev, 32863U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_WPTR2=0x%08X\n", tmp___12);
  tmp___13 = amdgpu_mm_rreg(adev, 32864U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_LO=0x%08X\n", tmp___13);
  tmp___14 = amdgpu_mm_rreg(adev, 32865U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_BASE_HI=0x%08X\n", tmp___14);
  tmp___15 = amdgpu_mm_rreg(adev, 32866U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_SIZE=0x%08X\n", tmp___15);
  tmp___16 = amdgpu_mm_rreg(adev, 32867U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_RPTR=0x%08X\n", tmp___16);
  tmp___17 = amdgpu_mm_rreg(adev, 32868U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_RB_WPTR=0x%08X\n", tmp___17);
  tmp___18 = amdgpu_mm_rreg(adev, 32958U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_CLOCK_GATING_A=0x%08X\n", tmp___18);
  tmp___19 = amdgpu_mm_rreg(adev, 32959U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_CLOCK_GATING_B=0x%08X\n", tmp___19);
  tmp___20 = amdgpu_mm_rreg(adev, 33263U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_UENC_CLOCK_GATING=0x%08X\n",
            tmp___20);
  tmp___21 = amdgpu_mm_rreg(adev, 33264U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_UENC_REG_CLOCK_GATING=0x%08X\n",
            tmp___21);
  tmp___22 = amdgpu_mm_rreg(adev, 34112U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_SYS_INT_EN=0x%08X\n", tmp___22);
  tmp___23 = amdgpu_mm_rreg(adev, 34205U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CTRL2=0x%08X\n", tmp___23);
  tmp___24 = amdgpu_mm_rreg(adev, 34214U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CTRL=0x%08X\n", tmp___24);
  tmp___25 = amdgpu_mm_rreg(adev, 34216U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_VM_CTRL=0x%08X\n", tmp___25);
  tmp___26 = amdgpu_mm_rreg(adev, 34221U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_SWAP_CNTL=0x%08X\n", tmp___26);
  tmp___27 = amdgpu_mm_rreg(adev, 34222U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_SWAP_CNTL1=0x%08X\n", tmp___27);
  tmp___28 = amdgpu_mm_rreg(adev, 34237U, 0);
  _dev_info((struct device const *)adev->dev, "  VCE_LMI_CACHE_CTRL=0x%08X\n", tmp___28);
  return;
}
}
static int vce_v3_0_set_interrupt_state(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                        unsigned int type , enum amdgpu_interrupt_state state )
{
  u32 val ;
  u32 tmp_ ;
  u32 tmp ;
  {
  val = 0U;
  if ((unsigned int )state == 1U) {
    val = val | 8U;
  } else {
  }
  tmp = amdgpu_mm_rreg(adev, 34112U, 0);
  tmp_ = tmp;
  tmp_ = tmp_ & 4294967287U;
  tmp_ = (val & 8U) | tmp_;
  amdgpu_mm_wreg(adev, 34112U, tmp_, 0);
  return (0);
}
}
static int vce_v3_0_process_interrupt(struct amdgpu_device *adev , struct amdgpu_irq_src *source ,
                                      struct amdgpu_iv_entry *entry )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((long )((int )drm_debug) & 1L, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("vce_v3_0_process_interrupt", "IH: VCE\n");
  } else {
  }
  switch (entry->src_data) {
  case 0U:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->vce.ring));
  goto ldv_43856;
  case 1U:
  amdgpu_fence_process((struct amdgpu_ring *)(& adev->vce.ring) + 1UL);
  goto ldv_43856;
  default:
  drm_err("Unhandled interrupt: %d %d\n", entry->src_id, entry->src_data);
  goto ldv_43856;
  }
  ldv_43856: ;
  return (0);
}
}
static int vce_v3_0_set_clockgating_state(void *handle , enum amd_clockgating_state state )
{
  {
  return (0);
}
}
static int vce_v3_0_set_powergating_state(void *handle , enum amd_powergating_state state )
{
  struct amdgpu_device *adev ;
  int tmp ;
  {
  adev = (struct amdgpu_device *)handle;
  if ((unsigned int )state == 0U) {
    return (0);
  } else {
    tmp = vce_v3_0_start(adev);
    return (tmp);
  }
}
}
struct amd_ip_funcs const vce_v3_0_ip_funcs =
     {& vce_v3_0_early_init, (int (*)(void * ))0, & vce_v3_0_sw_init, & vce_v3_0_sw_fini,
    & vce_v3_0_hw_init, & vce_v3_0_hw_fini, & vce_v3_0_suspend, & vce_v3_0_resume,
    & vce_v3_0_is_idle, & vce_v3_0_wait_for_idle, & vce_v3_0_soft_reset, & vce_v3_0_print_status,
    & vce_v3_0_set_clockgating_state, & vce_v3_0_set_powergating_state};
static struct amdgpu_ring_funcs const vce_v3_0_ring_funcs =
     {& vce_v3_0_ring_get_rptr, & vce_v3_0_ring_get_wptr, & vce_v3_0_ring_set_wptr,
    & amdgpu_vce_ring_parse_cs, & amdgpu_vce_ring_emit_ib, & amdgpu_vce_ring_emit_fence,
    & amdgpu_vce_ring_emit_semaphore, 0, 0, 0, & amdgpu_vce_ring_test_ring, & amdgpu_vce_ring_test_ib,
    & amdgpu_ring_test_lockup};
static void vce_v3_0_set_ring_funcs(struct amdgpu_device *adev )
{
  {
  adev->vce.ring[0].funcs = & vce_v3_0_ring_funcs;
  adev->vce.ring[1].funcs = & vce_v3_0_ring_funcs;
  return;
}
}
static struct amdgpu_irq_src_funcs const vce_v3_0_irq_funcs = {& vce_v3_0_set_interrupt_state, & vce_v3_0_process_interrupt};
static void vce_v3_0_set_irq_funcs(struct amdgpu_device *adev )
{
  {
  adev->vce.irq.num_types = 1U;
  adev->vce.irq.funcs = & vce_v3_0_irq_funcs;
  return;
}
}
int ldv_retval_12 ;
int ldv_retval_11 ;
extern int ldv_probe_15(void) ;
extern int ldv_release_15(void) ;
void ldv_initialize_amdgpu_ring_funcs_14(void)
{
  void *tmp ;
  {
  tmp = ldv_init_zalloc(632UL);
  vce_v3_0_ring_funcs_group0 = (struct amdgpu_ring *)tmp;
  return;
}
}
void ldv_initialize_amdgpu_irq_src_funcs_13(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(23352UL);
  vce_v3_0_irq_funcs_group0 = (struct amdgpu_device *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  vce_v3_0_irq_funcs_group1 = (struct amdgpu_irq_src *)tmp___0;
  return;
}
}
void ldv_main_exported_13(void)
{
  unsigned int ldvarg1037 ;
  enum amdgpu_interrupt_state ldvarg1038 ;
  struct amdgpu_iv_entry *ldvarg1036 ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = ldv_init_zalloc(20UL);
  ldvarg1036 = (struct amdgpu_iv_entry *)tmp;
  ldv_memset((void *)(& ldvarg1037), 0, 4UL);
  ldv_memset((void *)(& ldvarg1038), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_13 == 1) {
    vce_v3_0_set_interrupt_state(vce_v3_0_irq_funcs_group0, vce_v3_0_irq_funcs_group1,
                                 ldvarg1037, ldvarg1038);
    ldv_state_variable_13 = 1;
  } else {
  }
  goto ldv_43896;
  case 1: ;
  if (ldv_state_variable_13 == 1) {
    vce_v3_0_process_interrupt(vce_v3_0_irq_funcs_group0, vce_v3_0_irq_funcs_group1,
                               ldvarg1036);
    ldv_state_variable_13 = 1;
  } else {
  }
  goto ldv_43896;
  default:
  ldv_stop();
  }
  ldv_43896: ;
  return;
}
}
void ldv_main_exported_15(void)
{
  void *ldvarg192 ;
  void *tmp ;
  void *ldvarg201 ;
  void *tmp___0 ;
  void *ldvarg206 ;
  void *tmp___1 ;
  void *ldvarg199 ;
  void *tmp___2 ;
  void *ldvarg198 ;
  void *tmp___3 ;
  void *ldvarg193 ;
  void *tmp___4 ;
  void *ldvarg205 ;
  void *tmp___5 ;
  void *ldvarg197 ;
  void *tmp___6 ;
  void *ldvarg194 ;
  void *tmp___7 ;
  enum amd_clockgating_state ldvarg196 ;
  void *ldvarg202 ;
  void *tmp___8 ;
  void *ldvarg195 ;
  void *tmp___9 ;
  enum amd_powergating_state ldvarg200 ;
  void *ldvarg203 ;
  void *tmp___10 ;
  void *ldvarg204 ;
  void *tmp___11 ;
  int tmp___12 ;
  {
  tmp = ldv_init_zalloc(1UL);
  ldvarg192 = tmp;
  tmp___0 = ldv_init_zalloc(1UL);
  ldvarg201 = tmp___0;
  tmp___1 = ldv_init_zalloc(1UL);
  ldvarg206 = tmp___1;
  tmp___2 = ldv_init_zalloc(1UL);
  ldvarg199 = tmp___2;
  tmp___3 = ldv_init_zalloc(1UL);
  ldvarg198 = tmp___3;
  tmp___4 = ldv_init_zalloc(1UL);
  ldvarg193 = tmp___4;
  tmp___5 = ldv_init_zalloc(1UL);
  ldvarg205 = tmp___5;
  tmp___6 = ldv_init_zalloc(1UL);
  ldvarg197 = tmp___6;
  tmp___7 = ldv_init_zalloc(1UL);
  ldvarg194 = tmp___7;
  tmp___8 = ldv_init_zalloc(1UL);
  ldvarg202 = tmp___8;
  tmp___9 = ldv_init_zalloc(1UL);
  ldvarg195 = tmp___9;
  tmp___10 = ldv_init_zalloc(1UL);
  ldvarg203 = tmp___10;
  tmp___11 = ldv_init_zalloc(1UL);
  ldvarg204 = tmp___11;
  ldv_memset((void *)(& ldvarg196), 0, 4UL);
  ldv_memset((void *)(& ldvarg200), 0, 4UL);
  tmp___12 = __VERIFIER_nondet_int();
  switch (tmp___12) {
  case 0: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_hw_fini(ldvarg206);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_hw_fini(ldvarg206);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_hw_fini(ldvarg206);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 1: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_print_status(ldvarg205);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_print_status(ldvarg205);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_print_status(ldvarg205);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 2: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_early_init(ldvarg204);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_early_init(ldvarg204);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_early_init(ldvarg204);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 3: ;
  if (ldv_state_variable_15 == 2) {
    ldv_retval_12 = vce_v3_0_suspend(ldvarg203);
    if (ldv_retval_12 == 0) {
      ldv_state_variable_15 = 3;
    } else {
    }
  } else {
  }
  goto ldv_43918;
  case 4: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_sw_init(ldvarg202);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_sw_init(ldvarg202);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_sw_init(ldvarg202);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 5: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_set_powergating_state(ldvarg201, ldvarg200);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_set_powergating_state(ldvarg201, ldvarg200);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_set_powergating_state(ldvarg201, ldvarg200);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 6: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_wait_for_idle(ldvarg199);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_wait_for_idle(ldvarg199);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_wait_for_idle(ldvarg199);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 7: ;
  if (ldv_state_variable_15 == 3) {
    ldv_retval_11 = vce_v3_0_resume(ldvarg198);
    if (ldv_retval_11 == 0) {
      ldv_state_variable_15 = 2;
    } else {
    }
  } else {
  }
  goto ldv_43918;
  case 8: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_set_clockgating_state(ldvarg197, ldvarg196);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_set_clockgating_state(ldvarg197, ldvarg196);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_set_clockgating_state(ldvarg197, ldvarg196);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 9: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_hw_init(ldvarg195);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_hw_init(ldvarg195);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_hw_init(ldvarg195);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 10: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_soft_reset(ldvarg194);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_soft_reset(ldvarg194);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_soft_reset(ldvarg194);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 11: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_sw_fini(ldvarg193);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_sw_fini(ldvarg193);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_sw_fini(ldvarg193);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 12: ;
  if (ldv_state_variable_15 == 1) {
    vce_v3_0_is_idle(ldvarg192);
    ldv_state_variable_15 = 1;
  } else {
  }
  if (ldv_state_variable_15 == 3) {
    vce_v3_0_is_idle(ldvarg192);
    ldv_state_variable_15 = 3;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    vce_v3_0_is_idle(ldvarg192);
    ldv_state_variable_15 = 2;
  } else {
  }
  goto ldv_43918;
  case 13: ;
  if (ldv_state_variable_15 == 3) {
    ldv_release_15();
    ldv_state_variable_15 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_15 == 2) {
    ldv_release_15();
    ldv_state_variable_15 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_43918;
  case 14: ;
  if (ldv_state_variable_15 == 1) {
    ldv_probe_15();
    ldv_state_variable_15 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_43918;
  default:
  ldv_stop();
  }
  ldv_43918: ;
  return;
}
}
void ldv_main_exported_14(void)
{
  struct amdgpu_cs_parser *ldvarg350 ;
  void *tmp ;
  struct amdgpu_semaphore *ldvarg347 ;
  void *tmp___0 ;
  uint64_t ldvarg351 ;
  struct amdgpu_ib *ldvarg348 ;
  void *tmp___1 ;
  unsigned int ldvarg353 ;
  uint64_t ldvarg352 ;
  bool ldvarg346 ;
  u32 ldvarg349 ;
  int tmp___2 ;
  {
  tmp = ldv_init_zalloc(200UL);
  ldvarg350 = (struct amdgpu_cs_parser *)tmp;
  tmp___0 = ldv_init_zalloc(24UL);
  ldvarg347 = (struct amdgpu_semaphore *)tmp___0;
  tmp___1 = ldv_init_zalloc(272UL);
  ldvarg348 = (struct amdgpu_ib *)tmp___1;
  ldv_memset((void *)(& ldvarg351), 0, 8UL);
  ldv_memset((void *)(& ldvarg353), 0, 4UL);
  ldv_memset((void *)(& ldvarg352), 0, 8UL);
  ldv_memset((void *)(& ldvarg346), 0, 1UL);
  ldv_memset((void *)(& ldvarg349), 0, 4UL);
  tmp___2 = __VERIFIER_nondet_int();
  switch (tmp___2) {
  case 0: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_emit_fence(vce_v3_0_ring_funcs_group0, ldvarg352, ldvarg351, ldvarg353);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 1: ;
  if (ldv_state_variable_14 == 1) {
    vce_v3_0_ring_get_rptr(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 2: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_parse_cs(ldvarg350, ldvarg349);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 3: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_test_ring(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 4: ;
  if (ldv_state_variable_14 == 1) {
    vce_v3_0_ring_set_wptr(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 5: ;
  if (ldv_state_variable_14 == 1) {
    vce_v3_0_ring_get_wptr(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 6: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_emit_ib(vce_v3_0_ring_funcs_group0, ldvarg348);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 7: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_emit_semaphore(vce_v3_0_ring_funcs_group0, ldvarg347, (int )ldvarg346);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 8: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_vce_ring_test_ib(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  case 9: ;
  if (ldv_state_variable_14 == 1) {
    amdgpu_ring_test_lockup(vce_v3_0_ring_funcs_group0);
    ldv_state_variable_14 = 1;
  } else {
  }
  goto ldv_43946;
  default:
  ldv_stop();
  }
  ldv_43946: ;
  return;
}
}
bool ldv_queue_work_on_1041(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1042(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1043(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1044(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1045(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1055(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1057(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1056(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1059(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1058(struct workqueue_struct *ldv_func_arg1 ) ;
extern long drm_compat_ioctl(struct file * , unsigned int , unsigned long ) ;
long amdgpu_kms_compat_ioctl(struct file *filp , unsigned int cmd , unsigned long arg )
{
  unsigned int nr ;
  int ret ;
  long tmp ;
  long tmp___0 ;
  {
  nr = cmd & 255U;
  if (nr <= 63U) {
    tmp = drm_compat_ioctl(filp, cmd, arg);
    return (tmp);
  } else {
  }
  tmp___0 = amdgpu_drm_ioctl(filp, cmd, arg);
  ret = (int )tmp___0;
  return ((long )ret);
}
}
bool ldv_queue_work_on_1055(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1056(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1057(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1058(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1059(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1069(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1071(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1070(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1073(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1072(struct workqueue_struct *ldv_func_arg1 ) ;
extern int vga_switcheroo_register_handler(struct vga_switcheroo_handler * ) ;
extern void vga_switcheroo_unregister_handler(void) ;
extern acpi_status acpi_get_name(acpi_handle , u32 , struct acpi_buffer * ) ;
static struct amdgpu_atpx_priv amdgpu_atpx_priv ;
bool amdgpu_has_atpx(void)
{
  {
  return (amdgpu_atpx_priv.atpx_detected);
}
}
static union acpi_object *amdgpu_atpx_call(acpi_handle handle , int function , struct acpi_buffer *params )
{
  acpi_status status ;
  union acpi_object atpx_arg_elements[2U] ;
  struct acpi_object_list atpx_arg ;
  struct acpi_buffer buffer ;
  char const *tmp ;
  {
  buffer.length = 0xffffffffffffffffULL;
  buffer.pointer = (void *)0;
  atpx_arg.count = 2U;
  atpx_arg.pointer = (union acpi_object *)(& atpx_arg_elements);
  atpx_arg_elements[0].type = 1U;
  atpx_arg_elements[0].integer.value = (u64 )function;
  if ((unsigned long )params != (unsigned long )((struct acpi_buffer *)0)) {
    atpx_arg_elements[1].type = 3U;
    atpx_arg_elements[1].buffer.length = (u32 )params->length;
    atpx_arg_elements[1].buffer.pointer = (u8 *)params->pointer;
  } else {
    atpx_arg_elements[1].type = 1U;
    atpx_arg_elements[1].integer.value = 0ULL;
  }
  status = acpi_evaluate_object(handle, (acpi_string )0, & atpx_arg, & buffer);
  if (status != 0U && status != 5U) {
    tmp = acpi_format_exception(status);
    printk("failed to evaluate ATPX got %s\n", tmp);
    kfree((void const *)buffer.pointer);
    return ((union acpi_object *)0);
  } else {
  }
  return ((union acpi_object *)buffer.pointer);
}
}
static void amdgpu_atpx_parse_functions(struct amdgpu_atpx_functions *f , u32 mask )
{
  {
  f->px_params = (mask & 1U) != 0U;
  f->power_cntl = (mask & 2U) != 0U;
  f->disp_mux_cntl = (mask & 4U) != 0U;
  f->i2c_mux_cntl = (mask & 8U) != 0U;
  f->switch_start = (mask & 16U) != 0U;
  f->switch_end = (mask & 32U) != 0U;
  f->disp_connectors_mapping = (mask & 128U) != 0U;
  f->disp_detetion_ports = (mask & 256U) != 0U;
  return;
}
}
static int amdgpu_atpx_validate(struct amdgpu_atpx *atpx )
{
  union acpi_object *info ;
  struct atpx_px_params output ;
  size_t size ;
  u32 valid_bits ;
  unsigned long _min1 ;
  size_t _min2 ;
  {
  atpx->functions.power_cntl = 1;
  if ((int )atpx->functions.px_params) {
    info = amdgpu_atpx_call(atpx->handle, 1, (struct acpi_buffer *)0);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    memset((void *)(& output), 0, 10UL);
    size = (size_t )*((u16 *)info->buffer.pointer);
    if (size <= 9UL) {
      printk("ATPX buffer is too small: %zu\n", size);
      kfree((void const *)info);
      return (-22);
    } else {
    }
    _min1 = 10UL;
    _min2 = size;
    size = _min1 < _min2 ? _min1 : _min2;
    memcpy((void *)(& output), (void const *)info->buffer.pointer, size);
    valid_bits = output.flags & output.valid_flags;
    if ((valid_bits & 64U) != 0U) {
      atpx->functions.i2c_mux_cntl = 1;
      atpx->functions.disp_mux_cntl = 1;
    } else {
    }
    if ((valid_bits & 56U) != 0U) {
      atpx->functions.disp_mux_cntl = 1;
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_verify_interface(struct amdgpu_atpx *atpx )
{
  union acpi_object *info ;
  struct atpx_verify_interface output ;
  size_t size ;
  int err ;
  unsigned long _min1 ;
  size_t _min2 ;
  {
  err = 0;
  info = amdgpu_atpx_call(atpx->handle, 0, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  memset((void *)(& output), 0, 8UL);
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 7UL) {
    printk("ATPX buffer is too small: %zu\n", size);
    err = -22;
    goto out;
  } else {
  }
  _min1 = 8UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)(& output), (void const *)info->buffer.pointer, size);
  printk("ATPX version %u, functions 0x%08x\n", (int )output.version, output.function_bits);
  amdgpu_atpx_parse_functions(& atpx->functions, output.function_bits);
  out:
  kfree((void const *)info);
  return (err);
}
}
static int amdgpu_atpx_set_discrete_state(struct amdgpu_atpx *atpx , u8 state )
{
  struct acpi_buffer params ;
  union acpi_object *info ;
  struct atpx_power_control input ;
  {
  if ((int )atpx->functions.power_cntl) {
    input.size = 3U;
    input.dgpu_state = state;
    params.length = (acpi_size )input.size;
    params.pointer = (void *)(& input);
    info = amdgpu_atpx_call(atpx->handle, 2, & params);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_switch_disp_mux(struct amdgpu_atpx *atpx , u16 mux_id )
{
  struct acpi_buffer params ;
  union acpi_object *info ;
  struct atpx_mux input ;
  {
  if ((int )atpx->functions.disp_mux_cntl) {
    input.size = 4U;
    input.mux = mux_id;
    params.length = (acpi_size )input.size;
    params.pointer = (void *)(& input);
    info = amdgpu_atpx_call(atpx->handle, 3, & params);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_switch_i2c_mux(struct amdgpu_atpx *atpx , u16 mux_id )
{
  struct acpi_buffer params ;
  union acpi_object *info ;
  struct atpx_mux input ;
  {
  if ((int )atpx->functions.i2c_mux_cntl) {
    input.size = 4U;
    input.mux = mux_id;
    params.length = (acpi_size )input.size;
    params.pointer = (void *)(& input);
    info = amdgpu_atpx_call(atpx->handle, 4, & params);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_switch_start(struct amdgpu_atpx *atpx , u16 mux_id )
{
  struct acpi_buffer params ;
  union acpi_object *info ;
  struct atpx_mux input ;
  {
  if ((int )atpx->functions.switch_start) {
    input.size = 4U;
    input.mux = mux_id;
    params.length = (acpi_size )input.size;
    params.pointer = (void *)(& input);
    info = amdgpu_atpx_call(atpx->handle, 5, & params);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_switch_end(struct amdgpu_atpx *atpx , u16 mux_id )
{
  struct acpi_buffer params ;
  union acpi_object *info ;
  struct atpx_mux input ;
  {
  if ((int )atpx->functions.switch_end) {
    input.size = 4U;
    input.mux = mux_id;
    params.length = (acpi_size )input.size;
    params.pointer = (void *)(& input);
    info = amdgpu_atpx_call(atpx->handle, 6, & params);
    if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
      return (-5);
    } else {
    }
    kfree((void const *)info);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_switchto(enum vga_switcheroo_client_id id )
{
  u16 gpu_id ;
  {
  if ((unsigned int )id == 0U) {
    gpu_id = 0U;
  } else {
    gpu_id = 1U;
  }
  amdgpu_atpx_switch_start(& amdgpu_atpx_priv.atpx, (int )gpu_id);
  amdgpu_atpx_switch_disp_mux(& amdgpu_atpx_priv.atpx, (int )gpu_id);
  amdgpu_atpx_switch_i2c_mux(& amdgpu_atpx_priv.atpx, (int )gpu_id);
  amdgpu_atpx_switch_end(& amdgpu_atpx_priv.atpx, (int )gpu_id);
  return (0);
}
}
static int amdgpu_atpx_power_state(enum vga_switcheroo_client_id id , enum vga_switcheroo_state state )
{
  {
  if ((unsigned int )id == 0U) {
    return (0);
  } else {
  }
  amdgpu_atpx_set_discrete_state(& amdgpu_atpx_priv.atpx, (int )((u8 )state));
  return (0);
}
}
static bool amdgpu_atpx_pci_probe_handle(struct pci_dev *pdev )
{
  acpi_handle dhandle ;
  acpi_handle atpx_handle ;
  acpi_status status ;
  struct acpi_device *tmp ;
  {
  tmp = to_acpi_node(pdev->dev.fwnode);
  dhandle = acpi_device_handle(tmp);
  if ((unsigned long )dhandle == (unsigned long )((acpi_handle )0)) {
    return (0);
  } else {
  }
  status = acpi_get_handle(dhandle, (char *)"ATPX", & atpx_handle);
  if (status != 0U) {
    amdgpu_atpx_priv.other_handle = dhandle;
    return (0);
  } else {
  }
  amdgpu_atpx_priv.dhandle = dhandle;
  amdgpu_atpx_priv.atpx.handle = atpx_handle;
  return (1);
}
}
static int amdgpu_atpx_init(void)
{
  int r ;
  {
  r = amdgpu_atpx_verify_interface(& amdgpu_atpx_priv.atpx);
  if (r != 0) {
    return (r);
  } else {
  }
  r = amdgpu_atpx_validate(& amdgpu_atpx_priv.atpx);
  if (r != 0) {
    return (r);
  } else {
  }
  return (0);
}
}
static int amdgpu_atpx_get_client_id(struct pci_dev *pdev )
{
  struct acpi_device *tmp ;
  acpi_handle tmp___0 ;
  {
  tmp = to_acpi_node(pdev->dev.fwnode);
  tmp___0 = acpi_device_handle(tmp);
  if ((unsigned long )amdgpu_atpx_priv.dhandle == (unsigned long )tmp___0) {
    return (0);
  } else {
    return (1);
  }
}
}
static struct vga_switcheroo_handler amdgpu_atpx_handler = {& amdgpu_atpx_switchto, & amdgpu_atpx_power_state, & amdgpu_atpx_init, & amdgpu_atpx_get_client_id};
static bool amdgpu_atpx_detect(void)
{
  char acpi_method_name[255U] ;
  unsigned int tmp ;
  struct acpi_buffer buffer ;
  struct pci_dev *pdev ;
  bool has_atpx ;
  int vga_count ;
  bool tmp___0 ;
  bool tmp___1 ;
  {
  acpi_method_name[0] = 0;
  tmp = 1U;
  while (1) {
    if (tmp >= 255U) {
      break;
    } else {
    }
    acpi_method_name[tmp] = (char)0;
    tmp = tmp + 1U;
  }
  buffer.length = 255ULL;
  buffer.pointer = (void *)(& acpi_method_name);
  pdev = (struct pci_dev *)0;
  has_atpx = 0;
  vga_count = 0;
  goto ldv_37262;
  ldv_37261:
  vga_count = vga_count + 1;
  tmp___0 = amdgpu_atpx_pci_probe_handle(pdev);
  has_atpx = ((int )has_atpx | (int )tmp___0) != 0;
  ldv_37262:
  pdev = pci_get_class(196608U, pdev);
  if ((unsigned long )pdev != (unsigned long )((struct pci_dev *)0)) {
    goto ldv_37261;
  } else {
  }
  goto ldv_37265;
  ldv_37264:
  vga_count = vga_count + 1;
  tmp___1 = amdgpu_atpx_pci_probe_handle(pdev);
  has_atpx = ((int )has_atpx | (int )tmp___1) != 0;
  ldv_37265:
  pdev = pci_get_class(229376U, pdev);
  if ((unsigned long )pdev != (unsigned long )((struct pci_dev *)0)) {
    goto ldv_37264;
  } else {
  }
  if ((int )has_atpx && vga_count == 2) {
    acpi_get_name(amdgpu_atpx_priv.atpx.handle, 0U, & buffer);
    printk("\016VGA switcheroo: detected switching method %s handle\n", (char *)(& acpi_method_name));
    amdgpu_atpx_priv.atpx_detected = 1;
    return (1);
  } else {
  }
  return (0);
}
}
void amdgpu_register_atpx_handler(void)
{
  bool r ;
  {
  r = amdgpu_atpx_detect();
  if (! r) {
    return;
  } else {
  }
  vga_switcheroo_register_handler(& amdgpu_atpx_handler);
  return;
}
}
void amdgpu_unregister_atpx_handler(void)
{
  {
  vga_switcheroo_unregister_handler();
  return;
}
}
int ldv_retval_67 ;
extern int ldv_setup_12(void) ;
extern int ldv_release_12(void) ;
void ldv_main_exported_12(void)
{
  struct pci_dev *ldvarg793 ;
  void *tmp ;
  enum vga_switcheroo_state ldvarg791 ;
  enum vga_switcheroo_client_id ldvarg794 ;
  enum vga_switcheroo_client_id ldvarg792 ;
  int tmp___0 ;
  {
  tmp = __VERIFIER_nondet_pointer();
  ldvarg793 = (struct pci_dev *)tmp;
  ldv_memset((void *)(& ldvarg791), 0, 4UL);
  ldv_memset((void *)(& ldvarg794), 0, 4UL);
  ldv_memset((void *)(& ldvarg792), 0, 4UL);
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0: ;
  if (ldv_state_variable_12 == 2) {
    amdgpu_atpx_switchto(ldvarg794);
    ldv_state_variable_12 = 2;
  } else {
  }
  if (ldv_state_variable_12 == 1) {
    amdgpu_atpx_switchto(ldvarg794);
    ldv_state_variable_12 = 1;
  } else {
  }
  if (ldv_state_variable_12 == 3) {
    amdgpu_atpx_switchto(ldvarg794);
    ldv_state_variable_12 = 3;
  } else {
  }
  goto ldv_37287;
  case 1: ;
  if (ldv_state_variable_12 == 2) {
    amdgpu_atpx_get_client_id(ldvarg793);
    ldv_state_variable_12 = 2;
  } else {
  }
  if (ldv_state_variable_12 == 1) {
    amdgpu_atpx_get_client_id(ldvarg793);
    ldv_state_variable_12 = 1;
  } else {
  }
  if (ldv_state_variable_12 == 3) {
    amdgpu_atpx_get_client_id(ldvarg793);
    ldv_state_variable_12 = 3;
  } else {
  }
  goto ldv_37287;
  case 2: ;
  if (ldv_state_variable_12 == 2) {
    ldv_retval_67 = amdgpu_atpx_init();
    if (ldv_retval_67 == 0) {
      ldv_state_variable_12 = 3;
    } else {
    }
  } else {
  }
  goto ldv_37287;
  case 3: ;
  if (ldv_state_variable_12 == 2) {
    amdgpu_atpx_power_state(ldvarg792, ldvarg791);
    ldv_state_variable_12 = 2;
  } else {
  }
  if (ldv_state_variable_12 == 1) {
    amdgpu_atpx_power_state(ldvarg792, ldvarg791);
    ldv_state_variable_12 = 1;
  } else {
  }
  if (ldv_state_variable_12 == 3) {
    amdgpu_atpx_power_state(ldvarg792, ldvarg791);
    ldv_state_variable_12 = 3;
  } else {
  }
  goto ldv_37287;
  case 4: ;
  if (ldv_state_variable_12 == 1) {
    ldv_setup_12();
    ldv_state_variable_12 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_37287;
  case 5: ;
  if (ldv_state_variable_12 == 2) {
    ldv_release_12();
    ldv_state_variable_12 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  if (ldv_state_variable_12 == 3) {
    ldv_release_12();
    ldv_state_variable_12 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_37287;
  default:
  ldv_stop();
  }
  ldv_37287: ;
  return;
}
}
bool ldv_queue_work_on_1069(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1070(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1071(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1072(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1073(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static unsigned int __arch_hweight32(unsigned int w )
{
  unsigned int res ;
  {
  res = 0U;
  __asm__ ("661:\n\tcall __sw_hweight32\n662:\n.skip -(((6651f-6641f)-(662b-661b)) > 0) * ((6651f-6641f)-(662b-661b)),0x90\n663:\n.pushsection .altinstructions,\"a\"\n .long 661b - .\n .long 6641f - .\n .word ( 4*32+23)\n .byte 663b-661b\n .byte 6651f-6641f\n .byte 663b-662b\n.popsection\n.pushsection .altinstr_replacement, \"ax\"\n6641:\n\t.byte 0xf3,0x40,0x0f,0xb8,0xc7\n6651:\n\t.popsection": "=a" (res): "D" (w));
  return (res);
}
}
extern int strcmp(char const * , char const * ) ;
bool ldv_queue_work_on_1083(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1085(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1084(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1087(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1086(struct workqueue_struct *ldv_func_arg1 ) ;
extern int register_acpi_notifier(struct notifier_block * ) ;
extern int unregister_acpi_notifier(struct notifier_block * ) ;
extern void backlight_force_update(struct backlight_device * , enum backlight_update_reason ) ;
int amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev ) ;
int amdgpu_atif_handler(struct amdgpu_device *adev , struct acpi_bus_event *event ) ;
static union acpi_object *amdgpu_atif_call(acpi_handle handle , int function , struct acpi_buffer *params )
{
  acpi_status status ;
  union acpi_object atif_arg_elements[2U] ;
  struct acpi_object_list atif_arg ;
  struct acpi_buffer buffer ;
  char const *tmp ;
  long tmp___0 ;
  {
  buffer.length = 0xffffffffffffffffULL;
  buffer.pointer = (void *)0;
  atif_arg.count = 2U;
  atif_arg.pointer = (union acpi_object *)(& atif_arg_elements);
  atif_arg_elements[0].type = 1U;
  atif_arg_elements[0].integer.value = (u64 )function;
  if ((unsigned long )params != (unsigned long )((struct acpi_buffer *)0)) {
    atif_arg_elements[1].type = 3U;
    atif_arg_elements[1].buffer.length = (u32 )params->length;
    atif_arg_elements[1].buffer.pointer = (u8 *)params->pointer;
  } else {
    atif_arg_elements[1].type = 1U;
    atif_arg_elements[1].integer.value = 0ULL;
  }
  status = acpi_evaluate_object(handle, (char *)"ATIF", & atif_arg, & buffer);
  if (status != 0U && status != 5U) {
    tmp___0 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___0 != 0L) {
      tmp = acpi_format_exception(status);
      drm_ut_debug_printk("amdgpu_atif_call", "failed to evaluate ATIF got %s\n",
                          tmp);
    } else {
    }
    kfree((void const *)buffer.pointer);
    return ((union acpi_object *)0);
  } else {
  }
  return ((union acpi_object *)buffer.pointer);
}
}
static void amdgpu_atif_parse_notification(struct amdgpu_atif_notifications *n , u32 mask )
{
  {
  n->display_switch = (mask & 1U) != 0U;
  n->expansion_mode_change = (mask & 2U) != 0U;
  n->thermal_state = (mask & 4U) != 0U;
  n->forced_power_state = (mask & 8U) != 0U;
  n->system_power_state = (mask & 16U) != 0U;
  n->display_conf_change = (mask & 32U) != 0U;
  n->px_gfx_switch = (mask & 64U) != 0U;
  n->brightness_change = (mask & 128U) != 0U;
  n->dgpu_display_event = (mask & 256U) != 0U;
  return;
}
}
static void amdgpu_atif_parse_functions(struct amdgpu_atif_functions *f , u32 mask )
{
  {
  f->system_params = (mask & 1U) != 0U;
  f->sbios_requests = (mask & 2U) != 0U;
  f->select_active_disp = (mask & 4U) != 0U;
  f->lid_state = (mask & 8U) != 0U;
  f->get_tv_standard = (mask & 16U) != 0U;
  f->set_tv_standard = (mask & 32U) != 0U;
  f->get_panel_expansion_mode = (mask & 64U) != 0U;
  f->set_panel_expansion_mode = (mask & 128U) != 0U;
  f->temperature_change = (mask & 4096U) != 0U;
  f->graphics_device_types = (mask & 16384U) != 0U;
  return;
}
}
static int amdgpu_atif_verify_interface(acpi_handle handle , struct amdgpu_atif *atif )
{
  union acpi_object *info ;
  struct atif_verify_interface output ;
  size_t size ;
  int err ;
  unsigned long _min1 ;
  size_t _min2 ;
  long tmp ;
  {
  err = 0;
  info = amdgpu_atif_call(handle, 0, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  memset((void *)(& output), 0, 12UL);
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 11UL) {
    printk("\016[drm] ATIF buffer is too small: %zu\n", size);
    err = -22;
    goto out;
  } else {
  }
  _min1 = 12UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)(& output), (void const *)info->buffer.pointer, size);
  tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atif_verify_interface", "ATIF version %u\n", (int )output.version);
  } else {
  }
  amdgpu_atif_parse_notification(& atif->notifications, output.notification_mask);
  amdgpu_atif_parse_functions(& atif->functions, output.function_bits);
  out:
  kfree((void const *)info);
  return (err);
}
}
static int amdgpu_atif_get_notification_params(acpi_handle handle , struct amdgpu_atif_notification_cfg *n )
{
  union acpi_object *info ;
  struct atif_system_params params ;
  size_t size ;
  int err ;
  unsigned long _min1 ;
  size_t _min2 ;
  long tmp ;
  long tmp___0 ;
  {
  err = 0;
  info = amdgpu_atif_call(handle, 1, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    err = -5;
    goto out;
  } else {
  }
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 9UL) {
    err = -22;
    goto out;
  } else {
  }
  memset((void *)(& params), 0, 11UL);
  _min1 = 11UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)(& params), (void const *)info->buffer.pointer, size);
  tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atif_get_notification_params", "SYSTEM_PARAMS: mask = %#x, flags = %#x\n",
                        params.flags, params.valid_mask);
  } else {
  }
  params.flags = params.flags & params.valid_mask;
  if ((params.flags & 3U) == 0U) {
    n->enabled = 0;
    n->command_code = 0;
  } else
  if ((params.flags & 3U) == 1U) {
    n->enabled = 1;
    n->command_code = 129;
  } else {
    if (size <= 10UL) {
      err = -22;
      goto out;
    } else {
    }
    n->enabled = 1;
    n->command_code = (int )params.command_code;
  }
  out:
  tmp___0 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp___0 != 0L) {
    drm_ut_debug_printk("amdgpu_atif_get_notification_params", "Notification %s, command code = %#x\n",
                        (int )n->enabled ? (char *)"enabled" : (char *)"disabled",
                        n->command_code);
  } else {
  }
  kfree((void const *)info);
  return (err);
}
}
static int amdgpu_atif_get_sbios_requests(acpi_handle handle , struct atif_sbios_requests *req )
{
  union acpi_object *info ;
  size_t size ;
  int count ;
  unsigned long _min1 ;
  size_t _min2 ;
  long tmp ;
  unsigned int tmp___0 ;
  {
  count = 0;
  info = amdgpu_atif_call(handle, 2, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 12UL) {
    count = -22;
    goto out;
  } else {
  }
  memset((void *)req, 0, 13UL);
  _min1 = 13UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)req, (void const *)info->buffer.pointer, size);
  tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atif_get_sbios_requests", "SBIOS pending requests: %#x\n",
                        req->pending);
  } else {
  }
  tmp___0 = __arch_hweight32(req->pending);
  count = (int )tmp___0;
  out:
  kfree((void const *)info);
  return (count);
}
}
int amdgpu_atif_handler(struct amdgpu_device *adev , struct acpi_bus_event *event )
{
  struct amdgpu_atif *atif ;
  struct atif_sbios_requests req ;
  acpi_handle handle ;
  int count ;
  long tmp ;
  int tmp___0 ;
  struct acpi_device *tmp___1 ;
  long tmp___2 ;
  struct amdgpu_encoder *enc ;
  struct amdgpu_encoder_atom_dig *dig ;
  long tmp___3 ;
  {
  atif = & adev->atif;
  tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atif_handler", "event, device_class = %s, type = %#x\n",
                        (char *)(& event->device_class), event->type);
  } else {
  }
  tmp___0 = strcmp((char const *)(& event->device_class), "video");
  if (tmp___0 != 0) {
    return (0);
  } else {
  }
  if (! atif->notification_cfg.enabled || event->type != (u32 )atif->notification_cfg.command_code) {
    return (0);
  } else {
  }
  tmp___1 = to_acpi_node((adev->pdev)->dev.fwnode);
  handle = acpi_device_handle(tmp___1);
  count = amdgpu_atif_get_sbios_requests(handle, & req);
  if (count <= 0) {
    return (0);
  } else {
  }
  tmp___2 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp___2 != 0L) {
    drm_ut_debug_printk("amdgpu_atif_handler", "ATIF: %d pending SBIOS requests\n",
                        count);
  } else {
  }
  if ((req.pending & 128U) != 0U) {
    enc = atif->encoder_for_bl;
    if ((unsigned long )enc != (unsigned long )((struct amdgpu_encoder *)0)) {
      dig = (struct amdgpu_encoder_atom_dig *)enc->enc_priv;
      tmp___3 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
      if (tmp___3 != 0L) {
        drm_ut_debug_printk("amdgpu_atif_handler", "Changing brightness to %d\n",
                            (int )req.backlight_level);
      } else {
      }
      (*((adev->mode_info.funcs)->backlight_set_level))(enc, (int )req.backlight_level);
      backlight_force_update(dig->bl_dev, 0);
    } else {
    }
  } else {
  }
  return (32770);
}
}
static union acpi_object *amdgpu_atcs_call(acpi_handle handle , int function , struct acpi_buffer *params )
{
  acpi_status status ;
  union acpi_object atcs_arg_elements[2U] ;
  struct acpi_object_list atcs_arg ;
  struct acpi_buffer buffer ;
  char const *tmp ;
  long tmp___0 ;
  {
  buffer.length = 0xffffffffffffffffULL;
  buffer.pointer = (void *)0;
  atcs_arg.count = 2U;
  atcs_arg.pointer = (union acpi_object *)(& atcs_arg_elements);
  atcs_arg_elements[0].type = 1U;
  atcs_arg_elements[0].integer.value = (u64 )function;
  if ((unsigned long )params != (unsigned long )((struct acpi_buffer *)0)) {
    atcs_arg_elements[1].type = 3U;
    atcs_arg_elements[1].buffer.length = (u32 )params->length;
    atcs_arg_elements[1].buffer.pointer = (u8 *)params->pointer;
  } else {
    atcs_arg_elements[1].type = 1U;
    atcs_arg_elements[1].integer.value = 0ULL;
  }
  status = acpi_evaluate_object(handle, (char *)"ATCS", & atcs_arg, & buffer);
  if (status != 0U && status != 5U) {
    tmp___0 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___0 != 0L) {
      tmp = acpi_format_exception(status);
      drm_ut_debug_printk("amdgpu_atcs_call", "failed to evaluate ATCS got %s\n",
                          tmp);
    } else {
    }
    kfree((void const *)buffer.pointer);
    return ((union acpi_object *)0);
  } else {
  }
  return ((union acpi_object *)buffer.pointer);
}
}
static void amdgpu_atcs_parse_functions(struct amdgpu_atcs_functions *f , u32 mask )
{
  {
  f->get_ext_state = (mask & 1U) != 0U;
  f->pcie_perf_req = (mask & 2U) != 0U;
  f->pcie_dev_rdy = (mask & 4U) != 0U;
  f->pcie_bus_width = (mask & 8U) != 0U;
  return;
}
}
static int amdgpu_atcs_verify_interface(acpi_handle handle , struct amdgpu_atcs *atcs )
{
  union acpi_object *info ;
  struct atcs_verify_interface output ;
  size_t size ;
  int err ;
  unsigned long _min1 ;
  size_t _min2 ;
  long tmp ;
  {
  err = 0;
  info = amdgpu_atcs_call(handle, 0, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  memset((void *)(& output), 0, 8UL);
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 7UL) {
    printk("\016[drm] ATCS buffer is too small: %zu\n", size);
    err = -22;
    goto out;
  } else {
  }
  _min1 = 8UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)(& output), (void const *)info->buffer.pointer, size);
  tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
  if (tmp != 0L) {
    drm_ut_debug_printk("amdgpu_atcs_verify_interface", "ATCS version %u\n", (int )output.version);
  } else {
  }
  amdgpu_atcs_parse_functions(& atcs->functions, output.function_bits);
  out:
  kfree((void const *)info);
  return (err);
}
}
bool amdgpu_acpi_is_pcie_performance_request_supported(struct amdgpu_device *adev )
{
  struct amdgpu_atcs *atcs ;
  {
  atcs = & adev->atcs;
  if ((int )atcs->functions.pcie_perf_req && (int )atcs->functions.pcie_dev_rdy) {
    return (1);
  } else {
  }
  return (0);
}
}
int amdgpu_acpi_pcie_notify_device_ready(struct amdgpu_device *adev )
{
  acpi_handle handle ;
  union acpi_object *info ;
  struct amdgpu_atcs *atcs ;
  struct acpi_device *tmp ;
  {
  atcs = & adev->atcs;
  tmp = to_acpi_node((adev->pdev)->dev.fwnode);
  handle = acpi_device_handle(tmp);
  if ((unsigned long )handle == (unsigned long )((acpi_handle )0)) {
    return (-22);
  } else {
  }
  if (! atcs->functions.pcie_dev_rdy) {
    return (-22);
  } else {
  }
  info = amdgpu_atcs_call(handle, 3, (struct acpi_buffer *)0);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  kfree((void const *)info);
  return (0);
}
}
int amdgpu_acpi_pcie_performance_request(struct amdgpu_device *adev , u8 perf_req ,
                                         bool advertise )
{
  acpi_handle handle ;
  union acpi_object *info ;
  struct amdgpu_atcs *atcs ;
  struct atcs_pref_req_input atcs_input ;
  struct atcs_pref_req_output atcs_output ;
  struct acpi_buffer params ;
  size_t size ;
  u32 retry ;
  struct acpi_device *tmp ;
  unsigned long _min1 ;
  size_t _min2 ;
  u32 tmp___0 ;
  {
  atcs = & adev->atcs;
  retry = 3U;
  tmp = to_acpi_node((adev->pdev)->dev.fwnode);
  handle = acpi_device_handle(tmp);
  if ((unsigned long )handle == (unsigned long )((acpi_handle )0)) {
    return (-22);
  } else {
  }
  if (! atcs->functions.pcie_perf_req) {
    return (-22);
  } else {
  }
  atcs_input.size = 10U;
  atcs_input.client_id = (int )((u16 )(adev->pdev)->devfn) | ((int )((u16 )((adev->pdev)->bus)->number) << 8U);
  atcs_input.valid_flags_mask = 3U;
  atcs_input.flags = 2U;
  if ((int )advertise) {
    atcs_input.flags = (u16 )((unsigned int )atcs_input.flags | 1U);
  } else {
  }
  atcs_input.req_type = 1U;
  atcs_input.perf_req = perf_req;
  params.length = 10ULL;
  params.pointer = (void *)(& atcs_input);
  goto ldv_52154;
  ldv_52153:
  info = amdgpu_atcs_call(handle, 2, & params);
  if ((unsigned long )info == (unsigned long )((union acpi_object *)0)) {
    return (-5);
  } else {
  }
  memset((void *)(& atcs_output), 0, 3UL);
  size = (size_t )*((u16 *)info->buffer.pointer);
  if (size <= 2UL) {
    printk("\016[drm] ATCS buffer is too small: %zu\n", size);
    kfree((void const *)info);
    return (-22);
  } else {
  }
  _min1 = 3UL;
  _min2 = size;
  size = _min1 < _min2 ? _min1 : _min2;
  memcpy((void *)(& atcs_output), (void const *)info->buffer.pointer, size);
  kfree((void const *)info);
  switch ((int )atcs_output.ret_val) {
  case 1: ;
  default: ;
  return (-22);
  case 2: ;
  return (0);
  case 3:
  __const_udelay(42950UL);
  goto ldv_52152;
  }
  ldv_52152: ;
  ldv_52154:
  tmp___0 = retry;
  retry = retry - 1U;
  if (tmp___0 != 0U) {
    goto ldv_52153;
  } else {
  }
  return (0);
}
}
static int amdgpu_acpi_event(struct notifier_block *nb , unsigned long val , void *data )
{
  struct amdgpu_device *adev ;
  struct notifier_block const *__mptr ;
  struct acpi_bus_event *entry ;
  long tmp ;
  long tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  __mptr = (struct notifier_block const *)nb;
  adev = (struct amdgpu_device *)__mptr + 0xfffffffffffffec8UL;
  entry = (struct acpi_bus_event *)data;
  tmp___2 = strcmp((char const *)(& entry->device_class), "ac_adapter");
  if (tmp___2 == 0) {
    tmp___1 = power_supply_is_system_supplied();
    if (tmp___1 > 0) {
      tmp = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
      if (tmp != 0L) {
        drm_ut_debug_printk("amdgpu_acpi_event", "pm: AC\n");
      } else {
      }
    } else {
      tmp___0 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
      if (tmp___0 != 0L) {
        drm_ut_debug_printk("amdgpu_acpi_event", "pm: DC\n");
      } else {
      }
    }
    amdgpu_pm_acpi_event_handler(adev);
  } else {
  }
  tmp___3 = amdgpu_atif_handler(adev, entry);
  return (tmp___3);
}
}
int amdgpu_acpi_init(struct amdgpu_device *adev )
{
  acpi_handle handle ;
  struct amdgpu_atif *atif ;
  struct amdgpu_atcs *atcs ;
  int ret ;
  struct acpi_device *tmp ;
  long tmp___0 ;
  long tmp___1 ;
  struct drm_encoder *tmp___2 ;
  struct list_head const *__mptr ;
  struct amdgpu_encoder *enc ;
  struct drm_encoder const *__mptr___0 ;
  struct amdgpu_encoder_atom_dig *dig ;
  struct list_head const *__mptr___1 ;
  long tmp___3 ;
  {
  atif = & adev->atif;
  atcs = & adev->atcs;
  tmp = to_acpi_node((adev->pdev)->dev.fwnode);
  handle = acpi_device_handle(tmp);
  if ((unsigned long )adev->bios == (unsigned long )((uint8_t *)0U) || (unsigned long )handle == (unsigned long )((acpi_handle )0)) {
    return (0);
  } else {
  }
  ret = amdgpu_atcs_verify_interface(handle, atcs);
  if (ret != 0) {
    tmp___0 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___0 != 0L) {
      drm_ut_debug_printk("amdgpu_acpi_init", "Call to ATCS verify_interface failed: %d\n",
                          ret);
    } else {
    }
  } else {
  }
  ret = amdgpu_atif_verify_interface(handle, atif);
  if (ret != 0) {
    tmp___1 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
    if (tmp___1 != 0L) {
      drm_ut_debug_printk("amdgpu_acpi_init", "Call to ATIF verify_interface failed: %d\n",
                          ret);
    } else {
    }
    goto out;
  } else {
  }
  if ((int )atif->notifications.brightness_change) {
    __mptr = (struct list_head const *)(adev->ddev)->mode_config.encoder_list.next;
    tmp___2 = (struct drm_encoder *)__mptr + 0xfffffffffffffff8UL;
    goto ldv_52186;
    ldv_52185:
    __mptr___0 = (struct drm_encoder const *)tmp___2;
    enc = (struct amdgpu_encoder *)__mptr___0;
    if (((long )enc->devices & 34L) != 0L && (unsigned long )enc->enc_priv != (unsigned long )((void *)0)) {
      if ((int )adev->is_atom_bios) {
        dig = (struct amdgpu_encoder_atom_dig *)enc->enc_priv;
        if ((unsigned long )dig->bl_dev != (unsigned long )((struct backlight_device *)0)) {
          atif->encoder_for_bl = enc;
          goto ldv_52184;
        } else {
        }
      } else {
      }
    } else {
    }
    __mptr___1 = (struct list_head const *)tmp___2->head.next;
    tmp___2 = (struct drm_encoder *)__mptr___1 + 0xfffffffffffffff8UL;
    ldv_52186: ;
    if ((unsigned long )(& tmp___2->head) != (unsigned long )(& (adev->ddev)->mode_config.encoder_list)) {
      goto ldv_52185;
    } else {
    }
    ldv_52184: ;
  } else {
  }
  if ((int )atif->functions.sbios_requests && ! atif->functions.system_params) {
    atif->functions.system_params = 1;
  } else {
  }
  if ((int )atif->functions.system_params) {
    ret = amdgpu_atif_get_notification_params(handle, & atif->notification_cfg);
    if (ret != 0) {
      tmp___3 = ldv__builtin_expect((drm_debug & 2U) != 0U, 0L);
      if (tmp___3 != 0L) {
        drm_ut_debug_printk("amdgpu_acpi_init", "Call to GET_SYSTEM_PARAMS failed: %d\n",
                            ret);
      } else {
      }
      atif->notification_cfg.enabled = 0;
    } else {
    }
  } else {
  }
  out:
  adev->acpi_nb.notifier_call = & amdgpu_acpi_event;
  register_acpi_notifier(& adev->acpi_nb);
  return (ret);
}
}
void amdgpu_acpi_fini(struct amdgpu_device *adev )
{
  {
  unregister_acpi_notifier(& adev->acpi_nb);
  return;
}
}
bool ldv_queue_work_on_1083(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1084(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1085(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1086(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1087(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
__inline static long ldv__builtin_expect(long exp , long c ) ;
__inline static void INIT_HLIST_NODE(struct hlist_node *h )
{
  {
  h->next = (struct hlist_node *)0;
  h->pprev = (struct hlist_node **)0;
  return;
}
}
__inline static int hlist_unhashed(struct hlist_node const *h )
{
  {
  return ((unsigned long )h->pprev == (unsigned long )((struct hlist_node ** )0));
}
}
__inline static void __hlist_del(struct hlist_node *n )
{
  struct hlist_node *next ;
  struct hlist_node **pprev ;
  {
  next = n->next;
  pprev = n->pprev;
  *pprev = next;
  if ((unsigned long )next != (unsigned long )((struct hlist_node *)0)) {
    next->pprev = pprev;
  } else {
  }
  return;
}
}
__inline static void hlist_del_init(struct hlist_node *n )
{
  int tmp ;
  {
  tmp = hlist_unhashed((struct hlist_node const *)n);
  if (tmp == 0) {
    __hlist_del(n);
    INIT_HLIST_NODE(n);
  } else {
  }
  return;
}
}
__inline static void hlist_add_head(struct hlist_node *n , struct hlist_head *h )
{
  struct hlist_node *first ;
  {
  first = h->first;
  n->next = first;
  if ((unsigned long )first != (unsigned long )((struct hlist_node *)0)) {
    first->pprev = & n->next;
  } else {
  }
  h->first = n;
  n->pprev = & h->first;
  return;
}
}
__inline static void *ERR_PTR(long error ) ;
__inline static long PTR_ERR(void const *ptr ) ;
__inline static bool IS_ERR(void const *ptr ) ;
bool ldv_queue_work_on_1097(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_work_on_1099(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 ) ;
bool ldv_queue_delayed_work_on_1098(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
bool ldv_queue_delayed_work_on_1101(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 ) ;
void ldv_flush_workqueue_1100(struct workqueue_struct *ldv_func_arg1 ) ;
__inline static bool queue_work___10(struct workqueue_struct *wq , struct work_struct *work )
{
  bool tmp ;
  {
  tmp = ldv_queue_work_on_1097(8192, wq, work);
  return (tmp);
}
}
__inline static bool schedule_work___9(struct work_struct *work )
{
  bool tmp ;
  {
  tmp = queue_work___10(system_wq, work);
  return (tmp);
}
}
void disable_work_10(struct work_struct *work ) ;
void activate_work_10(struct work_struct *work , int state ) ;
void call_and_disable_all_10(int state ) ;
void call_and_disable_work_10(struct work_struct *work ) ;
void invoke_work_10(void) ;
extern int __mmu_notifier_register(struct mmu_notifier * , struct mm_struct * ) ;
extern void mmu_notifier_unregister(struct mmu_notifier * , struct mm_struct * ) ;
extern struct interval_tree_node *interval_tree_iter_next(struct interval_tree_node * ,
                                                          unsigned long , unsigned long ) ;
__inline static u64 hash_64(u64 val , unsigned int bits )
{
  u64 hash ;
  {
  hash = val;
  hash = hash * 0x9e37fffffffc0001ULL;
  return (hash >> (int )(64U - bits));
}
}
__inline static void hash_del(struct hlist_node *node )
{
  {
  hlist_del_init(node);
  return;
}
}
static void amdgpu_mn_destroy(struct work_struct *work )
{
  struct amdgpu_mn *rmn ;
  struct work_struct const *__mptr ;
  struct amdgpu_device *adev ;
  struct amdgpu_mn_node *node ;
  struct amdgpu_mn_node *next_node ;
  struct amdgpu_bo *bo ;
  struct amdgpu_bo *next_bo ;
  struct rb_node *____ptr ;
  struct rb_node *tmp ;
  struct rb_node const *__mptr___0 ;
  struct amdgpu_mn_node *tmp___0 ;
  struct list_head const *__mptr___1 ;
  struct list_head const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  struct rb_node *____ptr___0 ;
  struct rb_node *tmp___1 ;
  struct rb_node const *__mptr___4 ;
  struct amdgpu_mn_node *tmp___2 ;
  {
  __mptr = (struct work_struct const *)work;
  rmn = (struct amdgpu_mn *)__mptr + 0xffffffffffffffd8UL;
  adev = rmn->adev;
  mutex_lock_nested(& adev->mn_lock, 0U);
  mutex_lock_nested(& rmn->lock, 0U);
  hash_del(& rmn->node);
  tmp = rb_first_postorder((struct rb_root const *)(& rmn->objects));
  ____ptr = tmp;
  if ((unsigned long )____ptr != (unsigned long )((struct rb_node *)0)) {
    __mptr___0 = (struct rb_node const *)____ptr;
    tmp___0 = (struct amdgpu_mn_node *)__mptr___0;
  } else {
    tmp___0 = (struct amdgpu_mn_node *)0;
  }
  node = tmp___0;
  goto ldv_43842;
  ldv_43841:
  interval_tree_remove(& node->it, & rmn->objects);
  __mptr___1 = (struct list_head const *)node->bos.next;
  bo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffa98UL;
  __mptr___2 = (struct list_head const *)bo->mn_list.next;
  next_bo = (struct amdgpu_bo *)__mptr___2 + 0xfffffffffffffa98UL;
  goto ldv_43839;
  ldv_43838:
  bo->mn = (struct amdgpu_mn *)0;
  list_del_init(& bo->mn_list);
  bo = next_bo;
  __mptr___3 = (struct list_head const *)next_bo->mn_list.next;
  next_bo = (struct amdgpu_bo *)__mptr___3 + 0xfffffffffffffa98UL;
  ldv_43839: ;
  if ((unsigned long )(& bo->mn_list) != (unsigned long )(& node->bos)) {
    goto ldv_43838;
  } else {
  }
  kfree((void const *)node);
  node = next_node;
  ldv_43842: ;
  if ((unsigned long )node != (unsigned long )((struct amdgpu_mn_node *)0)) {
    tmp___1 = rb_next_postorder((struct rb_node const *)(& node->it.rb));
    ____ptr___0 = tmp___1;
    if ((unsigned long )____ptr___0 != (unsigned long )((struct rb_node *)0)) {
      __mptr___4 = (struct rb_node const *)____ptr___0;
      tmp___2 = (struct amdgpu_mn_node *)__mptr___4;
    } else {
      tmp___2 = (struct amdgpu_mn_node *)0;
    }
    next_node = tmp___2;
    goto ldv_43841;
  } else {
  }
  mutex_unlock(& rmn->lock);
  mutex_unlock(& adev->mn_lock);
  mmu_notifier_unregister(& rmn->mn, rmn->mm);
  kfree((void const *)rmn);
  return;
}
}
static void amdgpu_mn_release(struct mmu_notifier *mn , struct mm_struct *mm )
{
  struct amdgpu_mn *rmn ;
  struct mmu_notifier const *__mptr ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  {
  __mptr = (struct mmu_notifier const *)mn;
  rmn = (struct amdgpu_mn *)__mptr + 0xfffffffffffffff0UL;
  __init_work(& rmn->work, 0);
  __constr_expr_0.counter = 137438953408L;
  rmn->work.data = __constr_expr_0;
  lockdep_init_map(& rmn->work.lockdep_map, "(&rmn->work)", & __key, 0);
  INIT_LIST_HEAD(& rmn->work.entry);
  rmn->work.func = & amdgpu_mn_destroy;
  schedule_work___9(& rmn->work);
  return;
}
}
static void amdgpu_mn_invalidate_range_start(struct mmu_notifier *mn , struct mm_struct *mm ,
                                             unsigned long start , unsigned long end )
{
  struct amdgpu_mn *rmn ;
  struct mmu_notifier const *__mptr ;
  struct interval_tree_node *it ;
  struct amdgpu_mn_node *node ;
  struct amdgpu_bo *bo ;
  long r ;
  struct interval_tree_node const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  int tmp ;
  int tmp___0 ;
  struct list_head const *__mptr___2 ;
  {
  __mptr = (struct mmu_notifier const *)mn;
  rmn = (struct amdgpu_mn *)__mptr + 0xfffffffffffffff0UL;
  end = end - 1UL;
  mutex_lock_nested(& rmn->lock, 0U);
  it = interval_tree_iter_first(& rmn->objects, start, end);
  goto ldv_43877;
  ldv_43876:
  __mptr___0 = (struct interval_tree_node const *)it;
  node = (struct amdgpu_mn_node *)__mptr___0;
  it = interval_tree_iter_next(it, start, end);
  __mptr___1 = (struct list_head const *)node->bos.next;
  bo = (struct amdgpu_bo *)__mptr___1 + 0xfffffffffffffa98UL;
  goto ldv_43874;
  ldv_43873: ;
  if ((unsigned long )bo->tbo.ttm == (unsigned long )((struct ttm_tt *)0) || (unsigned int )(bo->tbo.ttm)->state != 0U) {
    goto ldv_43872;
  } else {
  }
  tmp = amdgpu_bo_reserve(bo, 1);
  r = (long )tmp;
  if (r != 0L) {
    drm_err("(%ld) failed to reserve user bo\n", r);
    goto ldv_43872;
  } else {
  }
  r = reservation_object_wait_timeout_rcu(bo->tbo.resv, 1, 0, 9223372036854775807UL);
  if (r <= 0L) {
    drm_err("(%ld) failed to wait for user bo\n", r);
  } else {
  }
  amdgpu_ttm_placement_from_domain(bo, 1U);
  tmp___0 = ttm_bo_validate(& bo->tbo, & bo->placement, 0, 0);
  r = (long )tmp___0;
  if (r != 0L) {
    drm_err("(%ld) failed to validate user bo\n", r);
  } else {
  }
  amdgpu_bo_unreserve(bo);
  ldv_43872:
  __mptr___2 = (struct list_head const *)bo->mn_list.next;
  bo = (struct amdgpu_bo *)__mptr___2 + 0xfffffffffffffa98UL;
  ldv_43874: ;
  if ((unsigned long )(& bo->mn_list) != (unsigned long )(& node->bos)) {
    goto ldv_43873;
  } else {
  }
  ldv_43877: ;
  if ((unsigned long )it != (unsigned long )((struct interval_tree_node *)0)) {
    goto ldv_43876;
  } else {
  }
  mutex_unlock(& rmn->lock);
  return;
}
}
static struct mmu_notifier_ops const amdgpu_mn_ops =
     {& amdgpu_mn_release, 0, 0, 0, 0, & amdgpu_mn_invalidate_range_start, 0, 0};
static struct amdgpu_mn *amdgpu_mn_get(struct amdgpu_device *adev )
{
  struct mm_struct *mm ;
  struct task_struct *tmp ;
  struct amdgpu_mn *rmn ;
  int r ;
  struct hlist_node *____ptr ;
  u64 tmp___0 ;
  struct hlist_node const *__mptr ;
  struct amdgpu_mn *tmp___1 ;
  struct hlist_node *____ptr___0 ;
  struct hlist_node const *__mptr___0 ;
  struct amdgpu_mn *tmp___2 ;
  void *tmp___3 ;
  void *tmp___4 ;
  struct lock_class_key __key ;
  struct rb_root __constr_expr_0 ;
  u64 tmp___5 ;
  void *tmp___6 ;
  {
  tmp = get_current();
  mm = tmp->mm;
  down_write(& mm->mmap_sem);
  mutex_lock_nested(& adev->mn_lock, 0U);
  tmp___0 = hash_64((u64 )mm, 7U);
  ____ptr = ((struct hlist_head *)(& adev->mn_hash) + tmp___0)->first;
  if ((unsigned long )____ptr != (unsigned long )((struct hlist_node *)0)) {
    __mptr = (struct hlist_node const *)____ptr;
    tmp___1 = (struct amdgpu_mn *)__mptr + 0xffffffffffffff88UL;
  } else {
    tmp___1 = (struct amdgpu_mn *)0;
  }
  rmn = tmp___1;
  goto ldv_44448;
  ldv_44447: ;
  if ((unsigned long )rmn->mm == (unsigned long )mm) {
    goto release_locks;
  } else {
  }
  ____ptr___0 = rmn->node.next;
  if ((unsigned long )____ptr___0 != (unsigned long )((struct hlist_node *)0)) {
    __mptr___0 = (struct hlist_node const *)____ptr___0;
    tmp___2 = (struct amdgpu_mn *)__mptr___0 + 0xffffffffffffff88UL;
  } else {
    tmp___2 = (struct amdgpu_mn *)0;
  }
  rmn = tmp___2;
  ldv_44448: ;
  if ((unsigned long )rmn != (unsigned long )((struct amdgpu_mn *)0)) {
    goto ldv_44447;
  } else {
  }
  tmp___3 = kzalloc(304UL, 208U);
  rmn = (struct amdgpu_mn *)tmp___3;
  if ((unsigned long )rmn == (unsigned long )((struct amdgpu_mn *)0)) {
    tmp___4 = ERR_PTR(-12L);
    rmn = (struct amdgpu_mn *)tmp___4;
    goto release_locks;
  } else {
  }
  rmn->adev = adev;
  rmn->mm = mm;
  rmn->mn.ops = & amdgpu_mn_ops;
  __mutex_init(& rmn->lock, "&rmn->lock", & __key);
  __constr_expr_0.rb_node = (struct rb_node *)0;
  rmn->objects = __constr_expr_0;
  r = __mmu_notifier_register(& rmn->mn, mm);
  if (r != 0) {
    goto free_rmn;
  } else {
  }
  tmp___5 = hash_64((u64 )mm, 7U);
  hlist_add_head(& rmn->node, (struct hlist_head *)(& adev->mn_hash) + tmp___5);
  release_locks:
  mutex_unlock(& adev->mn_lock);
  up_write(& mm->mmap_sem);
  return (rmn);
  free_rmn:
  mutex_unlock(& adev->mn_lock);
  up_write(& mm->mmap_sem);
  kfree((void const *)rmn);
  tmp___6 = ERR_PTR((long )r);
  return ((struct amdgpu_mn *)tmp___6);
}
}
int amdgpu_mn_register(struct amdgpu_bo *bo , unsigned long addr )
{
  unsigned long end ;
  unsigned long tmp ;
  struct amdgpu_device *adev ;
  struct amdgpu_mn *rmn ;
  struct amdgpu_mn_node *node ;
  struct list_head bos ;
  struct interval_tree_node *it ;
  long tmp___0 ;
  bool tmp___1 ;
  struct interval_tree_node const *__mptr ;
  unsigned long _min1 ;
  unsigned long _min2 ;
  unsigned long _max1 ;
  unsigned long _max2 ;
  void *tmp___2 ;
  {
  tmp = amdgpu_bo_size(bo);
  end = (tmp + addr) - 1UL;
  adev = bo->adev;
  node = (struct amdgpu_mn_node *)0;
  rmn = amdgpu_mn_get(adev);
  tmp___1 = IS_ERR((void const *)rmn);
  if ((int )tmp___1) {
    tmp___0 = PTR_ERR((void const *)rmn);
    return ((int )tmp___0);
  } else {
  }
  INIT_LIST_HEAD(& bos);
  mutex_lock_nested(& rmn->lock, 0U);
  goto ldv_44748;
  ldv_44747:
  kfree((void const *)node);
  __mptr = (struct interval_tree_node const *)it;
  node = (struct amdgpu_mn_node *)__mptr;
  interval_tree_remove(& node->it, & rmn->objects);
  _min1 = it->start;
  _min2 = addr;
  addr = _min1 < _min2 ? _min1 : _min2;
  _max1 = it->last;
  _max2 = end;
  end = _max1 > _max2 ? _max1 : _max2;
  list_splice((struct list_head const *)(& node->bos), & bos);
  ldv_44748:
  it = interval_tree_iter_first(& rmn->objects, addr, end);
  if ((unsigned long )it != (unsigned long )((struct interval_tree_node *)0)) {
    goto ldv_44747;
  } else {
  }
  if ((unsigned long )node == (unsigned long )((struct amdgpu_mn_node *)0)) {
    tmp___2 = kmalloc(64UL, 208U);
    node = (struct amdgpu_mn_node *)tmp___2;
    if ((unsigned long )node == (unsigned long )((struct amdgpu_mn_node *)0)) {
      mutex_unlock(& rmn->lock);
      return (-12);
    } else {
    }
  } else {
  }
  bo->mn = rmn;
  node->it.start = addr;
  node->it.last = end;
  INIT_LIST_HEAD(& node->bos);
  list_splice((struct list_head const *)(& bos), & node->bos);
  list_add(& bo->mn_list, & node->bos);
  interval_tree_insert(& node->it, & rmn->objects);
  mutex_unlock(& rmn->lock);
  return (0);
}
}
void amdgpu_mn_unregister(struct amdgpu_bo *bo )
{
  struct amdgpu_device *adev ;
  struct amdgpu_mn *rmn ;
  struct list_head *head ;
  struct amdgpu_mn_node *node ;
  struct list_head const *__mptr ;
  int tmp ;
  {
  adev = bo->adev;
  mutex_lock_nested(& adev->mn_lock, 0U);
  rmn = bo->mn;
  if ((unsigned long )rmn == (unsigned long )((struct amdgpu_mn *)0)) {
    mutex_unlock(& adev->mn_lock);
    return;
  } else {
  }
  mutex_lock_nested(& rmn->lock, 0U);
  head = bo->mn_list.next;
  bo->mn = (struct amdgpu_mn *)0;
  list_del(& bo->mn_list);
  tmp = list_empty((struct list_head const *)head);
  if (tmp != 0) {
    __mptr = (struct list_head const *)head;
    node = (struct amdgpu_mn_node *)__mptr + 0xffffffffffffffd0UL;
    interval_tree_remove(& node->it, & rmn->objects);
    kfree((void const *)node);
  } else {
  }
  mutex_unlock(& rmn->lock);
  mutex_unlock(& adev->mn_lock);
  return;
}
}
extern int ldv_probe_11(void) ;
void work_init_10(void)
{
  {
  ldv_work_10_0 = 0;
  ldv_work_10_1 = 0;
  ldv_work_10_2 = 0;
  ldv_work_10_3 = 0;
  return;
}
}
void ldv_initialize_mmu_notifier_ops_11(void)
{
  void *tmp ;
  void *tmp___0 ;
  {
  tmp = ldv_init_zalloc(24UL);
  amdgpu_mn_ops_group0 = (struct mmu_notifier *)tmp;
  tmp___0 = ldv_init_zalloc(2296UL);
  amdgpu_mn_ops_group1 = (struct mm_struct *)tmp___0;
  return;
}
}
void disable_work_10(struct work_struct *work )
{
  {
  if ((ldv_work_10_0 == 3 || ldv_work_10_0 == 2) && (unsigned long )ldv_work_struct_10_0 == (unsigned long )work) {
    ldv_work_10_0 = 1;
  } else {
  }
  if ((ldv_work_10_1 == 3 || ldv_work_10_1 == 2) && (unsigned long )ldv_work_struct_10_1 == (unsigned long )work) {
    ldv_work_10_1 = 1;
  } else {
  }
  if ((ldv_work_10_2 == 3 || ldv_work_10_2 == 2) && (unsigned long )ldv_work_struct_10_2 == (unsigned long )work) {
    ldv_work_10_2 = 1;
  } else {
  }
  if ((ldv_work_10_3 == 3 || ldv_work_10_3 == 2) && (unsigned long )ldv_work_struct_10_3 == (unsigned long )work) {
    ldv_work_10_3 = 1;
  } else {
  }
  return;
}
}
void activate_work_10(struct work_struct *work , int state )
{
  {
  if (ldv_work_10_0 == 0) {
    ldv_work_struct_10_0 = work;
    ldv_work_10_0 = state;
    return;
  } else {
  }
  if (ldv_work_10_1 == 0) {
    ldv_work_struct_10_1 = work;
    ldv_work_10_1 = state;
    return;
  } else {
  }
  if (ldv_work_10_2 == 0) {
    ldv_work_struct_10_2 = work;
    ldv_work_10_2 = state;
    return;
  } else {
  }
  if (ldv_work_10_3 == 0) {
    ldv_work_struct_10_3 = work;
    ldv_work_10_3 = state;
    return;
  } else {
  }
  return;
}
}
void call_and_disable_all_10(int state )
{
  {
  if (ldv_work_10_0 == state) {
    call_and_disable_work_10(ldv_work_struct_10_0);
  } else {
  }
  if (ldv_work_10_1 == state) {
    call_and_disable_work_10(ldv_work_struct_10_1);
  } else {
  }
  if (ldv_work_10_2 == state) {
    call_and_disable_work_10(ldv_work_struct_10_2);
  } else {
  }
  if (ldv_work_10_3 == state) {
    call_and_disable_work_10(ldv_work_struct_10_3);
  } else {
  }
  return;
}
}
void call_and_disable_work_10(struct work_struct *work )
{
  {
  if ((ldv_work_10_0 == 2 || ldv_work_10_0 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_10_0) {
    amdgpu_mn_destroy(work);
    ldv_work_10_0 = 1;
    return;
  } else {
  }
  if ((ldv_work_10_1 == 2 || ldv_work_10_1 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_10_1) {
    amdgpu_mn_destroy(work);
    ldv_work_10_1 = 1;
    return;
  } else {
  }
  if ((ldv_work_10_2 == 2 || ldv_work_10_2 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_10_2) {
    amdgpu_mn_destroy(work);
    ldv_work_10_2 = 1;
    return;
  } else {
  }
  if ((ldv_work_10_3 == 2 || ldv_work_10_3 == 3) && (unsigned long )work == (unsigned long )ldv_work_struct_10_3) {
    amdgpu_mn_destroy(work);
    ldv_work_10_3 = 1;
    return;
  } else {
  }
  return;
}
}
void invoke_work_10(void)
{
  int tmp ;
  {
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_work_10_0 == 2 || ldv_work_10_0 == 3) {
    ldv_work_10_0 = 4;
    amdgpu_mn_destroy(ldv_work_struct_10_0);
    ldv_work_10_0 = 1;
  } else {
  }
  goto ldv_44788;
  case 1: ;
  if (ldv_work_10_1 == 2 || ldv_work_10_1 == 3) {
    ldv_work_10_1 = 4;
    amdgpu_mn_destroy(ldv_work_struct_10_0);
    ldv_work_10_1 = 1;
  } else {
  }
  goto ldv_44788;
  case 2: ;
  if (ldv_work_10_2 == 2 || ldv_work_10_2 == 3) {
    ldv_work_10_2 = 4;
    amdgpu_mn_destroy(ldv_work_struct_10_0);
    ldv_work_10_2 = 1;
  } else {
  }
  goto ldv_44788;
  case 3: ;
  if (ldv_work_10_3 == 2 || ldv_work_10_3 == 3) {
    ldv_work_10_3 = 4;
    amdgpu_mn_destroy(ldv_work_struct_10_0);
    ldv_work_10_3 = 1;
  } else {
  }
  goto ldv_44788;
  default:
  ldv_stop();
  }
  ldv_44788: ;
  return;
}
}
void ldv_main_exported_11(void)
{
  unsigned long ldvarg122 ;
  unsigned long ldvarg123 ;
  int tmp ;
  {
  ldv_memset((void *)(& ldvarg122), 0, 8UL);
  ldv_memset((void *)(& ldvarg123), 0, 8UL);
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_state_variable_11 == 2) {
    amdgpu_mn_release(amdgpu_mn_ops_group0, amdgpu_mn_ops_group1);
    ldv_state_variable_11 = 1;
    ref_cnt = ref_cnt - 1;
  } else {
  }
  goto ldv_44799;
  case 1: ;
  if (ldv_state_variable_11 == 1) {
    amdgpu_mn_invalidate_range_start(amdgpu_mn_ops_group0, amdgpu_mn_ops_group1, ldvarg122,
                                     ldvarg123);
    ldv_state_variable_11 = 1;
  } else {
  }
  if (ldv_state_variable_11 == 2) {
    amdgpu_mn_invalidate_range_start(amdgpu_mn_ops_group0, amdgpu_mn_ops_group1, ldvarg122,
                                     ldvarg123);
    ldv_state_variable_11 = 2;
  } else {
  }
  goto ldv_44799;
  case 2: ;
  if (ldv_state_variable_11 == 1) {
    ldv_probe_11();
    ldv_state_variable_11 = 2;
    ref_cnt = ref_cnt + 1;
  } else {
  }
  goto ldv_44799;
  default:
  ldv_stop();
  }
  ldv_44799: ;
  return;
}
}
bool ldv_queue_work_on_1097(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_delayed_work_on_1098(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___0 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
bool ldv_queue_work_on_1099(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                            struct work_struct *ldv_func_arg3 )
{
  ldv_func_ret_type___1 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3);
  ldv_func_res = tmp;
  activate_work_2(ldv_func_arg3, 2);
  return (ldv_func_res);
}
}
void ldv_flush_workqueue_1100(struct workqueue_struct *ldv_func_arg1 )
{
  {
  flush_workqueue(ldv_func_arg1);
  call_and_disable_all_2(2);
  return;
}
}
bool ldv_queue_delayed_work_on_1101(int ldv_func_arg1 , struct workqueue_struct *ldv_func_arg2 ,
                                    struct delayed_work *ldv_func_arg3 , unsigned long ldv_func_arg4 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  bool tmp ;
  {
  tmp = queue_delayed_work_on(ldv_func_arg1, ldv_func_arg2, ldv_func_arg3, ldv_func_arg4);
  ldv_func_res = tmp;
  activate_work_2(& ldv_func_arg3->work, 2);
  return (ldv_func_res);
}
}
extern void *memset(void * , int , size_t ) ;
__inline static void ldv_error(void)
{
  {
  ERROR: ;
  __VERIFIER_error();
}
}
bool ldv_is_err(void const *ptr )
{
  {
  return ((unsigned long )ptr > 2012UL);
}
}
void *ldv_err_ptr(long error )
{
  {
  return ((void *)(2012L - error));
}
}
long ldv_ptr_err(void const *ptr )
{
  {
  return ((long )(2012UL - (unsigned long )ptr));
}
}
bool ldv_is_err_or_null(void const *ptr )
{
  bool tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )ptr == (unsigned long )((void const *)0)) {
    tmp___0 = 1;
  } else {
    tmp = ldv_is_err(ptr);
    if ((int )tmp) {
      tmp___0 = 1;
    } else {
      tmp___0 = 0;
    }
  }
  return ((bool )tmp___0);
}
}
int ldv_module_refcounter = 1;
void ldv_module_get(struct module *module )
{
  {
  if ((unsigned long )module != (unsigned long )((struct module *)0)) {
    ldv_module_refcounter = ldv_module_refcounter + 1;
  } else {
  }
  return;
}
}
int ldv_try_module_get(struct module *module )
{
  int module_get_succeeded ;
  {
  if ((unsigned long )module != (unsigned long )((struct module *)0)) {
    module_get_succeeded = ldv_undef_int();
    if (module_get_succeeded == 1) {
      ldv_module_refcounter = ldv_module_refcounter + 1;
      return (1);
    } else {
      return (0);
    }
  } else {
  }
  return (0);
}
}
void ldv_module_put(struct module *module )
{
  {
  if ((unsigned long )module != (unsigned long )((struct module *)0)) {
    if (ldv_module_refcounter <= 1) {
      ldv_error();
    } else {
    }
    ldv_module_refcounter = ldv_module_refcounter - 1;
  } else {
  }
  return;
}
}
void ldv_module_put_and_exit(void)
{
  {
  ldv_module_put((struct module *)1);
  LDV_STOP: ;
  goto LDV_STOP;
}
}
unsigned int ldv_module_refcount(void)
{
  {
  return ((unsigned int )(ldv_module_refcounter + -1));
}
}
void ldv_check_final_state(void)
{
  {
  if (ldv_module_refcounter != 1) {
    ldv_error();
  } else {
  }
  return;
}
}
void *external_alloc(void);
struct workqueue_struct *__alloc_workqueue_key(const char *arg0, unsigned int arg1, int arg2, struct lock_class_key *arg3, const char *arg4, ...) {
  return (struct workqueue_struct *)external_alloc();
}
void __const_udelay(unsigned long arg0) {
  return;
}
void __copy_from_user_overflow() {
  return;
}
void __copy_to_user_overflow() {
  return;
}
void __dynamic_dev_dbg(struct _ddebug *arg0, const struct device *arg1, const char *arg2, ...) {
  return;
}
void __free_pages(struct page *arg0, unsigned int arg1) {
  return;
}
void __init_rwsem(struct rw_semaphore *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
void __init_waitqueue_head(wait_queue_head_t *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
void __init_work(struct work_struct *arg0, int arg1) {
  return;
}
void *external_alloc(void);
void *__kmalloc(size_t arg0, gfp_t arg1) {
  return (void *)external_alloc();
}
void __list_add(struct list_head *arg0, struct list_head *arg1, struct list_head *arg2) {
  return;
}
void __list_del_entry(struct list_head *arg0) {
  return;
}
void __might_fault(const char *arg0, int arg1) {
  return;
}
void __might_sleep(const char *arg0, int arg1, int arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int __mmu_notifier_register(struct mmu_notifier *arg0, struct mm_struct *arg1) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int __msecs_to_jiffies(const unsigned int arg0) {
  return __VERIFIER_nondet_ulong();
}
void __mutex_init(struct mutex *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int __phys_addr(unsigned long arg0) {
  return __VERIFIER_nondet_ulong();
}
int __VERIFIER_nondet_int(void);
int __pm_runtime_resume(struct device *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int __pm_runtime_set_status(struct device *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int __pm_runtime_suspend(struct device *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void __pm_runtime_use_autosuspend(struct device *arg0, bool arg1) {
  return;
}
void __raw_spin_lock_init(raw_spinlock_t *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool __sg_page_iter_next(struct sg_page_iter *arg0) {
  return __VERIFIER_nondet_bool();
}
void __sg_page_iter_start(struct sg_page_iter *arg0, struct scatterlist *arg1, unsigned int arg2, unsigned long arg3) {
  return;
}
void __udelay(unsigned long arg0) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int __usecs_to_jiffies(const unsigned int arg0) {
  return __VERIFIER_nondet_ulong();
}
void *external_alloc(void);
void *__vmalloc(unsigned long arg0, gfp_t arg1, pgprot_t arg2) {
  return (void *)external_alloc();
}
void __wake_up(wait_queue_head_t *arg0, unsigned int arg1, int arg2, void *arg3) {
  return;
}
void __wake_up_locked(wait_queue_head_t *arg0, unsigned int arg1, int arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int __ww_mutex_lock(struct ww_mutex *arg0, struct ww_acquire_ctx *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int __ww_mutex_lock_interruptible(struct ww_mutex *arg0, struct ww_acquire_ctx *arg1) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _copy_from_user(void *arg0, const void *arg1, unsigned int arg2) {
  return __VERIFIER_nondet_ulong();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _copy_to_user(void *arg0, const void *arg1, unsigned int arg2) {
  return __VERIFIER_nondet_ulong();
}
void _dev_info(const struct device *arg0, const char *arg1, ...) {
  return;
}
void _raw_spin_lock(raw_spinlock_t *arg0) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _raw_spin_lock_irqsave(raw_spinlock_t *arg0) {
  return __VERIFIER_nondet_ulong();
}
void _raw_spin_unlock(raw_spinlock_t *arg0) {
  return;
}
void _raw_spin_unlock_irqrestore(raw_spinlock_t *arg0, unsigned long arg1) {
  return;
}
unsigned int __VERIFIER_nondet_uint(void);
acpi_status acpi_evaluate_object(acpi_handle arg0, acpi_string arg1, struct acpi_object_list *arg2, struct acpi_buffer *arg3) {
  return __VERIFIER_nondet_uint();
}
void *external_alloc(void);
const char *acpi_format_exception(acpi_status arg0) {
  return (const char *)external_alloc();
}
unsigned int __VERIFIER_nondet_uint(void);
acpi_status acpi_get_handle(acpi_handle arg0, acpi_string arg1, acpi_handle **arg2) {
  return __VERIFIER_nondet_uint();
}
unsigned int __VERIFIER_nondet_uint(void);
acpi_status acpi_get_name(acpi_handle arg0, u32 arg1, struct acpi_buffer *arg2) {
  return __VERIFIER_nondet_uint();
}
unsigned int __VERIFIER_nondet_uint(void);
acpi_status acpi_get_table_with_size(acpi_string arg0, u32 arg1, struct acpi_table_header **arg2, acpi_size *arg3) {
  return __VERIFIER_nondet_uint();
}
void *external_alloc(void);
struct page *alloc_pages_current(gfp_t arg0, unsigned int arg1) {
  return (struct page *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int arch_phys_wc_add(unsigned long arg0, unsigned long arg1) {
  return __VERIFIER_nondet_int();
}
void arch_phys_wc_del(int arg0) {
  return;
}
void *external_alloc(void);
struct backlight_device *backlight_device_register(const char *arg0, struct device *arg1, void *arg2, const struct backlight_ops *arg3, const struct backlight_properties *arg4) {
  return (struct backlight_device *)external_alloc();
}
void backlight_device_unregister(struct backlight_device *arg0) {
  return;
}
void backlight_force_update(struct backlight_device *arg0, enum backlight_update_reason arg1) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool cancel_delayed_work_sync(struct delayed_work *arg0) {
  return __VERIFIER_nondet_bool();
}
void cfb_copyarea(struct fb_info *arg0, const struct fb_copyarea *arg1) {
  return;
}
void cfb_fillrect(struct fb_info *arg0, const struct fb_fillrect *arg1) {
  return;
}
void cfb_imageblit(struct fb_info *arg0, const struct fb_image *arg1) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int clear_user(void *arg0, unsigned long arg1) {
  return __VERIFIER_nondet_ulong();
}
void console_lock() {
  return;
}
void console_unlock() {
  return;
}
void debug_dma_map_page(struct device *arg0, struct page *arg1, size_t arg2, size_t arg3, int arg4, dma_addr_t arg5, bool arg6) {
  return;
}
void debug_dma_map_sg(struct device *arg0, struct scatterlist *arg1, int arg2, int arg3, int arg4) {
  return;
}
void debug_dma_mapping_error(struct device *arg0, dma_addr_t arg1) {
  return;
}
void debug_dma_unmap_page(struct device *arg0, dma_addr_t arg1, size_t arg2, int arg3, bool arg4) {
  return;
}
void debug_dma_unmap_sg(struct device *arg0, struct scatterlist *arg1, int arg2, int arg3) {
  return;
}
int __VERIFIER_nondet_int(void);
int debug_lockdep_rcu_enabled() {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct dentry *debugfs_create_file(const char *arg0, umode_t arg1, struct dentry *arg2, void *arg3, const struct file_operations *arg4) {
  return (struct dentry *)external_alloc();
}
void debugfs_remove(struct dentry *arg0) {
  return;
}
long __VERIFIER_nondet_long(void);
loff_t default_llseek(struct file *arg0, loff_t arg1, int arg2) {
  return __VERIFIER_nondet_long();
}
void destroy_workqueue(struct workqueue_struct *arg0) {
  return;
}
void dev_err(const struct device *arg0, const char *arg1, ...) {
  return;
}
void dev_warn(const struct device *arg0, const char *arg1, ...) {
  return;
}
int __VERIFIER_nondet_int(void);
int device_create_file(struct device *arg0, const struct device_attribute *arg1) {
  return __VERIFIER_nondet_int();
}
void device_remove_file(struct device *arg0, const struct device_attribute *arg1) {
  return;
}
void *external_alloc(void);
void *dma_alloc_attrs(struct device *arg0, size_t arg1, dma_addr_t *arg2, gfp_t arg3, struct dma_attrs *arg4) {
  return (void *)external_alloc();
}
void dma_free_attrs(struct device *arg0, size_t arg1, void *arg2, dma_addr_t arg3, struct dma_attrs *arg4) {
  return;
}
int __VERIFIER_nondet_int(void);
int dma_set_mask(struct device *arg0, u64 arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int dma_supported(struct device *arg0, u64 arg1) {
  return __VERIFIER_nondet_int();
}
void down_read(struct rw_semaphore *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int down_read_trylock(struct rw_semaphore *arg0) {
  return __VERIFIER_nondet_int();
}
void down_write(struct rw_semaphore *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_add_edid_modes(struct drm_connector *arg0, struct edid *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_calc_vbltimestamp_from_scanoutpos(struct drm_device *arg0, int arg1, int *arg2, struct timeval *arg3, unsigned int arg4, const struct drm_crtc *arg5, const struct drm_display_mode *arg6) {
  return __VERIFIER_nondet_int();
}
long __VERIFIER_nondet_long(void);
long int drm_compat_ioctl(struct file *arg0, unsigned int arg1, unsigned long arg2) {
  return __VERIFIER_nondet_long();
}
void drm_connector_cleanup(struct drm_connector *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_connector_init(struct drm_device *arg0, struct drm_connector *arg1, const struct drm_connector_funcs *arg2, int arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_connector_register(struct drm_connector *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_connector_unregister(struct drm_connector *arg0) {
  return;
}
void drm_crtc_cleanup(struct drm_crtc *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_crtc_helper_set_config(struct drm_mode_set *arg0) {
  return __VERIFIER_nondet_int();
}
bool __VERIFIER_nondet_bool(void);
bool drm_crtc_helper_set_mode(struct drm_crtc *arg0, struct drm_display_mode *arg1, int arg2, int arg3, struct drm_framebuffer *arg4) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int drm_crtc_init(struct drm_device *arg0, struct drm_crtc *arg1, const struct drm_crtc_funcs *arg2) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct drm_display_mode *drm_cvt_mode(struct drm_device *arg0, int arg1, int arg2, int arg3, bool arg4, bool arg5, bool arg6) {
  return (struct drm_display_mode *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int drm_debugfs_create_files(const struct drm_info_list *arg0, int arg1, struct dentry *arg2, struct drm_minor *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_debugfs_remove_files(const struct drm_info_list *arg0, int arg1, struct drm_minor *arg2) {
  return __VERIFIER_nondet_int();
}
bool __VERIFIER_nondet_bool(void);
bool drm_detect_hdmi_monitor(struct edid *arg0) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int drm_dp_aux_register(struct drm_dp_aux *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_dp_aux_unregister(struct drm_dp_aux *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_dp_bw_code_to_link_rate(u8 arg0) {
  return __VERIFIER_nondet_int();
}
bool __VERIFIER_nondet_bool(void);
bool drm_dp_channel_eq_ok(const u8 *arg0, int arg1) {
  return __VERIFIER_nondet_bool();
}
bool __VERIFIER_nondet_bool(void);
bool drm_dp_clock_recovery_ok(const u8 *arg0, int arg1) {
  return __VERIFIER_nondet_bool();
}
long __VERIFIER_nondet_long(void);
ssize_t drm_dp_dpcd_read(struct drm_dp_aux *arg0, unsigned int arg1, void *arg2, size_t arg3) {
  return __VERIFIER_nondet_long();
}
int __VERIFIER_nondet_int(void);
int drm_dp_dpcd_read_link_status(struct drm_dp_aux *arg0, u8 *arg1) {
  return __VERIFIER_nondet_int();
}
long __VERIFIER_nondet_long(void);
ssize_t drm_dp_dpcd_write(struct drm_dp_aux *arg0, unsigned int arg1, void *arg2, size_t arg3) {
  return __VERIFIER_nondet_long();
}
unsigned char __VERIFIER_nondet_uchar(void);
u8 drm_dp_get_adjust_request_pre_emphasis(const u8 *arg0, int arg1) {
  return __VERIFIER_nondet_uchar();
}
unsigned char __VERIFIER_nondet_uchar(void);
u8 drm_dp_get_adjust_request_voltage(const u8 *arg0, int arg1) {
  return __VERIFIER_nondet_uchar();
}
unsigned char __VERIFIER_nondet_uchar(void);
u8 drm_dp_link_rate_to_bw_code(int arg0) {
  return __VERIFIER_nondet_uchar();
}
void drm_dp_link_train_channel_eq_delay(const u8 *arg0) {
  return;
}
void drm_dp_link_train_clock_recovery_delay(const u8 *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_edid_header_is_valid(const u8 *arg0) {
  return __VERIFIER_nondet_int();
}
bool __VERIFIER_nondet_bool(void);
bool drm_edid_is_valid(struct edid *arg0) {
  return __VERIFIER_nondet_bool();
}
void drm_edid_to_eld(struct drm_connector *arg0, struct edid *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_edid_to_sad(struct edid *arg0, struct cea_sad **arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_edid_to_speaker_allocation(struct edid *arg0, u8 **arg1) {
  return __VERIFIER_nondet_int();
}
void drm_encoder_cleanup(struct drm_encoder *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_encoder_init(struct drm_device *arg0, struct drm_encoder *arg1, const struct drm_encoder_funcs *arg2, int arg3) {
  return __VERIFIER_nondet_int();
}
void drm_err(const char *arg0, ...) {
  return;
}
void drm_fb_get_bpp_depth(u32 arg0, unsigned int *arg1, int *arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_blank(int arg0, struct fb_info *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_check_var(struct fb_var_screeninfo *arg0, struct fb_info *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_debug_enter(struct fb_info *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_debug_leave(struct fb_info *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_fb_helper_fill_fix(struct fb_info *arg0, u32 arg1, u32 arg2) {
  return;
}
void drm_fb_helper_fill_var(struct fb_info *arg0, struct drm_fb_helper *arg1, u32 arg2, u32 arg3) {
  return;
}
void drm_fb_helper_fini(struct drm_fb_helper *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_hotplug_event(struct drm_fb_helper *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_init(struct drm_device *arg0, struct drm_fb_helper *arg1, int arg2, int arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_initial_config(struct drm_fb_helper *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_pan_display(struct fb_var_screeninfo *arg0, struct fb_info *arg1) {
  return __VERIFIER_nondet_int();
}
void drm_fb_helper_prepare(struct drm_device *arg0, struct drm_fb_helper *arg1, const struct drm_fb_helper_funcs *arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_set_par(struct fb_info *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_setcmap(struct fb_cmap *arg0, struct fb_info *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_fb_helper_single_add_all_connectors(struct drm_fb_helper *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_framebuffer_cleanup(struct drm_framebuffer *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_framebuffer_init(struct drm_device *arg0, struct drm_framebuffer *arg1, const struct drm_framebuffer_funcs *arg2) {
  return __VERIFIER_nondet_int();
}
void drm_framebuffer_unregister_private(struct drm_framebuffer *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_gem_dumb_destroy(struct drm_file *arg0, struct drm_device *arg1, u32 arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_gem_handle_create(struct drm_file *arg0, struct drm_gem_object *arg1, u32 *arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_gem_object_init(struct drm_device *arg0, struct drm_gem_object *arg1, size_t arg2) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct drm_gem_object *drm_gem_object_lookup(struct drm_device *arg0, struct drm_file *arg1, u32 arg2) {
  return (struct drm_gem_object *)external_alloc();
}
void drm_gem_object_release(struct drm_gem_object *arg0) {
  return;
}
void *external_alloc(void);
struct dma_buf *drm_gem_prime_export(struct drm_device *arg0, struct drm_gem_object *arg1, int arg2) {
  return (struct dma_buf *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int drm_gem_prime_fd_to_handle(struct drm_device *arg0, struct drm_file *arg1, int arg2, u32 *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_gem_prime_handle_to_fd(struct drm_device *arg0, struct drm_file *arg1, u32 arg2, u32 arg3, int *arg4) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct drm_gem_object *drm_gem_prime_import(struct drm_device *arg0, struct dma_buf *arg1) {
  return (struct drm_gem_object *)external_alloc();
}
void *external_alloc(void);
struct edid *drm_get_edid(struct drm_connector *arg0, struct i2c_adapter *arg1) {
  return (struct edid *)external_alloc();
}
void *external_alloc(void);
const char *drm_get_format_name(u32 arg0) {
  return (const char *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int drm_get_pci_dev(struct pci_dev *arg0, const struct pci_device_id *arg1, struct drm_driver *arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_global_item_ref(struct drm_global_reference *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_global_item_unref(struct drm_global_reference *arg0) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool drm_handle_vblank(struct drm_device *arg0, int arg1) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int drm_hdmi_avi_infoframe_from_display_mode(struct hdmi_avi_infoframe *arg0, const struct drm_display_mode *arg1) {
  return __VERIFIER_nondet_int();
}
void drm_helper_connector_dpms(struct drm_connector *arg0, int arg1) {
  return;
}
void drm_helper_disable_unused_functions(struct drm_device *arg0) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool drm_helper_hpd_irq_event(struct drm_device *arg0) {
  return __VERIFIER_nondet_bool();
}
void drm_helper_mode_fill_fb_struct(struct drm_framebuffer *arg0, struct drm_mode_fb_cmd2 *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_helper_probe_single_connector_modes(struct drm_connector *arg0, u32 arg1, u32 arg2) {
  return __VERIFIER_nondet_int();
}
void drm_helper_resume_force_mode(struct drm_device *arg0) {
  return;
}
long __VERIFIER_nondet_long(void);
long int drm_ioctl(struct file *arg0, unsigned int arg1, unsigned long arg2) {
  return __VERIFIER_nondet_long();
}
int __VERIFIER_nondet_int(void);
int drm_irq_install(struct drm_device *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_irq_uninstall(struct drm_device *arg0) {
  return __VERIFIER_nondet_int();
}
void drm_kms_helper_poll_disable(struct drm_device *arg0) {
  return;
}
void drm_kms_helper_poll_enable(struct drm_device *arg0) {
  return;
}
void drm_kms_helper_poll_fini(struct drm_device *arg0) {
  return;
}
void drm_kms_helper_poll_init(struct drm_device *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_mm_dump_table(struct seq_file *arg0, struct drm_mm *arg1) {
  return __VERIFIER_nondet_int();
}
void drm_mode_config_cleanup(struct drm_device *arg0) {
  return;
}
void drm_mode_config_init(struct drm_device *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_mode_connector_attach_encoder(struct drm_connector *arg0, struct drm_encoder *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_mode_connector_update_edid_property(struct drm_connector *arg0, const struct edid *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_mode_create_scaling_mode_property(struct drm_device *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_mode_crtc_set_gamma_size(struct drm_crtc *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct drm_display_mode *drm_mode_duplicate(struct drm_device *arg0, const struct drm_display_mode *arg1) {
  return (struct drm_display_mode *)external_alloc();
}
unsigned int __VERIFIER_nondet_uint(void);
u32 drm_mode_legacy_fb_format(u32 arg0, u32 arg1) {
  return __VERIFIER_nondet_uint();
}
void *external_alloc(void);
struct drm_mode_object *drm_mode_object_find(struct drm_device *arg0, u32 arg1, u32 arg2) {
  return (struct drm_mode_object *)external_alloc();
}
void drm_mode_probed_add(struct drm_connector *arg0, struct drm_display_mode *arg1) {
  return;
}
void drm_mode_set_crtcinfo(struct drm_display_mode *arg0, int arg1) {
  return;
}
void drm_mode_set_name(struct drm_display_mode *arg0) {
  return;
}
void drm_object_attach_property(struct drm_mode_object *arg0, struct drm_property *arg1, uint64_t arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_open(struct inode *arg0, struct file *arg1) {
  return __VERIFIER_nondet_int();
}
void drm_pci_exit(struct drm_driver *arg0, struct pci_driver *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_pci_init(struct drm_driver *arg0, struct pci_driver *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_pci_set_busid(struct drm_device *arg0, struct drm_master *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_pcie_get_speed_cap_mask(struct drm_device *arg0, u32 *arg1) {
  return __VERIFIER_nondet_int();
}
unsigned int __VERIFIER_nondet_uint(void);
unsigned int drm_poll(struct file *arg0, struct poll_table_struct *arg1) {
  return __VERIFIER_nondet_uint();
}
void drm_prime_gem_destroy(struct drm_gem_object *arg0, struct sg_table *arg1) {
  return;
}
void *external_alloc(void);
struct sg_table *drm_prime_pages_to_sg(struct page **arg0, unsigned int arg1) {
  return (struct sg_table *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int drm_prime_sg_to_page_addr_arrays(struct sg_table *arg0, struct page **arg1, dma_addr_t *arg2, int arg3) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct drm_property *drm_property_create_enum(struct drm_device *arg0, int arg1, const char *arg2, const struct drm_prop_enum_list *arg3, int arg4) {
  return (struct drm_property *)external_alloc();
}
void *external_alloc(void);
struct drm_property *drm_property_create_range(struct drm_device *arg0, int arg1, const char *arg2, uint64_t arg3, uint64_t arg4) {
  return (struct drm_property *)external_alloc();
}
void drm_put_dev(struct drm_device *arg0) {
  return;
}
long __VERIFIER_nondet_long(void);
ssize_t drm_read(struct file *arg0, char *arg1, size_t arg2, loff_t *arg3) {
  return __VERIFIER_nondet_long();
}
int __VERIFIER_nondet_int(void);
int drm_release(struct inode *arg0, struct file *arg1) {
  return __VERIFIER_nondet_int();
}
void drm_send_vblank_event(struct drm_device *arg0, int arg1, struct drm_pending_vblank_event *arg2) {
  return;
}
void drm_ut_debug_printk(const char *arg0, const char *arg1, ...) {
  return;
}
void drm_vblank_cleanup(struct drm_device *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int drm_vblank_get(struct drm_device *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int drm_vblank_init(struct drm_device *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void drm_vblank_post_modeset(struct drm_device *arg0, int arg1) {
  return;
}
void drm_vblank_pre_modeset(struct drm_device *arg0, int arg1) {
  return;
}
void drm_vblank_put(struct drm_device *arg0, int arg1) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool drm_vma_node_is_allowed(struct drm_vma_offset_node *arg0, struct file *arg1) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int fb_alloc_cmap(struct fb_cmap *arg0, int arg1, int arg2) {
  return __VERIFIER_nondet_int();
}
void fb_dealloc_cmap(struct fb_cmap *arg0) {
  return;
}
void fb_set_suspend(struct fb_info *arg0, int arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int fence_add_callback(struct fence *arg0, struct fence_cb *arg1, void (*arg2)(struct fence *, struct fence_cb *)) {
  return __VERIFIER_nondet_int();
}
unsigned int __VERIFIER_nondet_uint(void);
unsigned int fence_context_alloc(unsigned int arg0) {
  return __VERIFIER_nondet_uint();
}
void fence_init(struct fence *arg0, const struct fence_ops *arg1, spinlock_t *arg2, unsigned int arg3, unsigned int arg4) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool fence_remove_callback(struct fence *arg0, struct fence_cb *arg1) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int fence_signal(struct fence *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int fence_signal_locked(struct fence *arg0) {
  return __VERIFIER_nondet_int();
}
long __VERIFIER_nondet_long(void);
long int fence_wait_timeout(struct fence *arg0, bool arg1, long arg2) {
  return __VERIFIER_nondet_long();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int find_first_zero_bit(const unsigned long *arg0, unsigned long arg1) {
  return __VERIFIER_nondet_ulong();
}
void *external_alloc(void);
struct vm_area_struct *find_vma(struct mm_struct *arg0, unsigned long arg1) {
  return (struct vm_area_struct *)external_alloc();
}
void finish_wait(wait_queue_head_t *arg0, wait_queue_t *arg1) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool flush_work(struct work_struct *arg0) {
  return __VERIFIER_nondet_bool();
}
void flush_workqueue(struct workqueue_struct *arg0) {
  return;
}
void *external_alloc(void);
struct fb_info *framebuffer_alloc(size_t arg0, struct device *arg1) {
  return (struct fb_info *)external_alloc();
}
void framebuffer_release(struct fb_info *arg0) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int gcd(unsigned long arg0, unsigned long arg1) {
  return __VERIFIER_nondet_ulong();
}
long __VERIFIER_nondet_long(void);
long int get_user_pages(struct task_struct *arg0, struct mm_struct *arg1, unsigned long arg2, unsigned long arg3, int arg4, int arg5, struct page **arg6, struct vm_area_struct **arg7) {
  return __VERIFIER_nondet_long();
}
long __VERIFIER_nondet_long(void);
ssize_t hdmi_avi_infoframe_pack(struct hdmi_avi_infoframe *arg0, void *arg1, size_t arg2) {
  return __VERIFIER_nondet_long();
}
void *external_alloc(void);
struct device *hwmon_device_register_with_groups(struct device *arg0, const char *arg1, void *arg2, const struct attribute_group **arg3) {
  return (struct device *)external_alloc();
}
void hwmon_device_unregister(struct device *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int i2c_add_adapter(struct i2c_adapter *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int i2c_bit_add_bus(struct i2c_adapter *arg0) {
  return __VERIFIER_nondet_int();
}
void i2c_del_adapter(struct i2c_adapter *arg0) {
  return;
}
void *external_alloc(void);
struct i2c_client *i2c_new_device(struct i2c_adapter *arg0, const struct i2c_board_info *arg1) {
  return (struct i2c_client *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int i2c_transfer(struct i2c_adapter *arg0, struct i2c_msg *arg1, int arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int idr_alloc(struct idr *arg0, void *arg1, int arg2, int arg3, gfp_t arg4) {
  return __VERIFIER_nondet_int();
}
void idr_destroy(struct idr *arg0) {
  return;
}
void *external_alloc(void);
void *idr_find_slowpath(struct idr *arg0, int arg1) {
  return (void *)external_alloc();
}
void *external_alloc(void);
void *idr_get_next(struct idr *arg0, int *arg1) {
  return (void *)external_alloc();
}
void idr_init(struct idr *arg0) {
  return;
}
void idr_remove(struct idr *arg0, int arg1) {
  return;
}
void init_timer_key(struct timer_list *arg0, unsigned int arg1, const char *arg2, struct lock_class_key *arg3) {
  return;
}
void interval_tree_insert(struct interval_tree_node *arg0, struct rb_root *arg1) {
  return;
}
void *external_alloc(void);
struct interval_tree_node *interval_tree_iter_first(struct rb_root *arg0, unsigned long arg1, unsigned long arg2) {
  return (struct interval_tree_node *)external_alloc();
}
void *external_alloc(void);
struct interval_tree_node *interval_tree_iter_next(struct interval_tree_node *arg0, unsigned long arg1, unsigned long arg2) {
  return (struct interval_tree_node *)external_alloc();
}
void interval_tree_remove(struct interval_tree_node *arg0, struct rb_root *arg1) {
  return;
}
unsigned int __VERIFIER_nondet_uint(void);
unsigned int ioread32(void *arg0) {
  return __VERIFIER_nondet_uint();
}
void *external_alloc(void);
void *ioremap_nocache(resource_size_t arg0, unsigned long arg1) {
  return (void *)external_alloc();
}
void iounmap(volatile void *arg0) {
  return;
}
void iowrite32(u32 arg0, void *arg1) {
  return;
}
unsigned int __VERIFIER_nondet_uint(void);
unsigned int jiffies_to_msecs(const unsigned long arg0) {
  return __VERIFIER_nondet_uint();
}
void *external_alloc(void);
void *kmemdup(const void *arg0, size_t arg1, gfp_t arg2) {
  return (void *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int kstrtoint(const char *arg0, unsigned int arg1, int *arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int kstrtouint(const char *arg0, unsigned int arg1, unsigned int *arg2) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
void __VERIFIER_assume(int);
ktime_t ktime_get() {
  union ktime *tmp = (union ktime*)external_alloc();
  __VERIFIER_assume(tmp != 0);
  return *tmp;
}
void kvfree(const void *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ldv_bind_104() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_105() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_107() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_46() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_47() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_49() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_57() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_58() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_bind_60() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_complete_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_104() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_105() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_107() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_46() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_47() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_49() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_57() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_58() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_connect_60() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_freeze_late_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_freeze_noirq_179() {
  return __VERIFIER_nondet_int();
}
void ldv_initialize() {
  return;
}
int __VERIFIER_nondet_int(void);
int ldv_open_160() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_open_161() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_open_174() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_poweroff_late_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_poweroff_noirq_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_prepare_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_102() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_106() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_108() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_11() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_111() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_114() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_116() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_119() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_120() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_121() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_15() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_159() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_165() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_166() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_168() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_170() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_172() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_18() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_21() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_22() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_27() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_28() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_33() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_39() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_44() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_48() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_50() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_55() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_59() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_61() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_62() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_64() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_67() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_70() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_72() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_74() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_77() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_78() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_79() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_82() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_85() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_86() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_91() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_probe_97() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_104() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_105() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_106() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_107() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_111() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_114() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_116() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_119() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_12() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_120() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_121() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_15() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_160() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_161() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_174() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_177() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_18() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_21() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_22() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_27() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_28() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_33() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_39() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_46() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_47() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_48() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_49() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_57() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_58() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_59() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_60() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_62() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_64() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_67() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_70() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_72() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_74() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_77() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_78() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_79() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_82() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_85() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_86() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_91() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_release_97() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_restore_early_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_restore_noirq_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_resume_early_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_resume_noirq_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_setup_12() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_shutdown_176() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_suspend_late_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_suspend_noirq_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_thaw_early_179() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ldv_thaw_noirq_179() {
  return __VERIFIER_nondet_int();
}
void list_del(struct list_head *arg0) {
  return;
}
void list_sort(void *arg0, struct list_head *arg1, int (*arg2)(void *, struct list_head *, struct list_head *)) {
  return;
}
void lock_acquire(struct lockdep_map *arg0, unsigned int arg1, int arg2, int arg3, int arg4, struct lockdep_map *arg5, unsigned long arg6) {
  return;
}
int __VERIFIER_nondet_int(void);
int lock_is_held(struct lockdep_map *arg0) {
  return __VERIFIER_nondet_int();
}
void lock_release(struct lockdep_map *arg0, int arg1, unsigned long arg2) {
  return;
}
void lockdep_init_map(struct lockdep_map *arg0, const char *arg1, struct lock_class_key *arg2, int arg3) {
  return;
}
void lockdep_rcu_suspicious(const char *arg0, const int arg1, const char *arg2) {
  return;
}
void mark_page_accessed(struct page *arg0) {
  return;
}
void mmu_notifier_unregister(struct mmu_notifier *arg0, struct mm_struct *arg1) {
  return;
}
void msleep(unsigned int arg0) {
  return;
}
void mutex_destroy(struct mutex *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int mutex_lock_interruptible_nested(struct mutex *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
void mutex_lock_nested(struct mutex *arg0, unsigned int arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int mutex_trylock(struct mutex *arg0) {
  return __VERIFIER_nondet_int();
}
void mutex_unlock(struct mutex *arg0) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int nsecs_to_jiffies(u64 arg0) {
  return __VERIFIER_nondet_ulong();
}
int __VERIFIER_nondet_int(void);
int pci_bus_read_config_word(struct pci_bus *arg0, unsigned int arg1, int arg2, u16 *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_bus_write_config_dword(struct pci_bus *arg0, unsigned int arg1, int arg2, u32 arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_bus_write_config_word(struct pci_bus *arg0, unsigned int arg1, int arg2, u16 arg3) {
  return __VERIFIER_nondet_int();
}
void pci_clear_master(struct pci_dev *arg0) {
  return;
}
void pci_disable_device(struct pci_dev *arg0) {
  return;
}
void pci_disable_msi(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_enable_device(struct pci_dev *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_enable_msi_range(struct pci_dev *arg0, int arg1, int arg2) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct pci_dev *pci_get_class(unsigned int arg0, struct pci_dev *arg1) {
  return (struct pci_dev *)external_alloc();
}
void pci_ignore_hotplug(struct pci_dev *arg0) {
  return;
}
void *external_alloc(void);
void *pci_iomap(struct pci_dev *arg0, int arg1, unsigned long arg2) {
  return (void *)external_alloc();
}
void pci_iounmap(struct pci_dev *arg0, void *arg1) {
  return;
}
void *external_alloc(void);
void *pci_map_rom(struct pci_dev *arg0, size_t *arg1) {
  return (void *)external_alloc();
}
void *external_alloc(void);
void *pci_platform_rom(struct pci_dev *arg0, size_t *arg1) {
  return (void *)external_alloc();
}
void pci_restore_state(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_save_state(struct pci_dev *arg0) {
  return __VERIFIER_nondet_int();
}
void pci_set_master(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_set_power_state(struct pci_dev *arg0, pci_power_t arg1) {
  return __VERIFIER_nondet_int();
}
void pci_unmap_rom(struct pci_dev *arg0, void *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int pcie_capability_read_dword(struct pci_dev *arg0, int arg1, u32 *arg2) {
  return __VERIFIER_nondet_int();
}
void pm_runtime_allow(struct device *arg0) {
  return;
}
void pm_runtime_forbid(struct device *arg0) {
  return;
}
void pm_runtime_set_autosuspend_delay(struct device *arg0, int arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int power_supply_is_system_supplied() {
  return __VERIFIER_nondet_int();
}
long __VERIFIER_nondet_long(void);
long int prepare_to_wait_event(wait_queue_head_t *arg0, wait_queue_t *arg1, int arg2) {
  return __VERIFIER_nondet_long();
}
int __VERIFIER_nondet_int(void);
int printk(const char *arg0, ...) {
  return __VERIFIER_nondet_int();
}
void put_page(struct page *arg0) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool queue_delayed_work_on(int arg0, struct workqueue_struct *arg1, struct delayed_work *arg2, unsigned long arg3) {
  return __VERIFIER_nondet_bool();
}
bool __VERIFIER_nondet_bool(void);
bool queue_work_on(int arg0, struct workqueue_struct *arg1, struct work_struct *arg2) {
  return __VERIFIER_nondet_bool();
}
void *external_alloc(void);
struct rb_node *rb_first_postorder(const struct rb_root *arg0) {
  return (struct rb_node *)external_alloc();
}
void *external_alloc(void);
struct rb_node *rb_next_postorder(const struct rb_node *arg0) {
  return (struct rb_node *)external_alloc();
}
bool __VERIFIER_nondet_bool(void);
bool rcu_is_watching() {
  return __VERIFIER_nondet_bool();
}
bool __VERIFIER_nondet_bool(void);
bool rcu_lockdep_current_cpu_online() {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int register_acpi_notifier(struct notifier_block *arg0) {
  return __VERIFIER_nondet_int();
}
void release_firmware(const struct firmware *arg0) {
  return;
}
void release_pages(struct page **arg0, int arg1, bool arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int remove_conflicting_framebuffers(struct apertures_struct *arg0, const char *arg1, bool arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int request_firmware(const struct firmware **arg0, const char *arg1, struct device *arg2) {
  return __VERIFIER_nondet_int();
}
void reservation_object_add_excl_fence(struct reservation_object *arg0, struct fence *arg1) {
  return;
}
void reservation_object_add_shared_fence(struct reservation_object *arg0, struct fence *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int reservation_object_reserve_shared(struct reservation_object *arg0) {
  return __VERIFIER_nondet_int();
}
bool __VERIFIER_nondet_bool(void);
bool reservation_object_test_signaled_rcu(struct reservation_object *arg0, bool arg1) {
  return __VERIFIER_nondet_bool();
}
long __VERIFIER_nondet_long(void);
long int reservation_object_wait_timeout_rcu(struct reservation_object *arg0, bool arg1, bool arg2, unsigned long arg3) {
  return __VERIFIER_nondet_long();
}
void schedule() {
  return;
}
long __VERIFIER_nondet_long(void);
long int schedule_timeout(long arg0) {
  return __VERIFIER_nondet_long();
}
int __VERIFIER_nondet_int(void);
int seq_printf(struct seq_file *arg0, const char *arg1, ...) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int seq_puts(struct seq_file *arg0, const char *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int set_page_dirty(struct page *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int sg_alloc_table_from_pages(struct sg_table *arg0, struct page **arg1, unsigned int arg2, unsigned long arg3, unsigned long arg4, gfp_t arg5) {
  return __VERIFIER_nondet_int();
}
void sg_free_table(struct sg_table *arg0) {
  return;
}
void *external_alloc(void);
struct scatterlist *sg_next(struct scatterlist *arg0) {
  return (struct scatterlist *)external_alloc();
}
unsigned long __VERIFIER_nondet_ulong(void);
size_t strlcpy(char *arg0, const char *arg1, size_t arg2) {
  return __VERIFIER_nondet_ulong();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int swiotlb_nr_tbl() {
  return __VERIFIER_nondet_ulong();
}
int __VERIFIER_nondet_int(void);
int trace_define_field(struct trace_event_call *arg0, const char *arg1, const char *arg2, int arg3, int arg4, int arg5, int arg6) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int trace_event_raw_init(struct trace_event_call *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int trace_event_reg(struct trace_event_call *arg0, enum trace_reg arg1, void *arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int trace_raw_output_prep(struct trace_iterator *arg0, struct trace_event *arg1) {
  return __VERIFIER_nondet_int();
}
void trace_seq_printf(struct trace_seq *arg0, const char *arg1, ...) {
  return;
}
void ttm_bo_add_to_lru(struct ttm_buffer_object *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_bo_clean_mm(struct ttm_bo_device *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
void ttm_bo_del_sub_from_lru(struct ttm_buffer_object *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_bo_device_init(struct ttm_bo_device *arg0, struct ttm_bo_global *arg1, struct ttm_bo_driver *arg2, struct address_space *arg3, uint64_t arg4, bool arg5) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_device_release(struct ttm_bo_device *arg0) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
size_t ttm_bo_dma_acc_size(struct ttm_bo_device *arg0, unsigned long arg1, unsigned int arg2) {
  return __VERIFIER_nondet_ulong();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_evict_mm(struct ttm_bo_device *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_init(struct ttm_bo_device *arg0, struct ttm_buffer_object *arg1, unsigned long arg2, enum ttm_bo_type arg3, struct ttm_placement *arg4, u32 arg5, bool arg6, struct file *arg7, size_t arg8, struct sg_table *arg9, struct reservation_object *arg10, void (*arg11)(struct ttm_buffer_object *)) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_init_mm(struct ttm_bo_device *arg0, unsigned int arg1, unsigned long arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_kmap(struct ttm_buffer_object *arg0, unsigned long arg1, unsigned long arg2, struct ttm_bo_kmap_obj *arg3) {
  return __VERIFIER_nondet_int();
}
void ttm_bo_kunmap(struct ttm_bo_kmap_obj *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_bo_lock_delayed_workqueue(struct ttm_bo_device *arg0) {
  return __VERIFIER_nondet_int();
}
void ttm_bo_mem_put(struct ttm_buffer_object *arg0, struct ttm_mem_reg *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_bo_mem_space(struct ttm_buffer_object *arg0, struct ttm_placement *arg1, struct ttm_mem_reg *arg2, bool arg3, bool arg4) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_mmap(struct file *arg0, struct vm_area_struct *arg1, struct ttm_bo_device *arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_move_accel_cleanup(struct ttm_buffer_object *arg0, struct fence *arg1, bool arg2, bool arg3, struct ttm_mem_reg *arg4) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_move_memcpy(struct ttm_buffer_object *arg0, bool arg1, bool arg2, struct ttm_mem_reg *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_bo_move_ttm(struct ttm_buffer_object *arg0, bool arg1, bool arg2, struct ttm_mem_reg *arg3) {
  return __VERIFIER_nondet_int();
}
void ttm_bo_unlock_delayed_workqueue(struct ttm_bo_device *arg0, int arg1) {
  return;
}
void ttm_bo_unref(struct ttm_buffer_object **arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_bo_validate(struct ttm_buffer_object *arg0, struct ttm_placement *arg1, bool arg2, bool arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_dma_populate(struct ttm_dma_tt *arg0, struct device *arg1) {
  return __VERIFIER_nondet_int();
}
void ttm_dma_tt_fini(struct ttm_dma_tt *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_dma_tt_init(struct ttm_dma_tt *arg0, struct ttm_bo_device *arg1, unsigned long arg2, u32 arg3, struct page *arg4) {
  return __VERIFIER_nondet_int();
}
void ttm_dma_unpopulate(struct ttm_dma_tt *arg0, struct device *arg1) {
  return;
}
void ttm_eu_backoff_reservation(struct ww_acquire_ctx *arg0, struct list_head *arg1) {
  return;
}
void ttm_eu_fence_buffer_objects(struct ww_acquire_ctx *arg0, struct list_head *arg1, struct fence *arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_eu_reserve_buffers(struct ww_acquire_ctx *arg0, struct list_head *arg1, bool arg2, struct list_head *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_fbdev_mmap(struct vm_area_struct *arg0, struct ttm_buffer_object *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_mem_global_init(struct ttm_mem_global *arg0) {
  return __VERIFIER_nondet_int();
}
void ttm_mem_global_release(struct ttm_mem_global *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_pool_populate(struct ttm_tt *arg0) {
  return __VERIFIER_nondet_int();
}
void ttm_pool_unpopulate(struct ttm_tt *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int ttm_tt_bind(struct ttm_tt *arg0, struct ttm_mem_reg *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int ttm_tt_set_placement_caching(struct ttm_tt *arg0, u32 arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int unregister_acpi_notifier(struct notifier_block *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int unregister_framebuffer(struct fb_info *arg0) {
  return __VERIFIER_nondet_int();
}
void up_read(struct rw_semaphore *arg0) {
  return;
}
void up_write(struct rw_semaphore *arg0) {
  return;
}
void usleep_range(unsigned long arg0, unsigned long arg1) {
  return;
}
void vfree(const void *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int vga_client_register(struct pci_dev *arg0, void *arg1, void (*arg2)(void *, bool ), unsigned int (*arg3)(void *, bool )) {
  return __VERIFIER_nondet_int();
}
void vga_switcheroo_client_fb_set(struct pci_dev *arg0, struct fb_info *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int vga_switcheroo_init_domain_pm_ops(struct device *arg0, struct dev_pm_domain *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int vga_switcheroo_process_delayed_switch() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int vga_switcheroo_register_client(struct pci_dev *arg0, const struct vga_switcheroo_client_ops *arg1, bool arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int vga_switcheroo_register_handler(struct vga_switcheroo_handler *arg0) {
  return __VERIFIER_nondet_int();
}
void vga_switcheroo_set_dynamic_switch(struct pci_dev *arg0, enum vga_switcheroo_state arg1) {
  return;
}
void vga_switcheroo_unregister_client(struct pci_dev *arg0) {
  return;
}
void vga_switcheroo_unregister_handler() {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool vgacon_text_force() {
  return __VERIFIER_nondet_bool();
}
void *external_alloc(void);
void *vzalloc(unsigned long arg0) {
  return (void *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int wake_up_process(struct task_struct *arg0) {
  return __VERIFIER_nondet_int();
}
void warn_slowpath_fmt(const char *arg0, const int arg1, const char *arg2, ...) {
  return;
}
void warn_slowpath_null(const char *arg0, const int arg1) {
  return;
}
void ww_mutex_unlock(struct ww_mutex *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int autoremove_wake_function(wait_queue_t *arg0, unsigned int arg1, int arg2, void * arg3) {
  return __VERIFIER_nondet_int();
}
void *__VERIFIER_nondet_pointer(void);
void *external_alloc(void) {
  return __VERIFIER_nondet_pointer();
}
void free(void *);
void kfree(void const *p) {
  free((void *)p);
}
